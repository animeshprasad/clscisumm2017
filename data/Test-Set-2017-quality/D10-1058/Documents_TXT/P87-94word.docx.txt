International Journal of Advanced Intelligence
Volume 4, Number 1, pp.87-94,  December, 2012.
⃝c  AIA International Advanced Information Institute




A  Fully Bayesian Inference for Word Alignment


Li Zezhong∗
Department of Computer Science, Ritsumeikan University Shiga, Japan
lizezhonglaile@163.com

Hideto  Ikeda
Department of Computer Science, Ritsumeikan University Shiga, Japan
hikeda@ritsumei.ac.jp


Received (11,  July, 2012) 
Revised (9, December, 2012)

Abstract

We present an  approximative IBM Model  4 for word  alignment. Diﬀerent with  the  most 
widely-used word  aligner GIZA++, which  implements all  the  5 IBM models and  HMM 
model  in the  framework of Expectation Maximum (EM),  we adopt a full  Bayesian infer- 
ence  which  integrates over all  possible parameter values, rather than  estimating a single 
parameter value. Empirical results show  promising improvements in alignment quality as 
well  as  in BLEU score  for the  translation performance over baselines.

Keywords : Bayesian inference; Word  alignment; Statistical machine translation.


1.  Introduction
Word alignment can be deﬁned as a procedure for detecting the corresponding words 
in a bilingual sentence pair. One of the notorious criticisms of word alignment is the 
inconsistence between the word alignment model to the phrase based translation 
model. In this paper, we have no intention to avoid mentioning this inherent weak- 
ness of word alignment, but we would say, as far as we know, word alignment is a 
fundamental component for most of the SMT systems. Phrase or the other higher 
level translation knowledge is extracted based on the word alignment, which is called 
two-stage approach. And even for approaches of so-called direct phrase alignment, 
they can rarely abandon word alignment thoroughly. Because of the computation 
complexity of phrase alignment, word alignment is usually used to constrain the 
inference.1 DeNero proposes a relative pure joint phrase model but still uses the 
word alignment as initialization and smoothing, which shows the least dependen- 
cy on word alignment.2  Neubig uses Bayesian methods and Inversion Transduction 
Grammar for joint phrase alignment,3  and the base distribution for the Dirichlet 
Process 5  prior is constructed by the word alignment model. Therefore, word align- 
ment is well worth concern. Our hope is to induce a better word alignment by


∗1-1-1, Noji-Higashi, Kusatsu, Shiga,  Japan.

87









88    Z.  Li,  H.  Ikeda

utilizing the state-of-the-art learning technology, and establish a better baseline for 
the word level alignment models.
   Bayesian inference, the approach we adopt in this paper, has been broadly ap- 
plied to various learning of latent structure. Goldwater points out that two theoret- 
ical factors contribute to the superiority of Bayesian inference.7 First,  integrating 
over parameter values leads to greater robustness in decision. One of the prob- 
lems that trouble EM algorithm is over-ﬁtting. Moore discusses details of how a
”Garbage collector” is generated.11 He also suggests a number of heuristic solution- 
s, but Bayesian inference can oﬀer a more principled solution. The second factor is 
that the integration permits the use of priors favoring sparse distributions, which 
proved to be more consistent with nature of natural language. Another practical 
advantage is that the implementation can be much easier than EM.12
   In the following sections, we will have a review for IBM Model 4 in Section 2, 
and reformulate it into a simpler and Bayesian form in Section 3. Section 4 gives the 
Bayesian inference, and Section 5 reports  results of experiment. Section 6 compares 
related research, and Section 7 concludes.


2.  IBM Model 4
Model 4 is a fertility-based alignment model, and can be viewed as the outstanding 
representative of all the IBM translation models. The model can be expressed as

P (F, A|E; n, t, d) = Pϕ (ϕI |E; n)Pτ (τ I |ϕI , E; t)Pπ (πI |ϕI , τ I , E; d) 	(1)


0	0	0
I	I


0     0     0
I   ϕi


1	I   ϕi


= n0 (ϕ0 | ∑ ϕi ) ∏ n(ϕi |ei ) ∏ ∏ t(τik |ei )


ϕ0 !


∏ ∏ pik (πik )


where


i=1


I


i=1

( ∑I


i=0 k=1

)
∑I


i=1 k=1


n0 (ϕ0 | ∑ ϕi ) =
i=1


i=1 ϕi
ϕ0


p0     i=1 ϕi −2ϕ0 p1 ϕ0 	(2)



{ d1 (j − cρi |A(eρi ), B(τi1 )) if k = 1


pik (πik ) =


d>1


(j − π



ik−1


|B(τik


)) 	if k > 1	(3)


Pϕ , Pτ   and Pπ  denote fertility model, lexical model and distortion model respec- 
tively, and their parameters can be described as n, t, and d. More details can be 
found in Brown et al.4


3.  Bayesian Model
Our Bayesian model almost repeats the same generative scenarios shown in the pre- 
vious section, but puts an appropriate prior for the parameters in the model. That 
is, parameter will be treated as variable, which makes a signiﬁcant diﬀerence to the 
traditional MLE or MAP approaches. In our proposed Bayesian setting, the fertility









A Fully  Bayesian Inference for  Word Alignment    89

ϕ and translation f for each target word e, both of which follow a Multinominal 
distribution, will be treated as a random variable with a prior, and Dirichlet dis- 
tribution is a natural choice for them, since it is conjugate to the Multinominal 
distribution. Since we can not specify the dimensions of the above distributions 
in advance, one solution is to take advantage of the nonparametric prior. Here we 
use the Dirichlet Process (DP)  which can ensure that the resulting distributions 
concentrate their probability mass on a small number of fertilities or translation 
candidates while retaining reasonable probability for unseen possibilities.


ne
∼
D
P 
(
α
, 
P 
o
is
s
o
n
(
1
, 
ϕ
)
)
(4)
ϕ|e
∼
n
e
(5)
te
∼
D
P 
(
β
, 
T
0 
(
f 
|e
)
)
(6)
f |e
∼
te
(7)
In the above distribution formulas, ne  denotes the fertility distribution for e, and
hyperparameter α  is a concentration parameter which aﬀects the variance of the
draws. We make P oisson(1, ϕ) as the base distribution for fertility which encodes
our prior knowledge about the properties of fertilities. Namely, high fertility should
be discouraged except that there is enough evidence. λ(e) denotes the expected
fertility for e, and for simplicity, we assign 1 as the value of expected fertility for all
the words. te is a translation distribution for e, and β is the concentration parameter.
As for base distribution T0 , shown as:

T0 (f |e) = ∑ p(et|e)p(f t|et)p(f |f t) 	(8)
et,f t
where et  denotes e’s Part-of-Speech (henceforth POS),  and f t denotes f ’s POS. 
p(f t|et) is a POS  translation model, p(et|e) is a transition probability from word 
to POS,  and p(f |f t) is a uniform distribution (over word types tagged with f t) 
for each word f . T0   encodes such a prior knowledge: POS  provides clues for the 
alignment.
   While our Bayesian model still has other free parameters, we still use p0  and 
p1  as parameters to model fertility for e0   as same as in IBM  models, but we ﬁx 
them to reasonable values in order to focus on learning for the other distributions. 
As for the distortion model, we simply adopt a distance penalty (not including the 
distortion for words generated by e0 ) shown as follows




1
pπ (A) ∝ ϕ  !


J
∏

j=1,aj ̸=0

{ πρi ϕρ



b|j−prev(j)|


(9)


prev(j) =


if k = 1
i
πik−1 if k > 1


(10)









90    Z.  Li,  H.  Ikeda

where b is a ﬁxed value less than 1, prev(j) means the position of predecessor for fj . 
ρi denotes the ﬁrst position to the left of ei for which has a non-zero fertility, and 
πik  is the position of word τik  for permutation π. The ﬁrst part of our distortion 
formula models the distortion procedure for words generated by e0 , which uses the 
same strategy as IBM  models that all these words are positioned only after the 
nonempty positions have been covered. Therefore, there are ϕ0 ! ways to order the 
ϕ0  words.
   Due to the above simpliﬁcation for fertility model, we will see a more convenient 
inference in following sections. Another theoretical reason is that we do not expect 
a skewed distribution for the above parameters as same as the fertility and lexical 
models. Therefore, it is unnecessary to put a prior for these parameters.


4.  Bayesian Inference
A frequent strategy to infer the posterior distribution is Gibbs sampling.12  For our 
concerned word alignment, instead of sampling the parameters explicitly, we sample 
the alignment structure directly with the parameters marginalized out. Then the 
Gibbs sampler is converted into a collapsed Gibbs sampler, and we have



P (F, A|E; α, β) =


∫

n,t


P (F, A, n, t, d|E; α, β) 	(11)


where n comprises all the ne   for each e, and t comprises all the te . d does not need 
integral since we do not treat this parameter as a random variable, and will be 
replaced by constant b in the left part of the integral formula. Due to the collapsed 
sampler, we need not sample the parameters explicitly, but directly sample the latent 
alignment structure in condition of ﬁxed α  and β. Our collapsed Gibbs sampler 
works by sampling each component of vector a alternatively. The probability for a 
new component value when the other values are ﬁxed can be written

P (aj |aj , F, E; α, β) ∝	(12)
Pϕ (aj |aj , F, E; α, β)Pτ (aj |aj , F, E; α, β)Pπ (aj |aj , F, E; α, β)
where aj  denotes the alignment exclude aj . Pϕ , Pτ  and Pπ represent fertility, trans- 
lation and distortion sub-models respectively. The probability of new sample can be 
calculated according to the three sub-models. This calculation is very similar with 
the procedure that ﬁnds the neighbour alignments in the E-step of EM, but in a way 
metaphorized as Chinese Restaurant Process instead of using ﬁxed parameters.12
First, we will investigate the translation model. Thanks to the exchangeability, we 
can write


C ount(eaj , fj ) + βT0 (fj |eaj )


Pτ (aj |aj , F, E; α, β) ∝


Σf C ount(eaj


(13)
, f ) + β









A Fully  Bayesian Inference for  Word Alignment    91

where C ount(e, f ) is the number of links between word pair (e, f ) in the other part 
of this sentence pair and other sentence pairs in the training corpus.
   As for the fertility model, because of the special treatment of the fertility for e0 , 
two cases should be considered. In the ﬁrst case aj ! = 0,


C ount(eaj , ϕaj   + 1) + αP oisson(1, ϕaj   + 1)


Pϕ (aj |aj , F, E; α, β) ∝


C ount(eaj


, ϕaj


) + αP oisson(1, ϕaj


(14)
)


where C ount(e, ϕ) is the frequency of cases where word e has a fertility ϕ, and the 
denominator encodes the fact that the new assignment will cause an instance of 
word-fertility to be removed from the cache as the new word-fertility is added. And 
in the second case, aj  = 0. As is described in the previous section, the fertility for 
empty word is not decided by itself, but decided by the number of words generated 
by nonempty words, which follows a binominal distribution. So we can infer




Pϕ (aj  = 0|aj , F, E; α, β) ∝


n0 (ϕ0  + 1| ∑I
I


ϕi )


∑I
=	i=1


ϕi − ϕ0 )p1


(15)


n0 (ϕ0 | ∑i=1 ϕi )


(ϕ0  + 1)p0


   The calculation for the distortion model is more direct since it is unnecessary to 
consider the cache model. Because of the special treatment for distortion of words 
aligned with the empty word, we also need to take account two cases, as for the 
ﬁrst case, aj ! = 0




Pπ (aj |aj , F, E; α, β) ∝ b|


j−prev(j)|+|next(j)−j|−|next(j)−prev(j)|


(16)


where the exponent means 3  distortions are changed, and next(j) is subject to 
j  == prev(next(j)). In the second case, where aj   = 0, we just need to consider 
the probability of a permutation of ϕ0  words in the remained uncovered positions. 
Notice that, the fertility value changes from ϕ0  to ϕ0 + 1 after this new assignment, 
then we have




Pπ (aj  = 0|aj , F, E; α, β) ∝ (ϕ


ϕ0 !	=
0 + 1)!	ϕ0


1
(17)
+ 1


   The ﬁnal probability for the new derivation should combine all the above in- 
ﬂuence factors, and the production of all the three factors as the ﬁnal probability. 
The algorithm is described in Table 1. To accelerate the convergence, we use HMM 
based Viterbi alignment as an initialization. After burn-in iterations, we begin to 
collect alignment counts from the samples.


5.  Experiments
All the corpus we used is Chinese-English corpus in patent domain, which is released 
by NTCIR9.15  We select 350K sentence pairs as training corpus, and 1000 pairs as









92    Z.  Li,  H.  Ikeda


Table 1. Gibbs  sampling for word  alignment.

For  each  sentence pair  (E, F ) in corpus
Initialize alignment
For  each  generation
For  each  sentence pair  (E, F ) in corpus
For  each  j in [1, |F |]
For  each  i in [0, |E|]
calculate p(aj  = i|aj , F, E; α, β)
Normalize p(aj |aj , F, E; α, β)
Sample a new  value for aj ; update the  cache  count
If (Current generation ≥ Burn-in)
Save alignment for (E, F )


development set. We also annotate 300  word aligned sentence pairs to evaluate
the quality of word alignment, and select 2000 bilingual pairs as the test set for
translation. Before running our Bayesian aligner, we should estimate the parameters
in T0 . We tagged the training corpus using some POS taggers, and replace each word
by its POS to get a POS parallel corpus. Then, we ran IBM model 1 on the POS
corpus to get the POS  translation probabilities. Through dividing the number of
occurrences of the word-tag pair (e, et) by the number of occurrences of e, we can
get p(et|e). Suppose word f is tagged with f t at least once in the training corpus,
then p(f |f t) is equal to the result of dividing 1 by the number of unique words
tagged with f t; otherwise, p(f |f t) is 0.
To contrast our approach with GIZA++, we need the Viterbi alignment ex-
tracted from the multiple samples, and one strategy is to assign each aj   as the
most frequent value in the collected samples. We set 1000 as the number of total
iterations and 0 as the burn-in value, and conﬁgure α and β with varying values.
We run GIZA++ in the standard conﬁguration (Training scheme is abbreviated
as 15 H 5 33 43 ). Both of the above two approaches need run in two directions and
symmetrization. Table 2 shows the comparison of AER between GIZA++ (EM)
and our Bayesian model. When α = 1 and β = 100, our proposed approach can get
the best performance, which reveals a satisfying improvement for alignment quality
in terms of AER, with a reduction of 3.41% over GIZA++.
For translation experiments, we use Moses as our decoder,10 and use SRILM  to
train 4-grams language models on both sides of the bilingual corpus. As is shown in
Table 3, we can see that the Bayesian approach outperforms EM approach in both
directions, which proves the eﬀectiveness of our proposed approach.


6.  Related Work
Our approach is similar with Coskun in spirit to Bayesian inference,9 where it places 
a prior for the model parameters and adopts a collapsed sampler, but they take 
Model 1 as the inference object, which we suppose somewhat harsh. Zhao proposes 
a brief fertility based HMM model,8  which also decreases the complexity of Model









A Fully  Bayesian Inference for  Word Alignment    93


Table 2. Performance of Word  Alignment.

Met
hod
A
E
R
EM
(GI
ZA
++)
1
6
.
1
2
%
Bay
esia
n(α 
= 
0.5, 
β = 
100)
1
3
.
4
3
%
Bay
esia
n(α 
= 1, 
β = 
100)
1
2
.
7
1
%
Bay
esia
n(α 
= 
1.5, 
β = 
100)
1
3
.
7
4
%
Bay
esia
n(α 
= 1, 
β = 
50)
1
5
.
0
4
%
Bay
esia
n(α 
= 1, 
β = 
200)
1
2
.
9
8
%


Table 3. Performance of Final Translation (BLEU-4).

Met
hod
C
h
i
n
es
e
-
E
n
gl
is
h
E
n
gl
is
h
-
C
hi
n
es
e
EM
(GI
ZA
++)
0.
2
7
6
6
0.
2
9
6
4
Bay
esia
n(α 
= 
0.5, 
β = 
100)
0.
2
7
8
7
0.
2
9
9
3
Bay
esia
n(α 
= 1, 
β = 
100)
0
.
2
7
9
8
0
.
3
0
1
1
Bay
esia
n(α 
= 
1.5, 
β = 
100)
0.
2
7
8
1
0.
2
9
8
6
Bay
esia
n(α 
= 1, 
β = 
50)
0.
2
7
7
8
0.
2
9
7
8
Bay
esia
n(α 
= 1, 
β = 
200)
0.
2
7
9
5
0.
3
0
0
3

4 but keeps the fertility as a component of modeling. But they do not place any
prior on the parameters, which can be viewed as a stochastic EM. They also assume
fertility follows a Poisson distribution, while ours adopts a DP  prior and Poisson
distribution as the base distribution in the DP prior. Darcey et al. use variational
Bayes which closely resembles the normal form of EM  algorithm to improve the
performance of GIZA++, as well as the BLEU score.14


7.  Conclusions and  Future Work
We have described an approximative IBM model 4 for word alignment, and adopt 
Bayesian inference which currently is a promising replacement for EM and already 
broadly applied for various tasks in the ﬁeld of NLP. Our pilot experiment shows 
a higher AER for word alignment as well as a modest improved BLEU score for 
translation. Our current research focuses on phrase extraction and reordering from 
multiple alignment samples generated by our Bayesian inference, and we expect a 
better performance.


References
1.   Hao Zhang,  et al. Bayesian Learning of Non-compositional Phrases with  Synchronous Parsing.
In Proceedings of ACL-HLT, pp.  97-105, 2008.
2.   John DeNero,  Alexandre Bouchard Cote,  Dan  Klein.  Sampling Alignment Structure Under  a
Bayesian Translation Model.  In Proceedings of EMNLP, pp.  314-323, 2008.
3.   Graham Neubig,  et al. An Unsupervised Model for Joint Phrase Alignment and  Extraction. In
Proceedings of ACL, pp.  632-641, 2011.
4.   Peter F. Brown  et al. The Mathematics of Statistical Machine  Translation: Parameter Estima-
tion.  Computational  Linguistics, 19(2):263-311, 1993.









94    Z.  Li,  H.  Ikeda

5.   Thomas S  Ferguson. A  Bayesian Analysis of  Some  Nonparametric Problems. In  Annals  of
Statistics,  1973.
6.   Franz Josef Och,  Hermann Ney.  A Systematic  Comparison of Various Statistical Alignment
Models.  In Computational  Linguistics, 29(1):19-51, 2003.
7.   Sharon Goldwater, Tom Griﬃths. A Fully Bayesian Approach to Unsupervised Part-of-Speech
Tagging. In Proceedings of the  ACL, pp.  744-751, 2007.
8.   Shaojun Zhao,  Daniel  Gildea. A  Fast Fertility Hidden  Markov Model  for  Word  Alignment
Using  MCMC. In Proceedings of EMNLP, pp.  596-605, 2010.
9.   Coskun  Mermer, Murat Saraclar. Bayesian Word  Alignment for  Statistical Machine  Transla-
tion.  In Proceedings of ACL, pp.  182-187, 2011.
10.   Philipp Koehn   et  al.  Moses:  Open  Source   Toolkit for  Statistical Machine   Translation.  In
Proceedings of ACL, pp.  177-180, 2007.
11.   Robert C.  Moore.  Improving IBM  Word  Alignment Model  1.  In  Proceedings of  ACL, pp.
518-525, 2004.
12.   Philip Resnik, Eric Hardisty. Gibbs  Sampling for the Uninitiated. Technical report, University
of Maryland, 2010.
13.   Daniel   Marcu   and  Daniel   Wong.  A  Phrase-based, Joint  Probability Model  for  Statistical
Machine  Translation. In Proceedings of EMNLP, pp.  133-139, 2002.
14.   Darcey Riley, Daniel Gildea. Improving the Performance of GIZA++ Using Variational Bayes.
University of Rochester. Technical Report, 2010.
15.   Tetsuya Sakai, Hideo  Joho. Overview of NTCIR-9.  In Workshop of  NTCIR-9, pp.  559-578,
2011.




Li Zezhong	(Member)
 

Hideto Ikeda 	(Member)




  He is currently a doctor  
student in Department of 
Com- puter Science, 
Ritsumeikan University, His 
main research interests 
include  Machine  Translation 
and  Natural Lan- guage  
Processing.









  He  received  the  PhD  from  
Hiroshima University. Dr. 
Ikeda  is currently a professor  
at  Ritsumeikan University. 
His main  research  interests 
include  Database, eLearning, 
Machine  Translation and  
Natural Language 
Processing.


