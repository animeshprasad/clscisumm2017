Exact Maximum Inference for the Fertility Hidden Markov Model

Chris Quirk 
Microsoft  Research 
One Microsoft  Way
Redmond, WA 98052, USA
chrisq@microsoft.com



Abstract

The notion of fertility in word alignment 
(the number of words emitted by a sin­ 
gle state) is useful but difficult to model. 
Initial attempts at modeling fertility used 
heuristic search methods.    Recent ap­ 
proaches instead use more principled ap­ 
proximate inference techniques such as 
Gibbs sampling for parameter estimation. 
Yet in practice we also need the single best 
alignment, which is difficult to find us­ 
ing Gibbs. Building on recent advances in 
dual decomposition, this paper introduces 
an exact algorithm for  finding the sin­ 
gle best alignment with a fertility HMM. 
Finding the best alignment appears impor­ 
tant, as this model leads to a substantial 
improvement in alignment quality.

1   Introduction

Word-based translation models intended to model 
the translation process have found new uses iden­ 
tifying word correspondences in sentence pairs. 
These word alignments are a crucial training com­ 
ponent in most machine translation systems. Fur­ 
thermore, they are useful in other NLP applica­ 
tions, such as entailment identification.
  The simplest models may use lexical infor­ 
mation alone.    The seminal Model 1  (Brown 
et al., 1993) has proved very powerful, per­ 
forming nearly as well as more complicated 
models in some phrasal systems (Koehn et al.,
2003).   With minor improvements to initializa­
tion (Moore, 2004) (which may be important 
(Toutanova and Galley, 2011)), it can be quite 
competitive.	Subsequent IBM models include 
more detailed information about context. Models


2 and 3 incorporate a positional model based on 
the absolute position of the word; Models 4 and
5 use a relative position model instead (an English 
word tends to align to a French word that is nearby 
the French word aligned to the previous English 
word).  Models 3, 4, and 5 all incorporate a no­ 
tion of"fertility": the number of French words that 
align to any English word.
  Although these latter models covered a broad 
range of phenomena, estimation techniques and 
MAP  inference  were  challenging.     The  au­ 
thors originally recommended heuristic proce­ 
dures based on local search for both. Such meth­ 
ods work reasonably well, but can be computation­ 
ally inefficient and have few guarantees.  Thus, 
many researchers have switched to the HMM 
model (Vogel et al., 1996) and variants with more 
parameters (He, 2007).   This captures the posi­ 
tional information in the IBM models in a frame­ 
work that admits exact parameter estimation infer­ 
ence, though the objective function is not concave: 
local maxima are a concern.
  Modeling fertility is challenging in the HMM 
framework as it violates the Markov assump­ 
tion. Where the HMM jump model considers only 
the prior state, fertility requires looking across 
the whole state space.   Therefore, the standard 
forward-backward and Viterbi algorithms do not 
apply. Recent work (Zhao and Gildea, 2010) de­ 
scribed an extension to the HMM with a fertility 
model, using MCMC techniques for parameter es­ 
timation.  However, they do not have a efficient 
means of MAP inference, which is necessary in 
many applications such as machine translation.
  This paper introduces a method for exact MAP 
inference with the fertility HMM using dual de­ 
composition.  The resulting model leads to sub­ 
stantial improvements in alignment quality.




7

Proceedings of the 51st Annual Meeting of the Association for Computational  Linguistics, pages 7-11, 
Sofia, Bulgaria, August 4-9 2013. @2013  Association for Computational Linguistics


2   IIMM alignment

Let us briefly review the HMM translation model 
as a starting point.   We are given a sequence of 
English words e = e,       1. This model pro­ 
duces distributions  over French  word sequences
f  = !I, ... , !J and word alignment vectors a =
at, ... , aJ, where aj E  [O..J] indicates the En­
glish word generating the jth French word, 0 rep­ 
resenting a special NULL state to handle systemat­ 
ically unaligned words.

J
Pr(f, ale)  = p(JII) ITp(ajlaj-t) p(Jilea;)
j=l

The generative story begins by predicting the num­ 
ber of words in the French sentence (hence the 
number of elements in the alignment vector). Then 
for each French word position, first the alignment 
variable (English word index used to generate the 
current French word) is selected based on only the 
prior alignment variable. Next the French word is 
predicted based on its aligned English word.
  Following prior work (Zhao and Gildea, 2010), 
we augment the standard HMM with a fertility dis­ 
tribution.

I
Pr(f, ale)  =p(JII) ITP(cPilei)



estimate the posterior distribution using Markov 
chain Monte Carlo methods such as Gibbs sam­ 
pling  (Zhao  and  Gildea,  2010).     In this  case, 
we make  some initial  estimate  of the a vector, 
potentially randomly.    We then repeatedly re­ 
sample each element  of that vector conditioned 
on all other positions  according  to the distribu­ 
tion Pr(aj la-j, e, f).  Given a complete assign­ 
ment of the alignment for all words except the cur­ 
rent, computing the complete probability includ­ 
ing transition, emission, and jump, is straightfor­ 
ward.  This estimate comes with a computational 
cost: we must cycle through all positions of the 
vector repeatedly to gather a good estimate.  In 
practice, a small number of samples will suffice.


2.2   MAP inference with dual decomposition

Dual decomposition, also known as Lagrangian 
relaxation, is a method for solving complex 
combinatorial optimization problems (Rush and 
Collins, 2012). These complex problems are sepa­ 
rated into distinct components with tractable MAP 
inference procedures.   The subproblems are re­ 
peatedly solved with some communication over 
consistency until a consistent and globally optimal 
solution is found.
Here we are interested in the problem of find­


i=l
J


(1)


ing the most likely alignment 
of a sentence pair


ITp(ajlaj-t) p(/j leai)
j=l

where cPi  = L,f=l 8(i, aj) indicates the number of 
times that state j is visited.  This deficient model 
wastes some probability mass on inconsistent con­ 
figurations where the number of times that a state 
iis visited does not match its fertility cPi·  Follow­ 
ing in the footsteps of older, richer, and wiser col­ 
leagues (Brown et al., 1993),we forge ahead un­ 
concerned by this complication.

2.1   Parameter estimation
Of  greater  concern  is  the exponential  complex­ 
ity of inference in this model.  For the standard 
HMM, there is a dynamic programming algorithm 
to compute the posterior probability over word 
alignments  Pr(ale, f).   These  are the  sufficient 
statistics gathered in the E step of EM.
  The structure of the fertility model violates the 
Markov assumptions used in this dynamic pro­ 
gramming method. However, we may empirically


e, f. Thus, we need to solve the combinatorial op­ 
timization  problem arg maxa Pr(f, ale).  Let us 
rewrite the objective function as follows:

h(a)     t,(logp(<P.Ie;) +;;logp ;le;))
+t,(logp(a;l•;-d + logp(;;je.,))



Because f is fixed, the p( J I I) term is constant and 
may be omitted.  Note how we've  split the opti­ 
mization into two portions. The first captures fer­ 
tility as well as some component of the translation 
distribution, and the second captures the jump dis­ 
tribution and the remainder of the translation dis­ 
tribution.
  Our dual decomposition method follows this 
segmentation. Define Ya as Ya (i, j) = 1 if aj = i, 
and 0 otherwise.  Let z  E  {0, 1}/xJ be a binary


u(o)(i,j) := 0  'ViE  l..J,j E l..J
fork=1toK
a(k) := argmaxa f(a)+I;.,,J. u(k-l)(i,j)ya(i,j))
z(k) := argmaxz g(z)- I;.,,J. u(k-l)(i,j)z(i, j))
if  Ya = Z
return a(k)
end if
u(k)(i,j) := u(k)(i,j) + lik  (Ya<•l (i,j)- z(k)(i,j))
end for
return a(K)


Figure 1:  The dual decomposition algorithm for 
the fertility HMM, where 6k is the step size at the 
kth iteration for 1kK, and K is the max
number of iterations.

matrix. Define the functions f and g as
/(a)       t,(logp(a;la;-1) +  logp(/; !e.;))

g(z)        L ( logp(¢ (zi)lei) +
i=l

z(i,j)	)
L.J 	2 -logp(filei)
j=l


Then we want to find

argmaxf(a) + g(z)
a,z

subject to the constraints Ya(i,j) = z(i,j)Vi,j. 
Note how this recovers the original objective func­ 
tion when matching variables are found.
  We  use  the  dual  decomposition  algorithm 
from  Rush   and   Collins  (2012),   reproduced 
here in  Figure  1.    Note how the langrangian 
adds  one  additional  term  word,  scaled  by  a 
value indicating whether that  word is  aligned 
in  the  current  position.     Because  it  is  only 
added  for  those  words  that  are  aligned,  we 
can  merge  this  with  the  log p(fi I eai)  terms
in  both  f  and  g.     Therefore,  we  can  solve
argmax8  (!(a)+  i,j u<k-l)(i,j)ya(i,j))  us­
ing the standard Viterbi algorithm.
  The g function, on the other hand, does not have 
a commonly used decomposition structure. Luck­ 
ily we can factor this maximization into pieces that 
allow for efficient computation. Note that g sums 
over arbitrary binary matrices. Unlike the HMM, 
where each French word must have exactly one 
English generator, this maximization allows each


z(i,j) := 0 \:l(i,j)  E [l..J]  X   [l..J]
v :=0 
fori=1toJ
forj=1toJ
x(j)  := (logpCJile;)  ,j)
end for
sort x in descending order by first component 
max:= logp(¢> =Ole;), arg := 0, sum:= 0 
forf=ltoJ
sum:= sum+ x[f, 1]
if sum+ logp(¢> =fie;) >max 
max:= sum+ logp(¢> =fie;) 
arg := f
end if
end for
v := v+max
for  f  = 1 to arg
z(i, x[f, 2]) := 1
end for
end for
retumz,v


Figure 2: Algorithm for finding the arg max and 
max of g,  the fertility-related component of the 
dual decomposition objective.


French word to have zero or many generators. Be­ 
cause assignments that are in accordance between 
this model and the HMM will meet the HMM's 
constraints, the overall dual decomposition algo­ 
rithm will return valid assignments, even though 
individual selections for this model may fail to 
meet the requirements.
  As the scoring function g can be decomposed 
into a sum of scores for each row9i (i.e., there
are no interactions between distinct rows of the 
matrix) we can maximize each row independently:

I 	I
m:X L9i(zi) = Lm:Xgi(zi)
i=l 	i=l

Within each row, we seek the best of all 2J pos­ 
sible configurations.  These configurations may be 
grouped into equivalence classes based on the 
number of non-zero entries.   In each class, the 
max assignment is the one using words with the 
highest log probabilities; the total score of this as­ 
signment is the sum those log probabilities and the 
log probability of that fertility.  Sorting the scores 
of each cell in the row in descending or­ der by 
log probability allows for linear time com­ putation 
of the max for each row. The algorithm described 
in Figure 2 finds this maximal assign­ ment in O(J 
Jlog J) time, generally faster than the 0(!2 J) 
time used by Viterbi.
We note in passing that this maximizer is pick­
ing from an unconstrained set of binary matri-



ces.  Since each English word may generate as 
many French words as it likes, regardless of all 
other words in the sentence, the underlying ma­ 
trix have many more or many fewer non-zero en­ 
tries than there are French words.  A straightfor­ 
ward extension to the algorithm of Figure 2 returns 
only z matrices with exactly J nonzero entries. 
Rather than maximizing each row totally indepen­ 
dently, we keep track of the best configurations 
for each number of words generated in each row, 
and then pick the best combination that sums to J: 
another straightforward exercise in dynamic pro­ 
gramming.  This refinement does not change the 
correctness of the dual decomposition algorithm; 
rather it speeds the convergence.

3   Fertility distribution parameters

Original IBM models used a categorical distribu­ 
tion of fertility, one such distribution for each En­ 
glish word. This gives EM a great amount of free­ 
dom in parameter estimation, with no smoothing 
or parameter tying of even rare words. Prior work 
addressed this by using the single parameter Pois­ 
son distribution, forcing infrequent words to share 
a global parameter estimated from the fertility of 
all words in the corpus (Zhao and Gildea, 2010).
  We explore instead a feature-rich  approach to 
address  this  issue.     Prior  work  has  explored 
feature-rich  approaches to modeling  the transla­ 
tion distribution  (Berg-Kirkpatrick  et al., 2010); 
we use the same technique, but only for the fertil­ 
ity model. The fertility distribution is modeled as 
a log-linear distribution ofF, a binary feature set: 
p(¢1e) ex  exp (0 · F(e, ¢)). We include a simple 
set of features:
• A binary indicator for each fertility ¢.  This 
feature is present for all words, acting as 
smoothing.
• A binary indicator for each word id and fer­
tility, if the word occurs more than 10 times.

• A binary indicator for each word length (in 
letters) and fertility.
• A binary indicator for each four letter word 
prefix and fertility.

Together  these  produce  a  distribution  that  can 
learn a reasonable distribution not only for com­ 
mon words,  but also for rare words.   Including
word length information aids in for languages with



Algorithm
AE
R(
G-
+E
)
AE
R(
E-
+G
)
H
M
M
FH
M
M 
Vit
erbi
FH
M
M 
Du
al-
dec
2
4
.
0
1
9
.
7
1
8
.
0
2
1
.
8
1
9
.
6
1
7
.
4

Table 1: Experimental results over the 120 evalu­ 
ation sentences. Alignment error rates in both di­ 
rections are provided here.


4   Evaluation

We explore the impact of this improved MAP in­ 
ference procedure on a task in German-English 
word alignment. For training data we use the news 
commentary data from the WMT 2012 translation 
task.1 120 of the training sentences were manually 
annotated with word alignments.
  The results in Table 1 compare several differ­ 
ent algorithms on this same data. The first line is a 
baseline HMM using exact posterior computa­ tion 
and inference with the standard dynamic pro­ 
gramming algorithms. The next line shows the fer­ 
tility HMM with approximate posterior computa­ 
tion from Gibbs sampling but with final alignment 
selected by the Viterbi algorithm.  Clearly fertil­ 
ity modeling is improving alignment quality. The 
prior work compared Viterbi with a form of local 
search (sampling repeatedly and keeping the max), 
finding little difference between the two (Zhao and 
Gildea, 2010).  Here, however, the difference be­ 
tween a dual decomposition and Viterbi is signifi­ 
cant: their results were likely due to search error.

5   Conclusions and future work

We have introduced a dual decomposition ap­ 
proach to alignment inference that substantially 
reduces alignment error. Unfortunately the algo­ 
rithm is rather slow to converge: after 40 iterations 
of the dual decomposition,  still only 55 percent 
of the test sentences have converged. We are ex­ 
ploring improvements to the simple sub-gradient 
method applied here in hopes of finding faster con­ 
vergence, fast enough to make this algorithm prac­ 
tical.  Alternate parameter estimation techniques 
appear promising given the improvements of dual 
decomposition over sampling.  Once the perfor­ 
mance issues of this algorithm are improved, ex­ 
ploring hard EM or some variant thereof might 
lead to more substantial improvements.


compounding:  long  words in one language may	 	


correspond to multiple words in the other.


1www.statmt.org/wmt12/trans1ation-task.html


References

Taylor Berg-Kirkpatrick,  Alexandre Bouchard-Cote, 
John DeNero, and Dan Klein.  2010.  Painless un­ 
supervised learning with features.  In Human Lan­ 
guage Technologies:  The 2010 Annual Conference 
of the North American  Chapter of the Association 
for Computational  Linguistics, pages 582-590, Los 
Angeles, California, June. Association for Compu­ 
tational Linguistics.

Peter  F.  Brown,   Vincent  J.  Della  Pietra,   Stephen 
A.  Della  Pietra,  and  Robert  L. Mercer.      1993. 
The mathematics of statistical machine translation: 
parameter  estimation.    Computational  Linguistics,
19(2):263-311.

Xiaodong He. 2007.  Using word-dependent  transition 
models in HMM-based  word alignment for statisti­ 
cal machine translation.  In Proceedings of the Sec­ 
ond Workshop on Statistical Machine Translation, 
pages 80-87, Prague, Czech Republic, June. Asso­ 
ciation for Computational Linguistics.

Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical  phrase-based translation.  In Proceedings 
of the 2003 Human  Language  Technology Confer­
ence of the North American Chapter of the Associa­
tion for Computational Linguistics.

Robert C. Moore.   2004.  Improving ibm word align­ 
ment model 1.   In Proceedings of the 42nd Meet­ 
ing of the Association for Computational Linguistics 
(ACL'04), Main Volume, pages 518-525, Barcelona, 
Spain, July.

Alexander M Rush and Michael Collins. 2012. A tuto­ 
rial on dual decomposition and lagrangian relaxation 
for inference in natural language processing.  Jour­ 
nal of Artificial Intelligence Research, 45:305-362.

Kristina Toutanova and Michel Galley.  2011.  Why 
initialization matters for ibm model 1: Multiple op­ 
tima and non-strict convexity. In Proceedings of the
49th Annual Meeting of the Association for Com­ 
putational Linguistics: Human Language Technolo­ 
gies, pages 461-466, Portland, Oregon, USA, June. 
Association for Computational Linguistics.

Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996.    HMM-based  word  alignment  in  statistical
translation.  In COUNG.

Shaojun Zhao and Daniel Gildea.  2010.  A fast fertil­ 
ity hidden markov model for word alignment using 
MCMC.  In Proceedings of the 2010 Conference on 
Empirical  Methods  in  Natural  Language  Process­ 
ing, pages 596-605, Cambridge, MA, October. As­ 
sociation for Computational Linguistics.

