Exact Maximum Inference for the Fertility Hidden Markov Model



Chris Quirk 
Microsoft Research 
One Microsoft Way
Redmond, WA 98052, USA
chrisq@microsoft.com








Abstract

The notion of fertility in word alignment 
(the number of words emitted by a sin- 
gle state) is useful but difficult to model. 
Initial attempts at modeling fertility used 
heuristic search  methods.  Recent  ap- 
proaches instead use more principled  ap- 
proximate  inference  techniques such as 
Gibbs sampling for parameter estimation. 
Yet in practice we also need the single best 
alignment,  which is difficult to find us- 
ing Gibbs. Building on recent advances in 
dual decomposition, this paper introduces 
an exact  algorithm for finding the sin- 
gle best alignment  with a fertility HMM. 
Finding  the best alignment  appears impor- 
tant, as this model leads to a substantial 
improvement in alignment quality.

1   Introduction

Word-based translation models intended to model 
the translation  process have found new uses iden- 
tifying word correspondences in sentence pairs. 
These word alignments are a crucial training com- 
ponent in most machine translation systems. Fur- 
thermore, they are useful  in other NLP applica- 
tions, such as entailment  identification.
  The simplest  models  may use  lexical infor- 
mation alone.   The seminal Model 1 (Brown 
et al.,  1993) has  proved very powerful, per- 
forming nearly as   well  as   more complicated 
models in some phrasal  systems (Koehn et al.,
2003).  With minor improvements  to initializa- 
tion (Moore, 2004) (which may be important 
(Toutanova  and Galley, 2011)), it can be quite 
competitive.     Subsequent IBM  models  include 
more detailed information  about context. Models


2 and 3 incorporate  a positional  model  based on 
the absolute position of the word; Models 4 and
5 use a relative position model instead (an English 
word tends to align to a French word that is nearby 
the French word aligned to the previous English 
word). Models 3, 4, and 5 all incorporate  a no- 
tion of “fertility”: the number of French words that 
align to any English word.
  Although  these latter models covered a broad 
range of phenomena, estimation techniques and 
MAP  inference were challenging.    The au- 
thors originally recommended  heuristic proce- 
dures based on local search for both. Such meth- 
ods work reasonably well, but can be computation- 
ally inefficient and have few guarantees.   Thus, 
many researchers  have  switched to the HMM 
model (Vogel et al., 1996) and variants with more 
parameters (He, 2007). This captures the posi- 
tional information in the IBM models in a frame- 
work that admits exact parameter estimation infer- 
ence, though the objective function  is not concave: 
local maxima are a concern.
  Modeling fertility is challenging in the HMM 
framework as  it  violates the Markov assump- 
tion. Where the HMM jump model considers only 
the prior state,  fertility requires looking across 
the whole state space.   Therefore,   the standard 
forward-backward  and Viterbi algorithms do not 
apply. Recent work (Zhao and Gildea, 2010) de- 
scribed an extension to the HMM with a fertility 
model, using MCMC techniques for parameter es- 
timation. However, they do not have  a efficient 
means of MAP inference, which is necessary in 
many applications  such as machine translation.
  This paper introduces a method for exact MAP 
inference with the fertility HMM using dual de- 
composition. The resulting model leads to sub- 
stantial improvements in alignment quality.




7

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–11, 
Sofia, Bulgaria, August 4-9 2013. Qc 2013 Association for Computational Linguistics


2   HMM alignment

Let us briefly review the HMM translation model 
as a starting  point. We are given a sequence  of 
English words e = e1, . . . , eI .  This model pro- 
duces distributions  over French word sequences 
f = f1, . . . , fJ  and word alignment vectors a =


estimate the posterior distribution  using Markov 
chain Monte Carlo methods such as Gibbs sam- 
pling (Zhao and Gildea, 2010).   In this case, 
we make  some initial estimate of the a vector, 
potentially randomly.   We  then repeatedly  re- 
sample each element  of that vector conditioned 
on all other positions according to the distribu-


a1, . . . , aJ , where aj   ∈ [0..J ] indicates the En-
glish word generating the jth French word, 0 rep-


tion Pr(aj |a−j


, e, f ).  Given a complete  assign-



resenting a special NULL state to handle systemat- 
ically unaligned words.

J
Pr(f , a|e) = p(J |I ) n p(aj |aj−1) p fj  ea


ment of the alignment for all words except the cur-
rent, computing the complete probability includ- 
ing transition,  emission, and jump, is straightfor- 
ward. This estimate comes with a computational 
cost: we must cycle through all positions of the



j=1


 
j
vector repeatedly 
to gather  a good  
estimate. In
practice, a small 
number of samples 
will suffice.


The generative story begins by predicting the num-
ber of words in the French sentence (hence the 
number of elements in the alignment vector). Then 
for each French word position, first the alignment 
variable (English word index used to generate the 
current French word)  is selected based on only the 
prior alignment variable. Next the French word is 
predicted based on its aligned English word.
  Following prior work (Zhao and Gildea, 2010), 
we augment the standard HMM with a fertility dis- 
tribution.

I
Pr(f , a|e) =p(J |I ) n p(φi|ei)




2.2   MAP inference with dual decomposition

Dual decomposition, also known as Lagrangian 
relaxation, is  a   method for  solving complex 
combinatorial optimization problems (Rush and 
Collins,  2012). These complex problems are sepa- 
rated into distinct components with tractable MAP 
inference procedures. The subproblems  are re- 
peatedly solved with some communication  over 
consistency until a consistent and globally  optimal 
solution is found.
Here we are interested in the problem of find-


i=1
J
n p(aj |aj−1) p fj  ea
j=1


(1)
 
j


ing the most likely alignment of a 
sentence  pair

e, f . Thus, we need to solve the 
combinatorial op- timization problem arg 
maxa Pr(f , a|e).   Let us
rewrite  the objective  function  as follows:


where φi  = LJ


δ(i, aj ) indicates 
the number of


times that state j is visited. This deficient model	I  



log p(fj |ei)


wastes some probability mass on inconsistent con-


h(a) = ) log p(φi|ei) + )	


figurations where the number of times that a state



i=1


2
j,aj =i


i is visited does not match its fertility φi. Follow-
ing in the footsteps of older, richer, and wiser col- 
leagues (Brown et al., 1993),we forge ahead un- 
concerned by this complication.


J
+ )
j=1


(
log p(aj |aj−1) +



log p fj  eaj  
2





2.1   Parameter estimation

Of greater concern is the exponential complex- 
ity of inference in this model. For the standard 
HMM, there is a dynamic programming algorithm 
to compute the posterior probability over word
alignments Pr(a|e, f ).   These  are the sufficient
statistics gathered in the E step of EM.
  The structure of the fertility model violates the 
Markov assumptions  used in this dynamic pro- 
gramming method. However, we may empirically


Because f is fixed, the p(J |I ) term is constant and
may be omitted. Note how we’ve split the opti-
mization into two portions. The first captures fer- 
tility as well as some component  of the translation 
distribution, and the second captures the jump dis- 
tribution  and the remainder of the translation dis- 
tribution.
  Our dual decomposition  method follows this 
segmentation. Define ya as ya(i, j) = 1 if aj  = i,
and 0 otherwise.  Let z ∈ {0, 1}I ×J be a binary


u(0) (i, j) := 0 ∀i ∈ 1..I , j ∈ 1..J
for k = 1 to K
a(k) := arg maxa   f (a) + 'S





i,j u





(k−1)




(i, j)ya (i, j)\


z(i, j) := 0 ∀(i, j) ∈ [1..I ] × [1..J ]
v := 0
for i = 1 to I
for j = 1 to J


z(k) := arg maxz   g(z) − 'S
if ya  = z
return a(k)
end if


i,j u


(k−1)


(i, j)z(i, j)\


x(j) := (log p(fj |ei ) , j)
end for
sort x in descending order by first 
component
max := log p(φ = 0|ei ) , arg := 0, 
sum := 0
for f = 1 to J


u(k) (i, j) := u(k) (i, j) + δk  y
end for 
return a(K)



a(k)


(i, j) − z(k) (i, j)\


sum := sum + x[f, 1]
if sum + log p(φ 
= f |ei ) > max 
max := sum + 
log p(φ = f |ei )
arg := f
end if


Figure 1: The dual decomposition algorithm for
the fertility HMM, where δk is the step size at the
kth iteration for 1 ≤ k ≤ K , and K is the max
number of iterations.


matrix. Define the functions f and g as


end for
v := v + max
for f = 1 to arg
z(i, x[f, 2]) := 1
    end for 
end for 
return z, v




f (a)   =



J
)

j=1
I



(
log p(aj 
|aj−1) +



1	\
2 log p fj 
eaj


Figure 2: 
Algorith
m for 
finding 
the arg 
max and 
max of 
g, the 
fertility-
related  
compone
nt of the 
dual 
decompo
sition 
objective
.


g(z)   =


) ( log 
p(φ 
(zi)|ei) +
i=1
J
) z(i, 
j) log 
p(f |e ) 
'



French 
word to 
have zero 
or many 
generator
s. Be- 
cause 
assignme
nts that 
are in 
accordan
ce 
between 
this 
model 
and the 
HMM 
will 
meet the 
HMM’s


     2	j  i 
j=1


Then we want to find

arg max f (a) + g(z)
a,z
subject to the constraints ya(i, j)  = z(i, j)∀i, j. 
Note how this recovers the original objective func-


constraints, the overall dual decomposition algo-
rithm will return valid assignments, even though 
individual selections for this model may fail to 
meet the requirements.

  As the scoring function g can be decomposed 
into a sum of scores for each row Li gi (i.e., there
are no interactions  between distinct rows of the 
matrix) we can maximize each row independently:


tion when matching variables are found.	I	I



We   use  the dual decomposition  algorithm


max ) gi(zi) = ) max gi(zi)



from  Rush and  Collins  (2012),	reproduced


z
i=1


z
i=1


here in Figure 1.    Note how the langrangian 
adds  one additional term word,  scaled by  a 
value indicating whether that word is aligned 
in  the current position.    Because  it  is  only 
added for  those words that are  aligned, we 
can merge this  with  the log p fj eaj        terms 
in  both f and g.    Therefore, we can solve


Within each row, we seek the best of all 2J  pos- 
sible configurations. These configurations  may 
be grouped into equivalence  classes based on the 
number of non-zero entries. In each class, the 
max assignment is the one using words with the 
highest log probabilities;  the total score of this as- 
signment is the sum those log probabilities and


arg maxa (f (a) + L


i,j


u(k−1)(i, j)ya(i, j)'  
us-


the log probability of 
that fertility.   Sorting 
the


ing the standard Viterbi algorithm.
  The g function, on the other hand, does not have 
a commonly  used decomposition  structure.  Luck- 
ily we can factor this maximization into pieces that 
allow for efficient computation. Note that g sums 
over arbitrary binary matrices. Unlike the HMM, 
where each French word must have exactly  one 
English generator, this maximization  allows each


scores of each cell in the row in descending or- 
der by log probability allows for linear time com- 
putation of the max for each row. The algorithm 
described in Figure 2 finds this maximal assign- 
ment in O(I J log J ) time, generally faster than 
the O(I 2J ) time used by Viterbi.
  We note in passing that this maximizer is pick- 
ing from an unconstrained  set of binary matri-



ces.   Since each English  word may generate  as 
many French words  as it likes,  regardless of all 
other words in the sentence, the underlying ma- 
trix have many more or many fewer non-zero en- 
tries than there are French words. A straightfor- 
ward extension to the algorithm of Figure 2 returns 
only z matrices with exactly J nonzero entries. 
Rather than maximizing each row totally indepen- 
dently, we keep track of the best configurations 
for each number of words generated in each row, 
and then pick the best combination that sums to J : 
another straightforward  exercise in dynamic pro- 
gramming. This refinement  does not change the 
correctness of the dual decomposition algorithm; 
rather it speeds the convergence.

3   Fertility distribution parameters

Original IBM models used a categorical  distribu- 
tion of fertility, one such distribution  for each En- 
glish word. This gives EM a great amount of free- 
dom in parameter estimation, with no smoothing 
or parameter tying of even rare words. Prior work 
addressed this by using the single parameter Pois- 
son distribution,  forcing infrequent words to share 
a global parameter estimated from the fertility of 
all words in the corpus (Zhao and Gildea, 2010).
  We explore  instead a feature-rich  approach to 
address this issue.  Prior work has explored 
feature-rich  approaches to modeling the transla- 
tion distribution (Berg-Kirkpatrick  et al., 2010); 
we use the same technique, but only for the fertil- 
ity model. The fertility distribution is modeled  as 
a log-linear distribution  of F , a binary feature set:
p(φ|e) ∝ exp (θ · F (e, φ)). We include a simple
set of features:
• A binary indicator for each fertility φ. This 
feature  is present  for all words, acting as 
smoothing.
• A binary indicator for each word id and fer-
tility, if the word occurs more than 10 times.
• A binary indicator for each word length (in
letters) and fertility.
• A binary indicator for each four letter word
prefix and fertility.

Together   these  produce  a  distribution that can 
learn a reasonable distribution  not only for com- 
mon words, but also for rare words. Including 
word length information  aids in for languages with



Al
go
rith
m
AE
R 
(G
→
E)
AE
R 
(E
→
G)
H
M
M
FH
M
M 
Vit
er
bi
FH
M
M 
Du
al-
de
c
2
4
.
0
1
9
.
7
1
8
.
0
2
1
.
8
1
9
.
6
1
7
.
4

Table 1: Experimental results over the 120 evalu- 
ation sentences. Alignment  error rates in both di- 
rections are provided here.


4   Evaluation

We explore the impact of this improved MAP in- 
ference procedure on a task in German-English 
word alignment.  For training  data we use the news 
commentary data from the WMT 2012 translation 
task.1 120 of the training sentences were manually 
annotated with word alignments.
  The results in Table 1 compare several differ- 
ent algorithms on this same data. The first line is 
a baseline  HMM using exact posterior computa- 
tion and inference with the standard dynamic pro- 
gramming algorithms. The next line shows the fer- 
tility HMM with approximate posterior computa- 
tion from Gibbs sampling but with final alignment 
selected by the Viterbi algorithm. Clearly fertil- 
ity modeling is improving alignment quality. The 
prior work compared Viterbi with a form of local 
search (sampling  repeatedly and keeping the max), 
finding little difference between the two (Zhao and 
Gildea, 2010). Here, however, the difference be- 
tween a dual decomposition  and Viterbi is signifi- 
cant: their results were likely due to search error.

5   Conclusions and future work

We  have  introduced   a  dual decomposition  ap- 
proach to alignment inference that substantially 
reduces alignment error. Unfortunately  the algo- 
rithm is rather slow to converge: after 40 iterations 
of the dual decomposition, still only 55 percent 
of the test sentences have converged.  We are ex- 
ploring improvements to the simple sub-gradient 
method applied here in hopes of finding faster con- 
vergence, fast enough to make this algorithm prac- 
tical.  Alternate parameter estimation techniques 
appear promising  given the improvements of dual 
decomposition over sampling. Once the perfor- 
mance issues of this algorithm are improved, ex- 
ploring hard EM or some variant thereof might 
lead to more substantial improvements.


compounding:  long words in one language may	 	


correspond to multiple words in the other.


1 www.statmt.org/wmt12/translation-task.html


References

Taylor Berg-Kirkpatrick, Alexandre  Bouchard-Coˆ te´, 
John DeNero, and Dan Klein.  2010. Painless un- 
supervised learning with features.  In Human Lan- 
guage Technologies:  The 2010 Annual Conference 
of the North American Chapter of the Association 
for Computational  Linguistics,  pages 582–590, Los 
Angeles, California,  June. Association for Compu- 
tational Linguistics.

Peter F. Brown, Vincent J.  Della Pietra, Stephen 
A.  Della Pietra, and Robert L.  Mercer.   1993. 
The mathematics of statistical machine translation: 
parameter estimation. Computational Linguistics,
19(2):263–311.

Xiaodong He. 2007. Using word-dependent transition 
models in HMM-based word alignment for statisti- 
cal machine translation.  In Proceedings of the Sec- 
ond Workshop on Statistical Machine Translation, 
pages 80–87, Prague, Czech Republic,  June. Asso- 
ciation for Computational Linguistics.

Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation.  In Proceedings 
of the 2003 Human Language Technology Confer- 
ence of the North American Chapter of the Associa- 
tion for Computational Linguistics.

Robert C. Moore. 2004. Improving ibm word align- 
ment model 1.  In Proceedings of the 42nd Meet- 
ing of the Association for Computational Linguistics 
(ACL’04), Main Volume, pages 518–525, Barcelona, 
Spain, July.

Alexander M Rush and Michael Collins. 2012. A tuto- 
rial on dual decomposition and lagrangian relaxation 
for inference in natural language processing.  Jour- 
nal of Artificial Intelligence  Research, 45:305–362.

Kristina Toutanova  and Michel Galley.  2011. Why 
initialization matters for ibm model 1: Multiple op- 
tima and non-strict convexity. In Proceedings of the
49th Annual Meeting of the Association for Com- 
putational Linguistics:  Human Language Technolo- 
gies, pages 461–466, Portland, Oregon, USA, June. 
Association for Computational Linguistics.

Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996.  HMM-based word alignment in statistical 
translation. In COLING.

Shaojun Zhao and Daniel Gildea. 2010. A fast fertil- 
ity hidden markov model for word alignment using 
MCMC. In Proceedings of the 2010 Conference on 
Empirical Methods in Natural Language Process- 
ing, pages 596–605, Cambridge, MA, October. As- 
sociation for Computational Linguistics.

