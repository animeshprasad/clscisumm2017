Monolingual Distributional Similarity for Text-to-Text Generation



Juri Ganitkevitch, Benjamin Van Dunne,   and Chris Callison-Burch
Center for Language and Speech Processing Human 
Language Technology Center of Excellence Johns 
Hopkins University
Baltimore, MD  21218, USA





Abstract

Previous work on paraphrase extraction and 
application has relied on either parallel 
datasets, or on distributional similarity met­ 
tics over large text corpora.  Our  approach 
combines these two orthogonal sources of in­ 
formation and directly integrates them into 
our paraphrasing system's log-linear model. 
We compare different distributional similar­ 
ity feature-sets and show significant improve­ 
ments in grarnmaticality and meaning reten­ 
tion on the example text-to-text generation 
task of sentence compression, achieving state­ 
of-the-art quality.


1   Introduction

A wide variety of applications in natural language 
processing can be cast in terms of text-to-text gen­ 
eration.    Given  input  in  the form  of  natural  lan­ 
guage, a text-to-text generation system produces 
natural language output that is subject to a set of 
constraints. Compression systems, for instance, pro­ 
duce  shorter  sentences.     Paraphrases,  i.e.  differ­ 
ing textual realizations  of the same meaning, are a 
crucial components of text-to-text generation sys­ 
tems, and have been successfully  applied  to tasks 
such as multi-document summarization  (Barzilay et 
a!.,  1999;  Barzilay,  2003),  query  expansion  (Arr­ 
ick and Tipirneni, 1999; Riezler eta!., 2007), ques­ 
tion answering (McKeown, 1979; Ravichandran and 
Hovy, 2002), sentence compression  (Cohn and La­ 
pata, 2008; Zhao et a!., 2009), and simplification 
(Wubben et a!., 2012).
  Paraphrase collections for text-to-text generation 
have been extracted from a variety of different cor­
pora.   Several  approaches  rely on bilingual  para!-


lei data (Bannard and Callison-Burch,  2005; Zhao 
et a!., 2008; Callison-Burch,  2008; Ganitkevitch et 
a!., 2011), while others leverage distributional meth­ 
ods on  monolingual  text corpora  (Lin and Pantel,
2001; Bhagat and Ravichandran, 2008). So far, how­
ever, ouly preliminary studies have been undertaken 
to combine the information from these two sources 
(Chan eta!., 2011).
  In this paper, we describe an extension of Gan­ 
itkevitch et a!. (2011)'s  bilingual data-based ap­ 
proach.  We augment the bilingually-sourced para­ 
phrases using features based on monolingual distri­ 
butional similarity. More specifically:

• We show that using monolingual distributional 
similarity features improves paraphrase quality 
beyond what we can achieve with features esti­ 
mated from bilingual data.

• We  define  distributional  similarity  for  para­ 
phrase patterns that contain constituent-level 
gaps, e.g.

sim(one JJ instance of NP, a JJ case of NP).

This generalizes  over distributional  similarity 
for contiguous phrases.

• We compare  different  types  of  monolingual 
distributional  information  and show that they 
can be used to achieve siguificant improve­ 
ments in grammaticality.

• Finally,  we  compare  our  method  to  several 
strong baselines on the text-to-text generation 
task of sentence compression.   Our method 
shows state-of-the-art  results, beating a purely 
bilingually sourced paraphrasing system.




256


First Joint Conference on Lexical and Computational Semantics (*SEM), pages 256-264,
Montreal, Canada, June 7-8, 2012. @2012 Association for Computational Linguistics




...their   IJII!nsithe term 	would...
-.,...-:.  ..	\
t'	......,
...ihre 	langfrisligen  P*le	wOrden...

...'*!ne sei,ne  lang en	e	aufzuQeben...
,.,....'    ....-----	:	/ 	·-.·. -"''
...without giving up his      long-lenn   plans


Figure 1:Pivot-based paraphrase extraction for con­ 
tiguous phrases.Two phrases translating to the same 
phrase in the foreign language are assumed to be 
paraphrases of one another.


2   Background

Approaches to paraphrase extraction differ based on 
their underlying data source. In Section 2.1we out­ 
line pivot-based paraphrase extraction from bilin­ 
gual data, while  the contextual features used to de­ 
termine closeness in meaning in  monolingual ap­ 
proaches is described in Section 2.2.

2.1   Paraphrase Extraction via Pivoting
Following Ganitkevitch et al. (2011), we formulate 
our  paraphrases as  a syntactically annotated syn­ 
chronous context-free grammar (SCFG) (Abo  and 
Ullman, 1972; Chiang, 2005). An SCFG rule has 
thefonn:


 

Figure 2: Extraction of syntactic paraphrases via the 
pivoting approach: We aggregate over different sur­ 
face realizations, matching the lexicalized portions 
of the rule and generalizing over the nonterminals.


To extract paraphrases we follow the intuition that
two  English strings e1 and  e2 that  translate to the 
same foreign string f can  be assumed to have the 
same meaning,as illustrated in Figure 1.1
  First, we use standard machine translation meth­ 
ods to extract a foreign-to-English translation gram­ 
mar from a bilingual parallel corpus (Koehn. 2010). 
Then, for  each  pair  of translation rules  where  the
left-hand side C and foreign string f match:

r1 = C -{!,e1, "'1,'P1)
r2 = C- {!,e2,"'2, 2),
wepivot over f to create a paraphrase rule rp:


r = C -{!, e,"',<jJ),
where  the left-hand side of the rule, C.is a nonter­



rp = C- (e1, e2,"'p



1c,Op),


minal and  the right-hand sides  f and  e are strings 
of  terminal and  nonterminal symbols.  There is a 
one-to-one correspondency between the nontermi­
nals in  f and e: each nonterminal symbol in  f has 
to also appear in e.The function"' captures this bi­ 
jective mapping between the nonterminals. Drawing 
on machine translation terminology, we refer to f as 
the source and e as the target side of the rule.
  Each ruleis annotated with a feature vector of fea­ 
ture functions rp = {'J'l..·'PN} that,  using  a corre­ 
sponding weight  vector  X, are combined in a log­
linear model to compute the cost of applying r:

cost(r) = -L,\i log'Pi·	(1)
i=l
A wide  variety  of  feature functions can be formu­ 
lated.  We detail  the feature-set used in our experi­ 
ments inSection 4.


with a combined nonterminal correspondency func­
tion  "'p· Not.e that  the common source side  f im­ 
pliesthat e1 and e2 share the same set of nontenninal 
symbols.
  The  paraphrase feature vector q}p  is  computed 
from the  translation feature vectors  q}1 and2  by 
following the pivoting idea.  For instance, we esti­ 
mate the conditional paraphrase probabilityp(e2le1) 
by margjnaJizing over  all shared foreign-language
translations f:

p(e2let) 	= Lp{e2, /let) 	(2)
I
Lp(e2l/, et)P(flet)	(3)
I
R1    LP(e2if)p(flet). 	(4)
I
1See Yao et al.(2012) for an analysis of this assumption.


 

Figure 3: An example of a synchronous paraphras­ 
tic derivation, here a sentence compression. Shaded 
words are deleted in the indicated rule applications.


Figure 2 illustrates syntax-constrained  pivoting and 
feature aggregation over multiple foreign language 
translations for a paraphrase pattern.
  After the SCFG has been extracted, it can be used 
within standard machine translation machinery, such 
as the Joshua decoder (Ganitkevitch et a!., 2012). 
Figure 3 shows an example for a synchronous para­ 
phrastic derivation produced as a result of applying 
our paraphrase grammar in the decoding process.
  The approach outlined relies on aligned bilingual 
texts to identify phrases and patterns that are equiva­ 
lent in meaning. When extracting paraphrases from 
monolingual text, we have to rely on an entirely dif­ 
ferent set of semantic cues and features.

2.2   Monolingual Distributional Similarity

Methods  based on monolingual  text corpora  mea­ 
sure the similarity of phrases based on contextual 
features.  To describe a phrase e, we define a set of 
features that capture the context of an occurrence of 
e in our corpus.  Writing the context vector for the
i-th occurrence of e as se,i. we can aggregate over 
all occurrences of e, resulting in a distributional sig­
nature for e, S. = L; S.,i· Following the intuition
that phrases with similar meanings occur in similar
contexts, we can then quantify the goodness of e' as 
a paraphrase of e by computing the cosine similarity 
between their distributional signatures:

.   (	') 	Be ·Be'



linguistically informed feature-sets that rely on de­ 
pendency and constituency parses, part-of-speech 
tags, or lemmatization have been proposed in widely 
known work such as by Church and Hanks (1991) 
and Lin and Pantel (2001).  For instance, a phrase 
is described by the various syntactic relations it has 
with lexical items in its context, such as: "for what 
verbs do we see with the phrase as the subject?", or 
"what adjectives modify the phrase?".
  However, when moving to vast text collections or 
collapsed representations of large text corpora, lin­ 
guistic annotations can become impractically expen­ 
sive to produce.  A straightforward and widely used 
solution is to fall back onto lexical n-gram features, 
e.g. "what words or bigrams have we seen to the left 
of this phrase?" A substantial body of work has fo­ 
cussed on using this type of feature-set for a variety 
of purposes in NLP (Lapata and Keller, 2005; Bha­ 
gat and Ravichandran,  2008; Lin et a!., 2010; Van 
Durme and Lall, 2010).

2.3    Other Related Work

Recently, Chan eta!. (2011) presented an initial in­ 
vestigation into combining phrasal paraphrases ob­ 
tained through bilingual pivoting with monolingual 
distributional  information.  Their work investigated 
a reranking approach and evaluated their method via 
a substitution task, showing that the two sources of 
information are complementary and can yield im­ 
provements in paraphrase quality when combined.

3   Incorporating Distributional Similarity

In order to incorporate  distributional  similarity  in­ 
formation  into  the  paraphrasing  system,  we  need 
to calculate similarity scores for the paraphrastic 
SCFG rules in our grammar.  For rules with purely 
lexical right-hand  sides  e1 and e2  this is a simple
task, and the similarity score sim(e1, e2) can be di­
rectly included in the rule's feature vector 'P. How­
ever, if e1 and e2 are long, their occurrences become 
sparse and their similarity can no longer be reliably 
estimated.  In our case, the right-hand  sides of our
rules often contain gaps and computing a similarity


szm e,e 	=



.
Se   Se' 1


score is less straightforward.
Figure 4 shows an 
example of such a 
discontin­


A wide variety of features  have been  used  to de­
scribe the distributional context of a phrase.  Rich,


uous rule and illustrates  our  solution:  we decom­
pose the discontinuous  patterns  that  make  up  the


NP---·i>   


.  ( ) _ 1(  . ( the long-term )	. ('s))
Slm r -	Slm  in the long term	nm  of


Figure 4: Scoring a rule by extracting and scoring 
contiguous  phrases consistent  with the alignment. 
The overall score of the rule is detennined by av­ 
eraging across all pairs of contiguous subphrases.


right-hand sides of a role r into pairs of contiguous 
phrases 1'(r)  = {(e, e')}. for which we can look 
up distributional  signatures and compute similarity 
scores.   This decomposition into phrases is non­ 
trivial, since our sentential paraphrase roles often 
involve significant reordering or structural changes. 
To avoid comparing  unrelated phrase pairs, were­ 
quire 1'(r)  to be consistent  with a token alignment 
a. The alignment  is defined analogously  to word 
alignments in machine translation. and computed by 
treating the source and target sides of our paraphrase 
roles as a parallel corpus.
  We define the overall similarity score of the role 
to be the average of the similarity scores of all ex­ 
tracted phrase pairs:

1
sim(r,a)= IP(a)l 	sim(e,e').



towards comparing  the substitutability  and context 
similarity of a pair of paraphrases.
  Our  two similarity  scores  are incorporated  into 
the  paraphraser  as  additional  rule  features  in  cp,
simngram and simllJifl• respectively. We estimate the 
corresponding weights along withthe otheras de­ 
tailed in Section 4.

4 	Experimental Setup

4.1    'IBsk:Sentence Compression

To evaluate our method on a real text-to-text appli­ 
cation, we use the sentence compression  task. To 
tune the parameters of our paraphrase system for 
sentence compression, we need an appropriate cor­ 
pus of reference compressions.  Since our model is 
designed to compress by paraphrasing rather than 
deletion, the commonly used deletion-based com­ 
pression data sets like the Ziff-Davis corpus are not 
suitable. We thus use the dataset introduced  in our 
previous work (Ganitkevitch et al., 2011).
  Beginning with 9570 tuples of parallel Engli• 
English sentences obtained from multiple reference 
translations for machine translation evaluation, we 
construct  a parallel compression  corpus by select­ 
ing the longest reference in each tuple as the source 
sentence and the shortest reference as the target sen­ 
tence.  We further retain only those sentence pairs
where the compression  ratio  cr falls in the range
0.5  < cr  0.8.  From these, we select 936 sen­
tences for the development set, as well as 560 sen­
tences for a test set that we use to gauge the perfor­


(e,e')E'P(a)

Since the distributional signatures for long, rare 
phrases may  be computed from  only a handful  of 
occurrences,  we additionally  query for the shorter 
sub-phrases  that are more likely  to have been ob­ 
served often enough to have reliable signatures and 
thus similarity estimates.
  Our  definition  of  the  similarity  of  two discon­ 
tinuous phrases substantially differs from others in 
the literature.    This  difference  is due  to  a differ­ 
ence in motivation.  Un and Pantel (2001), for in­ 
stance, seek to find new paraphrasepairs by compar­ 
ing their arguments. In this work. however, we try 
to add orthogonal information to existing paraphrase 
pairs.  Both our definition of pattern similarity and 
our feature-set (see Section 4.3) are therefore geared



mance of our system.
  We contrast our distributional similarity-informed 
paraphrase system with a pivoting-only baseline, as 
well as an implementation of Clarke and Lapata 
(2008)'s state--of-the-art compression  model which 
uses a series of constraints in an integer linear pro­ 
gramming (ILP) solver.

4.2   Baseline Paraphrase Grammar

We  extract   our   paraphrase   grammar   from   the 
French-English portion of the Europarl corpus (ver­ 
sion 5) (Koehn, 2005). The Berkeley aligner (Liang 
et al.,  2006) and  the Berkeley  parser  (Petrov  and 
Klein, 2007) are used to align the bitext and parse 
the English side, respectively.The paraphrase gram­ 
mar  is  produced  using  the  Hadoop-based   Thrax


Left
26 ,.....-;-;---l-----:---.1


Right


goals   23



.....................-.-.: -
: - -SR
11'.:••..,. ..... 	••••	••.amocf·· .
holdlrQ on  tO   the  
long.:term	Investment
VBG  IN  TO DT
	J
J 
	
NN


 



Figure 5: An example of the n-gra.m feature extrac­ 
tion on ann-gram corpus. Here, ''the long-term" is 
seen preceded by ''revise" (43 times) and followed 
by ''plans., (97 times). The corresponding left- and 
right-side features are added to the phrase signature 
with the counts of the n-gra.ms that gave rise to them.


grammar extractor's paraphrase mode (Ganitkevitch 
et al., 2012).  The syntactic nonterminallabels we 
allowed in the grammar were limited to constituent 
labels and CCG-style slashed categories.Paraphrase 
grammars extracted via pivoting tend to grow very 
large.  To keep the grammar size manageable,  we 
pruned away all paraphrase rules whose phrasal 
paraphrase  probabilities  p(e1le2) or p(e2le1) were 
smaller than 0.001.
  We extend the feature-set used in Ganitkevitch et 
al. (2011) with a number of features that aim to bet­ 
ter describe a rule's compressive  power: on top of 
the word count features  wcountsrc and  wcounttgt 
and the word count difference feature  wcountdiJJ, 
we addcharacter based count and difference features
ccount. rc.ccounttgt• and ccountdiff• as well  as log­ 
compression  ratio features  word cr = log =::!!!! 
and the analogously defined charcr = log ::;:!;:.
  For  model  tuning  and  decoding  we  used  the 
Joshua machine translation system (Ganitkevitch et 
al., 2012).The model weights are estimated using an 
implementation of the PRO tuning algorithm (Hop­ 
kins and May, 2011), with PRECIS as our objective 
function (Ganitkevitch et al., 2011).  The language 
model used in our paraphraser  and the Clarke and 
Lapata (2008) baseline system is a Kneser-Ney dis­ 
counted 5-gram  model estimated on the Gigaword 
corpus using the SRILM toolkit (Stolcke, 2002).


Figure  6:   An example  of  the  syntactic  feature­ 
set The phrase ''the long-term" is annotated  with 
position-aware lexical and part-of-speech n-gra.m 
features (e.g. "on to" on the left, and ''investment" 
and  "NN"  to its  right),  labeled  dependency  links 
(e.g.     amod- investment) and  features  derived 
from the phrase's CCG label NP /NN.



4.3   Distributional Similarlty Model

To investigate the impact of the feature-set used to 
construct distributional signatures, we contrast two 
approaches: a high-coverage collection of distribu­ 
tional signatures with a relatively simple feature-set, 
and a much smaller set of signatures with a rich, syn­ 
tactically informed feature-set.


4.3.1   n-gram Model

  The high-coverage model (from here on: n-gram 
model) is drawn from a web-scale n-gram corpus 
(Brants and Franz, 2006; Lin et al., 2010). We ex­ 
tract signatures for phrases up to a length of 4. For 
each phrase p we look at n-grams of the form wp 
and pv, where w  and v are single words.  We then 
extract the corresponding  features Wteft and Vnght· 
The feature count is set to the count of the n-gram, 
reflecting the frequency with which p was preceded 
or followed, respectively, by w and v in the data the 
n-gra.m corpus is based on. Figure  5 illustrates this 
feature extraction approach.The resulting collection 
comprises distributional signatures for the 200 mil­
lion most frequent 1-to-4-grams in the n-gram cor­
pus.


4.3.2   Syntactic Model
  For the syntactically informed signature model 
(from here on:   syntax model),  we use the 
constituency  and  dependency  parses  provided  in 
the  Annotated  Gigaword  corpus  (Napoles  et  al.,
2012).    We limit ourselves to the Los Angeles 
Times/Washington Post portion of the corpus and 
extract phrases up to a length of 4.  The following 
feature set is used to compute distributional  signa­ 
tures for the extracted phrases:

• Position-aware lexical and part-of-speech  uni­ 
gram and bigram features, drawn from a three­ 
word window to the right and left of the phrase.

• Features based on dependencies for both links 
into and out of the phrase, labeled with the cor­ 
responding lexical item and POS. If the phrase 
corresponds to a complete subtree in the con­ 
stituency parse we additionally include lexical 
and POS features for its head word.

• Syntactic features for any constituents govern­ 
ing the phrase, as well as for CCG-style slashed 
constituent labels for the phrase. The latter are 
split in governing constituent and missing con­ 
stituent (with directionality).

Fignre 6 illustrates the syntax model's feature ex­ 
traction for an example phrase occurrence.   Using 
this method we extract distributional  signatures for 
over 12 million 1-to-4-gram phrases.

4.3.3   Locality Sensitive Hashing
  Collecting distributional signatures for a large 
number of phrases quickly leads to unmanageably 
large datasets.  Storing  the syntax model's  12 mil­ 
lion  signatures  in  a  compressed  readable  format, 
for  instance,   requires  over  20GB  of  disk  space. 
Uke Ravichandran et al. (2005) and Bhagat and 
Ravichandran (2008),  we rely on locality  sensitive 
hashing (LSH) to make the use of these large collec­ 
tions practical.
  In order  to avoid explicitly  computing  the fea­ 
ture vectors, which can be memory intensive for fre­ 
quent phrases, we chose the online LSH variant de­ 
scribed  by Van Dunne  and Lall  (2010),  as imple­ 
mented  in  the Jerboa  toolkit  (Van Dunne,  2012). 
This method, based on the earlier work of Indyk and


Motwaui (1998) and Charikar (2002), approximates 
the cosine similarity between two feature vectors 
based on the Harurning distance in a dimensionality­ 
reduced  bitwise  representation.    Two feature  vec­ 
tors u,  v each  of  dimension  d are  first projected 
through a d x b random matrix populated with draws 
from N(O,1).   We then convert the resulting b­ 
dimensional vectors into bit-vectors by setting each 
bit of the signature conditioned on whether the cor­ 
responding  projected  value is less  than 0.    Now, 
given the bit signatures h(il)  and h(V), we can ap­ 
proximate the cosine similarity of u and v as:

 

where d(-,·) is the Harurning distance.   In our ex­ 
periments we use 256-bit signatures.  This reduces 
the memory requirements for the syntax model to 
around 600MB.

5   Evaluation Results

To rate the quality of our output, we solicit human 
judgments of the compressions along two five-point 
scales:  grammaticality and meaning preservation. 
Judges are instructed to decide how much the mean­ 
ing from a reference translation is retained in the 
compressed  sentence,  with a score of 5 indicating 
that all of the important information is present, and
1 being that the compression does not retain any of 
the original  meaning.   Similarly, a grammar  score 
of 5 indicates perfect grammaticality, while a score 
of 1 is assigned  to sentences  that are entirely  un­ 
grammatical.   We ran our evaluation on Mechaui­ 
cal Turk, where a total of 126 judges provided 3 re­ 
dundant judgments for each system output. To pro­ 
vide additional quality control, our illTs were aug­ 
mented with both positive and negative control com­ 
pressions. For the positive control we used the refer­ 
ence compressions from our test set.  Negative con­ 
trol was provided by adding a compression model 
based on random word deletions to the mix.
  In  Table  1 we  compare  our  distributional 
similarity-augmented systems to the plain pivoting­ 
based baseline and the lLP approach. The compres­ 
sion ratios of the paraphrasing systems are tuned to 
match the average compression ratio seen on the de­ 
velopment and test set.  The lLP system is config-


uredto loosely match this ratio,as to not overly con­ 
strain its search space. Our results indicate that the 
paraphrase  approach significantly outperforms  ll..P 
on meaning retention.   However, the baseline  sys­
tem  shows  notable weaknesses  in grammaticality.



300
2SO
200
150
100
so


.. pp


Adding  the n-gram  distributional  similarity  model 
to the paraphraser recovers some of the difference in 
grammaticality while simultaneously yielding some 
gain in the compressions' meaning retention. Mov­ 
ing to distributional similarity estimated on the syn­ 
tactic feature-set yields additional improvement, de­ 
spite the model's lower coverage.
  It is known that human evaluation scores correlate 
linearly with the compression ratio produced by a 
sentence compression system (Napoles et al., 2011). 
Thus, to ensure fairness in our comparisons, we pro­ 
duce  a pairwise  comparison  breakdown  that  only 
takes into account compressions of almost identical 
length.2 Figure 7 shows the results of this analysis, 
detailing the number of wins and ties in the human 
judgements.
  We note that the gains in meaning retention over 
both the baseline and the ll..P system are still present 
in the pairwise breakdown.    The gains over the 
paraphrasing baseline, as well as the improvement 
in meaning over ll..P  are statistically  significant at
p < 0.05 (using the sign test).
  We can observe that there is substantial  overlap 
between the baseline paraphraser and the n-gram 
model, while the syntax model appears to yield no­ 
ticeably different output far more often.
  Table 2 shows two example sentences drawn from 
our test set and the compressions produced by the 
different  systems.    It can  be  seen  that  both  the 
paraphrase-based and n.P systems produce good 
quality results, with the paraphrase system retaining 
the meaning of the source sentence more accurately.

6 	Conclusion

We presented a method to incorporate monolingual 
distributional similarity into linguistically informed 
paraphrases extracted from bilingual parallel data. 
Having extended the notion of similarity to dis­ 
contiguous pattern with multi-word gaps, we inves­ 
tigated  the effect  of  using  feature-sets  of  varying

2We require the compressions to be within ±10% length of
one another.


§3  F======l ==== F=====H
2SO
200
150
100
50
O L- ---J'--  --	--	--


Figure 7: A pairwise breakdown of the human judg­ 
ments comparing the systems.   Dark grey regions 
show the number of times the two systems were tied, 
and light grey shows how many times one system 
was judged to be better than the other.









I RandomDeletions I 0.78  I 	2.91 	2.53

Thble 1: Results of the human evaluation on longer 
compressions:  pairwise compression rates (CR), 
meaning and grammaticality  scores. Bold indicates
a statistically significance difference at p < 0.05.


complexity  to compute distributional similarity for 
our parapluase  collection.  We conclude that, com­ 
pared to a simple large-scale model, a rich, syntax­ 
based feature-set, even with significantly lower cov­ 
erage, noticeably improves output quality in a text­ 
to-text generation task. Our syntactic method sig­ 
nificantly improves grammaticality and meaning re­ 
tention over a strong paraphrastic baseline, and of­ 
fers substantial  gains in meaning retention  over a 
deletion-based state-<Jf-the-art system.

Acknowledgements   This research was supported 
in  part by  the  NSF under  grant llS-0713448 and 
in  part  by  the EuroMatrixPlus  project  funded  by 
the European Commission  (7th Framework Pro­ 
gramme).    Opinions, interpretations, and conclu­ 
sions are the authors' alone.




S
o
u
r
c
e

sh
ou
ld 
the
se 
po
liti
cal 
de
vel
op
me
nts 
ha
ve 
an 
im
pa
ct 
on 
sp
ort
s 
?

Re
fer
en
ce

sh
ou
ld 
the
se 
po
liti
cal 
ev
ent
s 
aff
ect 
sp
ort
s ?

S
y
n
t
a
x

sh
ou
ld 
the
se 
ev
ent
s 
ha
ve 
an 
im
pa
ct 
on 
sp
ort
s 
?

n
-
g
r
a
m

the
se 
pol
itic
al 
de
vel
op
me
nts 
im
pa
ct 
on 
sp
ort
s ?
p
p

sh
ou
ld 
the
se 
ev
ent
s 
im
pa
ct 
on 
sp
ort
s ?

l
l
.
.
P

pol
itic
al 
de
vel
op
me
nts 
ha
ve 
an 
im
pa
ct

S
o
u
r
c
e

n
o
w
 
w
e
 
h
a
v
e
 
t
o
 
t
h
i
n
k
 
a
n
d
 
m
a
k
e
 
a
 
d
e
c
i
s
i
o
n
 
a
b
o
n
t
 
o
u
r
 
d
i
r
e
c
t
i
o
n
 
a
n
d
 
c
h
o
o
s
e
 
o
u
l
y
 
o
n
e
 
w
a
y
 
.
 
t
h
a
n
k
s
.

Re
fer
en
ce

we 
sh
oul
d 
po
nd
er 
it 
an
d 
de
cid
e 
ou
r 
pat
h 
an
d 
fol
lo
w 
it , 
tha
nk
s .

S
y
n
t
a
x

no
w 
we 
thi
nk 
an
d 
de
cid
e 
on 
ou
r 
wa
y 
an
d 
ch
oo
se 
on
e 
wa
y . 
tha
nk
s .

n
-
g
r
a
m

no
w 
we 
ha
ve 
an
d 
de
cid
e 
on 
ou
r 
wa
y 
an
d 
ch
oo
se 
on
e 
wa
y . 
tha
nk
s .
p
p

no
w 
we 
ha
ve 
an
d 
de
cid
e 
on 
ou
r 
wa
y 
an
d 
ch
oo
se 
on
e 
wa
y . 
tha
nk
s .

l
l
.
.
P

we 
ha
ve 
to 
thi
nk 
an
d 
ma
ke 
a 
de
cis
ion 
an
d 
ch
oo
se 
wa
y 
tha
nk
s

Table  2: Example compressions produced by our systems and the baselines Table  I for three input sentences
from our test  data.




References

Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory 
of Parsing, Translation, and Compiling. Prentice Hall. 
Peter G. Anick and Suresh Tipimeni.    1999.  The para­ 
phrase  search  assistant:   terminological  feedback  for 
iterative information  seeking.   In Proceedings  of SI­
GIR.
Colin Bannard and Chris Callison-Burch. 2005.  Para­ 
phrasing with bilingual parallel corpora.  In Proceed­ 
ingsofACL.
Regina Barzilay, Kathleen R. McKeown, and Michael 
Elhadad.     1999.    Information  fusion  in the  context 
of multi-document summarization. In Proceedings of 
ACL.
Regina Barzilay.  2003.  Information Fusion for Mutli­ 
document Summarization: Paraphrasing and Genera­ 
tion. Ph.D. thesis, Columbia University, New York.
Rahul Bhagat and Deepak Ravichandran.   2008.   Large 
scale acquisition of paraphrases for learning surface 
patterns. In Proceedings of ACUHLT.
Thorsten Brants and Alex Franz. 2006.  Web 1T 5-gram
version 1.
Chris Callison-Burch.  2008.   Syntactic constraints on 
paraphrases  extracted  from parallel corpora.   In Pro­ 
ceedings of EMNLP.


Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van 
Durme. 2011. Reranking bilingually extracted para­ 
phrases using monolingual distributional similarity. In 
EMNLP Workshop on GEMS.
Moses Charikar. 2002. Similarity estimation  techniques 
from rounding algorithms. In Proceedings of STOC.
David Chiang. 2005. A hierarchical phrase-based  model 
for statistical machine translation.  In Proceedings  of 
ACL.
Kenneth Church and Patrick Hanks.   1991.   Word asso­ 
ciation  norms,  mutual information  and lexicography. 
Computational  Linguistics, 6(1):22-29.
James Clarke and Mirella  Lapata.   2008.   Global infer­ 
ence for sentence compression:  An integer linear pro­ 
gramming approach.  Journal qf Artificial Intelligence 
Research, 31:273-381.
Trevor Cohn and Mirella Lapata.  2008.  Sentence com­ 
pression beyond word deletion. In Proceedings of the 
COUNG.
Juri Ganitkevitch,  Chris Callison-Burch, Courtuey 
Napoles, and Benjamin Van Durme.  2011.  Learning 
sentential  paraphrases from bilingual parallel corpora 
for text-to-text generation. In Proceedings of EMNLP.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,


and Chris Callison-Burch. 2012. Joshua 4.0: Packing, 
PRO, and paraphrases. In Proceedings ofWMT12.
Mark Hopkins and Jonathan May. 2011. Thning as rank­
ing. In Proceedings of EMNLP.
Piotr lndyk and Rajeev Motwani.   1998.  Approximate 
nearest neighbors:  towards removing the curse of di­ 
mensionality. In Proceedings of STOC.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta­ 
tistical machine translation. In MT summit, volume 5. 
Philipp  Koehn.   2010.   Statistical Machine  Translation.
Cambridge University Press.
Mirella Lapata and Frank Keller. 2005. Web-based mod­ 
els for  natural  language  processing.   ACM Transac­ 
tions on Speech and Language Processing, 2(1).
Percy Liang, Ben Taskar, and Dan Klein.  2006.  Align­
ment by agreement.  In Proceedings of HLTINAACL.
Dekang Lin and Patrick Pantel.  2001. Discovery of infer­ 
ence rules from text. Natural Language Engineering. 
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, 
David Yarowsky, Shane Bergsma, Kailash Patil, Emily 
Pitler, Rachel Lathbury, V!kram Rao, Kapil Dalwani, 
and Sushant Narsale.  2010.  New tools for web-scale
n-grams. In Proceedings of LREC.
Kathleen R. McKeown.  1979.  Paraphrasing using given 
and new information  in a question-answer system.  In 
Proceedings of ACL.
Courtney  Napoles,  Chris  Callison-Burch, Juri Ganitke­
vitch, and Benjamin Van Durme.  2011.  Paraphrastic 
sentence compression with a character-based metric: 
Tightening without deletion.  Workshop on Monolin­ 
gual Text-To-Text Generation.
Courtney Napoles, Matt Gormley, and Benjamin Van 
Durme.  2012.  Annotated  gigaword.  In Proceedings 
of AKBC-WEKEX 2012.


Benjamin Van Durme.    2012.    Jerboa:  A toolkit for 
randomized and streaming algorithms.  Technical Re­ 
port 7, Human Language Technology Center of Excel­ 
lence, Johns Hopkins University.
Sander Wubben, Antal van den Bosch, and Ernie!Krah­ 
mer.   2012.   Sentence  simplification  by monolingual 
machine translation. In Proceedings of ACL.
Xuchen Yao, Benjamin Van Durme, and Chris Callison­ 
Burch. 2012.  Expectations of word sense in parallel 
corpora. In Proceedings of HLT/NAACL.
Shiqi  Zhao,  Haifeng  Wang,  Tmg  Liu,  and  Sheng  Li.
2008.   Pivot approach  for extracting  paraphrase  pat­ 
terns  from   bilingual  corpora.      In Proceedings   of 
ACUHLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.  2009.
Application-driven  statistical  paraphrase  generation.
In Proceedings of ACL.


Slav  Petrov  and  Dan  Klein. 	2007. 
ence  for  unlexicalized  parsing. 
HLT/NAACL.


Improved  infer­
In Proceedings  of


Deepak Ravichandran and Eduard Hovy. 2002. Learning 
sufrace text patterns for a question answering system. 
In Proceedings of ACL.
Deepak Ravichandran,  Patrick Pantel, and Eduard Hovy.
2005.  Randomized Algorithms and NLP: Using Lo­ 
cality Sensitive Hash Functions for High Speed Noun 
Clustering.  In Proceedings of ACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan­ 
taridis, V!bhu Mittal, and Yi Liu.   2007.   Statistical 
machine translation for query expansion in answer re­ 
trieval. In Proceedings of ACL.
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit.   In Proceeding of the International 
Conference on Spoken Language Processing.
Benjamin  Van Durme  and Ashwin Lall.   2010.   Online 
generation of locality sensitive hash signatures.   In 
Proceedings of ACL, Short Papers.

