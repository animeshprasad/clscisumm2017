<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Confusion network decoding has been the most successful approach in combining outputs from multiple machine translation (MT) systems in the recent DARPA GALE and NIST Open MT evaluations.</S>
		<S sid ="2" ssid = "2">Due to the varying word order between outputs from different MT systems, the hypothesis alignment presents the biggest challenge in confusion network decoding.</S>
		<S sid ="3" ssid = "3">This paper describes an incremental alignment method to build confusion networks based on the translation edit rate (TER) algorithm.</S>
		<S sid ="4" ssid = "4">This new algorithm yields significant BLEU score improvements over other recent alignment methods on the GALE test sets and was used in BBN’s submission to the WMT08 shared translation task.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Confusion network decoding has been applied in combining outputs from multiple machine translation systems.</S>
			<S sid ="6" ssid = "6">The earliest approach in (Bangalore et al., 2001) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks.</S>
			<S sid ="7" ssid = "7">The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).</S>
			<S sid ="8" ssid = "8">The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.</S>
			<S sid ="9" ssid = "9">The confusion networks are built around a “skeleton” hypothesis.</S>
			<S sid ="10" ssid = "10">The skeleton hypothesis defines the word order of the decoding output.</S>
			<S sid ="11" ssid = "11">Usually, the 1-best hypotheses from each system are considered as possible skeletons.</S>
			<S sid ="12" ssid = "12">Using the pairwise hypothesis alignment, the confusion networks are built in two steps.</S>
			<S sid ="13" ssid = "13">First, all hypotheses are aligned against the skeleton independently.</S>
			<S sid ="14" ssid = "14">Second, the confusion networks are created from the union of these alignments.</S>
			<S sid ="15" ssid = "15">The incremental hypothesis alignment algorithm combines these two steps.</S>
			<S sid ="16" ssid = "16">All words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses.</S>
			<S sid ="17" ssid = "17">As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.</S>
			<S sid ="18" ssid = "18">System weights and language model weights are tuned to optimize the quality of the decoding output on a development set.</S>
			<S sid ="19" ssid = "19">This paper is organized as follows.</S>
			<S sid ="20" ssid = "20">The incremental TER alignment algorithm is described in Section 2.</S>
			<S sid ="21" ssid = "21">Experimental evaluation comparing the incremental and pairwise alignment methods are presented in Section 3 along with results on the WMT08 Europarl test sets.</S>
			<S sid ="22" ssid = "22">Conclusions and future work are presented in Section 4.</S>
	</SECTION>
	<SECTION title="Incremental TER Alignment. " number = "2">
			<S sid ="23" ssid = "1">The incremental hypothesis alignment is based on an extension of the TER algorithm (Snover et al.,2006).</S>
			<S sid ="24" ssid = "2">The extension allows using a confusion net work as the reference.</S>
			<S sid ="25" ssid = "3">First, the algorithm finds the minimum edit distance between the hypothesis and the reference network by considering all word arcs between two consecutive nodes in the reference network as possible matches for a hypothesis word at 183 Proceedings of the Third Workshop on Statistical Machine Translation, pages 183–186, Columbus, Ohio, USA, June 2008.</S>
			<S sid ="26" ssid = "4">Qc 2008 Association for Computational Linguistics NULL (2) balloons (2) NULL (2) NULL (2) NULL (1) balloons (2) I (3) like (3) I (3) like (3) 1 2 3 4 5 6 1 2 3 4 5 6 big blue (1) blue (1) kites (1) big (1) blue (2) kites (1) Figure 1: Network after pairwise TER alignment.</S>
			<S sid ="27" ssid = "5">that position.</S>
			<S sid ="28" ssid = "6">Second, shifts of blocks of words that have an exact match somewhere else in the network are tried in order to find a new hypothesis word order with a lower TER.</S>
			<S sid ="29" ssid = "7">Each shifted block is considered a single edit.</S>
			<S sid ="30" ssid = "8">These two steps are executed iteratively as a greedy search.</S>
			<S sid ="31" ssid = "9">The final alignment between the reordered hypothesis and the reference network may include matches, substitutions, deletions, and insertions.</S>
			<S sid ="32" ssid = "10">The confusion networks are built by creating a simple confusion network from the skeleton hypothesis.</S>
			<S sid ="33" ssid = "11">If the skeleton hypothesis has words, the initial network has arcs and nodes.</S>
			<S sid ="34" ssid = "12">Each arc has a set of system specific confidence scores.</S>
			<S sid ="35" ssid = "13">The score for the skeleton system is set to and the confidences for other systems are set to zeros.</S>
			<S sid ="36" ssid = "14">For each non-skeleton hypothesis, a TER alignment against the current network is executed as described above.</S>
			<S sid ="37" ssid = "15">Each match found will increase the system specific word arc confidence by where Figure 2: Network after incremental TER alignment.</S>
			<S sid ="38" ssid = "16">each set of two consecutive nodes.</S>
			<S sid ="39" ssid = "17">Other scores for the word arc are set as in (Rosti et al., 2007).</S>
			<S sid ="40" ssid = "18">2.1 Benefits over PairWise TER Alignment.</S>
			<S sid ="41" ssid = "19">The incremental hypothesis alignment guarantees that insertions between a hypothesis and the current confusion network are always considered when aligning the following hypotheses.</S>
			<S sid ="42" ssid = "20">This is not the case in any pairwise hypothesis alignment algorithm.</S>
			<S sid ="43" ssid = "21">During the pairwise hypothesis alignment, an identical word in two hypotheses may be aligned as an insertion or a substitution in a different position with respect to the skeleton.</S>
			<S sid ="44" ssid = "22">This will result in undesirable repetition and lower confidence for that word in the final confusion network.</S>
			<S sid ="45" ssid = "23">Also, multiple insertions are not handled implicitly.</S>
			<S sid ="46" ssid = "24">For example, three hypotheses “I like balloons”, “I like big blue balloons”, and “I like blue kites” might be aligned by the pairwise alignment, assuming the first as the skeleton, as follows: is the rank of the hypothesis in that system’s -best list.</S>
			<S sid ="47" ssid = "25">Each substitution will generate a new word arc at the corresponding position in the network.</S>
			<S sid ="48" ssid = "26">The word arc confidence for the system is set to and the confidences for other systems are set to zeros.</S>
			<S sid ="49" ssid = "27">Each deletion will generate a new NULL word arc unless one exists at the corresponding position in the network.</S>
			<S sid ="50" ssid = "28">The NULL word arc confidences are adjusted as in the case of a match or a substitution depending on whether the NULL word arc exists or not.</S>
			<S sid ="51" ssid = "29">Finally, each insertion will generate a new node and two word arcs at the corresponding position in the network.</S>
			<S sid ="52" ssid = "30">The first word arc will have the inserted word with the confidence set as in the case of a substitution and the second word arc will have a NULL word with confidences set by assuming all previously aligned hypotheses and the skeleton generated the NULL word arc. After all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between I like NULL balloons NULL I like big blue balloons NULL I like NULL balloons NULL I like NULL blue kites which results in the confusion network shown in Figure 1.</S>
			<S sid ="53" ssid = "31">The number of hypotheses proposing each word is shown in parentheses.</S>
			<S sid ="54" ssid = "32">The alignment between the skeleton and the second hypothesis has two consecutive insertions “big blue” which are not available for matching when the third hypothesis is aligned against the skeleton.</S>
			<S sid ="55" ssid = "33">Therefore, the word “blue” appears twice in the confusion network.</S>
			<S sid ="56" ssid = "34">If many hypotheses have multiple insertions at the same location with respect to the skeleton, they have to be treated as phrases or a secondary alignment process has to be applied.</S>
			<S sid ="57" ssid = "35">Assuming the same hypotheses as above, the incremental hypothesis alignment may yield the following alignment: Sy ste m T E R B L E U M T R Sy ste m T E R B L E U M T R w or st 53 .2 6 33 .0 0 63 .1 5 w or st 59 .0 9 20 .7 4 57 .2 4 be st 42 .3 0 48 .5 2 67 .7 1 be st 48 .1 8 31 .4 6 62 .6 1 sy sc o m b p w 39 .8 5 52 .0 0 68 .7 3 sy sc o m b p w 46 .3 1 33 .0 2 63 .1 8 sy sc o m b gi za 40 .0 1 52 .2 4 68 .6 8 sy sc o m b gi za 46 .0 3 33 .3 9 63 .2 1 sy sc o m b in c 39 .2 5 52 .7 3 68 .9 7 sy sc o m b in c 45 .4 5 33 .9 0 63 .4 5 or acl e 21 .6 8 64 .1 4 78 .1 8 or acl e 27 .5 3 49 .1 0 71 .8 1 Table 1: Results on the Arabic GALE Phase 2 system combination tuning set with four reference translations.</S>
			<S sid ="58" ssid = "36">I like NULL NULL balloons I like big blue balloons I like NULL blue kites which results in the confusion network shown in Figure 2.</S>
			<S sid ="59" ssid = "37">In this case the word “blue” is available for matching when the third hypothesis is aligned.</S>
			<S sid ="60" ssid = "38">It should be noted that the final confusion network depends on the order in which the hypotheses are added.</S>
			<S sid ="61" ssid = "39">The experiments so far have indicated that different alignment order does not have a significant influence on the final combination results as measured by the automatic evaluation metrics.</S>
			<S sid ="62" ssid = "40">Usually, aligning the system outputs in the decreasing order of their TER scores on the development set yields the best scores.</S>
			<S sid ="63" ssid = "41">2.2 Confusion Network Oracle.</S>
			<S sid ="64" ssid = "42">The extended TER algorithm can also be used to estimate an oracle TER in a confusion network by aligning the reference translations against the confusion network.</S>
			<S sid ="65" ssid = "43">The oracle hypotheses can be extracted by finding a path with the maximum number of matches.</S>
			<S sid ="66" ssid = "44">These hypotheses give a lower bound on the TER score for the hypotheses which can be generated from the confusion networks.</S>
	</SECTION>
	<SECTION title="Experimental Evaluation. " number = "3">
			<S sid ="67" ssid = "1">The quality of the final combination output depends on many factors.</S>
			<S sid ="68" ssid = "2">Combining very similar outputs does not yield as good gains as combining outputs from diverse systems.</S>
			<S sid ="69" ssid = "3">It is also important that the development set used to tune the combination weights is as similar to the evaluation set as possible.</S>
			<S sid ="70" ssid = "4">This development set should be different from the one used to tune the individual systems to avoid bias toward any system that may be over-tuned.</S>
			<S sid ="71" ssid = "5">Due Table 2: Results on the Arabic GALE Phase 2 evaluation set with one reference translation.</S>
			<S sid ="72" ssid = "6">to the tight schedule for the WMT08, there was no time to experiment with many configurations.</S>
			<S sid ="73" ssid = "7">As more extensive experiments have been conducted in the context of the DARPA GALE program, results on the Arabic GALE Phase 2 evaluation setup are first presented.</S>
			<S sid ="74" ssid = "8">The translation quality is measured by three MT evaluation metrics: TER (Snover et al., 2006), BLEU (Papineni et al., 2002), and METEOR (Lavie and Agarwal, 2007).</S>
			<S sid ="75" ssid = "9">3.1 Results on Arabic GALE Outputs.</S>
			<S sid ="76" ssid = "10">For the Arabic GALE Phase 2 evaluation, nine systems were combined.</S>
			<S sid ="77" ssid = "11">Five systems were phrase- based, two hierarchical, one syntax-based, and one rule-based.</S>
			<S sid ="78" ssid = "12">All statistical systems were trained on common parallel data, tuned on a common genre specific development set, and a common English to- kenization was used.</S>
			<S sid ="79" ssid = "13">The English bi-gram and 5- gram language models used in the system combination were trained on about 7 billion words of English text.</S>
			<S sid ="80" ssid = "14">Three iterations of bi-gram decoding weight tuning were performed followed by one iteration of 5-gram re-scoring weight tuning.</S>
			<S sid ="81" ssid = "15">All weights were tuned to minimize the sum of TER and 1BLEU.</S>
			<S sid ="82" ssid = "16">The final 1-best outputs were true-cased and detokenized before scoring.</S>
			<S sid ="83" ssid = "17">The results on the newswire system combination development set and the GALE Phase 2 evaluation set are shown in Tables 1 and 2.</S>
			<S sid ="84" ssid = "18">The first two rows show the worst and best scores from the individual systems.</S>
			<S sid ="85" ssid = "19">The scores may be from different systems as the best performing system in terms of TER was not necessarily the best performing system in terms of the other metrics.</S>
			<S sid ="86" ssid = "20">The following three rows show the scores of three combination outputs where the only difference was the hypothesis alignment method.</S>
			<S sid ="87" ssid = "21">The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.</S>
			<S sid ="88" ssid = "22">to the pairwise TER alignment described in (Rosti et al., 2007).</S>
			<S sid ="89" ssid = "23">The second, syscomb giza, corresponds to the pairwise symmetric HMM alignments from GIZA++ described in (Matusov et al., 2006).</S>
			<S sid ="90" ssid = "24">The third, syscomb inc, corresponds to the incremental TER alignment presented in this paper.</S>
			<S sid ="91" ssid = "25">Finally, oracle corresponds to an estimate of the lower bound on the translation quality obtained by extracting the TER oracle output from the confusion networks generated by the incremental TER alignment.</S>
			<S sid ="92" ssid = "26">It is unlikely that there exists a set of weights that would yield the oracle output after decoding, though.</S>
			<S sid ="93" ssid = "27">The incremental TER alignment yields significant improvements over all individual systems and the combination outputs using the pairwise alignment methods.</S>
			<S sid ="94" ssid = "28">3.2 Results on WMT08 Europarl Outputs.</S>
			<S sid ="95" ssid = "29">On the WMT08 shared translation task, translations for two language pairs and two tasks were provided for the system combination experiments.</S>
			<S sid ="96" ssid = "30">Twelve systems participated in the GermanEnglish and fourteen in the French-English translation tasks.</S>
			<S sid ="97" ssid = "31">The translations of the Europarl test (test2008) were provided as the development set outputs and the translations of the News test (newstest2008) were provided as the evaluation set outputs.</S>
			<S sid ="98" ssid = "32">An English bi-gram, 4-gram, and true-caser language models were trained by using all English text available for the WMT08 shared task, including Europarl monolingual and news commentary parallel training sets.</S>
			<S sid ="99" ssid = "33">The outputs were tokenized and lower-cased before combination, and the final combination output was true-cased and detokenized.</S>
			<S sid ="100" ssid = "34">The results on the Europarl test set for both language pairs are shown in table 3.</S>
			<S sid ="101" ssid = "35">The first two rows have the NIST BLEU scores of the worst and the best individual systems.</S>
			<S sid ="102" ssid = "36">The last row, syscomb, corresponds to the system combination using the in cremental TER alignment.</S>
			<S sid ="103" ssid = "37">The improvements in the NIST BLEU scores are fairly modest which is probably due to low diversity of the system outputs.</S>
			<S sid ="104" ssid = "38">It is also unlikely that these weights are optimal for the out-of-domain News test set outputs.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "4">
			<S sid ="105" ssid = "1">This paper describes a novel hypothesis alignment algorithm for building confusion networks from multiple machine translation system outputs.</S>
			<S sid ="106" ssid = "2">The algorithm yields significant improvements on the Arabic GALE evaluation set outputs and was used in BBN’s submission to the WMT08 shared translation task.</S>
			<S sid ="107" ssid = "3">The hypothesis alignment may benefit from using stemming and synonymy in matching words.</S>
			<S sid ="108" ssid = "4">Also, special handling of punctuation may improve the alignment further.</S>
			<S sid ="109" ssid = "5">The future work will investigate the influence of better alignment to the final combination outputs.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="110" ssid = "6">This work was supported by DARPA/IPTO Contract No.</S>
			<S sid ="111" ssid = "7">HR001106-C-0022 under the GALE program.</S>
	</SECTION>
</PAPER>
