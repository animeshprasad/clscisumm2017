<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Given several systems’ automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step.</S>
		<S sid ="2" ssid = "2">We build our confusion networks using the method of Rosti et al.</S>
		<S sid ="3" ssid = "3">(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.</S>
		<S sid ="4" ssid = "4">Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Large improvements in machine translation (MT) may result from combining different approaches to MT with mutually complementary strengths.</S>
			<S sid ="6" ssid = "6">System-level combination of translation outputs is a promising path towards such improvements.</S>
			<S sid ="7" ssid = "7">Yet there are some significant hurdles in this path.</S>
			<S sid ="8" ssid = "8">One must somehow align the multiple outputs—to identify where different hypotheses reinforce each other and where they offer alternatives.</S>
			<S sid ="9" ssid = "9">One must then ∗This work was partially supported by the DARPA GALE program (Contract No HR001106-20001).</S>
			<S sid ="10" ssid = "10">Also, we would like to thank the IBM Rosetta team for the availability of several MT system outputs.</S>
			<S sid ="11" ssid = "11">use this alignment to hypothesize a set of new, composite translations, and select the best composite hypothesis from this set.</S>
			<S sid ="12" ssid = "12">The alignment step is difficult because different MT approaches usually reorder the translated words differently.</S>
			<S sid ="13" ssid = "13">Training the selection step is difficult because identifying the best hypothesis (relative to a known reference translation) means scoring all the composite hypotheses, of which there may be exponentially many.</S>
			<S sid ="14" ssid = "14">Most MT combination methods do create an exponentially large hypothesis set, representing it as a confusion network of strings in the target language (e.g., English).</S>
			<S sid ="15" ssid = "15">(A confusion network is a lattice where every node is on every path; i.e., each time step presents an independent choice among several phrases.</S>
			<S sid ="16" ssid = "16">Note that our contributions in this paper could be applied to arbitrary lattice topologies.)</S>
			<S sid ="17" ssid = "17">For example, Bangalore et al.</S>
			<S sid ="18" ssid = "18">(2001) show how to build a confusion network following a multistring alignment procedure of several MT outputs.</S>
			<S sid ="19" ssid = "19">The procedure (used primarily in biology, (Thompson et al., 1994)) yields monotone alignments that minimize the number of insertions, deletions, and substitutions.</S>
			<S sid ="20" ssid = "20">Unfortunately, monotone alignments are often poor, since machine translations (particularly from different models) can vary significantly in their word order.</S>
			<S sid ="21" ssid = "21">Thus, when Matusov et al.</S>
			<S sid ="22" ssid = "22">(2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment.</S>
			<S sid ="23" ssid = "23">The procedure described by Rosti et al.</S>
			<S sid ="24" ssid = "24">(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.</S>
			<S sid ="25" ssid = "25">(TER is defined as the minimum number of inser 81 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 81–84, Columbus, Ohio, USA, June 2008.</S>
			<S sid ="26" ssid = "26">Qc 2008 Association for Computational Linguistics tions, deletions, substitutions and block shifts between two strings.)</S>
			<S sid ="27" ssid = "27">A remarkable feature of that procedure is that it performs the alignment of the output translations (i) without any knowledge of the translation model used to generate the translations, and (ii) without any knowledge of how the target words in each translation align back to the source words.</S>
			<S sid ="28" ssid = "28">In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.</S>
			<S sid ="29" ssid = "29">For this, Rosti et al.</S>
			<S sid ="30" ssid = "30">(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.</S>
			<S sid ="31" ssid = "31">In this paper, we show that one can build better confusion networks (in terms of the best translation possible from the confusion network) when the pairwise alignments are computed not by tercom, which approximately minimizes TER, but instead by an exact minimization of invWER (Leusch et al., 2003), which is a restricted version of TER that permits only properly nested sets of block shifts, and can be computed in polynomial time.</S>
			<S sid ="32" ssid = "32">The paper is organized as follows: a summary of TER, tercom, and invWER, is presented in Section</S>
	</SECTION>
	<SECTION title="The system combination procedure is summa-. " number = "2">
			<S sid ="33" ssid = "1">rized in Section 3, while experimental (oracle) results are presented in Section 4.</S>
			<S sid ="34" ssid = "2">Conclusions are given in Section 5.</S>
			<S sid ="35" ssid = "3">2 Comparing tercom and invWER.</S>
			<S sid ="36" ssid = "4">The tercom script was created mainly in order to measure translation quality based on TER.</S>
			<S sid ="37" ssid = "5">As is proved by Shapira and Storer (2002), computation of TER is an NP-complete problem.</S>
			<S sid ="38" ssid = "6">For this reason, tercom uses some heuristics in order to compute an approximation to TER in polynomial time.</S>
			<S sid ="39" ssid = "7">In the rest of the paper, we will denote this approximation as tercomTER, to distinguish it from (the intractable) TER.</S>
			<S sid ="40" ssid = "8">The block shifts which are allowed in tercom have to adhere to the following constraints: (i) A block that has an exact match cannot be moved, and (ii) for a block to be moved, it should have an exact match in its new position.</S>
			<S sid ="41" ssid = "9">However, this sometimes leads to counter-intuitive sequences of edits; for instance, for the sentence pair “thomas jefferson says eat your vegetables” “eat your cereal thomas edison says”, tercom finds an edit sequence of cost 5, instead of the optimum 3.</S>
			<S sid ="42" ssid = "10">Furthermore, the block selection is done in a greedy manner, and the final outcome is dependent on the shift order, even when the above constraints are imposed.</S>
			<S sid ="43" ssid = "11">An alternative to tercom, considered in this paper, is to use the Inversion Transduction Grammar (ITG) formalism (Wu, 1997) which allows one to view the problem of alignment as a problem of bilingual parsing.</S>
			<S sid ="44" ssid = "12">Specifically, ITGs can be used to find the optimal edit sequence under the restriction that block moves must be properly nested, like parentheses.</S>
			<S sid ="45" ssid = "13">That is, if an edit sequence swaps adjacent substrings A and B of the original string, then any other block move that affects A (or B) must stay completely within A (or B).</S>
			<S sid ="46" ssid = "14">An edit sequence with this restriction corresponds to a synchronous parse tree under a simple ITG that has one nonterminal and whose terminal symbols allow insertion, deletion, and substitution.</S>
			<S sid ="47" ssid = "15">The minimum-cost ITG tree can be found by dynamic programming.</S>
			<S sid ="48" ssid = "16">This leads to invWER (Leusch et al., 2003), which is defined as the minimum number of edits (insertions, deletions, substitutions and block shifts allowed by the ITG) needed to convert one string to another.</S>
			<S sid ="49" ssid = "17">In this paper, the minimum- invWER alignments are used for generating confusion networks.</S>
			<S sid ="50" ssid = "18">The alignments are found with a 11- rule Dyna program (Dyna is an environment that facilitates the development of dynamic programs—see (Eisner et al., 2005) for more details).</S>
			<S sid ="51" ssid = "19">This program was further sped up (by about a factor of 2) with an A∗ search heuristic computed by additional code.</S>
			<S sid ="52" ssid = "20">Specifically, our admissible outside heuristic for aligning two substrings estimated the cost of aligning the words outside those substrings as if reordering those words were free.</S>
			<S sid ="53" ssid = "21">This was complicated somewhat by type/token issues and by the fact that we were aligning (possibly weighted) lattices.</S>
			<S sid ="54" ssid = "22">Moreover, the same Dyna program was used for the computation of the minimum invWER path in these confusion networks (oracle path), without having to invoke tercom numerous times to compute the best sentence in an N -best list.</S>
			<S sid ="55" ssid = "23">The two competing alignment procedures were Ge nr e C Ns wi th ter co m C N s w it h I T G N W 5 0.</S>
			<S sid ="56" ssid = "24">1 % ( 2 7.</S>
			<S sid ="57" ssid = "25">7 % ) 48 .8 % (2 8.</S>
			<S sid ="58" ssid = "26">3 % ) W B 5 1.</S>
			<S sid ="59" ssid = "27">0 % ( 2 5.</S>
			<S sid ="60" ssid = "28">5 % ) 50 .5 % (2 6.</S>
			<S sid ="61" ssid = "29">0 % ) Table 1: Comparison of average per-document tercomTER with invWER on the EVAL07 GALE Newswire (“NW”) and Weblogs (“WB”) data sets.</S>
			<S sid ="62" ssid = "30">used to estimate the TER between machine translation system outputs and reference translations.</S>
			<S sid ="63" ssid = "31">Table 1 shows the TER estimates using tercom and invWER.</S>
			<S sid ="64" ssid = "32">These were computed on the translations submitted by a system to NIST for the GALE evaluation in June 2007.</S>
			<S sid ="65" ssid = "33">The references used are the post-edited translations for that system (i.e., these are “HTER” approximations).</S>
			<S sid ="66" ssid = "34">As can be seen from the table, in all language and genre conditions, in- vWER gives a better approximation to TER than tercomTER.</S>
			<S sid ="67" ssid = "35">In fact, out of the roughly 2000 total segments in all languages/genres, tercomTER gives a lower number of edits in only 8 cases!</S>
			<S sid ="68" ssid = "36">This is a clear indication that ITGs can explore the space of string permutations more effectively than tercom.</S>
	</SECTION>
	<SECTION title="The System Combination Approach. " number = "3">
			<S sid ="69" ssid = "1">ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.</S>
			<S sid ="70" ssid = "2">(2007).</S>
			<S sid ="71" ssid = "3">The algorithm entails the following steps: • Computation of all pairwise alignments between system hypotheses (either using ITGs or tercom); for each pair, one of the hypotheses plays the role of the “reference”.</S>
			<S sid ="72" ssid = "4">• Selection of a system output as the “skeleton” of the confusion network, whose words are used as anchors for aligning all other machine translation outputs together.</S>
			<S sid ="73" ssid = "5">Each arc has a translation output word as its label, with the special token “NULL” used to denote an insertion/deletion between the skeleton and another system output.</S>
			<S sid ="74" ssid = "6">• Multiple consecutive words which are inserted relative to the skeleton form a phrase that gets Table 2: TercomTERs of invWER-oracles and (in parentheses) oracle BLEU scores of confusion networks generated with tercom and ITG alignments.</S>
			<S sid ="75" ssid = "7">The best results per row are shown in bold.</S>
			<S sid ="76" ssid = "8">aligned with an epsilon arc of the confusion network.</S>
			<S sid ="77" ssid = "9">• Setting the weight of each arc equal to the negative log (posterior) probability of its label; this probability is proportional to the number of systems which output the word that gets aligned in that location.</S>
			<S sid ="78" ssid = "10">Note that the algorithm of Rosti et al.</S>
			<S sid ="79" ssid = "11">(2007) used N -best lists in the combination.</S>
			<S sid ="80" ssid = "12">Instead, we used the single- best output of each system; this was done because not all systems were providing N -best lists, and an unbalanced inclusion would favor some systems much more than others.</S>
			<S sid ="81" ssid = "13">Furthermore, for each genre, one of our MT systems was significantly better than the others in terms of word order, and it was chosen as the skeleton.</S>
	</SECTION>
	<SECTION title="Experimental Results. " number = "4">
			<S sid ="82" ssid = "1">Table 2 shows tercomTERs of invWER-oracles (as computed by the aforementioned Dyna program) and oracle BLEU scores of the confusion networks.</S>
			<S sid ="83" ssid = "2">The confusion networks were generated using 9 MT systems applied to the Chinese GALE 2007 Dev set, which consists of roughly 550 Newswire segments, and 650 Weblog segments.</S>
			<S sid ="84" ssid = "3">The confusion networks which were generated with the ITG- based alignments gave significantly better oracle tercomTERs (significance tested with a Fisher sign test, p − 0.02) and better oracle BLEU scores.</S>
			<S sid ="85" ssid = "4">The BLEU oracle sentences were found using the dynamic-programming algorithm given in Dreyer et al.</S>
			<S sid ="86" ssid = "5">(2007) and measured using Philipp Koehn’s evaluation script.</S>
			<S sid ="87" ssid = "6">On the other hand, a comparison between the 1-best paths did not reveal significant differences that would favor one approach or the other (either in terms of tercomTER or BLEU).</S>
			<S sid ="88" ssid = "7">We also tried to understand which alignment method gives higher probability to paths “close” to the corresponding oracle.</S>
			<S sid ="89" ssid = "8">To do that, we computed the probability that a random path from a confusion network is within x edits from its oracle.</S>
			<S sid ="90" ssid = "9">This computation was done efficiently using finite-state-machine operations, and did not involve any randomization.</S>
			<S sid ="91" ssid = "10">Preliminary experiments with the invWER-oracles show that the probability of all paths which are within x = 3 edits from the oracle is roughly the same for ITG-based and tercom-based confusion networks.</S>
			<S sid ="92" ssid = "11">We plan to report our findings for a whole range of x-values in future work.</S>
			<S sid ="93" ssid = "12">Finally, a runtime comparison of the two techniques shows that ITGs are much more computationally intensive: on average, ITG-based alignments took1.5 hours/sentence (owing to their O(n6) complex ity), while tercom-based alignments only took 0.4 sec/sentence.</S>
	</SECTION>
	<SECTION title="Concluding Remarks. " number = "5">
			<S sid ="94" ssid = "1">We compared alignments obtained using the widely used program tercom with alignments obtained with ITGs and we established that the ITG alignments are superior in two ways.</S>
			<S sid ="95" ssid = "2">Specifically: (a) we showed that invWER (computed using the ITG alignments) gives a better approximation to TER between machine translation outputs and human references than tercom; and (b) in an oracle system combination experiment, we found that confusion networks generated with ITG alignments contain better oracles, both in terms of tercomTER and in terms of BLEU.</S>
			<S sid ="96" ssid = "3">Future work will include rescoring results with a language model, as well as exploration of heuristics (e.g., allowing only “short” block moves) that can reduce the ITG alignment complexity to O(n4).</S>
	</SECTION>
</PAPER>
