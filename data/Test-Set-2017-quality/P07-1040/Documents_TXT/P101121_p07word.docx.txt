BLEUSP, INVWER, CDER: Three improved MT evaluation measures



Gregor Leusch and Hermann Ney
RWTH Aachen University, Germany
{leusch,ney}@cs.rwth-aachen.de



Abstract


We   present three modifications of  well- 
established   automatic machine translation 
evaluation  measures, to improve correlation 
between  those measures and human evalua- 
tion.  Following Lin & Och, we present an 
improved version of the BLEU score, which 
uses a smoothed geometric  mean for combin- 
ing different  n-gram precisions.  We use seg- 
ment boundary markers to increase the weight 
of words near the segment boundaries in the 
BLEU score. Our second MT evaluation mea- 
sure is a variant of the WER which allows for 
block movements, but does not demand com- 
plete and disjoint coverage of the source sen- 
tence. As this might be problematic  if MT sys- 
tems are tuned on this score, we later investi- 
gate a linear combination  of this measure with 
PER.   Finally, we describe an edit distance 
similar to TER, which also allows for block re- 
ordering.  Our measure uses a full search, but 
with the constraint that block operations must 
be bracketed.  We describe this measure us- 
ing a Bracketing  Transduction  Grammar, and 
sketch a polynomial-time algorithm for its cal- 
culation.  We also modify the WER-like mea- 
sures such that they use word-dependent  sub- 
stitution  costs instead of fixed ones to model 
the similarity between  words.  Experimen- 
tal comparison of these measures show that 
our new measures correlate significantly better 
with human judgment than the original mea- 
sures.



1	Introduction

For a couple of reasons, automatic evaluation of Ma- 
chine Translation (MT) systems is a difficult task,


mostly  because it is difficult to define when a trans- 
lation is good, and when it is bad.  Or which of 
two given translations is better,  and which one is 
worse.  The main reason for this are ambiguities  in 
natural languages: Usually,  there is more than one 
correct translation for a source sentences; there are 
ambiguities in the choice of synonyms  as well as in 
the order of the words.  Because of the difficulty of 
this task, a multitude  of automatic MT evaluation 
measures have been defined over the last couple of 
years.  Some of these measures have become well- 
established, for example BLEU or TER, others are 
only of medium or small significance.  We expect 
that in the context of NIST’s Metrics MATR eval- 
uation, more measures will be added to the pool of 
evaluation measures.
  In a few previous papers, the proposed measures 
seemed to be more of theoretical interest than of 
practical use: While they certainly emphasis impor- 
tant linguistic effects, it is not investigated system- 
atically in how far these effects play a role in the 
difference in quality in different MT systems. Some 
other proposed evaluation  measures seemed to fo- 
cus on specific properties and features of the previ- 
ously generated translations they are trained or opti- 
mized on, which can (but does not need to) lead to 
evaluation  measures which are basically classifiers 
dividing previously good from previously poor sys- 
tems, or “easy” from “difficult” source sentences. If 
measures with this property  are used to tune a typ- 
ical statistical MT system, it can sometimes be ob- 
served that the MT system learns to “play” against 
this, and might even learn to produce translations 
which show the “good” features without actually 
being good translations.  For example, Rosti et al. 
(2007) report such an effect.  This is not to say


that all new measures share these problems, nor that 
there is no need for MT evaluation measures which 
go beyond lexical comparison – quite the opposite. 
But these issues were the motivation  for us to start 
from established evaluation  measures, with known 
properties especially with regard to tuning, and alter 
them at a few selected points to improve their corre- 
lation with human judgment.
  This paper is organized  as follows: In Section 2, 
we describe some modifications to the BLEU score, 
following Lin and Och (2004), and Leusch et al. 
(2005).  We  present a simple variant of WER in 
Section 3, called CDER, which allows for block 
transposition similar to TER, following Leusch et al. 
(2006). This measure can be efficiently calculated 
exactly, without having to resort to shift heuristics 
or greedy  search as in TER.  The tradeoff is that 
this measure by itself measures basically recall, not 
precision. To overcome this bias, we will later pro- 
pose a linear combination  of this measure and PER 
in Section 6. Before this, in Section 4, we describe 
another variant of TER which can be exactly cal- 
culated in polynomial time, this time by restricting 
possible shifts to ITG constrains.  This method fol- 
lows Leusch et al. (2003). We call this measure IN- 
VWER. In Section 5, we introduce two simple meth- 
ods following Leusch et al. (2006) to improve edit- 
operation–based measures like PER, WER, TER, and 
CDER/INVWER by taking into account the lexical 
difference of words in a substitution  operation.  Af- 
ter an experimental evaluation of our three proposed 
evaluation measures in Section 7, we conclude this 
paper in Section 8.

2   BLEUSP

BLEU  (Papineni et al., 2001) is a precision  mea- 
sure based on n-gram count vectors. The precision 
is modified  such that multiple references are com- 
bined into a single n-gram  count vector. Multiple 
occurrences of an n-gram in the candidate sentence 
are counted as correct  only up to the maximum oc- 
currence count within the reference sentences. Typ- 
ically, unigrams, bigrams, trigrams, and four-grams 
are used for BLEU; their four precisions  are com- 
bined using the geometric mean.
  To avoid a bias towards short candidate sentences 
consisting of “safe guesses” only, sentences shorter


than the reference length will be penalized with a 
brevity penalty.
  In  the original BLEU   definition there is  no 
smoothing for the geometric mean. This has the 
disadvantage that the whole score becomes zero al- 
ready if the four-gram precision is zero, which es- 
pecially  happens often with short or difficult trans- 
lations. As a result,  sentence-level  scores are often 
quite noisy, and not usable for evaluation. To allow 
for sentence-wise evaluation,  Lin and Och (2004) 
define the BLEUS measure, which is basically BLEU 
where all bi-, tri-, and four-gram  counts are initial- 
ized with 1 instead of 0. We have adopted this tech- 
nique for this study, as experiments showed a clear 
improvement over BLEU in terms of correlation with 
human judgment on segment and document  level; 
the effect on system level scores is negligible  due 
to the already high n-gram counts here.
  In our experiments, BLEU and BLEUS lack in an- 
other minor point: The position of a word within a 
sentence can be quite significant  for the correctness 
of the sentence.   WER, TER, and CDER/INVWER 
(Sections 3 and 4) explicitly take into account the 
ordering of the words in a sentence. This is not the 
case with BLEU, although the order of inner words 
is regarded implicitly by n-gram overlap. To model 
the position of words at the initial or the end of a 
sentence, we enclose the sentence with artificial sen- 
tence boundary tokens.
  For example, the sentence A B C is considered 
to consist of

• the unigrams [A], [B], and [C],
• the bigrams [<s> A], [A  B], [B  C], and
[C  </s>],
• the trigrams [<s> <s>  A], [<s> A B],
[A  B C], [B  C  </s>], and
[C  </s> </s>],
• etc.

  In the measure  we denote as BLEUSP,  all n- 
grams are counted like this for all candidate and ref- 
erence segments, and for these counts, the BLEUS 
score is calculated.

3	CDER

As translations of sentences are often ambiguous in 
the order of phrases, reorderings of whole blocks of



       . 
o’clock 
seven
    at 
airport 
the
 at 
met 
we


 


candida
te


be 
depicted 
using an 
alignme
nt grid 
as 
shown 
in Fig- 
ure 1: 
Here, 
each 
grid 
point 
corresp
onds to 
a pair of 
inter-
word 
position
s in 
candidat
e and 
referenc
e sen- 
tence, 
respecti
vely.   
dLJ  is 
the 
minimu
m cost 
of a 
path 
between 
the 
lower 
left 
(first) 
and the 
upper 
right 
(last) 
alignme
nt grid 
point 
which 
covers 
all 
refer- 
ence 
and 
candidat
e words.  
Deletion
s and 
insertio
ns 
corresp
ond to 
horizont
al  and 
vertical  
edges, 
respec- 
tively.  
Substitu
tions  
and 
identity 
operatio
ns 
corre- 
spond 
to 
diagonal  
edges.  
Edges 
between 
arbitrar
y grid 
points 
from 
the 
same 
row 
corresp
ond to 
long 
jump 
operatio
ns.  It 
is easy 
to see 
that dLJ  
is sym- 
metrica
l.
  Lopre
sti  and 
Tomkins  
(1997) 
showed 
that 
find- 
ing an 
optimal 
path in 
a long 
jump 
alignme
nt grid 
is


 	deletion


identity	best path


an NP-hard 
problem.  Our 
experiments 
showed that



insertion 
substitution



long jump block



start/
end node


the calculation 
of exact long 
jump distances 
thus be- comes 
impractical  for 
sentences 
longer than 
about
20 words.
A possible 
way to 
achieve 
polynomial  
run-time


Figure 1: Example of a long jump alignment grid. All
possible deletion, insertion, identity and substitution op- 
erations are depicted.  Only long jump edges from the best 
path are drawn.


words should not be penalized too hard by an MT 
evaluation  measure.  WER, which is based on the 
classical Levenshtein distance (Levenshtein, 1966), 
penalizes block reorderings rather hard – each word 
that has been shifted usually  needs to be deleted in 
its old position, and inserted in its new position. One 
approach here is to extend the Levenshtein distance 
by an additional operation, namely block movement 
(or shift, as it is called in TER (Snover et al., 2005)). 
Note that the number of blocks in a sentence is equal 
to the number of gaps among the blocks plus one. 
Thus, the block movements can equivalently  be ex- 
pressed as long jump operations that jump over the 
gaps between two blocks. The costs of a long jump 
are considered constant.  The blocks are read in the 
order of one of the sentences.   These long jumps 
are combined  with the “classical”  Levenshtein edit 
operations, namely insertion, deletion, substitution, 
and the zero-cost operation identity. The resulting 
long jump distance dLJ gives the minimum number 
of operations which are necessary to transform the 
candidate sentence into the reference sentence. Like 
the Levenshtein distance, the long jump distance can


is to restrict the number of admissible block per- 
mutations, for example  as in Section 4.  Alterna- 
tively, a heuristic  or approximative  distance can be 
calculated,  as in GTM (Turian et al., 2003). An im- 
plementation of both approaches at the same time 
can be found in TER. In the following section we 
will present another approach which has a suitable 
run-time, while still maintaining  completeness of 
the calculated measure.  The idea of the proposed 
method is to drop some restrictions on the alignment 
path.
  The long jump  distance as well as the Levenshtein 
distance require both reference and candidate trans- 
lation to be covered completely and disjointly. When 
extending the metric by block movements, we drop 
this constraint for the candidate translation. That is, 
only the words in the reference sentence have to be 
covered exactly  once, whereas those in the candi- 
date sentence can be covered zero, one, or multiple 
times. Dropping the constraints allows for an effi- 
cient computation of the distance. We drop the con- 
straints for the candidate  sentence and not for the 
reference sentence because we do not want any in- 
formation contained in the reference to be omitted. 
Moreover, the reference translation will not contain 
unnecessary repetitions of blocks.
  The new measure, which we call CDER, can thus 
be seen as a measure oriented  towards  recall, while


measures like BLEU are guided by precision. The 
CDER is based on the C DC D distance1 introduced 
by Lopresti and Tomkins (1997). The authors show 
that the problem of finding the optimal solution can 
be solved in O(I 2 · J ) time, where I is the length 
of the candidate  sentence and J the length of the 
reference sentence. Within this paper, we will refer 
to this distance as dCD . In (Leusch et al., 2006) we 
showed how it can be computed in O(I · J ) time 
using a modification  of the Levenshtein algorithm.
  We also studied the reverse direction  of the de- 
scribed measure; that is, we dropped the coverage 
constraints for the reference sentence instead of the 
candidate sentence.  Additionally, the maximum of 
both directions  has been considered as distance mea- 
sure.

4   INVWER

Another approach to circumvent the NP-hardness of 
the block reordering problem is to reduce the search 
space by restricting the number of admissible block 
permutations:

4.1   Bracketed transpositions

In order to reduce the complexity  of the search, we 
restrict consequent block transpositions to be brack- 
eted, i.e. the two blocks to be swapped must both 
lie either completely within or completely out of 
any blocks from previous operations. The following 
examples illustrate admissible and forbidden block 
transpositions. The brackets  indicate the blocks 
that are swapped.  In the transformation of ABC D 
into C DBA in (1), only transpositions within these 
blocks are performed. In (2), the transformation 
from BC DA into BDAC crosses the blocks BC D 
and A from the previous transposition and is there- 
fore forbidden.
1.	Admissible transpositions:
(A)(B C  D) →	((B) (C  D))(A)
→	((C D)  (B))(A)
2.	Forbidden transpositions:
(A)(B C  D) →	(B  C  D)(A)
/→	(B)(D A)(C)
  A concise definition of the Levenshtein and block 
transposition (shift) edit operations can be given us-

  1 C stands for cover and D for disjoint. We adopted this 
notion for our measures.


ing bracketing transduction grammars.


4.2   Bracketing Transduction Grammars

A  bracketing  transduction  grammar  (BTG) (Wu,
1995) is a pair-of-strings  model that generates two 
output strings s and t.  It consists of one common 
set of production rules for both output strings. A 
BTG always  generates a pair of sentences.  Termi- 
nals are pairs of symbols,  where each may be the 
empty word E.
  Concatenation of the terminals and nonterminals 
on the right hand side of a production  rule is either 
straight, denoted by [·], or inverted, denoted by (·). 
In the former  case, the parse subtree is to be read 
left-to-right in both s and t, and in the latter case it 
is to be read left-to-right in s and right-to-left in t. A 
BTG contains only the start symbol S and one non- 
terminal symbol A, and each production  rule con- 
sists of either a string of As or a terminal  pair.
  Using the BTG formalism, we can describe the 
edit operations  Inversion (=  Shift), Substitution, 
Deletion,  Insertion,  as production  rules, associated 
with a cost function  c:

1. Concatenation: A → [AA]
with c([αβ]) = c(α) + c(β)

2. Inversion:       A → (AA)
with c((αβ)) = c(α) + c(β) + cINV

3. Identity:         A → x/x
with c(x/x) = 0

4. Substitution:  A → x/y, where x /= y
with c(x/y)  = cSUB

5. Deletion:        A → x/E
with c(x/E) = cDEL

6. Insertion:       A → E/y
with c(E/y) = cINS

7. Start:              S → A; S → E/E
with c(E/E) = 0


  We define the inversion edit distance  between  a 
candidate sentence eI  and a reference sentence e˜J  to
1	1
be the minimum  cost of the set T (sI , tJ ) of all parse


1    1


trees generated by the BTG for this sentence pair:


(b) The triangular inequation holds: it is al- 
ways cheaper to replace e by e˜ than to re-


dinv (sI , tJ ) :=	min


c(τ ) 	(1)


place e by et and then et by e˜.


1    1
τ ∈T (sI ,tJ )
1   1



Note that, without the inversion rule, the minimum 
production cost equals the Levenshtein distance.
  We  use this distance  to define our error mea- 
sure, the Inversion Word Error Rate (INVWER), by 
normalizing it by the reference length.  The dis- 
tance can be calculated by an algorithm similar to 
a 2-dimensional  CYK algorithm in time O(I 3J 3)


3. The costs of substituting  a word e by e˜ are al- 
ways equal or lower than those of deleting  e 
and then inserting e˜. In short, cSUB ≤ 2.

  Under these conditions  the algorithms for WER 
and CDER can easily be modified to use  word- 
dependent substitution costs. For example, the only 
necessary modification in the CDER algorithm is to


and space O(I 2J 2), as described  in (Leusch et al.,


replace c



SUB


by c



SUB


(e, e˜) in Subsection 4.2.


2003). Because the algorithm  has basically  a time
complexity in Θ(I 6) if I ≈ J , it can become quite 
slow for long sentences.  Because of this, we split 
sentences longer  than 30 words, parallel in candi- 
date and reference on PER-optimal  split points.

5   Word-dependent  Substitution Costs

All automatic  error measures which  are based on the 
edit distance (for example WER, PER, TER, CDER, 
INVWER) assume fixed costs for the substitution of 
words. However, this is counter-intuitive, as replac- 
ing a word with another one which has  a similar 
meaning will rarely change the meaning of a sen- 
tence significantly.  On the other hand, replacing the 
same word with a completely different one probably 
will.  Therefore, it seems advisable to make substi- 
tution costs dependent on the semantical and/or syn- 
tactical dissimilarity of the words. METEOR (Baner- 
jee and Lavie, 2005)  uses a similar idea of grad- 
uated similarity between words (exact match, stem 
match, WORDNET match), but instead of assuming 
different  costs, it uses a matching  procedure  which 
matches the most similar  words first. The MT sys- 
tem combination approach of Ayan et al. (2008) uses 
WORDNET matches  as well as exact matches,  and 
uses different  costs for these matches.
  For algorithmic  reasons, it is helpful to demand 
that an arbitrary substitution cost function cSUB for 
two words e, e˜ meets the following requirements:

1. cSUB depends only on e and e˜.

2. cSUB is a metric; especially
  

For PER, it is no longer possible to use a linear 
time algorithm in the general case. Instead, we use 
a modification  of the Hungarian algorithm (Knuth,
1993).
  The question  is now how to define  the word- 
dependent substitution costs. A pragmatic approach 
is to compare the spelling of the words to be substi- 
tuted with each other. The more similar the spelling 
is, the more similar we consider the words to be, 
and the lower we want the substitution  costs between 
them to be. In English, this works well with similar 
tenses of the same verb, or with genitives or plurals 
of the same noun. Nevertheless,  a similar spelling 
is no guarantee for a similar  meaning, because pre- 
fixes such as “mis-”, “in-”, or “un-” can change 
the meaning of a word significantly.
  We have studied two different  approaches to use 
the similarity in the spelling of two words  as a sub- 
stitution cost:

5.1   Character-based Levenshtein Distance

An obvious way of comparing the spelling is the 
Levenshtein distance. Here, words are compared 
on character level. To normalize this distance into 
a range from 0 (for identical words) to 1 (for com- 
pletely different words), we divide the absolute dis- 
tance by the length of the Levenshtein alignment 
path.

5.2   Common Prefix Length

Another  character-based substitution  cost function 
we studied is based on the common prefix length 
of both words. In English, different tenses of the



Table 1: Example of word-dependent substitution costs.




7	6


5	4.5




and genders of most nouns and adjectives.   How- 
ever, it does not hold if verb prefixes are changed 
or removed. On the other hand, the common pre- 
fix length is sensitive  to critical prefixes such as 
“mis-” for the same reason. Consequently,  the 
common prefix length, normalized by the average 
length of both words, gives a reasonable measure for 
the similarity of two words. To transform the nor- 
malized common prefix length into costs, this frac- 
tion is then subtracted from 1. An example for these 
two approaches is shown in Table 1.

6	Linear Combination of evaluation 
measures

An interesting topic in MT evaluation  research is 
the question whether different  MT evaluation mea- 
sures can be combined into a consensus score, which 
hopefully  shows a better correlation  with the target
– human evaluation – than the single measures. Re- 
cently, Albrecht & Hwa (2007) have investigated on 
the combination of up to 53 measures and features 
in a regression model  and a classifier  as evaluation 
measure.  Also, a linear combination  of BLEU and 
TER has been successfully used for tuning MT sys- 
tems (Mauser et al., 2008; Rosti et al., 2008). In our 
approach, we only are interested in the linear combi- 
nation of two MT evaluation  measures, particularly 
the combination of CDER and PER. We expect this 
combination to have a higher  correlation  with hu- 
man evaluation than the measures alone. CDER (as 
opposed to PER) has the ability to reward correct lo- 
cal ordering,  whereas PER  (as opposed to CDER) 
penalizes overly long candidate sentences. The two 
measures were combined with linear interpolation. 
In order to determine the weights, we performed 
data analysis on seven different corpora in (Leusch 
et al., 2006). The results were consistent across all 
different  data collections and language pairs: a lin-



Table 2: Corpus statistics of the MATR MT06 corpus 
that was used for experimental evaluation of the proposed 
measures.

La
ng
ua
ge 
pa
ir
(A
ra
bi
c)
–
E
ng
lis
h
G
en
re
N
e
w
s
w
i
r
e
 
t
e
x
t
s
M
T 
sy
st
e
m
s
8
D
oc
u
m
en
ts
2
5
Se
g
m
en
ts
2
4
9
R
ef
er
en
ce
s/
se
g
4
H
yp
. 
le
ng
th
32
.5
Re
f. 
le
ng
th
34
.3
Hu
m
an 
ev
al
ua
tio
n
ad
eq
ua
cy 
(1. 
. . 
7)



ear combination of about 60% CDER and 40% PER 
has a significantly   higher correlation with human 
evaluation than each of the measures alone. Conse- 
quently, we chose these weights for the NIST Met- 
rics MATR evaluation  as well.

7   Experimental results

For an experimental  comparison  of the different 
evaluation  measures, we calculated the correlation 
between these measures and human evaluation,  in 
particular the “adequacy”, for the MT06 corpus pro- 
vided by NIST and LDC for the Metrics MATR
2008 evaluation (NIST, 2008). This corpus consists 
of translations of 25 Arabic newswire  documents 
into English, as generated  by 8 MT systems that 
participated in NIST’s 2006 MT evaluation.  Some 
statistics on this corpus are listed in Table 2.
  Within NIST’s Metrics MATR evaluation, another 
corpus was provided to participants for metrics eval- 
uation. But due to its extremely limited size – a sin- 
gle document consisting of 16 segments – correla- 
tion results generated on this corpus are quite noisy, 
and of limited significance.



Table 3: Pearson’s r and Kendall’s τ (absolute) between adequacy and automatic evaluation measures on different 
levels of the MATR MT06 data.

M
ea
su
re
r
τ
τ¯

se
g
do
c
sy
s
se
g
do
c
sy
s
sy
s/s
eg
sy
s/
do
c
W
ER
W
ER 
+
c(
w)
PE
R
PE
R 
+
c(
w)
0.
6
2
1
0.
6
2
6
0.
5
9
7
0.
5
8
6
0.
85
3
0.
86
0
0.
85
2
0.
85
8
0.
95
3
0.
95
4
0.
95
8
0.
96
6
0.
5
0
3
0.
5
0
6
0.
4
8
2
0.
4
8
3
0.
59
9
0.
60
8
0.
58
8
0.
59
0
0.
57
1
0.
57
1
0.
64
3
0.
71
4
0.
58
4
0.
58
0
0.
57
6
0.
55
9
0.
64
1
0.
65
3
0.
64
4
0.
64
6
B
LE
U 
(
mi
n 
R
ef
)
B
L
E
U
 
(
a
v
g
 
R
e
f
)
 
B
L
E
U
S
B
LE
U
SP
0.
5
9
2
0.
5
9
8
0.
6
7
2
0.
6
8
7
0.
84
4
0.
85
7
0.
86
0
0.
87
7
0.
94
3
0.
95
5
0.
95
5
0.
96
1
0.
4
7
6
0.
4
8
3
0.
5
4
1
0.
5
4
2
0.
58
0
0.
60
2
0.
60
6
0.
61
3
0.
57
1
0.
64
3
0.
64
3
0.
64
3
0.
57
8
0.
58
7
0.
59
0
0.
60
9
0.
58
8
0.
61
2
0.
61
5
0.
61
8
T
ER
IN
V
W
ER
IN
V
W
ER 
+
c(
w)
0.
5
9
7
0.
6
3
8
0.
6
4
9
0.
84
9
0.
86
7
0.
87
1
0.
95
7
0.
95
8
0.
95
8
0.
4
9
5
0.
5
0
9
0.
5
1
2
0.
60
2
0.
60
6
0.
60
6
0.
57
1
0.
57
1
0.
57
1
0.
59
5
0.
58
3
0.
57
3
0.
66
7
0.
64
3
0.
63
8
C
D
ER 
+
c(
w)
C
D
ER 
+ 
PE
R 
+
c(
w)
0.
7
0
8
0.
6
9
0
0.
89
1
0.
88
5
0.
97
3
0.
97
5
0.
5
5
8
0.
5
4
3
0.
64
3
0.
63
2
0.
71
4
0.
71
4
0.
62
3
0.
61
0
0.
67
1
0.
67
9
+c(w) denotes measures using word dependent substitution  costs.
“seg” is on segment level, “doc is on document level, “sys” is on system level.
“sys/seg” is average system ranking per segment, “sys/doc” is average system ranking per document.




  We  investigated two different aspects  of auto- 
matic evaluation  measures – their ability to give  a 
reliable absolute estimate of their translation qual- 
ity, and their ability to rank different translations 
with regard to their quality. As target for this, we 
chose the adequacy score, an integer value between
1 and 7, that was assigned to all translations in the 
corpus by a human  judge. We measured the abso- 
lute prediction  as Pearson’s correlation  coefficient  r 
(Casella and Berger, 1990), and the ranking capabil- 
ity as Kendall’s τ (Kendall, 1970). The latter has 
the big advantage over other coefficients like Spear- 
man’s ρ that it handles ties in a well-defined  and rea- 
sonable manner.  As there are only seven different 
outcomes for adequacy, but up to several thousand 
samples, ties are extremely frequent in these experi- 
ments.
  We measured both r and τ on three levels of gran- 
ularity – the system level, that is comparing the ac- 
cumulated  scores of all documents per system, the 
document level, and the segment level.
  Comparing  the scores of different  systems at the 
same  time as comparing   different (source) docu-


put of different MT systems, and to compare the out- 
put for different  source sentence. In other words, an 
evaluation metric gets a “bonus”  in terms of correla- 
tion already if it is able to divide easy from difficult 
source sentences (which typically have good and bad 
translations respectively), as well as for dividing dif- 
ferent MT systems.  In practice, we are mostly  in- 
terested in the latter: Our test corpus then is fixed, 
and there might be more efficient  methods for esti- 
mating the “difficulty” of a source sentence. We use 
the MT evaluation  measure here because we want to 
compare the actual MT systems, and not the source 
sentences, and the MT evaluation measure we chose 
should respect this.
  To divide these two effects in our experiments, 
we calculated the rank correlation (using τ ) over the 
different MT system for a fixed source segment or 
document respectively, and then calculated the arith- 
metic average over the different  τ for the individual 
source segments (or documents).2    We denote this 
averaged correlation  coefficient  as τ¯.  Our experi- 
mental results are listed in Table 3.

2 This is another advantage of τ over r or ρ – in our under-


7.1   BLEU and BLEUSP
Using the average reference  length instead of the 
minimum  reference length brings a very small im- 
provement in correlation with human judgment on 
segment level, and a slightly  larger improvement on 
document  and system level. The largest improve- 
ment for BLEU-like measures on the segment level 
comes from Lin & Och’s smoothed geometric mean 
in BLEUS: r raises from .60 to .67, τ from .48 to
.54, even though the ability to rank systems on the 
sentence or document level hardly increases. This 
can be increased using the segment boundary  to- 
kens: r improves from .67 to .68, and τ¯seg from .59 
to .61.

7.2   TER and INVWER
Even though TER and INVWER are structurally very 
similar, we see  that INVWER  has a significantly 
higher correlation than TER  with human  evalua- 
tion on segment and document  level, even though 
it seems that TER has a better capability  to actually 
judge between MT systems on smaller levels. Using 
word-dependent substitution costs brings small im- 
provements on the segment level, but seems to have 
a slightly negative effect on the ability to differenti- 
ate between systems.

7.3   Word-dependent  substitution costs
Compared with our results in (Leusch et al., 2006), 
we see only small improvements, or for PER  even 
a very small deterioration if we use word-dependent 
substitution  costs on this corpus, both on segment 
and system level. For our final submission, we made 
the substitution  costs dependent on the common pre- 
fix length in out Metrics MATR submission.

7.4	CDER and linear combination of 
evaluation measures
CDER, as a pure  recall measure, shows again the 
highest correlation  of all evaluation  measures, on 
segment,  document,  and system  level.   Unfortu- 
nately, it is unwise to use a purely recall-oriented 
measure in actual research. This is because MT sys- 
tems tuned for such a measure, either directly  or in- 
directly, tend to produce overly  long sentences con- 
taining many unnecessary or even wrong insertions. 
To avoid this, and because doing so showed a signif-


on most of our development corpora, we use a lin- 
ear combination  of CDER (with a weight of .6) and 
PER  (.4), both using word dependent substitution 
costs. Unfortunately,  this combined measure shows 
a slightly lower correlation with human judgment 
on the sentence level than pure CDER.  But even 
though, this measure has a significantly higher cor- 
relation on all levels than the original BLEU or TER 
score, and is better than or on par with even our im- 
proved scores INVWER and BLEUSP.

8   Conclusions

In this paper  we have  presented  three improved 
evaluation  measures for MT, based  on the well- 
established and understood  measures TER, BLEU, 
and a variant  of WER and PER.  While not exactly 
being revolutionary, our measures show a signifi- 
cant improvement in correlation with human judg- 
ment compared with the original  measures. We fur- 
ther refined the way this correlation is measured, 
taking into account typical statistical  and data ef- 
fects when looking evaluating MT evaluation mea- 
sures. We consider our results to be of importance 
because a multitude  of new, sometimes “revolution- 
ary” MT evaluation  measures have been proposed 
over the last years, sometimes to be compared only 
with BLEU or TER in terms of correlation, even for 
applications  where these baseline measures are not 
well suited for. One of our scientific contributions 
in this paper is to show that even with these only al- 
terations and additions, we can have a significantly 
higher correlation with human judgment. Along the 
way, we hope to raise the baseline for new evalua- 
tion measures significantly.
  The measures described in this paper,  as well as 
some additional  measures, have been implemented 
by us under  a python command line tool called 
PyET. The implementation   uses shared libraries 
written in C++ for performance.  Please contact the 
first author via E-Mail, or visit his web site3 to ob- 
tain a copy of this software.

9   Acknowledgments

This paper is  based   upon work  supported  by 
the Defense Advanced  Research Projects  Agency

3 http://www-i6.informatik.rwth-aachen←'


(DARPA) under Contract No. HR0011-06-C-0023.


References

J. Albrecht  and R. Hwa. 2007. A re-examination of ma- 
chine learning approaches for sentence-level mt eval- 
uation. In Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics, pp.
880–887, Prague, Czech Republic,  June. Association 
for Computational Linguistics.
N. F. Ayan, J. Zheng,  and W. Wang. 2008. Improving 
alignments for better confusion networks for combin- 
ing machine translation systems. In Proceedings of the
22nd International  Conference on Computational Lin- 
guistics (Coling 2008), pp. 33–40, Manchester, UK, 
August. Coling 2008 Organizing Committee.
S. Banerjee and A. Lavie. 2005. METEOR: An auto- 
matic metric for MT evaluation with improved corre- 
lation with human judgments.  In Proceedings of the 
ACL Workshop on Intrinsic and Extrinsic Evaluation 
Measures for Machine Translation and/or Summariza- 
tion, pp. 65–72, Ann Arbor, Michigan,  June. Associa- 
tion for Computational Linguistics.
G. Casella and R. L. Berger, 1990. Statistical Inference, 
chapter 4.5, pp. 160–168. Duxbury Press.
M. G. Kendall.	1970.	Rank Correlation Methods.
Charles Griffin & Co Ltd, London.

D. E. Knuth, 1993. The Stanford GraphBase: a platform 
for combinatorial  computing, pp. 74–87. ACM Press, 
New York, NY.
G. Leusch, N. Ueffing, and H. Ney.  2003.  A novel 
string-to-string  distance measure with applications to 
machine translation evaluation.  In Proc. MT Summit 
IX, pp. 240–247, New Orleans, LA, September.
G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005. Pre- 
processing and normalization for automatic evaluation 
of machine translation. In 43rd Annual Meeting of the 
Assoc. for Computational Linguistics: Proc. Workshop 
on Intrinsic and Extrinsic Evaluation Measures for MT 
and/or Summarization, pp. 17–24, Ann Arbor, Michi- 
gan, June. Association  for Computational Linguistics.
G. Leusch, N. Ueffing, and H. Ney. 2006. CDER: Effi- 
cient mt evaluation using block movements.  In Con- 
ference of the European Chapter of the Association for 
Computational Linguistics, pp. 241–248, Trento, Italy, 
April. European Chapter of the Association for Com- 
putational Linguistics.
V. I. Levenshtein.  1966. Binary codes capable of correct- 
ing deletions, insertions and reversals. Soviet Physics


C. Y. Lin and F. J. Och.  2004. Orange: a method for eval- 
uation automatic evaluation metrics for machine trans- 
lation. In Proc. COLING 2004, pp. 501–507, Geneva, 
Switzerland, August.
D. Lopresti and A. Tomkins.  1997. Block edit models for 
approximate string matching. Theoretical Computer 
Science, 181(1):159–179, July.
A. Mauser, S. Hasan, and H. Ney.  2008.  Automatic 
evaluation  measures for statistical machine translation 
system optimization. In International  Conference on 
Language Resources and Evaluation, Marrakech, Mo- 
rocco, May.
NIST.  2008.  NIST Metrics for Machine Translation 
Challenge 2008	. 
http://www.nist.gov/speech/tests/←' 
metricsmatr/2008/ .
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method  for automatic evaluation of machine 
translation. Technical Report RC22176 (W0109-022), 
IBM Research Division, Thomas J. Watson Research 
Center, September.
A. V. Rosti, S. Matsoukas, and R. Schwartz.  2007. Im- 
proved word-level system combination  for machine 
translation.  In Proceedings of the 45th Annual Meet- 
ing of the Association of Computational Linguistics, 
pp. 312–319, Prague, Czech Republic,  June. Associa- 
tion for Computational Linguistics.
A. V. Rosti, B. Zhang, S. Matsoukas, and R. Schwartz.
2008.  Incremental  hypothesis alignment for build- 
ing confusion networks with application to machine 
translation system combination.  In Proceedings of the 
Third Workshop on Statistical Machine Translation, 
pp. 183–186, Columbus, Ohio, June. Association  for 
Computational Linguistics.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci- 
ulla, and R. Weischedel. 2005. A study of translation 
error rate with targeted human annotation.  Technical 
Report LAMP-TR-126,  CS-TR-4755, UMIACS-TR-
  2005-58, University of Maryland, College Park, MD. 
J. P. Turian,  L. Shen, and I. D. Melamed. 2003. Eval-
uation of machine translation  and its evaluation.   In
Proc. MT Summit IX, pp. 23–28, New Orleans, LA, 
September.
D. Wu. 1995. An algorithm for simultaneously bracket- 
ing parallel texts by aligning words. In Proc. of the
33rd Annual Conf. of the Association for Computa- 
tional Linguistics, pp. 244–251.





