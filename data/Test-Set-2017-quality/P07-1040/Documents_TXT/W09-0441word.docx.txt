Fluency, Adequacy, or HTER?
Exploring Different  Human Judgments with a Tunable MT Metric


Matthew Snover§, Nitin Madnani§, Bonnie J. Dorr§ † & Richard Schwartz† ‡
§Laboratory for Computational Linguistics and Information Processing
§Institute for Advanced Computer Studies
§University of Maryland, College Park
†Human Language Technology Center of Excellence
‡BBN Technologies
{snover,nmadnani,bonnie}@umiacs.umd.edu	schwartz@bbn.com





Abstract

Automatic Machine Translation (MT) 
evaluation metrics have traditionally  been 
evaluated by the correlation of the scores 
they assign  to MT  output with human 
judgments of  translation performance. 
Different types of human judgments, such 
as Fluency,  Adequacy,  and HTER, mea- 
sure varying aspects of MT performance 
that can be captured  by automatic  MT 
metrics.  We explore these differences 
through the use of a new tunable MT met- 
ric: TER-Plus, which extends the Transla- 
tion Edit Rate evaluation metric with tun- 
able parameters and the incorporation of 
morphology,  synonymy and paraphrases. 
TER-Plus  was shown to be one of the 
top metrics in  NIST’s  Metrics MATR
2008 Challenge, having the highest aver- 
age rank in terms of Pearson and Spear- 
man correlation.  Optimizing TER-Plus 
to different types of  human judgments 
yields significantly improved correlations 
and meaningful  changes in the weight of 
different types of edits, demonstrating sig- 
nificant differences between the types of 
human judgments.
1   Introduction

Since the introduction of the BLEU metric (Pa- 
pineni et al., 2002), statistical MT systems have 
moved away from human evaluation of their per- 
formance and towards rapid evaluation using au- 
tomatic metrics.  These automatic  metrics are 
themselves evaluated by their ability to generate 
scores for MT output that correlate well with hu- 
man judgments of translation quality.   Numer- 
ous methods of judging MT output by humans


have  been  used,  including Fluency, Adequacy, 
and, more recently, Human-mediated Translation 
Edit Rate (HTER) (Snover et al., 2006). Fluency 
measures whether  a translation  is fluent, regard- 
less of the correct meaning, while Adequacy mea- 
sures whether the translation conveys the correct 
meaning,  even if the translation is not fully flu- 
ent. Fluency  and Adequacy  are frequently  mea- 
sured together  on a discrete  5 or 7 point scale, 
with their average being used  as a single score 
of translation quality. HTER is a more  complex 
and semi-automatic  measure in which humans do 
not score translations directly, but rather generate 
a new reference translation  that is closer to the 
MT output but retains the fluency and meaning 
of the original reference. This new targeted refer- 
ence is then used as the reference translation  when 
scoring the MT output using Translation Edit Rate 
(TER) (Snover et al., 2006) or when  used with 
other automatic  metrics  such as BLEU or ME- 
TEOR (Banerjee  and Lavie, 2005). One of the 
difficulties in the creation of targeted references 
is a further requirement that the annotator attempt 
to minimize the number of edits, as measured by 
TER, between the MT output and the targeted ref- 
erence, creating the reference that is as close  as 
possible to the MT output while still being ade- 
quate and fluent. In this way, only true errors in 
the MT output are counted. While  HTER  has been 
shown to be more consistent and finer grained than 
individual  human annotators of Fluency and Ade- 
quacy, it is much more time consuming  and tax- 
ing on human annotators than other types of hu- 
man judgments, making it difficult and expensive 
to use. In addition,  because HTER  treats all edits 
equally, no distinction  is made between serious er- 
rors (errors in names or missing subjects) and mi- 
nor edits (such as a difference  in verb agreement



   Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 259–268, 
Athens, Greece, 30 March – 31 March 2009. Qc 2009 Association for Computational Linguistics


or a missing determinator).
  Different types of translation errors vary in im- 
portance depending on the type of human judg- 
ment being used to evaluate the translation.  For 
example, errors in tense might barely affect the ad- 
equacy of a translation  but might  cause the trans- 
lation  be scored as less fluent. On the other hand, 
deletion of content words might not lower the flu- 
ency of a translation but the adequacy would  suf- 
fer.  In this paper, we examine  these differences 
by taking an automatic evaluation metric and tun- 
ing it to these these human judgments  and exam- 
ining the resulting differences in the parameteri- 
zation of the metric. To study this we introduce 
a new evaluation metric, TER-Plus (TERp)1 that 
improves over the existing Translation Edit Rate 
(TER) metric (Snover et al., 2006), incorporating 
morphology,  synonymy  and paraphrases, as well 
as tunable  costs for different  types of errors that 
allow for easy interpretation  of the differences be- 
tween human judgments.
  Section 2 summarizes the TER metric and dis- 
cusses how TERp improves  on it. Correlation re- 
sults with human judgments, including  indepen- 
dent results from the 2008 NIST Metrics MATR 
evaluation,  where TERp was consistently one of 
the top metrics, are presented in Section 3 to show 
the utility of TERp  as an evaluation  metric. The 
generation of paraphrases,  as well as the effect of 
varying the source of paraphrases, is discussed in 
Section 4. Section 5 discusses the results of tuning 
TERp to Fluency, Adequacy and HTER, and how 
this affects the weights of various edit types.

2   TER and TERp

Both TER and TERp are automatic  evaluation 
metrics for machine translation that score a trans- 
lation, the hypothesis, of a foreign  language text, 
the source, against a translation  of the source text 
that was created by a human  translator,  called  a 
reference  translation. The set  of possible cor- 
rect translations is very large—possibly infinite— 
and any single reference translation is just a sin- 
gle point in that space.   Usually multiple refer- 
ence translations,  typically 4, are provided to give 
broader sampling of the space of correct transla- 
tions. Automatic MT evaluation metrics compare 
the hypothesis against this set of reference trans- 
lations and assign a score to the similarity; higher

1 Named after the nickname–”terp”–of the University of
Maryland, College Park, mascot: the diamondback terrapin.


scores are given to hypotheses that are more simi- 
lar to the references.
  In addition to assigning  a score to a hypothe- 
sis, the TER metric also provides an alignment be- 
tween the hypothesis and the reference, enabling it 
to be useful beyond general translation evaluation. 
While  TER has been shown to correlate well with 
human judgments of translation quality, it has sev- 
eral flaws, including  the use of only a single  ref- 
erence translation and the measuring of similarity 
only by exact word matches between the hypoth- 
esis and the reference.  The handicap of using  a 
single reference can be addressed by the construc- 
tion of a lattice  of reference translations.   Such a 
technique has been used with TER to combine the 
output of multiple  translation systems (Rosti et al.,
2007). TERp does not utilize this methodology2
and instead focuses on addressing the exact match- 
ing flaw of TER. A brief description of TER is pre- 
sented in Section 2.1, followed by a discussion  of 
how TERp differs from TER in Section 2.2.

2.1   TER

One of the first automatic metrics used to evaluate 
automatic machine translation (MT) systems was 
Word Error Rate (WER) (Niessen et al., 2000), 
which is the standard evaluation  metric for Au- 
tomatic Speech Recognition.   WER is computed 
as the Levenshtein (Levenshtein,  1966) distance 
between the words of the system output and the 
words of the reference translation divided by the 
length of the reference translation.  Unlike speech 
recognition, there are many correct translations for 
any given foreign sentence. These correct transla- 
tions differ not only in their word choice but also 
in the order in which the words occur. WER is 
generally seen as inadequate  for evaluation for ma- 
chine translation  as it fails to combine knowledge 
from multiple  reference translations and also fails 
to model the reordering of words and phrases in 
translation.
  TER addresses the latter failing of WER by al- 
lowing block movement of words, called shifts. 
within the hypothesis.  Shifting a phrase  has the 
same edit cost as inserting,  deleting  or substitut- 
ing a word, regardless of the number of words 
being shifted. While a general solution  to WER 
with block movement is NP-Complete (Lopresti

  2 The technique of combining  references in this fashion 
has not been evaluated in terms of its benefit when correlating 
with human judgments. The authors hope to examine and 
incorporate such a technique in future versions of TERp.


and Tomkins,  1997), TER addresses this by using 
a greedy  search to select the words to be shifted, 
as well as further  constraints on the words to be 
shifted. These constraints are intended  to simu- 
late the way in which a human editor might choose 
the words to shift. For exact details on these con- 
straints, see Snover et al. (2006). There are other 
automatic metrics that follow the general formu- 
lation  as TER but address the complexity of shift- 
ing in different ways, such as the CDER evaluation 
metric (Leusch et al., 2006).
  When TER is used with multiple  references, it 
does not combine the references. Instead, it scores 
the hypothesis against each reference individually. 
The reference against which the hypothesis has the 
fewest number of edits is deemed the closet refer- 
ence, and that number of edits is used as the nu- 
merator for calculating the TER score. For the de- 
nominator, TER uses the average number  of words 
across all the references.

2.2   TER-Plus

TER-Plus (TERp) is an extension  of TER that 
aligns words in the hypothesis and reference not 
only when they are exact matches but also when 
the words  share a stem or are synonyms.   In ad- 
dition, it uses probabilistic  phrasal substitutions 
to align phrases in the hypothesis and reference. 
These phrases are generated by considering possi- 
ble paraphrases of the reference words. Matching 
using stems and synonyms  (Banerjee and Lavie,
2005) and using paraphrases (Zhou  et al., 2006; 
Kauchak and Barzilay,  2006) have previously been 
shown to be beneficial for automatic MT evalu- 
ation.  Paraphrases have  also been shown  to be 
useful in expanding the number of references used 
for parameter tuning (Madnani et al., 2007; Mad- 
nani et al., 2008) although they are not used di- 
rectly in this fashion within TERp. While all edit 
costs in TER are constant, all edit costs in TERp 
are optimized  to maximize  correlation  with human 
judgments. This is because while  a set of constant 
weights might prove adequate for the purpose of 
measuring translation quality—as evidenced by 
correlation with human judgments both for TER 
and HTER—they  may not be ideal for maximiz- 
ing correlation.
  TERp uses  all the edit operations of TER— 
Matches, Insertions, Deletions, Substitutions and 
Shifts—as well as three new edit operations: Stem 
Matches, Synonym  Matches and Phrase Substitu-


tions. TERp identifies words in the hypothesis and 
reference that share the same stem using the Porter 
stemming algorithm  (Porter, 1980). Two words 
are determined  to be synonyms if they share the 
same synonym  set according  to WordNet (Fell- 
baum, 1998).  Sequences of words in the reference 
are considered to be paraphrases of a sequence of 
words in the hypothesis if that phrase pair occurs 
in the TERp phrase table. The TERp phrase table 
is discussed in more detail in Section 4.
  With the exception of the phrase substitutions, 
the cost for all other edit operations is the same re- 
gardless of what the words in question are. That 
is, once the edit cost of an operation is determined 
via optimization, that operation costs the same no 
matter what words are under consideration.  The 
cost of a phrase substitution,   on the other hand, 
is a function  of the probability  of the paraphrase 
and the number of edits needed to align the two 
phrases according  to TERp. In effect, the proba- 
bility of the paraphrase is used to determine how 
much to discount the alignment of the two phrases. 
Specifically,  the cost of a phrase substitution  be- 
tween the reference phrase, p1 and the hypothesis 
phrase p2 is:

cost(p1, p2) =w1+
edit(p1, p2)×
(w2 log(Pr(p1, p2))
+ w3 Pr(p1, p2) + w4)

  where w1, w2, w3, and w4 are the 4 free param- 
eters of the edit cost, edit(p1, p2) is the edit cost 
according to TERp of aligning p1 to p2 (excluding 
phrase substitutions)  and Pr(p1, p2) is the prob- 
ability of paraphrasing p1 as p2, obtained from 
the TERp phrase table. The w parameters of the 
phrase substitution cost may be negative while still 
resulting in a positive  phrase substitution  cost, as 
w2 is multiplied  by the log probability, which is al- 
ways a negative number. In practice this term will 
dominate the phrase substitution  edit cost.
  This edit cost for phrasal substitutions is, there- 
fore, specified by four parameters,  w1, w2, w3 
and w4. Only paraphrases specified in the TERp 
phrase table are considered  for phrase substitu- 
tions. In addition, the cost for a phrasal substi- 
tution is limited to values greater than or equal to
0, i.e., the substitution cost cannot be negative. In 
addition, the shifting constraints of TERp are also 
relaxed to allow shifting of paraphrases, stems, 
and synonyms.


  In total TERp uses 11 parameters out of which 
four represent the cost of phrasal substitutions. 
The match cost is held fixed at 0, so that only the
10 other parameters can vary during optimization. 
All edit costs, except for the phrasal substitution 
parameters, are also restricted  to be positive.  A 
simple hill-climbing search is used to optimize  the 
edit costs by maximizing the correlation of human 
judgments with the TERp score. These correla- 
tions are measured  at the sentence,  or segment, 
level. Although it was done for the experiments 
described in this paper, optimization  could also 
be performed to maximize document level correla- 
tion – such an optimization  would give decreased 
weight to shorter segments as compared  to the seg- 
ment level optimization.

3   Correlation  Results

The optimization of the TERp edit costs, and com- 
parisons against several standard automatic eval- 
uation metrics, using human judgments of Ade- 
quacy is first described in Section 3.1. We then 
summarize, in Section 3.2, results of the NIST 
Metrics MATR workshop where TERp was eval- 
uated as one of 39 automatic metrics using many 
test conditions and types of human judgments.

3.1   Optimization of Edit Costs and
Correlation Results

As part of the 2008 NIST Metrics MATR work- 
shop (Przybocki  et al., 2008), a development sub- 
set  of translations from eight Arabic-to-English 
MT systems submitted  to NIST’s MTEval 2006 
was  released that had been annotated  for Ade- 
quacy. We divided this development  set into an 
optimization set and a test set, which we then used 
to optimize the edit costs of TERp and compare it 
against other evaluation metrics. TERp was op- 
timized to maximize the segment  level Pearson 
correlation with adequacy on the optimization  set. 
The edit costs determined by this optimization are 
shown in Table 1.
  We can compare TERp with other metrics by 
comparing  their Pearson  and Spearman  corre- 
lations with Adequacy, at the segment, docu- 
ment and system  level.   Document level Ade- 
quacy scores are determined by taking the length 
weighted average of the segment level scores. Sys- 
tem level scores  are  determined   by taking the 
weighted average of the document level scores in 
the same manner.


We compare TERp with BLEU (Papineni et al.,
2002), METEOR (Banerjee and Lavie, 2005), and 
TER (Snover et al., 2006). The IBM version of 
BLEU was  used in case insensitive  mode  with 
an ngram-size of 4 to calculate the BLEU scores. 
Case insensitivity  was used with BLEU as it was 
found to have much higher correlation with Ade- 
quacy. In addition, we also examined BLEU using 
an ngram-size of 2 (labeled as BLEU-2),  instead 
of the default ngram-size of 4, as it often has a 
higher correlation with human judgments. When 
using METEOR, the exact matching, porter stem- 
ming matching, and WordNet synonym matching 
modules were used. TER was also used in case 
insensitive mode.
  We show the Pearson and Spearman correlation 
numbers of TERp and the other automatic metrics 
on the optimization  set and the test set in Tables 2 
and 3.  Correlation  numbers that are statistically 
indistinguishable from the highest correlation, us- 
ing a 95% confidence interval,  are shown in bold 
and numbers that are actually not statistically  sig-
nificant  correlations are marked with a †.  TERp
has the highest Pearson correlation  in all condi-
tions, although not all differences  are statistically 
significant. When examining the Spearman cor- 
relation, TERp has the highest correlation  on the 
segment and system levels, but performs worse 
than METEOR  on the document level Spearman 
correlatons.


3.2   NIST Metrics MATR 2008 Results

TERp was one of 39 automatic metrics evaluated 
in the 2008 NIST Metrics MATR Challenge.  In 
order to evaluate the state of automatic MT eval- 
uation, NIST tested metrics  across a number  of 
conditions across 8 test sets. These conditions  in- 
cluded segment, document and system level corre- 
lations with human judgments of preference, flu- 
ency, adequacy and HTER.  The test sets included 
translations from Arabic-to-English,  Chinese-to- 
English, Farsi-to-English, Arabic-to-French, and 
English-to-French MT systems involved in NIST’s 
MTEval 2008, the GALE (Olive,  2005) Phase 2 
and Phrase 2.5 program,  Transtac January and July
2007, and CESTA run 1 and run 2, covering mul- 
tiple genres. The version of TERp submitted to 
this workshop was optimized as described in Sec- 
tion 3.1. The development data upon which TERp 
was optimized was not part of the test sets evalu- 
ated in the Challenge.




M
at
ch

In
se
rt

D
el
eti
o
n

S
u
bs
t.

St
e
m

S
yn
.

S
hi
ft
Phrase 
Substit
ution
w1	w2	w3	w4
0
.
0
0
.
2
6
1
.
4
3
1
.
5
6
0
.
0
0.
0
0.
5
6
-
0.
23
-
0.
15
-
0.
08
0.
18

Table 1: Optimized TERp Edit Costs



M
e
t
r
i
c
Op
tim
iza
tio
n 
Set
Seg 	Doc	Sys
T
e
s
t
 
S
e
t
Seg 	Doc	Sys
Optimi
zation+
Test
Seg 
	
Doc
	
Sys
B
L
E
U
0.
6
2
3
0.
8
6
7
0.
9
5
2
0.
5
6
3
0.
8
5
2
0.
9
4
8
0.
6
0
3
0.
8
6
1
0.
9
5
4
B
L
E
U
-
2
0.
6
6
1
0.
8
8
8
0.
9
4
6
0.
5
9
1
0.
8
7
6
0.
9
5
3
0.
6
3
7
0.
8
8
3
0.
9
5
2
M
E
T
E
O
R
0.
7
3
1
0.
8
9
4
0.
9
5
2
0.
7
5
1
0.
9
0
4
0.
9
5
7
0.
7
3
9
0.
8
9
8
0.
9
5
8
T
E
R
-
0.
6
0
9
-
0.
86
4
-
0.
95
7
-
0.
6
0
7
-
0.
86
0
-
0.
95
9
-
0.
6
0
9
-
0.
86
3
-
0.
96
1
T
E
R
p
-
0.
7
8
2
-
0.
91
2
-
0.
99
6
-
0.
7
8
7
-
0.
91
8
-
0.
98
5
-
0.
7
8
4
-
0.
91
4
-
0.
99
4

Table 2: Optimization & Test Set Pearson Correlation  Results




  Due to the wealth of testing conditions,  a sim- 
ple overall view of the official MATR08 results re- 
leased by NIST is difficult. To facilitate this anal- 
ysis, we examined the average rank of each metric 
across all conditions,  where the rank was deter- 
mined by their Pearson and Spearman correlation 
with human judgments.  To incorporate statistical 
significance, we calculated the 95% confidence in- 
terval for each correlation  coefficient  and found 
the highest and lowest rank from which the cor- 
relation coefficient was statistically indistinguish- 
able, resulting in lower and upper bounds of the 
rank for each metric in each condition.  The aver- 
age lower bound, actual, and upper bound ranks 
(where  a rank of 1 indicates the highest correla- 
tion) of the top metrics, as well as BLEU and TER, 
are shown in Table 4, sorted by the average upper 
bound Pearson correlation. Full descriptions of the 
other metrics3, the evaluation results, and the test 
set composition  are available  from NIST (Przy- 
bocki et al., 2008).
  This analysis shows that TERp was consistently 
one of the top metrics across test conditions  and 
had the highest average rank both in terms of Pear- 
son and Spearman correlations.   While this anal- 
ysis is not comprehensive, it does give a general 
idea of the performance of all metrics by syn- 
thesizing the results into a  single table.  There 
are striking differences between the Spearman and 
Pearson correlations for other metrics, in particu- 
lar the CDER metric (Leusch et al., 2006) had the 
second highest rank in Spearman correlations (af-

  3 System description of  metrics  are also  distributed 
by AMTA: http://www.amtaweb.org/AMTA2008. 
html


ter TERp), but was the sixth ranked metric accord- 
ing to the Pearson correlation. In several  cases, 
TERp was not the best metric (if a metric was the 
best in all conditions,  its average rank would be 1), 
although it performed well on average.  In partic- 
ular, TERp did significantly  better than the TER 
metric, indicating the benefit of the enhancements 
made to TER.

4   Paraphrases

TERp uses probabilistic   phrasal substitutions  to 
align phrases in the hypothesis with phrases in the 
reference. It does so by looking up—in a pre- 
computed phrase table—paraphrases of phrases in 
the reference and using its associated edit cost as 
the cost of performing   a match  against the hy- 
pothesis. The paraphrases used in TERp were ex- 
tracted using the pivot-based method as described 
in (Bannard and Callison-Burch,  2005) with sev- 
eral additional filtering mechanisms to increase 
the precision. The pivot-based method utilizes the 
inherent monolingual  semantic knowledge from 
bilingual corpora: we first identify English-to-F 
phrasal correspondences, then map from English 
to English by following translation units from En- 
glish to F and back. For example, if the two En- 
glish phrases e1 and e2 both correspond to the 
same foreign  phrase f, then they may be consid- 
ered to be paraphrases of each other with the fol- 
lowing probability:

p(e1|e2) ≈ p(e1|f ) ∗ p(f |e2)

If there are several pivot phrases that link the two
English phrases, then they are all used in comput-




M
e
t
r
i
c
O
pt
i
m
iz
at
io
n 
S
et
Seg 	Doc	Sys
T
e
s
t
 
S
e
t
Seg 	Doc	Sys
Opt
imi
zati
on+
Test
Seg 	Doc	Sys
B
L
E
U
0.
6
3
5
0.
8
1
6
0.
71
4†
0.
5
5
0
0.
7
4
0
0.
69
0†
0.
6
0
6
0.
7
9
4
0.
73
8†
B
L
E
U
-
2
0.
6
4
3
0.
8
2
3
0.
78
6†
0.
5
5
8
0.
7
4
7
0.
69
0†
0.
6
1
4
0.
7
9
9
0.
73
8†
M
E
T
E
O
R
0.
7
2
9
0.
8
8
6
0
.
8
8
1
0.
7
2
7
0.
8
5
3
0.
73
8†
0.
7
3
0
0.
8
7
6
0
.
9
2
2
T
E
R
-
0.
6
3
0
-
0.
79
4
-
0.
81
0†
-
0.
6
3
0
-
0.
79
7
-
0.
66
7†
-
0.
6
3
1
-
0.
80
1
-
0.
78
6†
T
E
R
p
-
0.
7
6
0
-
0.
83
4
-
0.
9
7
6
-
0.
7
3
7
-
0.
81
8
-
0.
8
8
1
-
0.
7
5
4
-
0.
83
4
-
0.
9
2
9

Table 3: MT06 Dev. Optimization & Test Set Spearman Correlation  Results


M
e
t
r
i
c
Av
er
ag
e 
Ra
nk 
by 
P
ea
rs
o
n
Av
er
ag
e 
Ra
nk 
by 
S
pe
ar
m
an
T
E
R
p
1
.
4
9
 
«
 
6
.
0
7
 
«
 
1
7
.
3
1
1
.
6
0
 
«
 
6
.
4
4
 
«
 
1
7
.
7
6
M
E
T
E
O
R
 
v
0
.
7
1
.
8
2
 
«
 
7
.
6
4
 
«
 
1
8
.
7
0
1
.
7
3
 
«
 
8
.
2
1
 
«
 
1
9
.
3
3
M
E
T
E
O
R 
r
a
n
k
i
n
g
2
.
3
9
 
«
 
9
.
4
5
 
«
 
1
9
.
9
1
2
.
1
8
 
«
 
1
0
.
1
8
 
«
 
1
9
.
6
7
M
E
T
E
O
R
 
v
0
.
6
2
.
4
2
 
«
 
1
0
.
6
7
 
«
 
1
9
.
1
1
2
.
4
7
 
«
 
1
1
.
2
7
 
«
 
1
9
.
6
0
E
D
P
M
2
.
4
5
 
«
 
8
.
2
1
 
«
 
2
0
.
9
7
2
.
7
9
 
«
 
7
.
6
1
 
«
 
2
0
.
5
2
C
D
E
R
2
.
9
3
 
«
 
8
.
5
3
 
«
 
1
9
.
6
7
1
.
6
9
 
«
 
8
.
0
0
 
«
 
1
8
.
8
0
B
l
e
u
S
P
3
.
6
7
 
«
 
9
.
9
3
 
«
 
2
1
.
4
0
3
.
1
6
 
«
 
8
.
2
9
 
«
 
2
0
.
8
0
N
I
S
T
-
v
1
1
b
3
.
8
2
 
«
 
1
1
.
1
3
 
«
 
2
1
.
9
6
4
.
6
4
 
«
 
1
2
.
2
9
 
«
 
2
3
.
3
8
B
L
E
U
-
1
 
(
I
B
M
)
4
.
4
2
 
«
 
1
2
.
4
7
 
«
 
2
2
.
1
8
4
.
9
8
 
«
 
1
4
.
8
7
 
«
 
2
4
.
0
0
B
L
E
U
-
4
 
(
I
B
M
)
6
.
9
3
 
«
 
1
5
.
4
0
 
«
 
2
4
.
6
9
6
.
9
8
 
«
 
1
4
.
3
8
 
«
 
2
5
.
1
1
T
E
R
 
v
0
.
7
.
2
5
8
.
8
7
 
«
 
1
6
.
2
7
 
«
 
2
5
.
2
9
6
.
9
3
 
«
 
1
7
.
3
3
 
«
 
2
4
.
8
0
BL
E
U-
4 
v1
2 
(N
IS
T)
1
0
.
1
6 
«
 
1
8
.
0
2 
«
 
2
7
.
6
4
1
0
.
9
6
 
«
 
1
7
.
8
2
 
«
 
2
8
.
1
6

Table 4: Average Metric Rank in NIST Metrics MATR 2008 Official Results




ing the probability:

p(e1|e2) ≈      p(e1|f  ) ∗ p(f  |e2)
f  

  The corpus used for extraction  was an Arabic- 
English newswire bitext containing a million sen- 
tences. A few examples of the extracted para- 
phrase pairs that were actually  used in a run of 
TERp on the Metrics MATR 2008 development 
set are shown below:
(brief → short) (controversy over → 
polemic about) (by using power → 
by force) (response → reaction)

  A discussion of paraphrase quality  is presented 
in Section 4.1, followed by a brief analysis of the 
effect of varying  the pivot corpus used by the auto- 
matic paraphrase generation upon the correlation 
performance of the TERp metric in Section 4.2.

4.1	Analysis of Paraphrase Quality
We analyzed the utility of the paraphrase probabil- 
ity and found that it was not always a very reliable


estimate of the degree to which the pair was se- 
mantically  related. For example, we looked at all 
paraphrase pairs that had probabilities greater than
0.9, a set that should ideally  contain pairs that are 
paraphrastic to a large degree. In our analysis, we 
found the following five kinds of paraphrases in 
this set:

(a) Lexical Paraphrases.  These paraphrase 
pairs are not phrasal paraphrases but instead 
differ in at most one word and may be con- 
sidered as lexical  paraphrases for all practical 
purposes. While these pairs may not be very 
valuable for TERp due to the obvious overlap 
with WordNet, they may help in increasing 
the coverage of the paraphrastic phenomena 
that TERp can handle. Here are some exam- 
ples:

(2500 polish troops → 2500 polish soldiers) 
(accounting firms → auditing firms) (armed 
source → military source)

(b) Morphological  Variants.   These phrasal 
pairs only differ in the morphological form


for one of the words. As the examples show, 
any knowledge that these pairs may provide 
is already available to TERp via stemming.

(50 ton → 50 tons)
(caused clouds → causing clouds)
(syria deny → syria denies)

(c) Approximate  Phrasal Paraphrases.   This 
set included pairs that only shared partial se- 
mantic content. Most paraphrases extracted 
by the pivot method are expected to be of this 
nature. These pairs are not directly beneficial 
to TERp since they cannot be substituted for 
each other in all contexts. However, the fact 
that they share at least some semantic content 
does suggest that they may not be entirely 
useless either. Examples include:

(mutual proposal → suggest) (them 
were exiled → them abroad) (my 
parents → my father)

(d) Phrasal Paraphrases.  We did indeed find 
a large number of pairs in this set that were 
truly paraphrastic and proved the most useful 
for TERp. For example:

(agence presse → news agency) 
(army roadblock → military barrier) 
(staff walked out → team withdrew)

(e) Noisy Co-occurrences. There are also pairs 
that are  completely unrelated  and happen 
to be extracted  as paraphrases based on the 
noise inherent in the pivoting  process. These 
pairs are much smaller  in number than the 
four sets described above and are not signif- 
icantly detrimental to TERp since they are 
rarely chosen for phrasal substitution. Exam- 
ples:

(counterpart salam → peace) 
(regulation dealing → list) 
(recall one → deported)

  Given this distribution of the pivot-based para- 
phrases, we experimented with a variant of TERp 
that did not use the paraphrase probability at all


can see that this variant works as well as the full 
version of TERp that utilizes  paraphrase probabil- 
ities. This confirms our intuition that the proba- 
bility computed via the pivot-method is not a very 
useful predictor of semantic equivalence for use in 
TERp.

4.2   Varying Paraphrase Pivot Corpora

To determine the effect that the pivot language 
might have on the quality and utility of the ex- 
tracted paraphrases in TERp, we used paraphrase 
pairsmade available by Callison-Burch (2008). 
These paraphrase pairs were extracted from Eu- 
roparl data using each of 10 European languages 
(German, Italian,  French etc.) as a pivot language 
separately and then combining  the extracted para- 
phrase pairs. Callison-Burch (2008) also extracted 
and made available syntactically  constrained para- 
phrase  pairs from the same  data  that are  more 
likely to be semantically related.
  We used both sets of paraphrases in TERp  as al- 
ternatives to the paraphrase pairs that we extracted 
from the Arabic newswire bitext. The results are 
shown in the last four rows of Table 5 and show 
that using a pivot language other than the one that 
the MT system is actually translating yields results 
that are almost  as good. It also shows that the 
syntactic constraints imposed by Callison-Burch 
(2008) on the pivot-based  paraphrase extraction 
process are useful and yield improved  results over 
the baseline pivot-method.  The results further sup- 
port our claim that the pivot paraphrase probability 
is not a very useful indicator of semantic related- 
ness.

5   Varying Human Judgments

To evaluate the differences between human judg- 
ment types we first align the hypothesis to the ref- 
erences using a fixed set of edit costs, identical to 
the weights in Table 1, and then optimize the edit 
costs to maximize the correlation, without  realign- 
ing. The separation of the edit costs used for align- 
ment from those used for scoring allows us to re- 
move the confusion of edit costs selected for align- 
ment purposes from those selected to increase cor- 
relation.
For Adequacy and Fluency judgments, the
4


but instead only used the actual edit distance be-


MTEval 2002 human  judgement  set


was used.


tween the two phrases to determine the final cost 
of a phrase substitution.  The results for this exper- 
iment are shown in the second row of Table 5. We


This set consists  of the output of ten MT sys-
tems, 3 Arabic-to-English systems and 7 Chinese-

4 Distributed to the authors by request from NIST.




P
a
r
a
p
h
r
a
s
e
 
S
e
t
u
p
P
e
a
r
s
o
n
Seg 	Doc	Sys
S
p
e
a
r
m
a
n
Seg 	Doc	Sys
Ar
ab
ic 
pi
vo
t
-
0.
7
8
7
-
0.
91
8
-
0.
98
5
-
0.
7
3
7
-
0.
81
8
-
0.
88
1
Ar
ab
ic 
pi
vo
t 
an
d 
no 
pr
ob
-
0.
7
8
7
-
0.
93
3
-
0.
98
6
-
0.
7
3
7
-
0.
84
1
-
0.
88
1
Eu
ro
pa
rl 
pi
vo
t
-
0.
7
7
5
-
0.
94
0
-
0.
98
3
-
0.
7
3
8
-
0.
86
5
-
0.
90
5
Eu
ro
pa
rl 
pi
vo
t 
an
d 
no 
pr
ob
-
0.
7
7
5
-
0.
94
0
-
0.
98
3
-
0.
7
3
7
-
0.
86
0
-
0.
90
5
Eu
ro
pa
rl 
pi
vo
t 
an
d 
sy
nt
act
ic 
co
ns
tra
int
s
-
0.
7
8
1
-
0.
94
1
-
0.
98
5
-
0.
7
3
9
-
0.
85
9
-
0.
88
1
Eu
ro
pa
rl 
piv
ot, 
sy
nt
ac
tic 
co
ns
tra
int
s 
an
d 
no 
pr
ob
-
0.
7
7
9
-
0.
94
6
-
0.
98
5
-
0.
7
3
7
-
0.
86
6
-
0.
97
6

Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.


H
u
m
a
n
J
u
d
g
m
e
nt

M
at
ch

In
se
rt

D
el
eti
o
n

S
u
bs
t.

St
e
m

S
yn
.

S
hi
ft
Phras
e 
Substi
tution
w1	w2	w3	w4
Ali
gn
m
en
t
0
.
0
0
.
2
6
1
.
4
3
1
.
5
6
0
.
0
0.
0
0.
5
6
-
0.
23
-
0.
15
-
0.
08
0.
1
8
A
d
e
q
u
ac
y
0
.
0
0
.
1
8
1
.
4
2
1
.
7
1
0
.
0
0.
0
0.
1
9
-
0.
38
-
0.
03
0.
2
2
0.
4
7
F
l
u
e
n
c
y
0
.
0
0
.
1
2
1
.
3
7
1
.
8
1
0
.
0
0.
0
0.
4
3
-
0.
63
-
0.
07
0.
1
2
0.
4
6
H
T
E
R
0
.
0
0
.
8
4
0
.
7
6
1
.
5
5
0.
9
0
0.
75
1.
0
7
-
0.
03
-
0.
17
-
0.
08
-
0.
09

Table 6: Optimized  Edit Costs




to-English  systems, consisting of a total, across 
all systems and both language pairs, of 7,452 seg- 
ments across 900 documents. To evaluate HTER, 
the GALE (Olive,  2005) 2007 (Phase 2.0) HTER 
scores were used. This set consists  of the out- 
put of 6 MT systems, 3 Arabic-to-English  systems 
and 3 Chinese-to-English  systems, although each 
of the systems in question is the product of system 
combination.  The HTER data consisted of a total, 
across all systems and language pairs, of 16,267 
segments across a total of 1,568 documents. Be- 
cause  HTER annotation is especially expensive 
and difficult, it is rarely performed, and the only 
source,  to the authors’ knowledge,  of available 
HTER annotations is on GALE evaluation data for 
which no Fluency and Adequacy judgments have 
been made publicly  available.

  The edit costs learned for each of these human 
judgments, along with the alignment edit costs are 
shown in Table 6. While all three types of human 
judgements differ from the alignment  costs used 
in alignment, the HTER edit costs differ most sig- 
nificantly. Unlike Adequacy and Fluency which 
have a low edit cost for insertions and a very high 
cost for deletions, HTER has a balanced cost for 
the two edit types. Inserted words are strongly pe- 
nalized against in HTER, as opposed  to in Ade- 
quacy and Fluency,  where such errors are largely 
forgiven.  Stem and synonym edits are also penal- 
ized against while these are considered equivalent


to a match for both Adequacy and Fluency.  
This penalty against stem matches can be 
attributed  to Fluency requirements in 
HTER that specifically penalize against 
incorrect morphology. The cost of shifts 
is also increased in HTER, strongly penal- 
izing the movement of phrases within the 
hypoth- esis, while Adequacy and Fluency  
give  a much lower cost to such errors. 
Some of the differences between HTER 
and both fluency and adequacy can be 
attributed to the different  systems used. The 
MT systems evaluated with HTER are all 
highly performing  state of the art systems, 
while the sys- tems used for adequacy and 
fluency are older MT systems.





  The differences between Adequacy and 
Fluency are smaller,  but there are still 
significant differ- ences. In particular, the 
cost of shifts is over twice as high for the 
fluency optimized system than the 
adequacy optimized  system,  indicating 
that the movement of phrases, as expected, 
is only slightly penalized when judging 
meaning, but can be much more harmful to 
the fluency of a translation.   Flu- ency 
however favors  paraphrases more 
strongly than the edit costs optimized  for 
adequacy.  This might  indicate that 
paraphrases are used to gener- ate a more 
fluent translation although at the poten- tial 
loss of meaning.


6   Discussion

We introduced a new evaluation metric, TER-Plus, 
and showed that it is competitive with state-of-the- 
art evaluation metrics when its predictions are cor- 
related with human judgments.  The inclusion  of 
stem, synonym and paraphrase edits allows  TERp 
to overcome some of the weaknesses of the TER 
metric and better align hypothesized translations 
with reference translations.  These new edit costs 
can then be optimized  to allow better correlation 
with human judgments.  In addition, we have ex- 
amined the use of other paraphrasing techniques, 
and shown that the paraphrase probabilities  esti- 
mated by the pivot-method may not be fully ad- 
equate for judgments of whether  a paraphrase in 
a translation  indicates a correct translation.   This 
line of research holds promise as an external eval- 
uation method of various paraphrasing methods.
  However  promising  correlation  results for an 
evaluation metric may be, the evaluation of the 
final output of an MT system is only a portion 
of the utility of an automatic translation metric. 
Optimization of the parameters of an MT system 
is now done using automatic metrics, primarily 
BLEU. It is likely that some features that make an 
evaluation metric good for evaluating the final out- 
put of a system would  make it a poor metric  for use 
in system tuning. In particular, a metric may have 
difficulty distinguishing  between outputs of an MT 
system that been optimized  for that same metric. 
BLEU, the metric most frequently  used to opti- 
mize systems, might therefore perform poorly in 
evaluation tasks compared to recall oriented met- 
rics such as METEOR  and TERp (whose tuning 
in Table 1 indicates  a preference towards recall). 
Future  research into the use of TERp and other 
metrics as optimization metrics is needed to better 
understand these metrics and the interaction  with 
parameter optimization.
  Finally, we explored  the difference  between 
three  types of human judgments  that are  often 
used to evaluate both MT systems and automatic 
metrics, by optimizing TERp to these human 
judgments and examining the resulting edit costs. 
While this can make no judgement as to the pref- 
erence of one type of human judgment over an- 
other, it indicates  differences  between these hu- 
man judgment  types, and in particular the differ- 
ence between HTER and Adequacy  and Fluency. 
This exploration is limited by the the lack of a 
large amount of diverse data annotated for all hu-


man judgment  types,  as well as the small num- 
ber of edit types used by TERp. The inclusion 
of additional more specific edit types could lead 
to a more detailed understanding of which trans- 
lation phenomenon and translation errors are most 
emphasized or ignored by which types of human 
judgments.

Acknowledgments

This work was supported, in part, by BBN Tech- 
nologies under the GALE Program, DARPA/IPTO 
Contract No. HR0011-06-C-0022  and in part by 
the Human Language Technology  Center of Ex- 
cellence.. TERp is available on the web for down-
load at: http://www.umiacs.umd.edu/∼snover/terp/.


References

Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im- 
proved Correlation with Human Judgments. In Pro- 
ceedings of the ACL 2005 Workshop on Intrinsic and 
Extrinsic Evaulation Measures for MT and/or Sum- 
marization.

Colin Bannard and Chris Callison-Burch.  2005. Para- 
phrasing with Bilingual Parallel Corpora. In Pro- 
ceedings of the 43rd Annual Meeting of the Asso- 
ciation for Computational Linguistics (ACL 2005), 
pages 597–604, Ann Arbor, Michigan, June.

Chris Callison-Burch.  2008. Syntactic constraints on 
paraphrases extracted from parallel corpora.  In Pro- 
ceedings of the 2008 Conference on Empirical Meth- 
ods in Natural Language  Processing,  pages 196–
205, Honolulu, Hawaii, October. Association  for
Computational Linguistics.

Christiane Fellbaum.      1998.      WordNet:    An 
Electronic Lexical Database.      MIT  Press. 
http://www.cogsci.princeton.edu/˜wn [2000, 
September 7].

David Kauchak and Regina Barzilay.  2006.  Para- 
phrasing for Automatic Evaluation. In Proceedings 
of the Human Language Technology Conference of 
the North  American  Chapter of the ACL, pages 455–
462.

Gregor Leusch,  Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using Block 
Movements. In Proceedings of the 11th Confer- 
enceof the European Chapter of the Association for 
Computational Linguistics (EACL 2006).

V. I. Levenshtein.  1966. Binary Codes Capable of Cor- 
recting Deletions, Insertions, and Reversals. Soviet 
Physics Doklady, 10:707–710.


Daniel Lopresti and Andrew Tomkins. 1997. Block 
edit models for approximate string matching. Theo- 
retical Computer Science, 181(1):159–179, July.

Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and 
Bonnie  J. Dorr.  2007. Using paraphrases for pa- 
rameter tuning in statistical machine translation.  In 
Proceedings of the Workshop on Statistical Machine 
Translation, Prague, Czech Republic,  June. Associ- 
ation for Computational Linguistics.

Nitin Madnani, Philip Resnik, Bonnie J.  Dorr, and 
Richard Schwartz. 2008. Are Multiple Reference 
Translations  Necessary? Investigating  the Value 
of Paraphrased Reference Translations  in Parameter 
Optimization. In Proceedings of the Eighth Confer- 
ence of the Association for Machine Translation in 
the Americas, October.

S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000. An 
evaluation tool for machine translation: Fast evalua- 
tion for MT research. In Proceedings of the 2nd In- 
ternational Conference on Language Resources and 
Evaluation (LREC-2000), pages 39–45.

Joseph Olive.   2005. Global Autonomous Language 
Exploitation (GALE).  DARPA/IPTO Proposer In- 
formation Pamphlet.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei- 
Jing Zhu.  2002.  Bleu: a Method  for Automatic 
Evaluation of Machine Traslation. In Proceedings 
of the 40th Annual Meeting of the Association for 
Computational Linguistics.

Martin F. Porter. 1980. An algorithm for suffic strip- 
ping. Program, 14(3):130–137.

Mark	 Przybocki,		Kay		Peterson,		and	Sebas- 
tian		Bronsart.			2008.		Official	 results 
of	the	NIST	 2008	”Metrics		for	MAchine 
TRanslation”	Challenge	(MetricsMATR08). 
http://nist.gov/speech/tests/metricsmatr/2008/results/, 
October.

Antti-Veikko Rosti, Spyros Matsoukas,  and Richard 
Schwartz. 2007. Improved word-level system com- 
bination for machine translation. In Proceedings 
of the 45th Annual Meeting of the Association of 
Computational Linguistics, pages 312–319, Prague, 
Czech Republic, June. Association  for Computa- 
tional Linguistics.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin- 
nea Micciulla, and John Makhoul. 2006. A Study 
of Translation Edit Rate with Targeted Human An- 
notation. In Proceedings of Association for Machine 
Translation in the Americas.

Liang Zhou, Chon-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating  Machine Translation Results with
Paraphrase  Support.  In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan- 
guage Processing (EMNLP 2006), pages 77–84.

