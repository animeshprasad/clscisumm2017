Machine Translation System Combination by Confusion Forest




Taro Watanabe and Eiichiro Sumita
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, 619-0289 JAPAN
{taro.watanabe,eiichiro.sumita}@nict.go.jp









Abstract

The state-of-the-art system combination 
method  for  machine  translation  (MT)  is 
based  on  confusion  networks  constructed 
by aligning hypotheses with regard to word 
similarities.   We introduce a novel system 
combination framework in which hypotheses 
are encoded as a confusion forest, a packed 
forest representing alternative trees.    The 
forest is generated using syntactic consensus 
among parsed hypotheses: First, MT outputs 
are parsed. Second, a context free grammar is 
learned by extracting a set of rules that con- 
stitute the parse trees. Third, a packed forest 
is generated starting from the root symbol of 
the extracted grammar through non-terminal 
rewriting.   The new hypothesis is produced 
by searching the best derivation in the forest. 
Experimental results on the WMT10 system 
combination shared task yield comparable 
performance to the conventional confusion 
network based method with smaller space.


1   Introduction

System combination techniques take the advantages 
of consensus among multiple systems and have been 
widely used in fields, such as speech recognition 
(Fiscus, 1997; Mangu et al., 2000) or parsing (Hen- 
derson and Brill, 1999). One of the state-of-the-art 
system combination methods for MT is based on 
confusion networks, which are compact graph-based 
structures representing multiple hypotheses (Banga- 
lore et al., 2001).
  Confusion  networks  are  constructed  based  on 
string similarity information. First, one skeleton or


backbone sentence is selected. Then, other hypothe- 
ses are aligned against the skeleton, forming a lattice 
with each arc representing alternative word candi- 
dates. The alignment method is either model-based 
(Matusov et al., 2006; He et al., 2008) in which a 
statistical word aligner is used to compute hypothe- 
sis alignment, or edit-based (Jayaraman and Lavie,
2005; Sim et al., 2007) in which alignment is mea- 
sured by an evaluation metric, such as translation er- 
ror rate (TER) (Snover et al., 2006). The new trans- 
lation hypothesis is generated by selecting the best 
path through the network.

  We present a novel method for system combina- 
tion which exploits the syntactic similarity of system 
outputs. Instead of constructing a string-based con- 
fusion network, we generate a packed forest (Billot 
and Lang, 1989; Mi et al., 2008) which encodes ex- 
ponentially many parse trees in a polynomial space. 
The packed forest, or confusion forest, is constructed 
by merging the MT outputs with regard to their 
syntactic consensus.  We employ a grammar-based 
method to generate the confusion forest: First, sys- 
tem outputs are parsed.  Second, a set of rules are 
extracted from the parse trees. Third, a packed for- 
est is generated using a variant of Earley’s algorithm 
(Earley, 1970) starting from the unique root symbol. 
New hypotheses are selected by searching the best 
derivation in the forest. The grammar, a set of rules, 
is limited to those found in the parse trees.  Spuri- 
ous ambiguity during the generation step is further 
reduced by encoding the tree local contextual infor- 
mation in each non-terminal symbol, such as parent 
and sibling labels, using the state representation in 
Earley’s algorithm.




1249


Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249–1257, 
Portland, Oregon, June 19-24, 2011. Qc 2011 Association for Computational Linguistics


  Experiments were carried out for the system 
combination task of the fifth workshop on sta- 
tistical machine translation (WMT10) in four di-
rections,  {Czech,  French,  German,  Spanish}-to-
English (Callison-Burch et al., 2010), and we found
comparable performance to the conventional con- 
fusion network based system combination in two 
language pairs, and statistically significant improve-



*  I	saw	the	forest
I  walked  the  blue  forest
I	saw	the	green  trees
the	forest   was   found

(a) Pairwise alignment using the first starred hypothesis as a 
skeleton.


ments in the others.
  First, we will review the state-of-the-art method 
which is a system combination framework based on
confusion networks (§2).  Then, we will introduce


I	saw

ϵ
ϵ	walked



the


blue


ϵ


forest


green


trees

ϵ
was


found


ϵ


a novel system combination method based on con-


(b) Confusion network from (a)


fusion forest (§3) and present related work in con- 
sensus translations (§4). Experiments are presented
in Section 5 followed by discussion and our conclu-
sion.



I	saw

ϵ
ϵ	walked




the



blue


green



forest


trees



was


ϵ



found


ϵ



2   Combination by Confusion Network

The system combination framework based on confu- 
sion network starts from computing pairwise align- 
ment between hypotheses by taking one hypothe- 
sis as a reference.  Matusov et al. (2006) employs 
a model based approach in which a statistical word 
aligner, such as GIZA++ (Och and Ney, 2003), is 
used to align the hypotheses. Sim et al. (2007) in- 
troduced TER (Snover et al., 2006) to measure the 
edit-based alignment.
  Then, one hypothesis is selected, for example by 
employing a minimum Bayes risk criterion (Sim et 
al., 2007), as a skeleton, or a backbone, which serves 
as a building block for aligning the rest of the hy- 
potheses. Other hypotheses are aligned against the 
skeleton using the pairwise alignment. Figure 1(b) 
illustrates an example of a confusion network con- 
structed from the four hypotheses in Figure 1(a), as- 
suming the first hypothesis is selected as our skele- 
ton.  The network consists of several arcs, each of 
which represents an alternative word at that position, 
including the empty symbol, ϵ.
  This pairwise alignment strategy is prone to spu- 
rious insertions and repetitions due to alignment er- 
rors such as in Figure 1(a) in which “green” in the 
third hypothesis is aligned with “forest” in the skele- 
ton.  Rosti et al. (2008) introduces an incremental 
method so that hypotheses are aligned incremen- 
tally to the growing confusion network, not only the


(c) Incrementally constructed confusion network

Figure 1: An example confusion network construc- 
tion


skeleton hypothesis. In our example, “green trees”
is aligned with “blue forest” in Figure 1(c).
  The confusion network construction is largely in- 
fluenced by the skeleton selection, which determines 
the global word reordering of a new hypothesis. For 
example, the last hypothesis in Figure 1(a) has a pas- 
sive voice grammatical construction while the others 
are active voice. This large grammatical difference 
may produce a longer sentence with spuriously in- 
serted words, as in “I saw the blue trees was found” 
in Figure 1(c).  Rosti et al. (2007b) partially re- 
solved the problem by constructing a large network 
in which each hypothesis was treated as a skeleton 
and the multiple networks were merged into a single 
network.

3   Combination by Confusion Forest

The confusion network approach to system com- 
bination encodes multiple hypotheses into a com- 
pact lattice structure by using word-level consensus. 
Likewise, we propose to encode multiple hypothe- 
ses into a confusion forest, which is a packed forest 
which represents multiple parse trees in a polyno- 
mial space (Billot and Lang, 1989; Mi et al., 2008) 
Syntactic consensus is realized by sharing tree frag-



S@ϵ






VP@2


Initialization:




Scan:



[TOP → •S, 0] : ¯1


NP@1 VBD@3 VP@4



VBD@2.1


[X → α • xβ, h] : u
[X → αx • β, h] : u


PRP DT


NN was


VBN
walke
d   
saw  
NP@2.
2


Predict:


I	the


forest


DT


found


JJ





DT@2.2.1 
NN@2.2.2


[X 
→ 
α • 
Yβ, 
h]
[Y → 
•γ, h 
+ 1] : 
u
Complete:


u
→ γ ∈ 
G , h < 
H





the


blue   green NN


the


forest


[X → α • 
Yβ, h] : u 	[Y → γ•, h + 1] : v
[
X
 
→
 
α
Y
 
•
 
β
,
 
h
]
 
:
 
u
 
⊗
 
v


forest   trees

Figure 2: An example packed forest representing hy- 
potheses in Figure 1(a).



Goal:


[TOP → S•, 0]




ments among parse trees. The forest is represented 
as a hypergraph which is exploited in parsing (Klein 
and Manning, 2001; Huang and Chiang, 2005) and 
machine translation (Chiang, 2007; Huang and Chi- 
ang, 2007).
More formally,  a  hypergraph is  a  pair ⟨V, E⟩
where V is the set of nodes and E is the set of hy-
peredges.  Each node in V  is represented as X @p
where X  ∈   N   is a non-terminal symbol and p
is an address (Shieber et al., 1995) that encapsu-
lates each node id relative to its parent.  The root 
node is given the address ϵ and the address of the 
first child of node p is given p.1.  Each hyperedge
e ∈  E  is represented as a pair ⟨head(e), tails(e)⟩
where head(e)  ∈ V is a head node and tails(e) ∈
V ∗  is  a  list  of  tail  nodes,  corresponding to  the
left-hand side and the right-hand side of an in- 
stance of a rule in a CFG, respectively.  Figure 2 
presents an example packed forest for the parsed 
hypotheses in  Figure 1(a).    For example,  VP@2
has two hyperedges, ⟨VP@2 , (VBD@3 , VP@4 )⟩ and
⟨VP@2 , (VBD@2.1 , NP@2.2)⟩,   leading  to  different
derivations where the former takes the grammatical
construction in passive voice while the latter in ac- 
tive voice.
  Given system outputs, we employ the following 
grammar based approach for constructing a confu- 
sion forest: First, MT outputs are parsed.  Second,



Figure 3: The deductive system for Earley’s genera- 
tion algorithm


a grammar is learned by treating each hyperedge as 
an instance of a CFG rule.  Third, a forest is gen- 
erated from the unique root symbol of the extracted 
grammar through non-terminal rewriting.

3.1   Forest Generation
Given the extracted grammar, we apply a variant of 
Earley’s algorithm (Earley, 1970) which can gener- 
ate strings in a left-to-right manner from the unique 
root symbol, TOP. Figure 3 presents the deductive 
inference rules (Goodman, 1999) for our generation
algorithm. We use capital letters X ∈ N  to denote 
non-terminals and x ∈ T  for terminals. Lowercase
Greek letters α, β and γ are strings of terminals and
non-terminals (T  ∪ N )∗. u and v are weights asso-
ciated with each item.
  The major difference compared to Earley’s pars- 
ing algorithm is that we ignore the terminal span in- 
formation each non-terminal covers and keep track 
of the height of derivations by h.   The scanning 
step will always succeed by moving the dot to the 
right. Combined with the prediction and completion 
steps, our algorithm may potentially generate a spu- 
riously deep forest. Thus, the height of the forest is 
constrained in the prediction step not to exceed H , 
which is empirically set to 1.5 times the maximum


height of the parsed system outputs.

3.2   Tree Annotation

The grammar compiled from the parsed trees is lo- 
cal in that it can represent a finite number of sen- 
tences translated from a specific input sentence. Al- 
though its coverage is limited, our generation algo-





NP PRP I


S



VBD

saw


VP DT
the







NP

NN

forest


rithm may yield a spuriously large forest. As a way
to reduce spurious ambiguities, we relabel the non- 
terminal symbols assigned to each parse tree before 
extracting rules.
Here, we replace each non-terminal symbol by	•S



(a) A parse tree for “I saw the forest”
•S
•S


the state representation of Earley’s algorithm corre-
sponding to the sequence of prediction steps starting 
from TOP.  Figure 4(a) presents an example parse 
tree with each symbol replaced by the Earley’s state


+ • NP : VP
•S 	•S
+ • NP : VP  +NP : •VP


+NP : •VP
•S
+NP : •VP


in Figure 4(b).  For example, the label for VBD is
replaced by •S + NP : •VP + •VBD : NP which


+ • PRP
I


+ • VBD : NP
saw 	•S


+VBD : •NP
•S


corresponds to the prediction steps of TOP →  •S,


+NP : •VP


+NP : •VP


S →  NP  • VP and VP →  •VBD NP. The context


+VBD : •NP  +VBD : •NP


represented in the Earley’s state is further limited by 
the vertical and horizontal Markovization (Klein and


+ • DT : NN
the


+DT : •NN
forest


Manning, 2003).  We define the vertical order v in
which the label is limited to memorize only v pre- 
vious prediction steps. For instance, setting v = 1
yields NP  :  •VP + •VBD  :  NP in our example.
Likewise, we introduce the horizontal order h which
limits the number of sibling labels memorized on the 
left and the right of the dotted label. Limiting h = 1 
implies that each deductive step is encoded with at 
most three symbols.
  No limits in the horizontal and vertical 
Markovization   orders   implies   memorizing   of 
all the deductions and yields a confusion forest 
representing the union of parse trees through the 
grammar collection and the generation processes. 
More relaxed horizontal orders allow more reorder- 
ing of subtrees in a confusion forest by discarding 
the sibling context in each prediction step.   Like- 
wise, constraining the vertical order generates a 
deeper forest by ignoring the sequence of symbols 
leading to a particular node.

3.3   Forest Rescoring

From the packed forest F , new k-best derivations 
are extracted from all possible derivations D by 
efficient forest-based algorithms for k-best parsing 
(Huang and Chiang, 2005). We use a linear combi-



(b) Earley’s state annotated tree for (a). The sub-labels in 
bold- face indicate the original labels.

Figure 4:  Label annotation by Earley’s alsogirhtm 
state


nation of features as our objective function to seek 
for the best derivation dˆ:
dˆ = arg max w⊤ · h(d, F ) 	(1)
d∈D

where h(d, F ) is a set of feature functions 
scaled by weight vector w. We use cube-pruning 
(Chiang,
2007; Huang and Chiang, 2007) to 
approximately intersect with non-local features, 
such as n-gram language models.  Then, k-best 
derivations are ex- tracted from the rescored forest 
using algorithm 3 of Huang and Chiang (2005).

4   Related 
Work

Consensus translations have been extensively stud- 
ied with many granularities.  One of the simplest 
forms is a sentence-based combination in which 
hypotheses are simply reranked without merging 
(Nomoto, 2004). Frederking and Nirenburg 
(1994)



proposed a phrasal combination by merging hy- 
potheses in a chart structure, while others depended 
on confusion networks, or similar structures, as a 
building block for merging hypotheses at the word 
level (Bangalore et al., 2001; Matusov et al., 2006; 
He et al., 2008; Jayaraman and Lavie, 2005; Sim 
et al., 2007). Our work is the first to explicitly ex- 
ploit syntactic similarity for system combination by 
merging hypotheses into a syntactic packed forest. 
The confusion forest approach may suffer from pars- 
ing errors such as the confusion network construc- 
tion influenced by alignment errors. Even with pars- 
ing errors, we can still take a tree fragment-level 
consensus as long as a parser is consistent in that 
similar syntactic mistakes would be made for simi- 
lar hypotheses.

  Rosti et al. (2007a) describe a re-generation ap- 
proach to consensus translation in which a phrasal 
translation table is constructed from the MT outputs 
aligned with an input source sentence. New transla- 
tions are generated by decoding the source sentence 
again using the newly extracted phrase table.  Our 
grammar-based approach can be regarded as a re- 
generation approach in which an off-the-shelf mono- 
lingual parser, instead of a word aligner, is used to 
annotate syntactic information to each hypothesis, 
then, a new translation is generated from the merged 
forest, not from the input source sentence through 
decoding.  In terms of generation, our approach is 
an instance of statistical generation (Langkilde and 
Knight, 1998; Langkilde, 2000).  Instead of gener- 
ating forests from semantic representations (Langk- 
ilde, 2000), we generate forests from a CFG encod- 
ing the consensus among parsed hypotheses.

  Liu et al. (2009) present joint decoding in which 
a translation forest is constructed from two distinct 
MT systems, tree-to-string and string-to-string, by 
merging forest outputs. Their merging method is ei- 
ther translation-level in which no new translation is 
generated, or derivation-level in that the rules shar- 
ing the same left-hand-side are used in both sys- 
tems. While our work is similar in that a new forest 
is constructed by sharing rules among systems, al- 
though their work involves no consensus translation 
and requires structures internal to each system such 
as model combinations (DeNero et al., 2010).




cz
-
en   
de
-
en   
es
-
en	fr-en
# 
of 
sy
ste
ms
6
	
1
6
	
8
	
1
4
avg
. 
wo
rds 
tun
e
tes
t
10
.6
K  
10
.9
K  
10
.9
K  
11
.0
K
50
.5
K  
52
.1
K  
52
.1
K  
52
.4
K
sen
ten
ces   
tun
e
tes
t
4
5
5
2
,
0
3
4

Table 1: WMT10 system combination tuning/testing 
data


5   Experiments

5.1   Setup

We  ran  our  experiments  for  the  WMT10  sys- 
tem combination task usinge four language pairs,
{Czech,    French,   German,   Spanish}-to-English
(Callison-Burch et al., 2010).  The data is summa-
rized in Table 1.  The system outputs are retok- 
enized to match the Penn-treebank standard, parsed 
by the Stanford Parser (Klein and Manning, 2003), 
and lower-cased.
  We   implemented   our   confusion   forest   sys- 
tem combination using an in-house developed 
hypergraph-based toolkit cicada which is motivated 
by  generic weighted logic  programming (Lopez,
2009), originally developed for a synchronous-CFG 
based machine translation system (Chiang, 2007). 
Input to our system is a collection of hypergraphs, 
a set of parsed hypotheses, from which rules are ex- 
tracted and a new forest is generated as described 
in Section 3. Our baseline, also implemented in ci- 
cada, is a confusion network-based system combi-
nation method (§2) which incrementally aligns hy-
potheses to the growing network using TER (Rosti
et al., 2008) and merges multiple networks into a 
large single network.  After performing epsilon re- 
moval, the network is transformed into a forest by
parsing with monotone rules of S →  X, S →  S X
and X →  x. k-best translations are extracted from
the forest using the forest-based algorithms in Sec-
tion 3.3.


5.2   Features

The feature weight vector w in Equation 1 is tuned 
by MERT over hypergraphs (Kumar et al., 2009).
We use three lower-cased 5-gram language mod-


els hi


(d):  
English 
Gigawor
d Fourth 
edition1, 
the


English side of French-English 109  corpus and the 
news commentary English data2.  The count based 
features ht (d) and he (d) count the number of ter- 
minals and the number of hyperedges in d, respec- 
tively.  We employ M  confidence measures hm (d) 
for M systems, which basically count the number of 
rules used in d originally extracted from mth system 
hypothesis (Rosti et al., 2007a).
  Following Macherey and Och (2007), BLEU (Pa- 
pineni et al., 2002) correlations are also incorporated 
in our system combination. Given M system outputs 
e1...eM , M BLEU scores are computed for d using 
each of the system outputs em as a reference








∞










Table 2:  Translation results in lower-case BLEU.


( 1
b (d) = BP (e, em ) · exp	4


4
∑

n=1


)

log ρn (e, em )


CN for confusion network and 
CF for confusion forest with 
different vertical (v) and 
horizontal (h) Markovization 
order.


where e = yield(d) is a terminal yield of d, BP (·)
and ρn(·) respectively denote brevity penalty and
n-gram precision.  Here, we use approximated un-
clipped n-gram counts (Dreyer et al., 2007) for com-
puting ρn (·) with a compact state representation (Li
and Khudanpur, 2009).
  Our baseline confusion network system has an ad- 
ditional penalty feature, hp (m),  which is the total 
edits required to construct a confusion network us- 
ing the mth system hypothesis as a skeleton, normal- 
ized by the number of nodes in the network (Rosti et 
al., 2007b).

5.3   Results

Table 2 compares our confusion forest approach 
(CF) with different orders, a confusion network 
(CN) and max/min systems measured by BLEU (Pa- 
pineni et al., 2002). We vary the horizontal orders,
h  = 1, 2, ∞  with vertical orders of v  = 3, 4, ∞.
Systems without statistically significant differences
from the best result (p < 0.05) are indicated by bold
face. Setting v = ∞  and h = ∞  achieves compa-
rable performance to CN. Our best results in three
languages come from setting v  = ∞  and h  = 2,
which favors little reordering of phrasal structures.
In general, lower horizontal and vertical order leads 
to lower BLEU.
1 LDC catalog No. LDC2009T13
    2 Those data are available from http://www.statmt. 
org/wmt10/.




la
ng
ua
ge
cz
-
en	de-en	es-en	fr-en
rer
an
k
29
.4
0	32.32	36.83	36.59
C
N
38
.5
2	34.97	47.65	46.37
C
Fv
=
∞,
h=
∞
C
Fv
=
∞,
h=
2
C
Fv
=
∞,
h=
1
30
.5
1	34.07	38.69	38.94
30
.6
1	34.25	38.87	39.10
31
.0
9	34.65	39.27	39.51
C
Fv
=4
,h
=
∞
C
Fv
=4
,h
=2
C
Fv
=4
,h
=1
30
.8
6	34.19	39.17	39.39
30
.9
6	34.32	39.35	39.57
31
.4
4	34.62	39.69	39.90
C
Fv
=3
,h
=
∞
C
Fv
=3
,h
=2
C
Fv
=3
,h
=1
31
.0
3	34.30	39.29	39.57
31
.2
5	34.97	39.61	40.00
31
.5
5	34.60	39.72	39.97

Table 3: Oracle lower-case BLEU



  Table 3 presents oracle BLEU achievable by each 
combination method.  The gains achievable by the 
CF over simple reranking are small, at most 2-3 
points, indicating that small variations are encoded 
in confusion forests. We also observed that a lower 
horizontal and vertical order leads to better BLEU 
potentials.   As briefly pointed out in Section 3.2, 
the higher horizontal and vertical order implies more 
faithfulness to the original parse trees. Introducing 
new tree fragments to confusion forests leads to new 
phrasal translations with enlarged forests, as pre- 
sented in Table 4, measured by the average number



la
ng
c
z
-
e
n	de-en	es-en	fr-en
C
N
l
a
tt
i
c
e
2,2
22.
68  
47,
23
1.2
0  
2,9
32.
24  
11,
96
9.4
0
1,7
23.
91  
41,
40
3.9
0  
2,3
30.
04  
10,
11
9.1
0
C
Fv
=
∞
C
Fv
=4
C
Fv
=3
2
3
0
.
0
8	540.03	262.30	386.79
2
5
4
.
4
5	651.10	302.01	477.51
2
8
6
.
0
1	802.79	349.21	575.17



Table 4:  Hypegraph size measured by the average 
number of hyperedges (h = 1 for CF). “lattice” is 
the average number of edges in the original CN.


of hyperedges3. The larger potentials do not imply 
better translations, probably due to the larger search 
space with increased search errors. We also conjec- 
ture that syntactic variations were not captured by 
the n-gram like string-based features in Section 5.2, 
therefore resulting in BLEU loss, which will be in- 
vestigated in future work.
  In contrast, CN has more potential for generat- 
ing better translations, with the exception of the 
German-to-English direction, with scores that are 
usually 10 points better than simple sentence-wise 
reranking.  The low potential in German should be 
interpreted in the light of the extremely large confu- 
sion network in Table 4.  We postulate that the di- 
vergence in German hypotheses yields wrong align- 
ments, and therefore amounts to larger networks 
with incorrect hypotheses. Table 4 also shows that 
CN produces a forest that is an order of magnitude 
larger than those created by CFs. Although we can- 
not directly relate the runtime and the number of 
hyperedges in CN and CFs, since the shape of the 
forests are different, CN requires more space to en- 
code the hypotheses than those by CFs.
  Table 5 compares the average length of the min- 
imum/maximum hypothesis that each method can 
produce.   CN may generate shorter hypotheses, 
whereby CF prefers longer hypotheses as we de- 
crease the vertical order.  Large divergence is also 
observed for German, such as for hypergraph size.

6   Conclusion

We presented a confusion forest based method for 
system combination in which system outputs are 
merged into a packed forest using their syntactic

  3 We measure the hypergraph size before intersecting with 
non-local features, like n-gram language models.








Table 5:  Average min/max hypothesis length pro- 
ducible by each method (h = 1 for CF).


similarity.   The forest construction is treated as a 
generation from a CFG compiled from the parsed 
outputs. Our experiments indicate comparable per- 
formance to a strong confusion network baseline 
with smaller space, and statistically significant gains 
in some language pairs.
  To our knowledge, this is the first work to directly 
introduce syntactic consensus to system combina- 
tion by encoding multiple system outputs into a sin- 
gle forest structure.  We believe that the confusion 
forest based approach to system combination has 
future exploration potential.  For instance, we did 
not employ syntactic features in Section 5.2 which 
would be helpful in discriminating hypotheses in 
larger forests.  We would also like to analyze the 
trade-offs, if any, between parsing errors and confu- 
sion forest constructions by controlling the parsing 
qualities.  As an alternative to the grammar-based 
forest generation, we are investigating an edit dis- 
tance measure for tree alignment, such as tree edit 
distance (Bille, 2005) which basically computes in- 
sertion/deletion/replacement of nodes in trees.

Acknowledgments

We would like to thank anonymous reviewers and 
our colleagues for helpful comments and discussion.


References

Srinivas Bangalore, German Bordel, and Giuseppe Ric- 
cardi.  2001.  Computing consensus translation from 
multiple machine translation systems. In Proceedings 
of Automatic Speech Recognition and Understanding 
(ASRU), 2001, pages 351 – 354.


Philip Bille.  2005.  A survey on tree edit distance and 
related problems.  Theor. Comput. Sci., 337:217–239, 
June.
Sylvie Billot and Bernard Lang.   1989.   The structure 
of shared forests in ambiguous parsing.  In Proceed- 
ings of the 27th Annual Meeting of the Association for 
Computational Linguistics, pages 143–151, Vancou- 
ver, British Columbia, Canada, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz, 
Kay Peterson,  Mark Przybocki,  and Omar Zaidan.
2010.   Findings of the 2010 joint workshop on sta- 
tistical machine translation and metrics for machine 
translation. In Proceedings of the Joint Fifth Workshop 
on Statistical Machine Translation and MetricsMATR, 
pages 17–53, Uppsala, Sweden, July. Revised August
2010.
David Chiang. 2007. Hierarchical phrase-based transla- 
tion. Computational Linguistics, 33(2):201–228.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz 
Och.   2010.  Model combination for machine trans- 
lation.  In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of 
the Association for Computational Linguistics, pages
975–983, Los Angeles, California, June.
Markus Dreyer,  Keith Hall,  and  Sanjeev Khudanpur.
2007.  Comparing reordering constraints for smt us- 
ing efficient bleu oracle computation. In Proceedings 
of SSST, NAACL-HLT 2007 / AMTA Workshop on Syn- 
tax and Structure in Statistical Translation, pages 103–
110, Rochester, New York, April.
Jay Earley. 1970. An efficient context-free parsing algo- 
rithm.  Communications of the Association for Com- 
puting Machinery, 13:94–102, February.
J.G. Fiscus. 1997. A post-processing system to yield re- 
duced word error rates: Recognizer output voting error 
reduction (rover). In Proceedings of Automatic Speech 
Recognition and Understanding (ASRU), 1997, pages
347 –354, December.
Robert Frederking and Sergei Nirenburg.  1994.  Three 
heads are better than one. In Proceedings of the fourth 
conference on Applied natural language processing, 
pages 95–100, Morristown, NJ, USA.
Joshua Goodman.  1999.  Semiring parsing.  Computa- 
tional Linguistics, 25:573–605, December.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, 
and Robert Moore.  2008.  Indirect-HMM-based hy- 
pothesis alignment for combining outputs from ma- 
chine translation systems. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan- 
guage Processing, pages 98–107, Honolulu, Hawaii, 
October.
John C. Henderson and Eric Brill.   1999.   Exploiting 
diversity in natural language processing: Combining


parsers.  In Proceedings of the Fourth Conference on 
Empirical Methods in Natural Language Processing, 
pages 187–194.
Liang Huang and David Chiang.   2005.   Better k-best 
parsing.   In Proceedings of the Ninth International 
Workshop on Parsing Technology, pages 53–64, Van- 
couver, British Columbia, October.
Liang Huang and David Chiang. 2007. Forest rescoring: 
Faster decoding with integrated language models.  In 
Proceedings of the 45th Annual Meeting of the Asso- 
ciation of Computational Linguistics, pages 144–151, 
Prague, Czech Republic, June.
Shyamsundar Jayaraman and Alon Lavie.  2005.  Multi- 
engine machine translation guided by explicit word 
matching.   In Proceedings of the ACL 2005 on In- 
teractive poster and demonstration sessions, ACL ’05, 
pages 101–104, Morristown, NJ, USA.
Dan Klein and Christopher D. Manning.  2001.  Parsing 
and hypergraphs.  In Proceedings of the Seventh In- 
ternational Workshop on Parsing Technologies (IWPT-
2001), pages 123–134.
Dan Klein and Christopher D. Manning.  2003.  Accu- 
rate unlexicalized parsing. In Proceedings of the 41st 
Annual Meeting of the Association for Computational 
Linguistics, pages 423–430, Sapporo, Japan, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and 
Franz Och. 2009. Efficient minimum error rate train- 
ing and minimum bayes-risk decoding for translation 
hypergraphs and lattices.  In Proceedings of the Joint 
Conference of the 47th Annual Meeting of the ACL and 
the 4th International Joint Conference on Natural Lan- 
guage Processing of the AFNLP, pages 163–171, Sun- 
tec, Singapore, August.
Irene Langkilde and Kevin Knight.  1998.   Generation 
that exploits corpus-based statistical knowledge.   In 
Proceedings of the 36th Annual Meeting of the As- 
sociation for Computational Linguistics and 17th In- 
ternational Conference on Computational Linguistics
- Volume 1, ACL-36, pages 704–710, Morristown, NJ, 
USA.
Irene Langkilde. 2000. Forest-based statistical sentence 
generation. In Proceedings of the 1st North American 
chapter of the Association for Computational Linguis- 
tics conference, pages 170–177, San Francisco, CA, 
USA.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extrac- 
tion of oracle-best translations from hypergraphs.  In 
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap- 
ter of the Association for Computational Linguistics, 
Companion Volume: Short Papers, pages 9–12, Boul- 
der, Colorado, June.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu.  2009.
Joint decoding with multiple translation models.   In


Proceedings of the Joint Conference of the 47th An- 
nual Meeting of the ACL and the 4th International 
Joint Conference on Natural Language Processing of 
the AFNLP, pages 576–584, Suntec, Singapore, Au- 
gust.
Adam Lopez. 2009. Translation as weighted deduction.
In Proceedings of the 12th Conference of the Euro- 
pean Chapter of the ACL (EACL 2009), pages 532–
540, Athens, Greece, March.
Wolfgang Macherey and Franz J. Och. 2007. An empir- 
ical study on computing consensus translations from 
multiple machine translation systems. In Proceedings 
of the 2007 Joint Conference on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL), pages
986–995, Prague, Czech Republic, June.
Lidia Mangu, Eric Brill, and Andreas Stolcke.   2000.
Finding consensus in speech recognition: word error 
minimization and other applications of confusion net- 
works.  Computer Speech & Language, 14(4):373 –
400.
Evgeny Matusov,  Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple 
machine translation systems using enhanced hypothe- 
ses alignment. In Proceedings of the 11th Conference 
of the European Chapter of the Association for Com- 
putational Linguistics, pages 33–40.
Haitao Mi, Liang Huang, and Qun Liu.  2008.  Forest- 
based translation.  In Proceedings of ACL-08:  HLT, 
pages 192–199, Columbus, Ohio, June.
Tadashi Nomoto.  2004.  Multi-engine machine transla- 
tion with voted language model. In Proceedings of the
42nd Meeting of the Association for Computational 
Linguistics (ACL’04), Main Volume, pages 494–501, 
Barcelona, Spain, July.
Franz Josef Och and Hermann Ney.  2003.  A system- 
atic comparison of various statistical alignment mod- 
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei- 
Jing Zhu.  2002.  Bleu: a method for automatic eval- 
uation of machine translation. In Proceedings of 40th 
Annual Meeting of the Association for Computational 
Linguistics, pages 311–318, Philadelphia, Pennsylva- 
nia, USA, July.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy- 
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007a.   Combining outputs from multiple machine 
translation systems.   In Human Language Technolo- 
gies 2007:  The Conference of the North American 
Chapter of the Association for Computational Linguis- 
tics; Proceedings of the Main Conference, pages 228–
235, Rochester, New York, April.


Antti-Veikko Rosti, Spyros Matsoukas, and Richard 
Schwartz.  2007b.  Improved word-level system com- 
bination for machine translation.   In Proceedings of 
the 45th Annual Meeting of the Association of Com- 
putational Linguistics, pages 312–319, Prague, Czech 
Republic, June.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and 
Richard Schwartz.   2008.   Incremental hypothesis 
alignment for building confusion networks with appli- 
cation to machine translation system combination. In 
Proceedings of the Third Workshop on Statistical Ma- 
chine Translation, pages 183–186, Columbus, Ohio, 
June.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira.   1995.   Principles and implementation of 
deductive parsing.   Journal of Logic Programming,
24(1–2):3–36, July–August.
K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi, and P.C.
Woodland.  2007.   Consensus network decoding for 
statistical machine translation system combination. In 
Proceedings of Acoustics, Speech and Signal Process- 
ing (ICASSP), 2007, volume 4, pages IV–105 –IV–
108, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea 
Micciulla, and John Makhoul. 2006. A study of trans- 
lation edit rate with targeted human annotation. In In 
Proceedings of Association for Machine Translation in 
the Americas, pages 223–231.

