Lattice-based System Combination for Statistical Machine Translation



Yang Feng, Yang Liu, Haitao Mi, Qun Liu, Yajuan Lu¨ 
Key Laboratory of Intelligent Information Processing 
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{fengyang, yliu, htmi, liuqun, lvyajuan}@ict.ac.cn







Abstract

Current  system  combination  methods  usu- 
ally use confusion networks to find consensus 
translations among different systems. Requir- 
ing one-to-one mappings between the words 
in candidate translations, confusion networks 
have difficulty in handling more general situa- 
tions in which several words are connected to 
another several words. Instead, we propose a 
lattice-based system combination model that 
allows for such phrase alignments and uses 
lattices to encode all candidate translations. 
Experiments show that our approach achieves 
significant improvements over the state-of- 
the-art baseline system on Chinese-to-English 
translation test sets.


1   Introduction

System combination aims to find consensus transla- 
tions among different machine translation systems. 
It has been proven that such consensus translations 
are usually better than the output of individual sys- 
tems (Frederking and Nirenburg, 1994).
  In recent several years, the system combination 
methods based on confusion networks developed 
rapidly (Bangalore et al., 2001; Matusov et al., 2006; 
Sim et al., 2007; Rosti et al., 2007a; Rosti et al.,
2007b; Rosti et al., 2008; He et al., 2008), which 
show state-of-the-art performance in benchmarks. A 
confusion network consists of a sequence of sets of 
candidate words. Each candidate word is associated 
with a score. The optimal consensus translation can 
be obtained by selecting one word from each set to 
maximizing the overall score.
  

To construct a confusion network, one first need 
to choose one of the hypotheses (i.e., candidate 
translations) as the backbone (also called “skeleton” 
in the literature) and then decide the word align- 
ments of other hypotheses to the backbone.  Hy- 
pothesis alignment plays a crucial role in confusion- 
network-based system combination because it has a 
direct effect on selecting consensus translations.
  However, a confusion network is restricted in 
such a way that only 1-to-1 mappings are allowed 
in hypothesis alignment.  This is not the fact even 
for word alignments between the same languages. It 
is more common that several words are connected 
to another several words.  For example, “be capa- 
ble of” and “be able to” have the same meaning. 
Although confusion-network-based approaches re- 
sort to inserting null words to alleviate this problem, 
they face the risk of producing degenerate transla- 
tions such as “be capable to” and “be able of”.
  In this paper, we propose a new system combina- 
tion method based on lattices.  As a more general 
form of confusion network, a lattice is capable of 
describing arbitrary mappings in hypothesis align- 
ment.  In a lattice, each edge is associated with a 
sequence of words rather than a single word. There- 
fore, we select phrases instead of words in each 
candidate set and minimize the chance to produce 
unexpected translations  such as  “be capable  to”. 
We compared our approach with the state-of-the-art 
confusion-network-based system (He et al., 2008) 
and achieved a significant absolute improvement of
1.23 BLEU points on the NIST 2005 Chinese-to- 
English test set and 0.93 BLEU point on the NIST
2008 Chinese-to-English test set.








1105

Proceedings of the 2009 Conference on Empirical  Methods in Natural  Language Processing, pages 1105–1113, 
Singapore, 6-7 August 2009. Qc 2009 ACL and AFNLP



He   feels   like apples 
He       prefer    apples 
He   feels   like apples 
He  is fond of  apples
(a) unidirectional alignments



He   feels   like apples 
He       prefer    apples 
He   feels   like apples 
He  is fond of  apples
(b) bidirectional alignments


lations. Note that the 
phrase “is fond of” is 
attached to an edge. Now, 
it is unlikely to obtain a 
translation like “He is like 
of apples”.
A lattice G = (V, E) is 
a directed acyclic graph,
formally a weighted 
finite state automation 
(FSA),
where V is the set of nodes 
and E is the set of edges. 
The nodes in a lattice are 
usually labeled according


ε 	prefer 	of
   He      feels 	like 	ε 	apples

is 	fond
(c) confusion network

ε 	prefer
 	he 		feels	like	  apples 	

is fond of
(d) lattice


Figure 1: Comparison of a confusion network and a lat- 
tice.


2	Background

2.1	Confusion Network and Lattice
We use an example shown in Figure 1 to illustrate 
our idea. Suppose that there are three hypotheses:


to an appropriate numbering to reflect how to pro- 
duce a translation. Each edge in a lattice is attached 
with a sequence of words as well as the associated 
probability.
  As lattice is a more general form of confusion 
network (Dyer et al., 2008), we expect that replac- ing 
confusion networks with lattices will further im- prove 
system combination.


2.2   IHMM-based Alignment Method

Since  the  candidate  hypotheses  are  aligned  us- ing 
Indirect-HMM-based (IHMM-based) alignment 
method (He et al., 2008) in both direction, we briefly 
review the IHMM-based alignment method first. Take 
the direction that the hypothesis is aligned to the 
backbone as an example. The conditional prob- ability 
that the hypothesis is generated by the back- bone is 
given by





He feels like apples


J
1	1 ) = ) n[p(a |a −1, I )p(e′ |e



)]l 	(1)


He prefer apples
He is fond of apples


p(e′ J |eI


j  j
J  j=1
1


j  aj






We choose the first sentence as the backbone.


Where eI


= (e1 , ..., eI ) is the backbone, e′J   =


Then, we perform hypothesis alignment to build a


(e′1, ..., e′ J ) is a hypothesis aligned to eI , and aJ  =


1	1


confusion network, as shown in Figure 1(a).  Note
that although “feels like” has the same meaning with 
“is fond of”, a confusion network only allows for 
one-to-one mappings.   In the confusion network 
shown in Figure 1(c), several null words ε are in-


(a1 , .., aJ ) is the alignment that specifies the posi-
tion of backbone word that each hypothesis word is 
aligned to.
  The translation probability p(e′ |ei ) is a linear in- 
terpolation of semantic similarity	(   |e ) and


serted to ensure that each hypothesis has the same


surface similarity psur (e′ |ei) and α is the interpo-


length. As each edge in the confusion network only
has a single word, it is possible to produce inappro- 
priate translations such as “He is like of apples”.
  In contrast,  we allow many-to-many mappings 
in the hypothesis alignment shown in Figure 2(b). 
For example, “like” is aligned to three words: “is”, 
“fond”, and “of”.  Then, we use a lattice shown in 
Figure 1(d) to represent all possible candidate trans-


lation factor:

p(ej |ei ) = α·psem(ej |ei )+(1−α)·psur (ej |ei ) (2)
′ 	′ 	′


The semantic similarity model is derived by using the 
source word sequence as a hidden layer, so the 
bilingual dictionary is necessary. The semantic sim-


ilarity model is given by

K
psem(e′ |ei) = ) p(fk |ei)p(e′ |fk , ei )
  

(2) Choose the backbone from the 
hypotheses. This is performed using a 
sentence-level Minimum Bayes Risk (MBR) 
method. The hypothesis with the


j
k=0
K
)
≈
k=0


j


p(fk |ei)p(e′ |fk )



(3)


minimum cost of edits 
against all hypotheses is 
se- lected. The backbone 
is significant for it 
influences not only the 
word order, but also the 
following align- ments. 
The backbone is selected 
as follows:



The surface similarity model is estimated by calcu-


EB 	=	argmin ) T ER(E′, E) 	(8)


lating the literal matching rate:


E′∈E


E∈E


psur (e′ |ei ) = exp{ρ · [s(e′ , ei ) − 1]}	(4)
j	j
where s(e′j , ei ) is given by



  (3) Get the alignments of the backbone and hy- 
pothesis pairs. First, each pair is aligned in both di- 
rections using the IHMM-based alignment method.


s(e′ , ei ) =


M (e′ , ei )
max(|e′ |, |ei |)



(5)


In the IHMM 
alignment model, 
bilingual dictionar- ies 
in both directions are 
indispensable.  Then, 
we apply a grow-diag-
final algorithm which 
is widely


where M (e′ , ei) is the length of the longest matched
prefix (LMP) and ρ is a smoothing factor that speci- 
fies the mapping.
The distortion probability p(aj  = i|aj−1  = i′, I )
is estimated by only considering the jump distance:
c(i − i′)


used in bilingual phrase extraction (Koehn et al.,
2003) to monolingual alignments.   The bidirec- 
tional alignments are combined to one resorting to 
the grow-diag-final algorithm, allowing n-to-n map- 
pings.
(4)Normalize the alignment pairs.  The word or-


p(i|i′, I ) =
i=1 c(l − i′)


(6)


der of the backbone determines the 
word order of
consensus outputs, so the word order of 
hypotheses



The distortion parameters c(d) are grouped into 11 
buckets, c(≤ −4), c(−3), ..., c(0), ..., c(5), c(≥ 6).
Since the alignments are in the same language, the 
distortion model favor monotonic alignments and 
penalize non-monotonic alignments.  It is given in 
a intuitive way
c(d) = (1 + |d − 1|)−K , d = −4, ..., 6	(7)

where K is tuned on held-out data.
  Also the probability p0 of jumping to a null word 
state is tuned on held-out data. So the overall distor- 
tion model becomes

 p0	if i = null state


must be consistent with that of the backbone.  All 
words of a hypotheses are reordered according to 
the alignment to the backbone. For a word aligned 
to null, an actual null word may be inserted to the 
proper position. The  alignment units are extracted 
first and then the hypothesis words in each unit are 
shifted as a whole.
  (5) Construct the lattice in the light of phrase 
pairs extracted on the normalized alignment pairs. 
The expression ability of the lattice depends on the 
phrase pairs.
  (6) Decode the lattice using a model similar to the 
log-linear model.
The confusion-network-based system combina-


p(i|i′, I ) =


(1 − p0 )


· p(i|i′, I



) 	otherwise


tion model 
goes in a 
similar way. 
The first two 
steps
are the same 
as the 
lattice-based 
model. The 
differ-



3	Lattice-based System Combination
Model

Lattice-based system combination involves the fol- 
lowing steps:
  (1) Collect the hypotheses from the candidate sys- 
tems.


ence is that the hypothesis pairs are aligned just in 
one direction due to the expression limit of the con- 
fusion network.  As a result, the normalized align- 
ments only contain 1-to-1 mappings (Actual null 
words are also needed in the case of null alignment). 
In the following, we will give more details about the 
steps which are different in the two models.


4	Lattice Construction



EB :   e1 	e2 	e3


e2 	ε 	e1


′	′


Unlike a confusion network that operates words 
only, a lattice allows for phrase pairs.  So phrase 
pairs must be extracted before constructing a lat- 
tice.  A major difficulty in extracting phrase pairs


 	
Eh  :	e′ 	e′
1	2	(a)

EB :	e1	e2	 	


   e1      e2       e3 	


e′ e′ e′
1  2  3
e1 	e2


is that the word order of hypotheses is not consistent


Eh  :  e′ 	′ 	′


1	e2	e3


e′   ′


with that of the backbone.  As a result, hypothesis 
words belonging to a phrase pair may be discon-
tinuous.  Before phrase pairs are extracted, the hy-


(b)

EB :	e1	e2	 	


1e3

1	e2 	e3
e′ 	′ 	′
   e       ε      e 	



pothesis words should be normalized to make sure


1	2
Eh  :  e′ 	′ 	′



the words in a phrase pair is continuous. We call a


1	e2 	e3


(c)


phrase pair before normalization a alignment unit.
The problem mentioned above is shown in Fig-
ure 2. In Figure 2 (a), although (e′ e′ , e2) should be



Figure 3: Different cases of null insertion


1  3
1	3	ik ∈ e¯i,	ea′


∈ e¯i


a phrase pair, but	e′


and	e′


are discontin-


e′	′
i
k


uous, so the phras e pai r can n ot be extracted. Only


aik   = null or eaik   ∈ e¯i


after the words of the hypothesis are reordered ac- 
cording to the corresponding words in the backbone


• ∀ eik   ∈ e¯i,	e′	′ 	′
′


as shown in Figure 2 (b),


and	e


be-


• ∄ a¯j


= (e¯j , e¯j ),


e¯j


= ei1


, ..., eik


or  e¯j   =


1	3	, ..., ein ,	k ∈ [1, n]


come continuous and the phrase pair (e′ e′ , e2) can
e extracted. The procedure of reorder	is called


k
Where a′



denotes the position of the word in the


b
alignment normalization


ing


ik
backbone that e′   is 
aligned to, and aik



denotes the



Eh:   e′ 	′ 	′


:  e	e	e



Eh:   e′ 	′ 	′


e	e	e


position of the word 
in the hypothesis that 
eik   is
aligned to.
  An actual null 
word may be inserted 
to a proper position if 
a word, either from the 
hypothesis or from


EB 	1


2	3
(a)


EB  :


1	2	3
(b)


the 
backbone, 
is aligned 
to null.  In 
this way, 
the 
minimum 
alignment 
set is 
extended to 
an 
alignment


Figure 2: An example of alignment units


4.1   Alignment Normalization
After the final alignments are generated in the grow- 
diag-final algorithm, minimum alignment units are 
extracted.  The hypothesis words of an alignment 
unit are packed as a whole in shift operations.
See the example in Figure 2 (a) first.  All mini-
mum alignment units are as follows: (e′ , e1 ), (e′ e′ ,


unit set, which includes not only minimum align- 
ment units but also alignment units which are gener- 
ated by adding null words to minimum alignment 
units.   In general, the following three conditions 
should be taken into consideration:
• A backbone word is aligned to null.  A null 
word is inserted to the hypothesis as shown in 
Figure 3 (a).


2	1  3


A hypothesis word is aligned to null and it is


e2 ) and (ε, e3 ). (e′ e′ e′ , e1 e2 ) is an alignment unit,	•


1  2  3
but not a minimum alignment unit.
Let a¯i   = (e¯′ , e¯i ) denote a minimum alignment
unit, and assume that the word string e¯′ covers words
i1   ,..., eim   on the hypothesis side, and the word


between the span of a minimum alignment unit.
A new alignment unit is generated by insert- ing the 
hypothesis word aligned to null to the minimum 
alignment unit. The new hypothesis string must 
remain the original word order of


e′	′


string e¯i covers the consecutive words ei1  ,..., ein  on
the backbone side.  In an alignment unit, the word
string on the hypothesis side can be discontinuous. 
The minimum unit a¯i   = (e¯′ , e¯i ) must observe the
following rules:


the hypothesis. It is illustrated in Figure 3 (b).
• A hypothesis word is aligned to null and it is not 
between the hypothesis span of any mini- mum 
alignment unit. In this case, a null word


e¯′


e¯′


e¯′


e¯′


e¯′


e¯′


e¯′


e¯′


e¯′


e1	e2	ε 	e3


e1	ε 	e2	e3


e1	ε 	e2	e3







(a)




(b)


e¯′




(c)





e¯′


e¯′


e¯′


e¯′


e¯′


e¯′


e1	ε 	e2	ε 	e3


e1	ε 	e2	ε 	e3





e¯4


e¯′


e¯4


e¯′


e¯′



(d)



(e)



Figure 4: A toy instance of lattice construction




are inserted to the backbone. This is shown in
Figure 3 (c).

4.2   Lattice Construction Algorithm
The lattice is constructed by adding the normalized 
alignment pairs incrementally. One backbone arc in 
a lattice can only span one backbone word. In con- 
trast, all hypothesis words in an alignment unit must 
be packed into one hypothesis arc. First the lattice is 
initialized with a normalized alignment pair.  Then 
given all other alignment pairs one by one, the lat- 
tice is modified dynamically by adding the hypothe- 
sis words of an alignment pair in a left-to-right fash-


• e2 is compared with e2 . There is no hypothesis
arc coming from the same node with the bone
arc e2  in the alignment pair, so the lattice re- 
mains the same. Both go to the next backbone 
words.
• ε is compared with e3 . A null backbone arc is 
inserted into the lattice between e2 and e3 . The 
hypothesis arc e¯5  is inserted to the lattice, too. 
The modified lattice is shown in Figure 4(d). 
The alignment pair goes to the next backbone 
word e3 .
• e3 is compared with e3. For they are the same


ion.


and 
there 
is no 
hypo
thesi
s arc 
e¯′


in 
the 
lattic
e,


A toy instance is given in Figure 4 to illustrate the
procedure of lattice construction.  Assume the cur- 
rent inputs are: an alignment pair as in Figure 4 (a), 
and a lattice as in Figure 4 (b). The backbone words 
of the alignment pair are compared to the backbone 
words of the lattice one by one. The procedure is as 
follows:

• e1  is compared with e1 .   Since they are the 
same, the hypothesis arc e¯′ , which comes from

the same node with e1  in the alignment pair, 
is compared with the hypothesis arc e¯′ , which
comes from the same node with e1  in the lat- 
tice. The two hypothesis arcs are not the same,


e¯′  is inserted to the lattice as in Figure 4(e).
• Both arrive at the end and it is the turn of the 
next alignment pair.

  When comparing a backbone word of the given 
alignment pair with a backbone word of the lattice, 
the following three cases should be handled:
• The current backbone word of the given align- 
ment pair is a null word while the current back- 
bone word of the lattice is not.  A null back- 
bone word is inserted to the lattice.
• The current backbone word of the lattice is a


so e¯′


is added to 
the lattice 
as shown in 
Figure


null word 
while the 
current 
word of 
the given


4(c). Both go to the next backbone words.
• e2 is compared with ε. The lattice remains the 
same.  The lattice goes to the next backbone 
word e2 .


alignment pair is not.  The current null back-
bone word of the lattice is skipped with nothing to 
do. The next backbone word of the lattice is 
compared with the current backbone word of the 
given alignment pair.


Algorithm 1 Lattice construction algorithm.
1:  Input: alignment pairs {pn}N
2:  L ← p1
3:  U nique(L)
4:  for n ← 2 .. N do
5:	pnode = pn · f irst
6:	lnode = L · f irst
7:	while pnode · barcnext /= N U LL do
8:	if lnode · barcnext  = N U LL   or  pnode ·
bword = null and lnode · bword /= null then
9:	INSE RT BARC(lnode,  null)
10:	pnode = pnode · barcnext
11:	else
12:	if pnode · bword  /= null  and  lnode ·
bword = null then
13:	lnode = lnode · barcnext
14:	else
15:	for each harc of pnode do
16:	if NotExist(lnode, pnode · harc)
then
17:	INSE RT HARC(lnode, pnode ·
harc)
18:	pnode = pnode · barcnext
19:	lnode = lnode · barcnext
20:  Output: lattice L


• The current backbone words of the given align-



Line 9 inserts a null bone arc to the position right 
before the current node. Line 12 − 13 deal with con-
dition 2 and jump to the next backbone word of the 
lattice. Line 15 − 19 handle condition 3 and function
InsertHarc inserts to the lattice a harc with the same 
hypothesis words and the same backbone word span 
with the current hypothesis arc.

5   Decoding

In confusion network decoding, a translation is gen- 
erated by traveling all the nodes from left to right. 
So a translation path contains all the nodes.  While 
in lattice decoding, a translation path may skip some 
nodes as some hypothesis arcs may cross more than 
one backbone arc.
  Similar to the features in Rosti et al. (2007a), the 
features adopted by lattice-based model are arc pos- 
terior probability, language model probability, the 
number of null arcs, the number of hypothesis arcs 
possessing more than one non-null word and the 
number of all non-null words. The features are com- 
bined in a log-linear model with the arc posterior 
probabilities being processed specially as follows:


ment pair and the lattice are the same.	Let
{harcl } denotes  the  set  of  hypothesis  arcs,
which come from the same node with the cur-



log p(e/f ) =


Narc
)

i=1


Ns
log ()
s=1



λsps(arc))





(9)


rent backbone arc in the lattice, and harch  de- 
notes one of the corresponding hypothesis arcs
in the given alignment pair.   In the {harcl },
if there is no arc which is the same with the
harch, a hypothesis arc projecting to harch  is 
added to the lattice.

  The algorithm of constructing a lattice is illus- 
trated in Algorithm  1. The backbone words of the 
alignment pair and the lattice are processed one by 
one in a left-to-right manner.  Line 2 initializes the 
lattice with the first alignment pair, and Line 3 re- 
moves the hypothesis arc which contains the same 
words with the backbone arc. barc denotes the back- 
bone arc, storing one backbone word only, and harc 
denotes the hypothesis arc, storing the hypothesis 
words. For there may be many alignment units span 
the same backbone word range, there may be more
than one harc coming from one node. Line 8 − 10
consider the condition 1 and function InsertBarc in


+ ζ L(e) + αNnullarc(e)
+ βNlongarc(e) + γNword(e)

where f denotes the source sentence, e denotes a 
translation generated by the lattice-based system, 
Narc  is the number of arcs the path of e covers, 
Ns is the number of candidate systems and λs is the 
weight of system s. ζ is the language model weight 
and L(e) is the LM log-probability. Nnullarcs(e) is 
the number of the arcs which only contain a null 
word, and Nlongarc(e) is the number of the arcs 
which store more than one non-null word.   The 
above two numbers are gotten by counting both 
backbone arcs and hypothesis arcs. α and β are the 
corresponding weights of the numbers, respectively. 
Nword(e) is the non-null word number and γ is its 
weight.
  Each arc has different confidences concerned with 
different systems, and the confidence of system s 
is denoted by ps(arc).    ps(arc)  is increased by


1/(k + 1) if the hypothesis ranking k in the system s
contains the arc (Rosti et al., 2007a; He et al., 2008).
  Cube pruning algorithm with beam search is em- 
ployed to search for the consensus output (Huang 
and Chiang, 2005).  The nodes in the lattice are 
searched in a topological order and each node re- 
tains a list of N best candidate partial translations.

6   Experiments

The candidate systems participating in the system 
combination are as listed in Table 1: System A is a 
BTG-based system using a MaxEnt-based reorder- 
ing model; System B is a hierarchical phrase-based 
system; System C is the Moses decoder  (Koehn et 
al., 2007); System D is a syntax-based system. 10- 
best hypotheses from each candidate system on the 
dev and test sets were collected as the input of the 
system combination.
  In our experiments, the weights were all tuned on 
the NIST MT02 Chinese-to-English test set, includ- 
ing 878 sentences, and the test data was the NIST 
MT05 Chinese-to-English test set, including 1082 
sentences, except the experiments in Table 2.  A 5- 
gram language model was used which was trained 
on the XinHua portion of Gigaword corpus. The re- 
sults were all reported in case sensitive BLEU score 
and the weights were tuned in Powell’s method to 
maximum BLEU score.   The IHMM-based align-


the lattice-based model. On the dev set, the lattice- 
based system was 3.92 BLEU points higher than the 
best single system and 0.36 BLEU point higher than 
the IHMM-based system. On the test set, the lattice- 
based system got an absolute improvement by 3.73
BLEU points over the best single system and 1.23
BLEU points over the IHMM-based system.

Sy
ste
m
M
T0
2
B
L
E
U
%
M
T0
5
B
L
E
U
%
Sy
ste
m
A
31
.9
3
30
.6
8
Sy
ste
m
B
32
.1
6
32
.0
7
Sy
ste
m
C
32
.0
9
31
.6
4
Sy
ste
m
D
33
.3
7
31
.2
6
IH
M
M
36
.9
3
34
.5
7
La
tti
ce
37
.2
9
35
.8
0

Table 1: Results on the MT02 and MT05 test sets

  The results on another test sets are reported in Ta- 
ble 2. The parameters were tuned on the newswire 
part of NIST MT06 Chinese-to-English test set, in- 
cluding 616 sentences, and the test set was NIST 
MT08 Chinese-to-English test set, including 1357 
sentences. The BLEU score of the lattice-based sys- 
tem is 0.93 BLEU point higher than the IHMM- 
based system and 3.0 BLEU points higher than the 
best single system.


ment module was implemented according  to He et	 	


al. (2008), He (2007) and Vogel et al. (1996). In all 
experiments, the parameters for IHMM-based align- 
ment module were set to: the smoothing factor for 
the surface similarity model, ρ = 3; the controlling 
factor for the distortion model, K = 2.

6.1   Comparison with
Confusion-network-based model
In order to compare the lattice-based system with 
the confusion-network-based system fairly, we used 
IHMM-based system combination model on behalf 
of the confusion-network-based model described in 
He et al. (2008). In both lattice-based and IHMM- 
based systems, the bilingual dictionaries were ex- 
tracted on the FBIS data set which included 289K 
sentence pairs. The interpolation factor of the simi- 
larity model was set to α = 0.1.
  The results are shown in Table 1.  IHMM stands 
for the IHMM-based model and Lattice stands for


System	MT06	MT08
BLEU%	BLEU%

SystemA	32.51	25.63
SystemB	31.43	26.32
SystemC	31.50	23.43
SystemD	32.41	26.28
IHMM	36.05	28.39
Lattice	36.53	29.32

Table 2: Results on the MT06 and MT08 test sets

  We take a real example from the output of the 
two systems (in Table 3) to show that higher BLEU 
scores correspond to better alignments and better 
translations. The translation of System C is selected 
as the backbone.  From Table 3, we can see that 
because of 1-to-1 mappings, “Russia” is aligned to 
“Russian” and “’s” to “null” in the IHMM-based 
model, which leads to the error translation “Russian


Source:

SystemA:





 Ru ssi a m er 
ger of sta te -
ow ne d o il c 
om pa ny an d  
the sta te- run 
g as 
company in 
Russia


SystemB:	Russia ’s state-owned oil company is working with Russia ’s state-run gas company mergers 
SystemC:	Russian state-run oil company is combined with the Russian state-run gas company 
SystemD:	Russia ’s state-owned oil companies are combined with Russia ’s state-run gas company

IHMM:	Russian ’s state-owned oil company working with Russia ’s state-run gas company
Lattice:	Russia ’s state-owned oil company is combined with the Russian state-run gas company

Table 3: A real translation example




’s”. Instead, “Russia ’s” is together aligned to ”Rus- 
sian” in the lattice-based model.  Also due to 1-to-
1 mappings, null word aligned to “is” is inserted.
As a result, “is” is missed in the output of IHMM- 
based model.  In contrast, in the lattice-based sys- 
tem, “is working with” are aligned to “is combined 
with”, forming a phrase pair.

6.2   Effect of Dictionary Scale

The dictionary is important to the semantic similar- 
ity model in IHMM-based alignment method.  We 
evaluated the effect of the dictionary scale by using 
dictionaries extracted on different data sets. The dic- 
tionaries were respectively extracted on similar data 
sets: 30K sentence pairs, 60K sentence pairs, 289K 
sentence pairs (FBIS corpus) and 2500K sentence 
pairs.  The results are illustrated in Table 4.  In or- 
der to demonstrate the effect of the dictionary size 
clearly, the interpolation factor of similarity model 
was all set to α = 0.1.
  From Table 4,  we can see that when the cor- 
pus size rise from 30k to 60k, the improvements 
were not obvious both on the dev set and on the 
test set.  As the corpus was expanded to 289K, al- 
though on the dev set, the result was only 0.2 BLEU 
point higher, on the test set, it was 0.63 BLEU point 
higher.  As the corpus size was up to 2500K, the 
BLEU scores both on the dev and test sets declined. 
The reason is that, on one hand, there are more noise 
on the 2500K sentence pairs; on the other hand, the
289K sentence pairs cover most of the words appear- 
ing on the test set.  So we can conclude that in or- 
der to get better results, the dictionary scale must be 
up to some certain scale.  If the dictionary is much 
smaller, the result will be impacted dramatically.




M
T0
2
B
L
E
U
%
M
T0
5
B
L
E
U
%
30
k
36
.9
4
35
.1
4
60
k
37
.0
9
35
.1
7
28
9k
37
.2
9
35
.8
0
25
00
k
37
.1
4
35
.6
2

Table 4: Effect of dictionary scale


6.3   Effect of Semantic Alignments

For the IHMM-based alignment method, the transla- 
tion probability of an English word pair is computed 
using a linear interpolation of the semantic similar- 
ity and the surface similarity. So the two similarity 
models decide the translation probability together 
and the proportion is controlled by the interpolation 
factor. We evaluated the effect of the two similarity 
models by varying the interpolation factor α.
  We used the dictionaries extracted on the FBIS 
data set. The result is shown in Table 5. We got the 
best result with α  = 0.1.  When we excluded the 
semantic similarity model (α = 0.0) or excluded the 
surface similarity model (α = 1.0), the performance 
became worse.

7   Conclusion

The alignment model plays an important role in 
system combination.   Because of the expression 
limitation of confusion networks, only 1-to-1 map- 
pings are employed in the confusion-network-based 
model.  This paper proposes a lattice-based system 
combination model. As a general form of confusion 
networks, lattices can express n-to-n mappings. So 
a lattice-based model processes phrase pairs while




M
T0
2
B
L
E
U
%
M
T0
5
B
L
E
U
%
α 
= 
1.
0
36
.4
1
34
.9
2
α 
= 
0.
7
37
.2
1
35
.6
5
α 
= 
0.
5
36
.4
3
35
.0
2
α 
= 
0.
4
37
.1
4
35
.5
5
α 
= 
0.
3
36
.7
5
35
.6
6
α 
= 
0.
2
36
.8
1
35
.5
5
α 
= 
0.
1
37
.2
9
35
.8
0
α 
= 
0.
0
36
.4
5
35
.1
4

Table 5: Effect of semantic alignments


a confusion-network-based model processes words 
only. As a result, phrase pairs must be extracted be- 
fore constructing a lattice.
  On NIST MT05 test set, the lattice-based sys- 
tem gave better results with an absolute improve- 
ment of 1.23 BLEU points over the confusion- 
network-based system (He et al., 2008) and 3.73
BLEU  points  over  the  best  single  system.    On 
NIST MT08 test set, the lattice-based system out- 
performed the confusion-network-based system by
0.93 BLEU point and outperformed the best single 
system by 3.0 BLEU points.

8   Acknowledgement

The authors were supported by National Natural Sci- 
ence Foundation of China Contract 60736014, Na- 
tional Natural Science Foundation of China Con- 
tract 60873167 and High Technology R&D Program 
Project No. 2006AA010108. Thank Wenbin Jiang, 
Tian Xia and Shu Cai for their help.  We are also 
grateful to the anonymous reviewers for their valu- 
able comments.


References

Srinivas Bangalore, German Bordel, and Giuseppe Ric- 
cardi.  2001.  Computing consensus translation from 
multiple machine translation systems.   In Proc. of 
IEEE ASRU, pages 351–354.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008.  Generalizing word lattice translation.  In Pro- 
ceedings of ACL/HLT 2008, pages 1012–1020, Colum- 
bus, Ohio, June.
Robert Frederking and Sergei Nirenburg.  1994.  Three



heads are better than one.  In Proc. of ANLP, pages
95–100.
Xiaodong He, Mei Yang, Jangfeng Gao, Patrick Nguyen, 
and Robert Moore.   2008.   Indirect-hmm-based hy- 
pothesis alignment for computing outputs from ma- 
chine translation systems. In Proc. of EMNLP, pages
98–107.
Xiaodong He.  2007.  Using word-dependent translation 
models in hmm based word alignment for statistical 
machine translation. In Proc. of COLING-ACL, pages
961–968.
Liang Huang and David Chiang.   2005.   Better k-best 
parsing.   In Proceedings of the Ninth International 
Workshop on Parsing Technologies (IWPT), pages 53–
64.
Philipp Koehn, Franz J. Och, and Daniel Marcu.  2003.
Statistical phrase-based translation.  In Proc. of HLT- 
NAACL, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, 
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris- 
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, 
Alexandra Constantin, and Evan Herbst.    2007. 
Moses:  Open source toolkit for statistical machine 
translation. In Proc. of the 45th ACL, Demonstration 
Session.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple 
machine translation systems using enhanced hypothe- 
ses alignment. In Proc. of IEEE EACL, pages 33–40.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz.  2007a.  Improved word-level system com- 
bination for machine translation.   In Proc. of ACL, 
pages 312–319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J. 
Dorr.  2007b.  Combining outputs from multiple ma- 
chine translation systems.  In Proc. of NAACL-HLT, 
pages 228–235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis 
alignment for building confusion networks with appli- 
cation to machine translaiton system combination. In 
Proc. of the Third ACL WorkShop on Statistical Ma- 
chine Translation, pages 183–186.
Khe  Chai  Sim,  William  J.  Byrne,  Mark  J.F.  Gales,
Hichem Sahbi, and Phil C. Woodland.  2007.  Con- 
sensus network decoding for statistical machine trans- 
lation system combination. In Proc. of ICASSP, pages
105–108.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans- 
lation. In Proc. of COLING, pages 836–841.

