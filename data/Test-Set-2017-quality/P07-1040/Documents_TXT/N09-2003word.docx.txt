Efficient Extraction of Oracle-best Translations from Hypergraphs


Zhifei Li 	and	Sanjeev Khudanpur
Center for Language and Speech Processing and Department of Computer Science 
The Johns Hopkins University,  Baltimore,  MD 21218, USA 
zhifei.work@gmail.com	and	khudanpur@jhu.edu






Abstract

Hypergraphs are used in  several  syntax- 
inspired methods  of machine translation  to 
compactly  encode exponentially  many trans- 
lation hypotheses.  The hypotheses closest to 
given reference translations  therefore cannot 
be found via brute force, particularly for pop- 
ular measures of closeness such as BLEU. We 
develop a dynamic program for extracting the 
so called oracle-best hypothesis from a hyper- 
graph by viewing it as the problem  of finding 
the most likely hypothesis under an n-gram 
language model trained from only the refer- 
ence translations. We further identify and re- 
move  massive redundancies in the dynamic 
program  state due to the sparsity of n-grams 
present in the reference translations, resulting 
in a very efficient program. We present run- 
time statistics for this program, and demon- 
strate successful application  of the hypothe- 
ses thus found  as the targets for discriminative 
training of translation system components.


1   Introduction

A hypergraph,  as demonstrated by Huang and Chi- 
ang (2007), is a compact data-structure that can en- 
code an exponential number of hypotheses gener- 
ated by a regular  phrase-based machine translation 
(MT) system (e.g., Koehn et al. (2003)) or a syntax- 
based MT system (e.g., Chiang (2007)). While the 
hypergraph  represents  a very large set of transla- 
tions, it is quite possible that some desired transla- 
tions (e.g., the reference translations)  are not con- 
tained in the hypergraph, due to pruning or inherent 
deficiency of the translation model. In this case, one 
is often required to find the translation(s) in the hy- 
pergraph that are most similar to the desired transla- 
tions, with similarity computed via some automatic


metric  such as BLEU (Papineni et al., 2002). Such 
maximally similar translations will be called oracle- 
best translations,  and the process of extracting them 
oracle extraction.  Oracle extraction is a nontrivial 
task because computing  the similarity of any one 
hypothesis requires information scattered over many 
items in the hypergraph, and the exponentially  large 
number of hypotheses makes a brute-force  linear 
search intractable. Therefore, efficient algorithms 
that can exploit the structure of the hypergraph are 
required.
  We  present an efficient oracle extraction  algo- 
rithm, which involves two key ideas. Firstly, we 
view the oracle extraction  as a bottom-up  model 
scoring process on a hypergraph, where the model is 
“trained” on the reference translation(s). This is sim- 
ilar to the algorithm proposed for a lattice by Dreyer 
et al. (2007). Their algorithm,  however, requires 
maintaining  a separate dynamic  programming  state 
for each distinguished sequence of “state” words and 
the number of such sequences can be huge, mak- 
ing the search very slow. Secondly, therefore, we 
present a novel look-ahead technique, called equiv- 
alent oracle-state  maintenance, to merge multiple 
states that are equivalent  for similarity computation. 
Our experiments show that the equivalent oracle- 
state maintenance technique significantly  speeds up 
(more than 40 times) the oracle extraction.
  Efficient oracle extraction  has at least three im- 
portant applications in machine translation.
Discriminative Training:  In discriminative train- 
ing, the objective is to tune the model parameters, 
e.g. weights of a perceptron model  or conditional 
random field, such that the reference translations are 
preferred over competitors.  However, the reference 
translations may not be reachable by the translation 
system,  in which case  the oracle-best hypotheses 
should be substituted in training.



9
Proceedings of NAACL HLT 2009: Short Papers, pages 9–12,
Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics


System Combination: In a typical  system combi- 
nation task, e.g. Rosti et al. (2007), each compo- 
nent system  produces  a set of translations, which 
are then grafted to form a confusion  network. The 
confusion network is then rescored, often employ- 
ing additional  (language) models, to select the fi- 
nal translation.  When measuring the goodness of a 
hypothesis in the confusion network,  one requires 
its score under each component  system.  However, 
some translations in the confusion network may not 
be reachable by some component systems, in which 
case a system’s  score for the most similar reachable 
translation  serves as a good approximation.
Multi-source  Translation: In  a  multi-source 
translation task (Och and Ney, 2001) the input is 
given in multiple source  languages. This leads 
to a  situation analogous  to system combination, 
except that each component translation system now 
corresponds to a specific source language.

2   Oracle Extraction on a Hypergraph

In this section, we present the oracle extraction  al- 
gorithm: it extracts one or more translations in a hy- 
pergraph that have the maximum BLEU score1 with 
respect to the corresponding reference translation(s).
  The BLEU score of a hypothesis  h relative to a 
reference r may be expressed in the log domain as,


and use this LM as the only model to do a Viterbi 
search on the hypergraph to find the hypothesis that 
has the maximum (oracle) LM score. Essentially, 
the LM is simply  a table memorizing  the counts of 
n-grams found in the reference translation(s),  and 
the LM score is the log-BLEU value (instead of log- 
probability,  as in a regular  LM). During  the search, 
the dynamic programming (DP) states maintained 
at each  item include the left- and right-side LM 
context,  and the length of the partial translation. 
To compute the n-gram precisions pn incrementally 
during the search, the algorithm  also memorizes at 
each item a vector of maximum numbers of n-gram 
matches between the partial translation and the ref- 
erence(s). Note however that the oracle state of an 
item (which  decides the uniqueness of an item) de- 
pends only on the LM contexts and span lengths, not 
on this vector of n-gram match counts.
  The computation  of  BLEU  also requires the 
brevity penalty, but since there is no explicit align- 
ment between the source and the reference(s), we 
cannot get the exact reference length |r| at an inter- 
mediate item. The exact value of brevity penalty is 
thus not computable. We approximate the true refer- 
ence length for an item with a product  between the 
length of the source string spanned by that item and 
a ratio (which is between the lengths of the whole 
reference and the whole source sentence). Another


4   1	approximation is that we do not consider the effect



log BLEU(r, h) = min


1− |r| , 0
|h|


+ )  
4
n=1



log pn.



of clipping, since it is 
a global feature, 
making  the strict 
computation  intractable.  
This does not signifi-


The first component is the brevity penalty  when
|h|<|r|, while the second component corresponds to 
the geometric mean of n-gram precisions pn (with 
clipping). While BLEU is normally defined at the 
corpus level, we use the sentence-level BLEU for 
the purpose of oracle extraction.
  Two key ideas for extracting the oracle-best hy- 
pothesis from a hypergraph are presented next.

2.1   Oracle Extraction  as Model Scoring

Our first key idea is to view the oracle extraction 
as a bottom-up  model  scoring  process on the hy- 
pergraph. Specifically,  we train a 4-gram language 
model (LM) on only the reference  translation(s),

  1 We believe our method is general and can be extended to 
other metrics capturing only n-gram dependency and other com- 
pact data structures, e.g. lattices.


cantly affect the quality of the oracle-best hypothesis 
as shown  later. Table 1 shows an example how the 
BLEU scores are computed in the hypergraph.
  The process above may be used either in a first- 
stage decoding or a hypergraph-rescoring  stage. In 
the latter case, if the hypergraph  generated by the 
first-stage decoding does not have a set of DP states 
that is a superset of the DP states required for ora- 
cle extraction, we need to split the items of the first- 
stage hypergraph  and create new items with suffi- 
ciently detailed states.
  It is worth mentioning that if the hypergraph items 
contain the state information  necessary for extract- 
ing the oracle-best hypothesis, it is straightforward 
to further extract the k-best hypotheses in the hyper- 
graph (according to BLEU) for any k ≥ 1 using the 
algorithm of Huang and Chiang (2005).


|�






Table 1: Example computation when items A and B are


EQ-L-STATE (em)
1	els  ← em
2	or i ← m to 1	✄ right to left
3	if IS-A-SUFFIX(ei
4	break	✄ stop reducing els
5	else


combined by a rule to produce item C.  r  is the approxi-
mated reference length as described in �   text.


6	els  ← ei−1
7	return els


✄ reduce state



2.2	Equivalent Oracle State Maintenance

The process above, while able to extract the oracle-



Figure 1: Equivalent Left LM State Computation.
EQ-R-STATE (em)


best hypothesis from a hypergraph, is very slow due 
to the need to maintain a dedicated item for each or- 
acle state (i.e., a combination  of left-LM state, right- 
LM state, and hypothesis length). This is especially 
true if the baseline system uses a LM whose order is 
smaller than four, since we need to split the items in


1	ers ← em
2	for i ← 1 to m 	✄ left to right
3	if IS-A-PREFIX (em)
4	break	✄ stop reducing ers
5	else
6	ers ← em 	✄ reduce state


the original hypergraph into many sub-items during
the search.  To speed up the extraction, our second
key idea is to maintain an equivalent oracle state.
  Roughly  speaking, instead of maintaining  a dif- 
ferent state for different language model words, we


7	return ers

Figure 2: Equivalent Right LM State Computation.

maintenance of Section 2.1. This multiplicative fac- 
tor under the equivalent  state maintenance above is


collapse them into a single state whenever it does not


O(m˜ 2), where m˜


is the number of unique n-grams


affect BLEU. For example, if we have two left-side


in the reference translations.  Clearly,  m˜


« m by


LM states a  b  c and a  b  d, and we know that
the reference(s) do not have any n-gram ending with 
them, then we can reduce them both to a  b and ig- 
nore the last word. This is because the combination 
of neither left-side LM state (a  b  c or a  b  d) can 
contribute an n-gram match to the BLEU computa- 
tion, regardless of which prefix in the hypergraph 
they combine with. Similarly, if we have two right- 
side LM states a  b  c and d  b  c, and if we know 
that the reference(s) do not have any n-gram starting 
with either, then we can ignore the first word and re- 
duce them both to b  c. We can continue this reduc- 
tion recursively  as shown in Figures 1 and 2, where 
IS-A-PREFIX(em) (or IS-A-SUFFIX(ei )) checks if


several orders of magnitude, leading to effectively 
much fewer items to process in the chart.
  One may view this idea of maintaining equivalent 
states more generally  as an outside look-ahead  dur- 
ing bottom-up inside parsing.  The look-ahead uses 
some external information,  e.g. IS-A-SUFFIX(·), to 
anticipate  whether maintaining  a detailed state now 
will  be of consequence later; if  not then the in- 
side parsing eliminates or collapses the state into 
a coarser state. The technique proposed by Li and 
Khudanpur (2008a) for decoding with large LMs is 
a special case of this general theme.


i
em 	i


1
	
3
	
Experimental 
Results


i  (resp. e1) is a prefix (suffix) of any n-gram in
the reference translation(s). For BLEU, 1 ≤ n ≤ 4.
  This equivalent oracle state maintenance  tech- 
nique, in practice, dramatically  reduces the number 
of distinct items preserved in the hypergraph for or- 
acle extraction.  To understand this, observe that if 
all hypotheses in the hypergraph together contain m 
unique n-grams, for any fixed n, then the total num- 
ber of equivalent  items takes a multiplicative factor 
that is O(m2) due to left- and right-side  LM state



We report experimental results on a Chinese to En- 
glish task, for a system that is trained using a similar 
pipeline  and data resource as in Chiang (2007).

3.1   Goodness of the Oracle-Best  Translations

Table 2 reports the average speed (seconds/sentence) 
for oracle extraction.   Hypergraphs were generated 
with a trigram  LM and expanded on the fly for 4- 
gram BLEU computation.


Basic DP	Collapse equiv. states	speed-up
25.4 sec/sent	0.6 sec/sent	× 42


Table 2: Speed of oracle extraction from hypergraphs. 
The basic dynamic program (Sec. 2.1) improves signifi- 
cantly by collapsing equivalent oracle states (Sec. 2.2).




  Table 3 reports the goodness of the oracle-best hy- 
potheses on three standard  data sets.  The highest 
achievable BLEU score in a hypergraph  is clearly 
much higher than in the 500-best unique strings. 
This shows that a hypergraph provides a much better 
basis, e.g., for reranking than an n-best list.
  As mentioned in Section 2.1, we use several ap- 
proximations in computing BLEU (e.g., no clipping 
and approximate reference length).  To justify these 
approximations, we first extract 500-best unique or- 
acles from the hypergraph, and then rerank the ora- 
cles based on the true sentence-level  BLEU. The last 
row of Table 3 reports the reranked one-best oracle 
BLEU scores.  Clearly,  the approximations do not 
hurt the oracle BLEU very much.

Hy
po
th
esi
s 
sp
ac
e
M
T’
04
M
T’
05
M
T’
06
1-
be
st 
(B
as
eli
ne
)
3
5
.
7
3
2
.
6
2
8
.
3
50
0-
un
iq
ue
-
be
st
4
4
.
0
4
1
.
2
3
5
.
1
H
yp
er
gr
ap
h
5
2
.
8
5
1
.
8
3
7
.
8
50
0-
be
st 
or
ac
le
s
5
3
.
2
5
2
.
2
3
8
.
0

Table 3: Baseline and oracle-best 4-gram BLEU scores 
with 4 references for NIST Chinese-English MT datasets.

3.2   Discriminative Hypergraph-Reranking

Oracle extraction   is  a   critical  component for 
hypergraph-based discriminative  reranking,  where 
millions of model parameters  are discriminatively 
tuned to prefer the oracle-best hypotheses over oth- 
ers. Hypergraph-reranking in MT is similar to the 
forest-reranking for monolingual parsing (Huang,
2008). Moreover,  once the oracle-best hypothesis 
is identified, discriminative  models may be trained 
on hypergraphs in the same way as on n-best lists 
(cf e.g. Li and Khudanpur (2008b)).  The results in 
Table 4 demonstrate that hypergraph-reranking with 
a discriminative  LM or TM improves upon the base- 
line models on all three test sets.  Jointly training 
both the LM and TM likely suffers from over-fitting.


Table 4: BLEU scores after discriminative hypergraph- 
reranking. Only the language model (LM) or the transla- 
tion model (TM) or both (LM+TM) may be discrimina- 
tively trained to prefer the oracle-best hypotheses.

4   Conclusions

We have presented an efficient  algorithm  to extract 
the oracle-best translation hypothesis from a hyper- 
graph. To this end, we introduced a novel technique 
for equivalent oracle state maintenance, which sig- 
nificantly speeds up the oracle extraction  process. 
Our algorithm  has clear applications  in diverse tasks 
such as discriminative  training, system combination 
and multi-source translation.


References

D. Chiang. 2007. Hierarchical  phrase-based translation.
Computational Linguistics, 33(2):201-228.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar- 
ing Reordering Constraints for SMT Using Efficient 
BLEU Oracle Computation. In Proc. of SSST.
L. Huang. 2008. Forest Reranking: Discriminative  Pars- 
ing with Non-Local Features. In Proc. of ACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. of IWPT.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster 
Decoding with Integrated Language Models. In Proc. 
of ACL.
P.  Koehn, F. J.  Och, and D. Marcu.2003.  Statistical 
phrase-based translation.  In Proc. of NAACL.
Z. Li and S. Khudanpur.  2008a. A Scalable Decoder for 
Parsing-based  Machine Translation with Equivalent 
Language Model State Maintenance. In Proc. SSST.
Z. Li and S. Khudanpur. 2008b. Large-scale Discrimina- 
tive n-gram Language Models for Statistical Machine 
Translation. In Proc. of AMTA.
F. Och and H. Ney. 2001. Statistical multisource transla- 
tion. In Proc. MT Summit VIII.
K. Papineni,  S. Roukos,  T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine 
translation. In Proc. of ACL.
A.I. Rosti, S. Matsoukas, and R. Schwartz. 2007. Im- 
proved word-level system combination  for machine 
translation. In Proc. of ACL.

