What’s in a Preposition?
Dimensions of Sense Disambiguation for an Interesting Word Class



Dirk Hovy, Stephen Tratz, and Eduard Hovy
Information Sciences Institute
University of Southern California
{dirkh, stratz, hovy}@isi.edu





Abstract

Choosing the right parameters for a word 
sense disambiguation   task is critical to 
the success of the experiments.   We ex- 
plore this idea for prepositions,  an of- 
ten overlooked word class. We examine 
the parameters that must be considered in 
preposition disambiguation, namely con- 
text, features,  and granularity.   Doing 
so delivers an increased performance that 
significantly improves over two state-of- 
the-art systems, and shows potential  for 
improving  other word sense disambigua- 
tion tasks. We report accuracies of 91.8% 
and 84.8% for coarse  and fine-grained 
preposition sense disambiguation,  respec- 
tively.

1   Introduction

Ambiguity is one of the central topics in NLP. A 
substantial amount of work has been devoted  to 
disambiguating prepositional attachment, words, 
and names. Prepositions,  as with most other word 
types, are ambiguous.  For example, the word in 
can assume both temporal (“in May”) and spatial 
(“in the US”) meanings,  as well as others,  less 
easily classifiable (“in that vein”).  Prepositions 
typically have more  senses than nouns or verbs 
(Litkowski and Hargraves,  2005), making them 
difficult to disambiguate.
  Preposition   sense disambiguation   (PSD) has 
many potential  uses.   For example,  due to the 
relational nature of prepositions, disambiguating 
their senses can help with all-word sense disam- 
biguation. In machine translation, different senses 
of the same English preposition often correspond
to different translations in the foreign language. 
Thus, disambiguating prepositions correctly may 
help improve translation quality.1 Coarse-grained 
PSD can also be valuable for information extrac- 
tion, where  the sense acts as a label. In a recent 
study, Hwang et al. (2010) identified preposition 
related features, among them the coarse-grained 
PP labels used here, as the most informative  fea- 
ture in identifying caused-motion constructions. 
Understanding the constraints that hold for prepo- 
sitional constructions could help improve  PP at- 
tachment in parsing,  one of the most frequent 
sources of parse errors.
  Several papers have successfully  addressed 
PSD with a variety of different  approaches (Rudz- 
icz and Mokhov, 2003; O’Hara and Wiebe, 2003; 
Ye and Baldwin, 2007; O’Hara and Wiebe, 2009; 
Tratz and Hovy, 2009). However, while it is often 
possible to increase accuracy by using  a differ- 
ent classifier and/or more features, adding more 
features creates two problems:  a) it can lead to 
overfitting,  and b) while possibly improving ac- 
curacy, it is not always clear where this improve- 
ment comes from and which features are actually 
informative.   While parameter studies exist  for 
general word sense disambiguation  (WSD) tasks 
(Yarowsky and Florian, 2002), and PSD accuracy 
has been steadily  increasing,  there has been no 
exploration of the parameters of prepositions to 
guide engineering decisions.
  We go beyond simply improving accuracy to 
analyze various parameters in order to determine 
which ones are actually informative.   We explore 
the different options for context  and feature se-

  1 See (Chan et al., 2007) for the relevance of word sense 
disambiguation and (Chiang et al., 2009) for the role of 
prepositions in MT.


454
Coling  2010: Poster Volume, pages 454–462, 
Beijing,  August 2010


lection, the influence of different preprocessing 
methods, and different levels of sense granular- 
ity. Using the resulting parameters in a Maximum 
Entropy classifier, we are able to improve signif- 
icantly over existing results. The general outline 
we present can potentially  be extended to other 
word classes and improve  WSD  in general.

2   Related Work

Rudzicz and Mokhov (2003) use syntactic  and 
lexical features from the governor and the preposi- 
tion itself in coarse-grained PP classification  with 
decision heuristics. They reach an average  F- 
measure of 89% for four classes. This shows that 
using a very small context can be effective.  How- 
ever, they did not include the object of the prepo- 
sition and used only lexical features for classifi- 
cation. Their results vary widely for the different 
classes.
  O’Hara  and Wiebe (2003) made use of a win- 
dow size  of five  words and features  from the 
Penn Treebank  (PTB) (Marcus et al., 1993) and 
FrameNet (Baker et al., 1998) to classify prepo- 
sitions.  They show that using high level fea- 
tures, such as semantic roles, significantly aid dis- 
ambiguation. They caution that using colloca- 
tions and neighboring words indiscriminately may 
yield high accuracy, but has the risk of overfit- 
ting.  O’Hara and Wiebe (2009) show compar- 
isons of various semantic repositories as labels for 
PSD approaches. They also provide some results 
for PTB-based coarse-grained senses, using a five- 
word window for lexical and hypernym features in 
a decision tree classifier.
SemEval 2007 (Litkowski  and Hargraves,
2007) included a task for fine-grained PSD (more 
than 290 senses). The best participating  system, 
that of Ye and Baldwin (2007), extracted part-of- 
speech and WordNet (Fellbaum, 1998) features 
using a word window of seven words in a Max- 
imum Entropy classifier.  Tratz and Hovy (2009) 
present a higher-performing system using a set of
20 positions that are syntactically  related to the 
preposition instead of a fixed window size.
  Though using a variety  of different extraction 
methods,  contexts,  and feature words, none of 
these approaches explores the optimal configura- 
tions for PSD.


3   Theoretical Background

The following parameters are applicable  to other 
word classes as well. We will demonstrate their 
effectiveness for prepositions.
  Analyzing the syntactic  elements of preposi- 
tional phrases, one discovers three recurring  ele- 
ments that exhibit syntactic dependencies and de- 
fine a prepositional  phrase. The first one is the 
governing word (usually  a noun, verb, or adjec- 
tive)2, the preposition itself, and the object of the 
preposition.
  Prepositional  phrases can be fronted (“In May, 
prices dropped by 5%”), so that the governor (in 
this case the verb “drop”) occurs later in the sen- 
tence.  Similarly, the object can be fronted  (con- 
sider “a dessert to die for”).
  In the simplest version, we can do classification 
based only on the preposition and the governor or 
object alone.3 Furthermore, directly neighboring 
words can influence the preposition, mostly two- 
word prepositions  such as “out of” or “because 
of”.
  To extract the words discussed above, one can 
either employ a fixed window size, (which has 
to be large enough to capture the words), or se- 
lect them based on heuristics or parsing informa- 
tion. The governor and object can be hard to ex- 
tract if they are fronted, since they do not occur in 
their unusual positions relative to the preposition. 
While syntactically  related words improve over 
fixed-window-size   approaches (Tratz and Hovy,
2009), it is not clear which words contribute most. 
There should be an optimal context, i.e., the small- 
est set of words that achieves the best accuracy. It 
has to be large enough to capture all relevant infor- 
mation, but small enough to avoid noise words.4
We surmise that earlier approaches were not uti- 
lizing that optimal context, but rather include a lot 
of noise.
  Depending on the task, different levels of sense 
granularity may be used. Fewer  senses increase 
the likelihood of correct classification, but may in-

  2 We will  refer to the governing word, irrespective of 
class, as governor.
     3 Basing classification on the preposition alone is not fea- 
sible, because of the very polysemy we try to resolve.
     4 It is not obvious how much information  a sister-PP can 
provide, or the subject of the superordinate clause.




correctly conflate prepositions. A finer granular- 
ity can help distinguish  nuances and better fit the 
different contexts. However, it might suffer from 
sparse data.





18000  16995

16000

14000

12000


PTB class distrib



4   Experimental Setup

We explore the different context types (fixed win- 
dow size vs. selective), the influence of the words 
in that context, and the preprocessing method 
(heuristics vs. parsing) on both coarse and fine-



10000

8000

6000

4000

2000

0


10332





5414








1781








1071
280    44


grained disambiguation.  We use a most-frequent- 
sense baseline. In addition, we compare to the 
state-of-the-art  systems for both types of granu- 
larity (O’Hara and Wiebe, 2009; Tratz and Hovy,
2009). Their results show what has been achieved 
so far in terms of accuracy, and serve as a second 
measure for comparison beyond the baseline.

4.1   Model

We  use the MALLET  implementation (McCal- 
lum, 2002) of  a  Maximum Entropy classifier 
(Berger et al., 1996) to construct our models. This 
classifier  was also used  by two state-of-the-art 
systems (Ye and Baldwin,  2007; Tratz and Hovy,
2009). For fine-grained PSD, we train a separate 
model for each preposition  due to the high num- 
ber of possible classes for each individual prepo- 
sition. For coarse-grained PSD, we use a single 
model for all prepositions, because they all share 
the same classes.

4.2   Data

We use two different  data sets from existing re- 
sources for coarse and fine-grained  PSD to make 
our results as comparable to previous work as pos- 
sible.
  For the coarse-grained disambiguation,  we use 
data from the POS tagged version  of the Wall 
Street Journal  (WSJ) section of the Penn Tree- 
Bank. A subset of the prepositional  phrases in 
this corpus is labelled with a set of seven classes: 
beneficial (BNF), direction (DIR), extent (EXT), 
location (LOC), manner (MNR), purpose (PRP), 
and temporal (TMP).  We extract only those prepo- 
sitions  that head a PP labelled  with such a class 
(N  = 35, 917).  The distribution of classes is 
highly skewed (cf.  Figure 1).  We compare the


LOC   TMP    DIR    MNR   PRP    EXT    BNF
c
l
a
s
s
e
s


Figure 1: Distribution of Class Labels in the 
WSJ Section of the Penn TreeBank.


results of this task to the findings of O’Hara and
Wiebe (2009).
  For the fine-grained task, we use  data 
from the SemEval 2007 workshop 
(Litkowski and Har- graves, 2007), separate 
XML files for the 34 most frequent English 
prepositions, comprising 16, 557 training 
and 8096 test sentences, each instance 
containing  one example of the respective 
prepo- sition. Each preposition  has between 
two and 25 senses (9.76  on average)  as 
defined by The Prepo- sition Project 
(Litkowski and Hargraves, 2005). We 
compare our results directly to the 
findings from Tratz and Hovy (2009). As 
in the original workshop  task, we train and 
test on separate sets.

5	Results

In this section we show experimental 
results for the influence of word extraction 
method (parsing vs. POS-based heuristics), 
context, and feature se- lection on accuracy.   
Each section compares the results for both 
coarse and fine-grained granular- ity. 
Accuracy for the coarse-grained task is in 
all experiments higher than for the fine-
grained one.

5.1	Word Extraction
In order to analyze the impact of the 
extracPtaiogen 1 method, we compare parsing  
versus POS-based heuristics for word 
extraction.
  Both O’Hara and Wiebe (2009) and Tratz 
and Hovy (2009)  use constituency  parsers 
to prepro- cess the data. However, parsing 
accuracy varies,




and the problem of PP attachment ambiguity  in- 
creases the likelihood of wrong extractions. This 
is especially  troublesome  in  the present  case, 
where we focus on prepositions.5 We  use the 
MALT parser (Nivre et al., 2007), a state-of-the- 
art dependency parser, to extract the governor and 
object.
  The alternative is a POS-based heuristics  ap- 
proach. The only preprocessing  step needed is 
POS tagging of the data, for which we used the 
system of Shen et al. (2007).  We then use simple 
heuristics to locate the prepositions and their re- 
lated words. In order to determine the governor 
in the absence of constituent  phrases, we consider 
the possible governing noun, verb, and adjective. 
The object of the preposition is extracted  as first 
noun phrase head to the right. This approach is 
faster than parsing, but has problems  with long- 
range dependencies and fronting of the PP (e.g., 
the PP appearing earlier  in the sentence than its
governor).




C
o
n
t
e
x
t
co
ars
e
fi
n
e
2-
wor
d 
win
do
w
9
1
.
6
8
0
.
4
3-
wor
d 
win
do
w
9
2
.
0
8
1
.
4
4-
wor
d 
win
do
w
9
1
.
6
7
9
.
8
5-
wor
d 
win
do
w
9
1
.
0
7
8
.
7
Go
ver
nor, 
pre
p
8
0
.
7
7
8
.
9
Pre
p, 
obj
ect
9
4
.
2
5
6
.
9
Go
ver
nor, 
pre
p, 
obj
ect
9
4
.
0
8
4
.
8

Table 2:  Accuracies (%) for Different Context
Types and Sizes



  The results show that the approach using both 
governor and object is the most accurate one. Of 
the fixed-window-size  approaches, three words to 
either side works best. This does not necessarily 
reflect a general property of that window size, but 
can be explained by the fact that most governors 
and objects occur within this window size.7 This


context


word sedleicstitoannce can vary from corpus to corpus, so win-
dow size would have to be determined  individu- 
ally for each task. The difference between using 
governor and preposition  versus preposition  and 
object between coarse and fine-grained classifica- 
tion might reflect the annotation process:  while


Table 1: Accuracies (%) for Word-Extraction Us- 
ing MALT Parser or Heuristics.


  Interestingly,  the extraction  method  does not 
significantly affect the final score for fine-grained 
PSD (see Table 1). The high score achieved when 
using the MALT  parse for coarse-grained PSD 
can be explained by the fact that the parser was 
originally trained on that data set.  The good re- 
sults we see when using heuristics-based  extrac- 
tion only, however,  means we can achieve high- 
accuracy PSD even without parsing.

5.2	Context
We compare the effects of fixed window size ver-


Litkowski and Hargraves (2007)  
selected exam- ples based on a search for 
governors8, most anno- tators in the PTB 
may have based their decision of the PP 
label on the object that occurs in it. We 
conclude that syntactically related words 
present a better context for classification 
than fixed window sizes.

5.3   
Featu
res

Having  established the context we want to 
use, we now turn to the details of 
extracting the feature words from that 
context.9  Using higher-level fea- tures 
instead of lexical  ones helps accounting 
for sparse training  data (given an 
infinite amount of data, we would not 
need to take any higher-level


sus syntactically related words as context. Table 2	 	


shows the results for the different  types and sizes 
of contexts.6

     5 Rudzicz and Mokhov (2003) actually motivate  their 
work as a means to achieve better PP attachment resolution.
     6 See also (Yarowsky and Florian, 2002) for experiments 
on the effect of varying window size for WSD.
  

7 Based on such statistics, O’Hara and Wiebe (2003) ac- 
tually set their window  size to 5.
8 Personal communication.
     9 As one reviewer pointed out, these two dimensions are 
highly interrelated and influence each other. To examine the 
effects, we keep one dimension  constant while varying the 
other.








Page 1


features into account, since every case would be 
covered). Compare O’Hara and Wiebe (2009).
  Following the prepocessing,  we use  a set  of 
rules to select the feature words,  and then gen- 
erate feature  values  from them using a  variety 
of  feature-generating  functions.10    The word- 
selection rules are listed below.

Word-Selection Rules
• Governor from the MALT parse
• Object from the MALT parse
• Heuristically determined object of the prepo- 
sition
• First verb to the left of the preposition
• First verb/noun/adjective to the left of the 
preposition
• Union  of  (First  verb to  the left,  First 
verb/noun/adjective to the left)
• First word to the left

  The feature-generating  functions, many of 
which utilize WordNet (Fellbaum, 1998), are 
listed below. To conserve space, curly braces are 
used to represent multiple functions in a single 
line. The name of each feature is the combination 
of the word-selection rule and the output from the 
feature-generating function.

WordNet-based Features
• {Hypernyms,   Synonyms}   for  {1st,   all}
sense(s) of the word
• All terms in the definitions (‘glosses’) of the 
word
• Lexicographer file names for the word
• Lists of all link types (e.g., meronym links)
associated with the word
• Part-of-speech indicators for the existence of
NN/VB/JJ/RB  entries for the word
• All sentence frames for the word
• All {part, member, substance}-of holonyms 
for the word
• All sentence frames for the word

Other Features
• Indicator that the word-finding  rule found  a 
word
10 Some words may be selected by multiple word-selection


• Capitalization indicator
• {Lemma, surface form} of the word
• Part-of-speech tag for the word
• General POS tag for the word (e.g. NNS →
NN, VBZ → VB)
• The {first, last} {two, three} letters of 
each word
• Indicators  for   suffix   types  (e.g.,   
de- adjectival,     de-nominal    
[non]agentive, de-verbal [non]agentive)
• Indicators for a wide variety of other 
affixes including those related to degree, 
number, or- der, etc. (e.g., ultra-, poly-, 
post-)
• Roget’s Thesaurus divisions for the word

  To establish the impact of each feature word 
on the outcome, we use leave-one-out and 
only-one evaluation.11   The results can be found 
in Table 3. A word that does not perform well 
as the only at- tribute may still be important in 
conjunction with others. Conversely, leaving 
out a word may not hurt performance, despite 
being a good single at-
tribute.                                              word 
selection


coars
e
f
i
n
e
W
o
r
d
LO
O
Onl
y
LO
O
Onl
y
MAL
T 
gov
erno
r
92
.1
80
.1
84
.3
78
.9
MAL
T 
obje
ct
93
.4
94
.2
84
.9
56
.3
Heu
ristic
s 
VB 
to 
left
92
.0
77
.9
85
.0
62
.1
Heu
r. 
NN/
VB/
ADJ 
to 
left
92
.1
78
.7
84
.3
78
.5
Heu
r. 
Gov
erno
r 
Unio
n
92
.1
78
.4
84
.5
81
.0
Heu
ristic
s 
wor
d to 
left
92
.0
78
.8
84
.4
77
.2
Heu
ristic
s 
obje
ct
91
.9
93
.0
84
.9
56
.8
non
e
91
.8
–
84
.8
–

Table  3:     Accuracies (%)  for   Leave-
One- Out (LOO) and Only-One  Word-
Extraction-Rule Evaluation.  none includes all 
words and serves for comparison. Important 
words reduce accuracy for LOO,  but rank high 
when used as only rule.


  Independent of the extraction method 
(MALT parser or POS-based heuristics),  the 
governor is the most informative word.  
Combining several heuristics to locate the 
governor is the best sin- gle feature for fine-
grained classification. The rule looking only for 
a governing  verb fails to account


rules.   For  example,  the  governor  of  the  preposition  may	 	


be identified by the Governor from MALT parse rule, first 
noun/verb/adjective to left, and the first word to the left rule.
  

11 Since the feature words are not independent of one 
an- other, neither of the two measures is decisive on its 
own.


full both


f
i
n
e
coar
se

f
i
n
e
coar
se
P
r
e
p
Tot
al
Ac
c
Tot
al
Ac
c

P
r
e
p
Tot
al
Ac
c
To
tal
Ac
c
abo
ard
–
–
6
10
0.0

like
1
2
5
9
0.
4
5
3
4
7.
2
abo
ut
3
6
4
9
4.
0
5
8
0.
0

nea
r
–
–
7
4
9
3.
2
abo
ve
2
3
6
9.
6
7
8
6
5.
4

nea
rest
–
–
1
0
.
0
acr
oss
1
5
1
9
6.
7
8
7
7
9.
3

nex
t
–
–
7
7
1.
4
afte
r
5
3
7
9.
2
8
4
1
9
2.
5

of
14
78
8
7.
9
7
1
6
4.
8
aga
inst
9
2
9
2.
4
1
6
4
3.
8

off
7
6
8
4.
2
2
8
7
5.
0
alo
ng
1
7
3
9
6.
0
4
5
7
1.
1

on
4
4
1
8
1.
4
2
2
8
7
9
0.
8
alo
ngsi
de
–
–
5
8
0.
0

ont
o
5
8
9
1.
4
1
5
5
3.
3
ami
d
–
–
5
8
7
0.
7

out
–
–
9
0
6
8.
9
am
ong
5
0
8
0.
0
3
5
8
9
3.
9

out
side
–
–
6
2
9
0.
3
am
ong
st
–
–
1
0
.
0

ove
r
9
8
7
9.
6
4
1
7
8
9.
4
aro
und
1
5
5
6
9.
0
1
0
7
8
6.
0

pas
t
–
–
6
8
3.
3
as
8
4
10
0.0
2
3
2
8
4.
5

per
–
–
3
10
0.0
astr
ide
–
–
2
5
0.
0

rou
nd
8
2
6
5.
9
–
–
at
3
6
7
8
6.
4
30
78
9
2.
0

sinc
e
–
–
4
4
9
9
4.
4
ato
p
–
–
5
10
0.0

tha
n
–
–
2
0
.
0
bec
aus
e
–
–
4
2
0
9
1.
7

thro
ugh
2
0
8
4
8.
1
3
6
4
6
9.
0
bef
ore
2
0
9
0.
0
3
8
4
8
3.
3

thro
ugh
out
–
–
6
2
9
3.
5
beh
ind
6
8
7
7.
9
6
5
8
7.
7

till
–
–
3
10
0.0
bel
ow
–
–
9
4
7
1.
3

to
5
7
2
8
9.
7
3
1
6
6
9
7.
5
ben
eat
h
2
8
7
8.
6
1
1
7
2.
7

tow
ard
–
–
5
5
6
5.
5
besi
de
2
9
10
0.0
4
10
0.0

tow
ard
s
1
0
2
9
7.
1
2
10
0.0
besi
des
–
–
1
0
.
0

und
er
–
–
6
0
4
9
1.
4
bet
wee
n
1
0
2
9
4.
1
9
8
8
4.
7

und
ern
eat
h
–
–
2
5
0.
0
bey
ond
–
–
4
5
6
4.
4

until
–
–
2
0
8
9
4.
2
by
2
4
8
8
8.
3
13
41
8
7.
5

up
–
–
2
0
7
5.
0
dow
n
1
5
3
8
1.
7
1
6
5
6.
2

upo
n
–
–
2
3
7
3.
9
duri
ng
3
9
8
7.
2
5
4
7
9
2.
1

via
–
–
2
2
4
0.
9
exc
ept
–
–
1
0
.
0

whe
ther
–
–
1
10
0.0
for
4
7
8
8
2.
4
14
55
8
4.
5

whil
e
–
–
3
3
3.
3
fro
m
5
7
8
8
5.
5
17
12
9
0.
5

with
5
7
8
8
4.
4
2
7
2
6
9.
5
in
6
8
8
7
7.
0
157
06
9
5.
0

with
in
–
–
2
1
3
9
6.
2
insi
de
3
8
7
3.
7
2
4
9
1.
7

with
out
–
–
6
9
6
3.
8
into
2
9
7
8
6.
2
4
1
5
8
0.
0







Ov
eral
l
80
96
8
4.
8
35
91
7
9
1.
8

Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics.  Sorted by 
preposition.




for noun governors, which consequently leads to 
a slight improvement when left out.

  Curiously, the word directly to the left is a bet- 
ter single feature than the object (for fine-grained 
classification). Leaving either of them out in-


creases accuracy,  which implies that their infor- 
mation can be covered by other words.

Page 1


coarse both 2009


Most 
Frequent 
Sense
O'Hara/Wieb
e 2009
10-fold 
CV
Cla
ss
pre
c
re
c
f
1
pre
c
re
c
f
1
pre
c
re
c
f
1
L
O
C 
T
M
P 
DI
R 
M
N
R 
P
R
P 
E
X
T 
B
N
F
7
1.
8
7
7.
5
9
1.
6
6
9.
9
7
8.
2
0
.
0
0
.
0
9
7.
4
3
9.
4
9
4.
2
4
3.
2
4
8.
8
0
.
0
0
.
0
8
2.
6
5
2.
3
9
2.
8
5
3.
4
6
0.
1
0
.
0
0
.
0
9
0.
8
8
4.
5
9
5.
6
8
2.
6
7
9.
3
8
1.
7
–
9
3.
2
8
5.
2
9
6.
5
5
5.
8
7
0.
1
8
4.
6
–
9
2.
0
8
4.
8
9
6.
1
6
6.
1
7
4.
4
8
2.
9
–
9
4.
7
9
4.
6
9
4.
6
8
3.
3
9
0.
6
8
7.
5
7
5.
0
9
6.
4
9
4.
6
9
4.
5
7
5.
0
8
3.
8
8
2.
1
3
4.
1
9
5.
6
9
4.
6
9
4.
5
7
8.
9
8
7.
1
8
4.
7
4
6.
9

Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification. Comparison to O’Hara 
and Wiebe (2009).  Classes ordered by frequency




5.4   Comparison with Related Work
To situate  our experimental  results  within the 
body of work on PSD, we compare them to both 
a most-frequent-sense baseline and existing  work 
for both granularities  (see Table 6).  The results 
use a syntactically   selective  context  of preposi- 
tion, governor,  object, and word to the left as 
determined by combined extraction information 
(POS tagging and parsing).


strictly comparable, yet they provide  a sense of 
the general task difficulty. See Table 5. We note 
that both systems perform  better than the most- 
frequent-sense baseline. DIR is reliably classified 
using the baseline, while EXT and BNF are never 
selected for any preposition. Our method  adds 
considerably to the scores for most classes.  The 
low score for BNF is mainly due to the low num- 
ber of instances in the data, which is why it was


accuracieexs cluded by O’Hara and Wiebe (2009).










Table 6:  Accuracies (%) for Different Classifi- 
cations.  Comparison  with O’Hara and Wiebe 
(2009)*, and Tratz and Hovy (2009)**.


  Our system easily exceeds the baseline for both 
coarse and fine-grained  PSD (see Table 6). Com- 
parison with related work shows that we achieve 
an improvement of 6.5% over Tratz and Hovy 
(2009), which is significant at p < .0001, and 
of 4.5% over O’Hara and Wiebe (2009), which is 
significant at p < .0001.
  A detailed overview over all prepositions for 
frequencies  and accuracies  of both coarse  and 
fine-grained PSD can be found in Table 4.
  In addition to overall accuracy,  O’Hara and 
Wiebe (2009) also measure precision, recall and 
F-measure for the different classes. They omitted 
BNF because it is so infrequent.  Due to different 
training  data and models, the two systems are not


6   Conclusion

To  get maximal accuracy in  disambiguating 
prepositions—and  also other word classes—one 
needs to consider context, features, and granular- 
ity. We presented an evaluation of these parame- 
ters for preposition sense disambiguation (PSD).
  We find that selective  context  is better than 
fixed window size. Within the context for prepo- 
sitions, the governor (head of the NP or VP gov- 
erning the preposition), the object of the prepo- 
sition (i.e., head of the NP to the right), and the 
word directly to the left of the preposition have 
the highest influence.12  This corroborates the lin- 
guistic intuition  that close mutual constraints hold 
between the elements of the PP. Each word syn- 
tactically  and semantically restricts the choice of 
the other elements.  Combining  different  extrac- 
tion mePtahgoed1s (POS-based heuristics  and depen- 
dency parsing) works better than either one in iso- 
lation, though high accuracy can be achieved just 
using heuristics.  The impact of context and fea- 
tures varies somewhat for different granularities.

12 These will likely differ for other word classes.


Not surprisingly, we see higher scores for coarser 
granularity than for the more fine-grained one.
  We measured success in accuracy, precision, re- 
call, and F-measure, and compared our results to 
a most-frequent-sense baseline and existing work. 
We were able to improve over state-of-the-art sys- 
tems in both coarse and fine-grained PSD, achiev- 
ing accuracies of 91.8% and 84.8% respectively.

Acknowledgements

The authors would like to thank Steve DeNeefe, 
Victoria Fossum, and Zornitsa Kozareva for com- 
ments and suggestions. StephenTratz is supported 
by a National Defense Science and Engineering 
fellowship.



References

Baker, C.F., C.J. Fillmore, and J.B. Lowe.   1998.
The Berkeley FrameNet Project. In Proceedings of 
the 17th international conference on Computational 
linguistics-Volume 1, pages 86–90. Association  for 
Computational Linguistics Morristown, NJ, USA.

Berger, A.L., V.J. Della Pietra, and S.A. Della Pietra.
1996.  A maximum  entropy approach to natural 
language processing. Computational Linguistics,
22(1):39–71.

Chan, Y.S., H.T. Ng, and D. Chiang. 2007. Word sense 
disambiguation improves statistical machine trans- 
lation. In Annual Meeting – Association For Com- 
putational Linguistics, volume 45, pages 33–40.

Chiang, D., K. Knight, and W. Wang. 2009. 11,001 
new features  for  statistical machine translation. 
In Proceedings of Human Language Technologies: 
The 2009 Annual Conference of the North American 
Chapter of the Association for Computational Lin- 
guistics, pages 218–226, Boulder,  Colorado,  June. 
Association for Computational Linguistics.

Fellbaum, C.  1998. WordNet: an electronic lexical 
database. MIT Press USA.

Hwang,  J. D., R. D. Nielsen,  and M. Palmer.  2010.
Towards a domain independent semantics: Enhanc- 
ing semantic representation with construction gram- 
mar. In Proceedings of the NAACL HLT Workshop 
on Extracting and Using Constructions in Computa- 
tional Linguistics, pages 1–8, Los Angeles, Califor- 
nia, June. Association  for Computational Linguis- 
tics.


Litkowski, K. and O. Hargraves.  2005. The preposi- 
tion project. ACL-SIGSEM Workshop on “The Lin- 
guistic Dimensions of Prepositions and Their Use in 
Computational Linguistic Formalisms and Applica- 
tions”, pages 171–179.

Litkowski, K. and O. Hargraves. 2007. SemEval-2007
Task 06: Word-Sense Disambiguation  of Preposi- 
tions. In Proceedings of the 4th International  Work- 
shop on Semantic  Evaluations (SemEval-2007), 
Prague, Czech Republic.

Marcus, M.P., M.A. Marcinkiewicz,  and B. Santorini.
1993.  Building a  large annotated corpus of En- 
glish: the Penn TreeBank.  Computational  Linguis- 
tics, 19(2):313–330.

McCallum, A.K. 2002. MALLET: A Machine Learn- 
ing for Language Toolkit. 2002. http://mallet. cs. 
umass. edu.

Nivre, J., J. Hall, J. Nilsson,  A. Chanev, G. Eryigit, 
S. Ku¨ bler, S. Marinov,  and E. Marsi. 2007. Malt- 
Parser:  A language-independent system for data- 
driven dependency parsing.  Natural  Language En- 
gineering, 13(02):95–135.

O’Hara, T. and J. Wiebe. 2003. Preposition semantic 
classification via Penn Treebank and FrameNet.  In 
Proceedings of CoNLL,  pages 79–86.

O’Hara, T. and J. Wiebe. 2009. Exploiting seman- 
tic role resources for preposition disambiguation. 
Computational Linguistics, 35(2):151–184.

Rudzicz, F.  and S. A.  Mokhov.     2003.     To- 
wards  a    heuristic  categorization of   prepo- 
sitional		phrases     in    english   with    word- 
net.	Technical report,  Cornell  University, 
arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.

Shen, L., G. Satta, and A. Joshi. 2007. Guided learn- 
ing for bidirectional sequence classification. In Pro- 
ceedings of the 45th Annual Meeting of the Associa- 
tion of Computational Linguistics, volume 45, pages
760–767.

Tratz, S. and D. Hovy.   2009.  Disambiguation of 
preposition sense using linguistically motivated fea- 
tures. In Proceedings of Human Language Tech- 
nologies: The 2009 Annual Conference of the North 
American Chapter of the Association for Computa- 
tional Linguistics,  Companion Volume: Student Re- 
search Workshop and Doctoral Consortium,  pages
96–100, Boulder,  Colorado,  June. Association  for
Computational Linguistics.

Yarowsky, D. and R. Florian. 2002. Evaluating  sense 
disambiguation  across diverse parameter spaces. 
Natural Language Engineering, 8(4):293–310.


Ye, P. and T. Baldwin. 2007. MELB-YB: Preposition 
Sense Disambiguation   Using Rich Semantic Fea- 
tures. In Proceedings of the 4th International  Work- 
shop on Semantic  Evaluations (SemEval-2007), 
Prague, Czech Republic.


