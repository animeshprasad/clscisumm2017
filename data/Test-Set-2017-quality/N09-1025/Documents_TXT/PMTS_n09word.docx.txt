Domain Adaptation in Statistical Machine Translation of User-Forum Data 
using Component-Level Mixture Modelling

Pratyush Banerjee, Sudip Kumar Naskar, Johann Roturier1, Andy Way2∗, Josef van Genabith
CNGL, School of Computing, Dublin City University, Dublin, Ireland
{pbanerjee,snaskar,josef}@computing.dcu.ie
1 Symantec Limited, Dublin, Ireland
johann roturier@symantec.com
2 Applied Language Solutions, Delph, UK
andy.way@appliedlanguage.com





Abstract

This paper reports experiments on adapting 
components of a Statistical Machine Trans- 
lation (SMT) system for the task of trans- 
lating online user-generated forum data from 
Symantec.   Such data is monolingual, and 
differs from available bitext MT training re- 
sources in a number of important respects. For 
this reason, adaptation techniques are impor- 
tant to achieve optimal results.  We investi- 
gate the use of mixture modelling to adapt 
our models for this speciﬁc task.  Individual 
models, created from different in-domain and 
out-of-domain data sources, are combined us- 
ing linear and log-linear weighting methods 
for the different components of an SMT sys- 
tem. The results show a more profound effect 
of language model adaptation over translation 
model adaptation with respect to translation 
quality. Surprisingly, linear combination out- 
performs log-linear combination of the mod- 
els.  The best adapted systems provide a sta- 
tistically signiﬁcant improvement of 1.78 ab- 
solute BLEU points (6.85% relative) and 2.73 
absolute BLEU points (8.05% relative) over 
the baseline system for English–German and 
English–French, respectively.


1   Introduction

In recent years, Statistical Machine Translation 
(SMT) technology has been used in many online 
applications, concentrating on professionally edited 
enterprise  quality  online  content.    At  the  same 
time,  very little research has gone into adapting
∗Work done while at CNGL, School of Computing, DCU


SMT technology to the translation of user-
generated content on the web.   While translation 
of online chats (Flournoy and Callison-Burch, 
2000) has re- ceived some attention, there is 
surprisingly little work on translation of online 
user forum data, de- spite growing interest in the 
area (Flournoy and Rueppel, 2010). In this paper 
we describe our efforts in building a system to 
address this particular appli- cation area. Our 
experiments are conducted on data collected from 
online forums on Symantec Security tools and 
services.1    For a multinational company like 
Symantec, the primary motivation behind trans- 
lation of user forum data is to enable access across 
language barriers to information in the forums. Fo- 
rum posts are rich in information about issues and 
problems with tools and services provided by the 
company, and often provide solutions to problems 
even before traditional customer-care help lines are 
even aware of them.
  The major challenge in developing MT systems 
for user forum data concerns the lack of proper 
parallel training material.   Forum data is mono- 
lingual and hence cannot be used directly to train 
SMT systems.  We use parallel training data in the 
form of Symantec Enterprise Translation 
Memories (TMs) from different product and 
service domains to train the SMT models. As an 
auxiliary source, we also used portions of the 
Europarl dataset2 (Koehn,
2005), selected according to their similarity with 
the forum data (Section 3.2), to supplement the 
TM- based training data.   Symantec TM data, 
being a part of enterprise documentation, is 
professionally

1 http://community.norton.com/
2 http://www.statmt.org/europarl/


edited and by and large conforms to the Symantec 
controlled language guidelines, and is signiﬁcantly 
different in nature from the user forum data, which 
is loosely moderated and does not use controlled 
language at all.   In contrast Europarl data is out- 
of-domain with respect to the forum data. The dif- 
ferences between available training and test datasets 
necessitate the use of adaptation techniques for op- 
timal translation.   We use mixture model adapta- 
tion (Foster and Kuhn, 2007), creating individual 
models from different sources of data and combin- 
ing them using different weights.  Monolingual fo- 
rum posts were used for language modelling along 
with the target side of the TM training data. A sys- 
tem trained only on the Symantec TM and forum 
data serves as the baseline system.  All our exper- 
iments are conducted on the English-German (En– 
De) and English-French (En–Fr) language pairs with 
a special emphasis on translation from English. For 
the sake of completeness however, we report trans- 
lation scores for both directions here.
  Apart from using models created from concate- 
nation of in-domain (Symantec TM) and out-of- 
domain (Europarl) datasets, we used linear and log- 
linear combination frameworks to combine individ- 
ual models.  Both translation models and language 
models were separately combined using the two 
methods and the effect of the adaptation was mea- 
sured on the translation output using established au- 
tomatic evaluation metrics. Our experiments reveal 
that for the current task, in terms of translation qual- 
ity, language model adaptation is more effective than 
translation model adaptation and linear combination 
performs slightly better than the log-linear setting.
  The remainder of this paper is organized as fol- 
lows: Section 2 brieﬂy describes related work rel- 
evant to the context.   Section 3 reports the tools 
and algorithms used along with a description of the 
datasets used.  Section 4 focuses on the mixture 
modelling experiments and how weights are learnt 
in different settings. Section 5 presents the experi- 
ments and analysis of results, followed by conclu- 
sions and future work in Section 6.

2   Related Work

Mixture Modelling (Hastie et al., 2001), a well- 
established technique for combining multiple mod-


els, has been extensively used for language model 
adaptation, especially in speech recognition.  Iyer 
and Ostendorf (1996) use this technique to cap- 
ture topic dependencies of words across sentences 
within language models.   Cache-based language 
models (Kuhn and De Mori, 1990) and dynamic 
adaptation of language models (Kneser and Stein- 
biss, 1993) for speech recognition successfully use 
this technique for sub-model combinations.
  Langlais (2002) introduced the concept of domain 
adaptation in SMT by integrating domain-speciﬁc 
lexicons in the translation model, resulting in sig- 
niﬁcant improvement in Word Error Rate.  Eck et 
al. (2004) utilized information retrieval theories to 
propose a language model adaptation technique in 
SMT. Hildebrand (2005) utilized this approach to se- 
lect similar sentences from available training data to 
adapt translation models, which improved transla- 
tion performance with respect to a baseline system. 
Wu et al. (2008) used a combination of in-domain 
bilingual dictionaries and monolingual data to per- 
form domain adaptation for SMT in a setting where 
in-domain bilingual data was absent. Integrating an 
in-domain language model with an out-of-domain 
one using log-linear features of a phrase-based SMT 
system is reported by Koehn and Schroeder (2007). 
Foster and Kuhn (2007) used mixture modelling 
to combine multiple models trained on different 
sources and learn mixture weights based on distance 
of the test set from the training data.  Civera and 
Juan (2007) further suggested a mixture adaptation 
approach to word alignment, generating domain- 
speciﬁc Viterbi alignments to feed a state-of-the-art 
phrase-based SMT system.
  Our work follows the line of research presented 
in Foster and Kuhn (2007) using mixture modelling 
and linear/log-linear combination frameworks, but 
differs in terms of the test set and development sets 
used for tuning and evaluation.  While Foster and 
Kuhn (2007) used test and development sets which 
were essentially a combination of data from differ- 
ent training genres, in our case test data (user forum) 
are inherently different from the training data. Our 
methods of estimating the linear weights for lan- 
guage and translation models are also different to 
the ones proposed in Foster and Kuhn (2007).  As 
part of our experiments, we also resort to selecting 
portions of relevant bitext from out-of-domain cor-


pora to augment available training data as described 
in Hildebrand et al. (2005).  However, our work is 
different from their approach in the use of language 
model perplexity as an indicator of relevance of the 
selected data.  Furthermore, due to the differences 
between the training and target datasets, we selected 
additional data in terms of its relevance to the target 
domain instead of the training domain.


3   Datasets, Pre-processing and Tools

3.1   Symantec Datasets

Our primary training data consists of En–De and 
En–Fr bilingual datasets in the form of Symantec 
TMs.   Monolingual Symantec forum posts in all 
three languages served as language modelling data. 
As the purpose of our experiments is to translate 
forum posts, the data for the development and the 
test sets were randomly selected from the monolin- 
gual English forum data. After being translated us- 
ing Google Translate,3 these datasets were manu- 
ally post-edited by professional translators follow- 
ing guidelines4 for achieving ‘good enough quality’, 
in order to generate bilingual development (dev) and 
test sets. The selected test data was excluded from 
the English forum data used to create language mod- 
els in the experiments.


Da
ta 
Se
t
E
n
–
D
e
E
n
–
F
r
Sy
ma
nte
c 
T
M
6
3
8
6
0
0
5
6
7
6
4
1
Eu
ro
par
l
7056
76 
(∼40
%)
4146
67 
(∼23
%)
De
vel
op
me
nt 
Set
5
0
0
5
0
0
Te
st 
Set
6
1
2
6
1
2
En
gli
sh 
Fo
ru
m
1
0
6
9
4
6
4
Ge
rm
an 
Fo
ru
m
2
5
1
6
9
Fr
en
ch 
Fo
ru
m
2
2
9
3
2

Table 1: Number of Sentences for bilingual training, de- 
velopment and test and monolingual forum data sets


  Apart from the Symantec datasets, we used por- 
tions of the Europarl dataset (Section 3.2) to supple- 
ment the training data. Table 1 presents the numbers 
of sentences for each of the resources used in our 
experiments.

3 http://translate.google.com/
4 http://www.cngl.ie/node/2542


3.2   Extracting Relevant Data from Europarl
Given that we needed additional resources to im- 
prove translation coverage, we selected the Europarl 
dataset, containing parallel sentences of the proceed- 
ings of the European Parliament. However, Europarl 
data is clearly out-of-domain given our speciﬁc task, 
but much larger in size than the Symantec TM data. 
For this reason, we decided to select only a por- 
tion of the Europarl data in order to balance the 
amount of in-domain and out-of-domain data.   In 
order to achieve this, the entire set of Europarl sen- 
tences were ranked using the sentence-level perplex- 
ity scores with respect to language models created 
on the monolingual forum data.  Only a portion of 
the ranked list with scores lower than a manually 
chosen threshold (perplexity value of 350) were se- 
lected for our experiments. Lower perplexity scores 
of the included sentences indicate a closer ﬁt (hence 
higher relevance) to the forum data. This technique 
enables us to select the most ‘forum-like’ sentences 
from Europarl.  The number of selected Europarl 
sentences, as reported in Table 1, constitute about
40% and 23% of the total Europarl sentences for En– 
De and En–Fr language pairs respectively.

3.3   Preprocessing and Data Cleanup

Re: No right click scan
No i copyed the ﬁle in stead of creating shortcut,LOL I did it with 
the shortcut and it works just ﬁne, :) Thanks
2008-10-19T23:14:38+00:00
Re: Norton AntiBot - possible vulnerability?
This	has		been	answered		on	a	 separate	thread: 
http://community.norton.com/norton/board/message?board.id= 
other&thread.id=2533&jump=true I am locking this thread now; 
avibuzz		wrote:Did		not		work		I	 went		  the 
highkey		 below	 and	could	  not	ﬁnd		any-
thing...HKEY LOCAL MACHINE\SOFTWARE\Microsoft\
Windows\CurrentVersion\Run What did you ﬁnd when you click
on that key?

Table 2:  Few examples of the untranslatable tokens in 
forum posts

  The Symantec TM datasets and the forum posts 
contain many tokens unsuitable for translation in- 
cluding:   URLs,  ﬁle paths and ﬁle names,  Win- 
dows registry entries, date and time stamps, XML 
and HTML tags, smilies, text-speak and garbage 
characters.  Table 2 shows a few examples of fo- 
rum posts containing such tokens, which we han- 
dled in the pre-processing steps using regular ex- 
pressions to replace them with unique place hold-


ers.  In the post-processing step, the place holders 
were replaced with the actual tokens, except for the 
smilies, text-speak and garbage characters. For en- 
tries with multiple tokens of a single type, tokens 
were replaced in the translation in the same order 
as they appeared in the source.  Furthermore, prior 
to training, all datasets involved in the experiments 
were subjected to deduplication, lower casing and 
tokenization.


3.4   Translation and Language Models

For our translation experiments we used OpenMa- 
TrEx (Stroppa and Way, 2006), an open source SMT 
system which provides a wrapper around the stan- 
dard log-linear phrase-based SMT system Moses 
(Koehn et al., 2007).   Word alignment was per- 
formed using Giza++ (Och and Ney, 2003).  The 
phrase and the reordering tables were built on the 
word alignments using the Moses training script. 
The feature weights for the log-linear combination 
of the feature functions were tuned using Minimum 
Error Rate Training (MERT) (Och, 2003) on the de- 
vset in terms of BLEU (Papineni et al., 2002).
  We used 5-gram language models in all our ex- 
periments created using the IRSTLM (Federico et 
al., 2008) language modelling toolkit using Modi- 
ﬁed Kneser-Ney smoothing (Kneser and Ney, 1995). 
Learning linear mixture weights for combining mul- 
tiple language models with respect to the develop- 
ment set was performed using the IRSTLM lan- 
guage model interpolation tools. Results of transla- 
tions in every phase of our experiments were eval- 
uated using BLEU and NIST (Doddington, 2002) 
scores.


4   Mixture Adaptation

In the experiments reported in this paper, mixture 
adaptation is involved in creating individual models 
from separate data sources, learning mixture weights 
for each model and ﬁnally using the weighted mix- 
ture of models to translate the forum data test set 
sentences. The models were combined using linear 
and log-linear combination frameworks to compare 
the effect of the combination techniques on transla- 
tion. This section details the different aspects of the 
mixture adaptation.


4.1   Model Combination using Linear Weights
Individual translation or language models were lin- 
early interpolated using the formula in (1):

p(x|h) =     λsps(x|h)	(1)
s

where p(x|h) is the language model probability or 
the translation model probability, ps(x|h) is the par-
ticular model trained on the training resource s, and 
λs  is the corresponding weight of the particular re- 
source, all of which sum up to 1.  For a linear- 
interpolated model, the resource weights are global 
weights unlike the model feature weights mentioned 
in Sub-section 3.4.  Hence, during tuning, the lin- 
ear mixture weights do not directly participate in the 
log-linear combination of model features.
  In  order  to  set  the  linear  mixture  weights 
for language models, we used the Expectation- 
Maximization  (EM)  algorithm  (Dempster  et  al.,
1977) to estimate optimal weights of the individ- 
ual language models with respect to the target side 
of the devset.  Initially all models are uniformly 
weighted and the EM algorithm iteratively opti- 
mizes the weights until a predeﬁned convergence 
criterion is met.   For translation models, we used 
a slightly different method to estimate the mixture 
weights for multiple phrase tables from different re- 
sources. Since the maximum phrase length for our 
SMT phrase-tables had been set to 7, we constructed
7-gram language models using the source side of 
the training data for each resource.  The mixture 
weights of these language models were estimated on 
the devset, again using the EM algorithm.  Finally 
the weights learned were used to combine different 
phrase tables. The weights set by the EM algorithm 
essentially denote the ﬁtness of each data source 
with respect to the devset. Standard algorithms like 
MERT cannot effectively be used in estimating lin- 
ear weights for the translation models as they are de- 
signed speciﬁcally for ﬂat log-linear models (Foster 
and Kuhn, 2007).
  The phrase tables constructed from the training 
data using Moses feature ﬁve sets of scores.
1. Inverse phrase translation probability: φ(f |e)
2. Inverse lexical weight: lex(f |e)
3. Direct phrase translation probabilities: φ(e|f )


4. Direct lexical weight: lex(e|f )
5. Phrase penalty: (always exp(1) = 2.718)

where f is the source phrase and e denotes the cor- 
responding target phrase.   Linearly mixing differ- 
ent phrase tables required combining their phrase 
translation probabilities and lexical weights as per 
equation (1) with linear mixture weights learnt us- 
ing the EM algorithm.  However, only the phrase 
pairs common to all the phrase tables were mixed; 
other phrase pairs were simply added to generate a 
single mixture-adapted phrase table.

4.2   Model Combination using Log-Linear
Weights
We combine multiple models under the log-linear 
combination framework as described in equation 
(2):
p(x|h) = n ps(x|h)αs	(2)
s
where αs  is the log-linear weight for the model
ps(x|h) trained on the training resource s.
The advantage of using the log-linear mixture of
models is that it easily ﬁts into the log-linear frame- 
work that the SMT model is built upon. The mixture 
weights were estimated by running MERT on the de- 
vset with multiple phrase tables and language mod- 
els. Since MERT directly optimizes the feature func- 
tion weights for each available model, simply adding 
the different phrase tables and/or language models 
to the Moses conﬁguration and using the multiple 
decoding path functionality (Koehn and Schroeder,
2007) of the decoder allowed us to estimate the log- 
linear mixture weights for each model. An added ad- 
vantage is the fact that the weights are optimized not 
in terms of ﬁtness to the target domain, but directly 
in terms of translation scores for the target domain. 
However, using multiple phrase tables and language 
models greatly increases the number of features to 
be optimized, thus reducing the chances of success- 
ful convergence of the MERT algorithm.

5   Experiments and Results

The adaptation experiments were conducted in three 
separate phases with different adaptation settings for 
the translation models. Within each phase, three dif- 
ferent adaptation settings for language models were 
used. Conducting separate experiments for language


and translation model adaptation allowed us to ex- 
amine the effect of mixture modelling for the task at 
hand, as well as observing the effect of adaptation 
at each component level of an SMT system.  The 
details of the baseline system and each phase are de- 
scribed in the following sections.

5.1   Baseline: Unadapted Model
The baseline system used in our experiments was 
a vanilla Moses system trained with the different 
Symantec datasets we had at our disposal. The trans- 
lation models were trained on the Symantec TM 
data, and the language models were trained on the 
monolingual forum data along with the target side 
of the bilingual TM data. In order to keep the base- 
line model unadapted, the selected ‘forum-like’ Eu- 
roparl data was deliberately excluded in training the 
baseline system, since using relevant out-of-domain 
data for training can be considered to be a type of 
adaptation.

5.2   Phase-1: Language Model Adaptation with
Unadapted Translation Model
In this phase of experiments our primary objective 
was to observe the effect of mixture adaptation on 
the language models for the task of forum data trans- 
lation. In order to keep the translation model free of 
any adaptation, we simply concatenated together the 
Symantec TM and ‘forum-like’ Europarl (TM+EP) 
datasets to create a single model. For language mod- 
elling, we had three distinct data sources at our dis- 
posal: the monolingual forum posts, the target side 
of the Symantec TM data, and the target side of the 
Europarl data.  In this way we created the follow- 
ing three types of language models from the data 
sources and used them for translation.
1. conc: a language model trained on the concate- 
nated data sets from all three sources, monolin- 
gual forum posts, target side of Symantec TMs 
and target side of ‘forum-like’ sub-parts of Eu- 
roparl.
2. linmix: an adapted language model using lin- 
ear mixture of weights.
3. logmix: an adapted language model using log- 
linear mixture of weights.
Table 3 reports the evaluation results for all phases 
of experiments. The ﬁrst row gives the scores for the





T
M

L
M
D
e-
E
n
E
n–
De
Fr
-
E
n
En
–
Fr



B
L
E
U
NI
ST
B
L
E
U
NI
ST
B
L
E
U
NI
ST
B
L
E
U
NI
ST
bl
T
M
T
M
+f
or
um
3
5.
3
8
7.
2
6
2
5.
9
9
6.
4
4
3
6.
4
2
7.
4
3
3
3
.
9
6.
7
8
ph
ase
-1
T
M
+E
P
co
nc
3
5.
4
4
7.
3
2
2
6.
8
4
6.
6
9
3
6.
8
1
7.
4
9
3
5.
7
4
7.
1
8

T
M
+E
P
lin
mi
x
3
5.
6
1
7.
3
5
2
7.
0
5
6.
7
1
3
6.
9
2
7
.
5
3
6.
4
6
7.
2
2

T
M
+E
P
log
mi
x
3
5.
4
9
7.
3
1
2
6.
9
8
6
.
5
3
6.
7
4
7.
4
6
3
5.
8
9
7.
1
3
ph
ase
-2
lin
mi
x
co
nc
3
5.
0
3
7.
2
7
2
6.
9
8
6.
5
3
3
6.
5
6
7.
4
1
3
5.
9
9
7.
1
7

lin
mi
x
lin
mi
x
3
5.
3
2
7.
3
7
2
7.
7
7
6
.
7
3
7
.
1
7.
4
9
3
6.
6
3
7.
2
3

lin
mi
x
log
mi
x
3
6.
5
7
7.
3
8
2
7.
3
9
6.
6
7
3
6.
7
4
7.
4
2
3
4.
5
1
7.
0
2
ph
ase
-3
log
mi
x
co
nc
3
4.
8
2
7.
3
1
2
7.
2
3
6.
7
1
3
4.
8
8
7.
3
2
3
2.
6
5
6.
9
1

log
mi
x
lin
mi
x
3
5.
5
5
7.
3
2
2
7.
7
1
6
.
7
3
6.
5
2
7.
4
2
3
6.
4
8
7
.
2

log
mi
x
log
mi
x
3
4
7.
1
8
2
7.
0
3
6.
4
4
3
6.
3
9
7.
3
6
3
4.
8
7
6.
9
4

Table 3: Evaluation results for all combinations of mixture adapted language and translation models: Baseline(bl)
scores are italicized, best scores are in bold




baseline system. As is evident from the table, all the 
phase-1 experiments improve the evaluation scores 
over the baseline.  Adding the Europarl data for 
training gives a slight improvement over the base- 
line, and both linear and log-linear mixture-adapted 
models further improve the scores. Surprisingly, the 
linear mixture results are slightly better than the log- 
linear ones. Since MERT directly optimizes the log- 
linear weights on the devset BLEU scores, as com- 
pared to the linear weights which were learnt by op- 
timizing the maximum likelihood on the target side 
of the devset, we expected the former to provide bet- 
ter results in terms of BLEU. However, in the tuning 
phase, MERT was observed to iterate to the maxi- 
mum allowable iteration limit (25) in order to com- 
plete, rather than converging automatically based 
on the evaluation metric criterion. This observation 
conﬁrms previous ﬁndings (Chiang et al., 2009) re- 
garding the inability of the MERT algorithm to con- 
verge on an optimal set of weights for a reasonably 
large number of parameters. Linear mixture adapta- 
tion caused the translation scores to improve by 1.06 
absolute BLEU points (4.08% relative) for En–De 
and 2.56 absolute points (7.55% relative) for En–Fr 
over the baseline. For De-En and Fr-En the improve- 
ments were 0.23 absolute BLEU points (0.65% rela- 
tive) and 0.5 absolute BLEU points (1.37% relative) 
respectively. When translating from English the im- 
provements were statistically signiﬁcant (with 97% 
and 99.8% reliability for En–De and En–Fr respec- 
tively), at the p=0.05 level using bootstrap resam- 
pling (Koehn, 2004).  This is due to the fact that


German and French forum data were smaller than 
the English corpus. When translating into English, 
however, the huge amount of monolingual English 
forum data used for language modelling seemed to 
reduce the effect of adaptation, resulting in smaller 
statistically insigniﬁcant absolute improvements.
  Notably,  in spite of being slightly worse than 
the linear-mixture scores, the log-linear scores are 
also better than the baseline scores, indicating the 
effectiveness of adaptation in the current setting. 
The NIST scores reported in the table also follow 
a similar trend to the BLEU scores, but the log- 
linear scores are slightly worse than the concate- 
nated model scores.  This might be due to the fact 
that MERT optimizes on BLEU scores rather than 
NIST to learn log-linear weights.

5.3   Phase-2: Linear Mixture Adaptation of
Translation Models

In the second phase of our experiments, we ex- 
tended mixture adaptation to the translation mod- 
els, adapting the phrase tables using linear mixture 
weights.  Two independent phrase tables were pre- 
pared from the Symantec TMs and ‘forum-like’ Eu- 
roparl datasets which were linearly combined us- 
ing weights learnt according to the process elabo- 
rated in Section 4.1. The combined phrase table was 
then used in combination with the different language 
models mentioned in Section 5.2.
  The Phase-2 labelled rows in Table 3 show the re- 
sults for this phase, which show very similar trends 
compared to Table 3 with the linear mixture-adapted


language models, which resulted in best translation 
scores.   The log-linear mixture-adapted language 
model performs better only for De-En translations. 
Using the concatenated language model with the 
adapted phrase table provides slightly higher trans- 
lation scores compared to the ones reported in Sec- 
tion 5.2, suggesting a positive effect of phrase-table 
adaptation.
  Linear mixture adaptation on phrase tables re- 
sulted in an improvement of 1.78 absolute BLEU 
points (6.85% relative) for En–De and 2.73 absolute 
BLEU points (8.05% relative) for En–Fr, over the 
baseline, which are better than the improvements re- 
ported in the previous section. Both these improve- 
ments are statistically signiﬁcant with a reliability 
of 99.6% and 99.8% respectively.  For De-En and 
Fr-En, the improvements are 1.19 absolute BLEU 
points (3.36% relative) and 0.68 absolute BLEU 
points (1.87% relative), respectively.  Similarly for 
the concatenated translation model, improvements 
were slightly bigger when translating from English. 
The NIST scores followed the same trend as the 
BLEU scores in terms of relative variations.

5.4   Phase-3: Log-linear Mixture Adaptation of
Translation Models

Finally, we combined multiple translation models 
using a log-linear combination and used them with 
three different language models, as in the ﬁrst and 
second phases, and obtained the set of results re- 
ported in the phase-3 section of Table 3.
  The scores follow the same trend as in the two 
previous phases, with the linear-adapted language 
model providing the best scores.  The evaluation 
scores when translating from English were better 
compared to those in phase 1, but poorer than those 
in phase 2. The BLEU score improvements over the 
baseline for this adaptation model were 1.72 abso- 
lute (6.62% relative) points for En–De, 2.58 abso- 
lute (7.61% relative) points for En–Fr, 0.17 absolute 
(0.48% relative) points for De-En and 0.1 absolute 
(0.28% relative) points for Fr-En. As in the previous 
phases, the improvements are statistically signiﬁcant 
for translations from English. The MERT algorithm 
is known to be unable to learn optimal weights for 
large parameter settings (Chiang et al., 2009). In the 
current scenario, two phrase tables, two reordering 
models and three language models resulted in a con-


siderable number of parameters, causing the algo- 
rithm to learn sub-optimal mixture weights leading 
to poorer performance.

6   Conclusion and Future Work

The overall trends of the results emphasize the im- 
portance of linear mixture adaptation for both lan- 
guage and translation models. However, comparing 
the scores of different translation model adaptations 
against those of the language models indicates that 
language model adaptation was slightly more signif- 
icant in improving translation quality, compared to 
translation model adaptation, for the task at hand. 
Although log-linear mixture adaptation ﬁts well into 
the SMT framework, the inability of MERT to con- 
verge on optimal weights in different settings caused 
poor performance in terms of evaluation scores.
  Here the weights for linear combination of mul- 
tiple phrase tables were estimated using language 
models. Directly learning linear weights by optimiz- 
ing translation quality in terms of the development 
set would be the prime direction in future. We would 
also like to look into alternative tuning techniques, 
especially ones based on the MIRA algorithm to im- 
prove the quality of log-linear mixture adaptation 
in large parameter settings (Chiang et al., 2009). 
Enhancing the translation quality further with third 
party forum data would also be another objective 
in this direction.  Finally we would also like to in- 
vestigate further on different ranking schemes and 
empirical threshold selection for selecting relevant 
datasets to supplement training data for improving 
translation quality.

Acknowledgments

This work is supported by Science Foundation Ire- 
land (Grant No. 07/CE/I1142) as part of the Centre 
for Next Generation Localisation (www.cngl.ie) at 
Dublin City University. We thank the reviewers for 
their insightful comments. We also thank Symantec 
for kindly providing us with data and support.


References

Chiang, D., Knight, K. and Wang, W.   2009.   11,001 
new features for statistical machine translation.   In 
Proceedings of Human Language Technologies:  The


2009 Annual Conference of the North American Chap- 
ter (NAACL’09) Boulder, CO. pp 218–226.
Civera, J. and Juan, A. (2007).   Domain adaptation in
statistical machine translation with mixture modelling. 
In ACL 2007: Proceedings of the Second Workshop 
on Statistical Machine Translation Prague, Czech Re- 
public. pp 177–180.
Dempster, A. P., Laird, N. M. and Rubin, D. B.  1977.
Maximum likelihood from incomplete data via the EM 
algorithm. In Journal of the Royal Statistical Society, 
Series B Vol 39:1, pp 1–38.
Doddington, G.   2002.   Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence 
statistics.  In Proceedings of the Second international 
conference on Human Language Technology Research 
San Diego, CA, pp 138–145.
Eck, M., Vogel, S. and Waibel, A.   2004.   Language
model adaptation for statistical machine translation 
based on information retrieval  In Proceedings of the
4th International  Conference on language resources 
and evaluation (LREC-2004) Lisbon, Portugal, pp.
327–330
Flournoy, R., and Callison-Burch, C. 2000. Reconciling
User Expectations and Translation Technology to Cre- 
ate a Useful Real-world Application In Proceedings of 
the 22nd International  Conference on Translating and 
the Computer London.
Flournoy,  R.,  and Rueppel,  J.	2010.	One technol-
ogy: many solutions  In Proceedings of AMTA 2010: 
the Ninth conference of the Association for Machine 
Translation in the Americas Denver, CO, pp. 6–12
Foster, G. and Kuhn, R. 2007. Mixture-model adaptation
for SMT.  in ACL 2007: Proceedings of the Second 
Workshop on Statistical Machine Translation Prague, 
Czech Republic, pp.128–135.
Federico,  M.,  Bertoldi,  N.  and  Cettolo,  M.	2008.
  IRSTLM:  an  Open  Source  Toolkit  for  Handling 
Large Scale Language Models.	In Proceedings of 
Interspeech-2008 Brisbane, Australia, pp.1618–1621. 
Hastie, T., Tibshirani, R. and Freidman, J. 2001. In The
Elements of Statistical Learning. Springer-Verlag
Hildebrand,  A.S.,Eck,  M.,  Vogel S.,  and  Waibel,  A.
2005. Adaptation of the translation model for statisti- 
cal machine translation based on information retrieval 
In 10th EAMT Conference: Practical Applications of 
Machine Translation, Conference Proceedings Bu- 
dapest, Hungary, pp.119–125.
Iyer, R. and Ostendorf, M.  1996.  Modelling long dis-
tance dependence in language: Topic mixtures vs. dy-


Kneser, R. and Steinbiss, V. 1993. On the dynamic adap- 
tation of stochastic language models.  In IEEE Inter- 
national  Conference on Acoustics, Speech, and Signal 
Processing, 1993. (ICASSP-93.), vol.2 Minneapolis, 
MN, pp.586–589.
Koehn, P.   2004.   Statistical signiﬁcance tests for ma-
chine translation evaluation.   In Proceedings of the
2004 Conference  on Empirical Methods in Natural 
Language Processing (EMNLP)  Barcelona, Spain, 
pp.388–395.
Koehn, P.  2005.  Europarl: A Parallel Corpus for Sta-
tistical Machine Translation.  In MT Summit X: The 
Tenth Machine Translation Summit Phuket, Thailand, 
pp.79–86.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-
erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, 
C., Zens, R., Dyer, C., Bojar, O., Constantin A. and 
Herbst H. 2007. Moses: open source toolkit for statis- 
tical machine translation. In ACL-2007: Proceedings 
of demo and poster sessions Prague, Czech Republic, 
pp.177–180.
Koehn,  P. and Schroeder, J.	2007.	Experiments in
domain adaptation for statistical machine translation. 
In Proceedings of the Second Workshop  on Statisti- 
cal Machine Translation  Prague, Czech Republic, 
pp.224–227.
Kuhn, R. and De Mori, R.  1990.  A cache-based natu-
ral language model for speech recognition.  In IEEE 
Transactions on Pattern Analysis and Machine Intelli- 
gence vol.12, no.6, pp.570–583.
Langlais, P.   2002.   Improving a general-purpose sta-
tistical translation engine by terminological lexicons. 
In Proceedings of Coling-2002: Second international 
workshop on computational terminology (COMPUT- 
ERM 2002) Taipei, Taiwan, pp.1–7.
Och, F. J. and H. Ney  2003. A Systematic Comparison
of Various Statistical Alignment Models In Computa- 
tional Linguistics volume 29, (1), pp. 19–51.
Och, F. J. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of the 41st An- 
nual Meeting on Association For Computational Lin- 
guistics - Volume 1 Sapporo, Japan, pp.160–167.
Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. 2002.
BLEU: a method for automatic evaluation of machine 
translation In ACL-2002:  40th Annual meeting of the 
Association for Computational Linguistics Philadel- 
phia, PA, pp. 311–318
Stroppa, N. and Way., A. 2006. MaTrEx: DCU machine
translation system for IWSLT 2006.  In Proceedings 
of the International  Workshop on Spoken Language


namic cache models. In IEEE Transactions on Speech
  

Translation Kyoto, Japan, pp. 31–36. Wu, H., Wang, 
H., and Zong, C. (2008).



Domain adap-


and Audio Processing pp.236–239.
Kneser, R. and Ney, H. 1995. Improved Backing-off for
M-gram Language Modeling  In IEEE International 
Conference  on Acoustics, Speech, and Signal Process- 
ing. vol.1 pp.181–184.


tation for statistical machine translation with domain 
dictionary and monolingual corpora. In Coling 2008,
22nd International  Conference on Computational Lin- 
guistics Manchester, UK. pp 993–1000

