Structural Support Vector Machines for Log-Linear Approach in Statistical Machine  Translation

Katsuhiko Hayashi†, Taro Watanabe††∗, Hajime Tsukada††, Hideki Isozaki††

Department of Information Science and Technology
†University of Doshisha, Japan
dti0708@mail4.doshisha.ac.jp
††NTT Comunication Science Labolatories, Japan
{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp




Abstract

Minimum error rate training (MERT) is a widely used learn-
ful tools for many tasks in natural language processing [6][7]. 
The core of the form consists of a smooth convex regularizer
ing method for statistical machine translation.  In this pa-
such as 1 ||w||2 and the empirical risk term of hinge loss. In
per, we present a SVM-based training method to enhance 
generalization ability.  We extend MERT optimization by 
maximizing the margin between the reference and incor- 
rect translations under the L2-norm prior to avoid overfit- 
ting problem. Translation accuracy obtained by our proposed 
methods is more stable in various conditions than that ob- 
tained by MERT. Our experimental results on the French- 
English WMT08 shared task show that degrade of our pro- 
posed methods is smaller than that of MERT in case of small 
training data or out-of-domain test data.

1.  Introduction

The state of the art statistical machine translation systems 
have been modeled by the log-linear approach which is a 
generalization of the noizy-channel approach. This approach 
has achieved a lot of great advances because it has provided 
a natural extention to integrate many useful components [1]. 
To estimate the weights toward these components according 
to their performance, minimum error rate training (MERT) 
[2] was introduced by Och (2003). MERT improves statisti- 
cal machine translation quality by optimizing the parameter 
of the log-linear function by using such automatic translation 
evaluation metrics as the BLEU scores [3].
  To train a small number of real-valued features used on a 
standard phrase-based statistical machine translation system 
like Moses [16], MERT with BLEU-based objective func- 
tion is very effective due to line-search algorithm proposed 
by Och (2003).  However, MERT tends to overfit to train- 
ing data because its objective function consists of no reg- 
ularizer.  To enhance generalization ability, we would like 
to use other state-of-the-art machine learning techniques for 
machine translation.
Support vector machines (SVM) have proven to be power-
†This research was conducted as an internship program 2008 of NTT.
∗Taro Watanabe now belongs to National Institute of Information and
Communications Technology (NICT) in Japan.


this paper we present an approach to optimize the parame-
ter of the log-liner model using the primal form of structural 
SVM [12]. We expect the convex regularizer or the factor of 
enlarging the margin (between the reference and the incor- 
rect translation) of SVM to reduce the overfitting problem 
and enhance generalization ability. Using the BLEU scores to 
define the hinge loss, our proposed method also inherits the 
advantages of MERT, which enhance the BLEU scores of 
translations.
  In this paper we carried out on a French-English task and 
exprimentally compared our proposed method with MERT. 
We achieved more significant improvements than these ob- 
tained by conventional MERT, especially in case of small 
training data or out-of-domain test data.
  The remainder of this paper is organized as follows. Sec- 
tion 2 briefly reviews the framework for the MERT and 
Section 3 performs the formulations of the structural SVM 
and the cutting-plane algorithms to introduce our proposed 
method in Section 4.  Section 4 describes the integration of 
the line-search algorithm with S-slack and 1-slack structural 
SVM. We experimentally compare structural SVM to MERT on 
the WMT08 French-English task in Section 5. In section
6, we conclude with summary and future work.

2.  Minimum Error Rate Training

2.1.  Log-Linear  Approach

To translate source sentence f into another target language e, 
the log-linear approach seeks a maximum solution:
ˆe = argmax(w, h(e, f )),	(1)
e

where h(e, f ) is a feature vector and w is a weight vector 
that scales the contributions from all features. This approach 
has the advantage that addtional models or feature functions 
can be easily integrated into the overall system.  However, it 
must appropriately optimize a weight vector to obtain the


2.3.  Evaluation Metrics BLEU

The BLEU score [3] used in MERT is defined as follows:

BLEU({ˆe}S ; {r}



  1
)= BP ·exp


N
) log pn ({ˆe}S


 
, {r}S )


1	1	N


1 	1
n=1



where pn (·) is the n-gram precision of hypothesezed trans-
lations {e}S  given the reference {r}S  and BP is a brevity
1 	1
penalty. The BLEU scores are calculated in terms of a whole
corpus.








Figure 1:  each line of the 6-best translations and BLEU 
scores with 1-best translation selected by the current param- 
eter α



good translation results.


3.  Structural Support Vector Machines

In recent years, Support Vector Machines (SVM) for 
conven- tional binary classification have been 
generalized for multi- classification and structured output 
problems [12][13]. The generalized SVM is called a 
structural SVM [12].  In the generalized margin-
maximization principle we maximize the separation 
margin, which is the score difference between the 
reference rs  and incorrect translation ˆes  scores.  To 
allow errors in training data, we introduce slack 
variables ξs  and optimize with soft-margin criteria:





2.2.  Minimum Error Rate Training

Minimum error rate training (MERT) obtains w that max-



argmin
w,ξ≥0



λ 1w12 +
2


S
) ξs 	(4)
S
s=1


imizes BLEU scores on  S-size training data  {(rs , fs )}S
and  a  set  of  K different  candidate  translations  Cs     =
{ˆes,1 , ··· , ˆes,K } for each input sentence fs :


s.t.


∀ˆe1


∈ C1


\ r1  :

.


(w, δh1


)≥ 1 − ξ1


(
argmax BLEU(
w


 S
rs , argmax(w, h(ˆes , fs 
))
ˆes ∈Cs 
	
1



). 	(2)


s.t. 
∀ˆe
S  ∈ 
CS  
\ 
rS  
:  
(w, 
δh
S 
)≥ 
1 − 
ξS ,
where δhs  is hs (rs , 
fs ) − hs (ˆes , fs ).  
The constraints 
state


If we use the argmin function, we need to calculate the ob- 
jective function as 1.0 − BLEU.
  Och’s line-search, one of the efficient algorithms for 
optimizing an objective function, maximizes the function 
through a sequence of line maximizations along vector direc- 
tions d. To compute the most probable sentence ˆes,best  from 
hypotheses Cs , the optimization problem is defined with α 
as follows:
ˆes,best  = argmax(w + αd, h(ˆes , ˆfs ))


that for each training example (rs , fs ), the score w · h(rs , 
fs )
of the correct structure rs  must be greater than the 
score
w · h(ˆes , fs ) of all incorrect structures ˆe by the margin 
1.
The standard structural SVM optimization problem has
also been generalized in several ways [12][13].  
Tsochan- taridis et al.  (2005) introduced two different 
ways of using a hinge loss to the convex upper bound of 
the loss, namely,
”margin-rescaling” and ”slack-rescaling”. In this paper, 
we use only a margin-rescaling. The margin-rescaling is 
for the
special case of the Hamming loss.  Each margin-
rescaling


ˆes ∈Cs
⎧
⎪⎨


⎫
.	(3)
⎬


constraint has the 
following form:


= argmax


(w, h(ˆes , ˆf  )) 
+α (d, h(ˆe  , ˆf  
))


s.t. ∀ˆes  ∈ Cs  \ 
rs  :  (w, δhs 
)≥ .6(rs , ˆes ) 
− ξs


s 	s    s


ˆes ∈Cs    ⎪⎩


inte r cept    _


 	sl o pe 	_⎪⎭



where this 
rescaling can 
penalize ˆe  /=  r 
with high loss


Each translation in the hypotheses corresponds to a line with
the slope and the intercept in the argmax of Eq.3. Figure.1 
shows the example of lines for the 6-best translations.  For 
any particular choice of α, the decoder seeks the translation 
that yields the largest score. Och’s algorithm shifts α from
−∞ to ∞ to obtain the best parameter w while recording the
points where two or more lines intersect and selects α which
is able to determine the translation obtaining the highest eval- 
uation scores.


Δ(r, ˆe) more severely than that with small loss.

3.1.  Cutting-Plane Algorithm

The optimization problem in Eq.4 with margin-rescaling has
O(S|C|) constraints. In general, |C| is extremely large or in-
finite. To cut a large number of these constraints, the cutting-
plane algorithm (Algorithm 1) iteratively finds the most 
vio- lated constraint through the training data:








Algorithm 1 Cutting-plane training for S-slack Formulation 
with margin-rescaling


Algorithm  2 Cutting-plane training for 1-slack Formulation 
with margin-rescaling


1:  Input : w, CS , {rs , fs }S ,E 


1:  Input  : w, CS , {rs , fs   S


1 	1
2:	1   = {}



2:  W


1 	1
= {}


3:  repeat


3:  repeat



λ	2 	1 	S


4:	for s = 1 ...S do


4:	w = argminw,ξ≥0   2 
1w1


+ S Ls=1 ξs


5:	ˆes  = argmaxˆes ∈Cs {Δ(rs , ˆes ) − (w, δhs )}


s.t.   ∀(ˆe1 , ··· , es )   ∈   W	:	λ Ls=1 (w, δhs )  ≥


6:	if Δ(rs , ˆes ) − (w, δhs )>ξs  + E then


1 LS
S 	s=1


.6(rs ,


S
ˆes ) − ξs


7:	add(Ws , ˆes )



λ	2 	1 	S


5:	for s = 1 ...S do


8:	w = argminw,ξ≥0   2 1w1


s=1 ξs


6:	ˆes  = argmaxˆes ∈Cs {Δ(rs , ˆes ) − 
(w, δhs )}


s.t. ∀ˆe1  ∈ W1 :  (w, δh1 )≥ .6(r1 , ˆe1 ) − ξ1
.
∀ˆeS  ∈ WS :  (w, δhS )≥ .6(rS , ˆeS ) − ξS


7:	end for
8:	add(W , (ˆe1 , ··· , ˆeS ))
9:  until


9:	end if
10:	end for
11:  until



This algorithm iteratively constructs a working set W =
W1 ∪· · ·∪ WS of constraints. The most violated constraint
for margin-rescaling (line 5) is defined as follows:



  It iteratively constructs a working set W of constrains. In 
each iteration, this algorithm optimizes the parameter over the 
current working set W , find the most violated constrait, and 
add it to the working set. Unlike the S-slack algorithm, only a 
single costraint is added in each iteration.

4.  Minimum Error Rate Training based on
Structural SVM


ξs  =	max


{.6(rs , ˆes ) − (w, δhs )}.	(5)


ˆes ∈Cs \rs


If this constraint is violated by more than the desired preci- 
sion E (line 6), the constraint is added to the working set W 
and the QP is solved over the W (line 8).


3.2.  1-Slack Formulation

Joachims et al.  (2009) proposed to replace the S cutting- 
plane models of the hinge loss with a single cutting-plane 
model for the sum of the hinge losses. This model has only 
one slack variable ξ that is shared across all constraints:


argmin λ 1w12 + ξ.	(6)
w,ξ≥0   2

The constraint for 1-slack formulation with margin-rescaling 
is

s.t. ∀(ˆe1 , ··· , ˆeS ) ∈ CS  :


In this section we describe how to apply the structural SVM
to the MERT objective function.  The first subsection ad- 
dresses how to calculate the feature score of the ”correct” 
sentence.  In the second subsection we define loss function 
Δ(rs , ˆes ) using the BLEU scores, and the third subsection 
extends Och’s line-search algorithm as the optimization al- 
gorithm for the SVM-based objective function.

4.1.  Selecting the Configuration in the K -best list

For structural SVM, we need to label a correct candidate 
translation for each source sentence from its K -best list of 
candidates, which is called a configuration. Since the BLEU 
scores are not cumulative, we cannot efficiently select the 
best configuration from the K -best list. So we approximate it 
by a greedy search algorithm [14].
  This algorithm considers the impact on the training set 
score when selecting an alternative translation by subtracting 
the statistics for the current configuration choice from the ac- 
cumulated statistics and adding those for the alternative and 
selects the translation which results in the highest score. Re- 
peat this process and continue untill there are no configura-


1 )
S
s=1


1
(w, δhs )≥ S


S
)

s=1



Δ(rs , ˆes ) − ξ.


tion changes.  The 
configuration 
obtained by this 
algorithm
specifies the correct 
candidate for each K 
-best list, and the 
BLEU scores are the 
upper bound for the 
BLEU scores on the 
training set.


It is proven that the S-slack and 1-slack formulations are 
equivalent in some points [13].  In the next section we use 
this formulation to calculate the whole corpus-wise BLEU 
scores.
Cutting-plane training for 1-slack formulation is showed
in Algoritm 2:



4.2.  Loss Function for Rescaling

4.2.1.  Approximation for the Sentence-wise BLEU

The BLEU scores are defined in terms of a whole corpus and 
not over individual sentences.  However we need to calcu-








late the sentence-wise BLEU scores to define the loss func- 
tion Δ(rs , ˆes ). To calculate sentence-wise BLEU we use the 
approximated BLEU like the one proposed by Watanabe et


Algorithm  3 extended Och’s line-search algorithm for S- 
slack formulation 	
1:  Input  : w, d, CS , {rs , ˆe∗, fs   S


al.  (2006).  Given the configurations for S input sentences


2:  I


= {}


1 	s 	1


{ˆe∗ ... ˆe∗ }, this approximated BLEU scores on translation


3:  for s = 1 ...S do


1 	S


candidate ˆes  for s-th input sentence are calculated by substi- 
tuting ˆe∗ with ˆes .


4:	for all ˆes  ∈ Cs  \ ˆe∗ do
5:	ˆe  .m = (d, δh )


s 	s 	s
6:	ˆes .b = Δ(ˆe∗, ˆes ) − (w, δhs )


4.2.2.  Loss Functions based on Sentence-wise and Corpus-
wise BLEU

The sentence-wise loss function Δ(rs , ˆes ) is defined as fol-


7:	end for
8:	i =0 
9:	li = argminˆes ∈Cs \ˆe∗ ˆes .m


lows by using this approximated BLEU scores:


10:	xi


= −∞



Q × {BLEU({rs , ˆe∗}



) − apBLEU({rs , ˆes }



)},


11:	repeat
12:	i = i +1 


s   1 	1


li .m−li−1 .m } > xi−1  then


where Q is a constant for scaling the BLEU loss function. If


14:	li = argminˆe


C   ˆe∗ { ˆe	l


.m }


li  1 .b−ˆes .b


we set the parameter Q on high, we regard the loss function


s ∈  s \ s


s .m− i−1


as important. We use the sentence-wise BLEU loss function


li  1 .b−li .b
li .m−li−1 .m }


Δ(rs , ˆes ) to calculate the objective function of the S-slack
SVM formulation.
Since we believe that the average sentence-wise BLEU


16:	end if
17:	until no more intersections
18:	add(I , xi )
add(I , max(I )+ 2E)


scores are less reliable than the whole corpus-wise BLEU
scores, we tried to apply the corpus-wise BLEU to the ob- 
jective function using a 1-slack formulation SVM assum-


19:
20:	xbest 	=	argminx∈I  Obj(w	+	(x 	−
E)d, Cs 	∗	s


ing  1 LS


Δ(rs , es )   
∝  corpus-
wise BLEU  
loss  func-


1
 
,
 
{
r
j
 
,
 
ˆ
e
j
 
,
 
f
j
 
}
j
=
1
 
)
21:
	w+
= (xbest  − 
E)


S 	s=1
tion Δ({rs , es  S ).   The corpus-wise BLEU loss function
}1
S


22:	delete(I , max(I )+ 2E)
23:  end for


Δ({ˆe∗, ˆes }


) is as 
follows:


24:  return  
w


s 	1


Q × {BLEU({rs , ˆe∗  S


) − BLEU({rs , ˆes   S


)}.


s   1 	1

1-slack formulation with margin-rescaling is constructed us-
S


ing the corpus-wise BLEU loss function Δ({ˆe∗, ˆes }  ):


In case of S-slack formulation (Algorithm 3), the max


s 	1
function in Eq. 5 corresponds to the argmax function in Eq.


argmin λ 1w12 + ξ.	(7)



3, so we can find a translation that has the most violated con-


w,ξ≥0   2



straint ξs



by a line-search 
algorithm with the 
following slope


s.t. ∀(ˆe1 , ··· , ˆeS ) ∈ CS  :


and intercept (line 5,6) :


1 )
S	(w, δhs )≥ Δ({ˆe∗, ˆes }


) − ξ.


s 	1
s=1 	⎧	⎫



Unlike the margin infused relaxed algorithm (MIRA) [9] and


⎪⎨	⎪⎬
argmax	Δ(e∗


s , es ) − (w, δhs ) +α (d, δhs )	.


S-slack formulation, we can directly apply the corpus-wise
BLEU to the SVM objective function without approximating 
the BLEU scores.


ˆes ∈Cs    ⎪⎩


intercept 	_



slope


_⎪⎭





4.3.  Optimization Algorithm

4.3.1.  Line-search Algorithm

Next we describe extended Och’s line-search algorithms for 
S-slack and 1-slack formulation of the Structural SVM. 
These pseudocodes for the line-search to optimize parame- 
ter w are given by Algorithm 3,4. In Algorithm 3,4 we can 
find the range of values along the direction vector d to which 
each candidate translation is assigned the best score. Algo- 
rithm 3 is the line-search algorithm for S-slack formulation 
of the structural SVM:


Algorithm 3 iteratively constructs the line intersections I
through the training examples and estimates the parameter 
variation (x − E) which is able to select the translation min-
imizing the objective function.  The process of Algorithm
3 which extracts the most violated translation is similar to 
the cutting-plane training in Algorithm 1. In case of the S- 
slack formulation with margin-rescaling, the Obj  function 
(line 20) is constructed by each of the most violated transla- 
tions, as in Eq. 4, using margin-rescaling constraints.
  Algorithm 4 is the line-search algorithm for 1-slack for- 
mulation of the structural SVM:


Algorithm  4 extended Och’s line-search algorithm for 1-
slack formulation 	
1:  Input  : w, d, CS , {rs , ˆe∗, fs   S


• Three	orientation	types	reordering	model[17]:
p(m|f , e)  ,   p(s|f , e)  ,   p(d|f , e)  to  capture  the
lexicalized information.


2:  I = {}


1 	s 	1



• Word , 
Phrase penalty: 
To control the 
target length 
and


3:  for s = 1 ...S do
4:	for all ˆes  ∈ Cs  \ ˆe∗ do



the average length of the phrases.


5:	ˆes .m


= (d, hs (ˆes , fs ))


In this paper, we 
trained these 
small number of 
features and


6:	ˆes .b = (w, hs (ˆes , fs ))
7:	end for
8:	i =0 
9:	li = argminˆes ∈Cs \ˆe∗ ˆes .m
10:	xi  = −∞
11:	repeat
12:	i = i +1 
li .m−li−1 .m } > xi−1  then
li−1 .b−ˆes .b


phrases were extracted using a typical approach 
[16] that ran GIZA++ [18]. We used a Katz 
smoothing 5-gram language model that was 
created using the SRILM toolkit [19].

5.
2.  
D
at
a 
Se
t

For experiments we used the French-English 
data provided for the Europarl-based WMT08 
shared task. Europarl corpus was collected from 
the proceedings of European Parliament


14:	li = argminˆes ∈Cs \ˆe∗ { ˆe  .m−l
s	s
li  1 .b−li .b
li .m−li−1 .m }
16:	end if


i−1 .m }


[20].  This training corpus contains about 
1.3 M sentences. Parameters were tuned 
over the provided development set 
(dev2006) that consisted of 2000 sentences 
with one refer-


17:	until no more intersections
18:	add(I , xi )
19:  end for
20:  add(I , max(I )+ 2E)
21:  xbest 	=	argminx∈I  Obj(w	+	(x 	−
S


ence. We used two open test sets: Europarl test 2008, consist- 
ing of 2000 sentences with one reference, and News news- 
test 2008 (out-of-domain), consisting of 1500 sentences. Ta- 
ble 1 shows these contents in more detail.


E)d, CS , {rs , ˆe∗, fs }  )
1 	s 	1


22:  return w + (xbest  − E)d


Table 1: The Data statistics






  For the 1-slack formulation we should calculate slopes m 
and intercepts b the same as the S-slack formulation, but, 
to avoid bias toward the line-search procedure by sentence- 
wise BLEU, we computed them the same as MERT (line 5-
6). Unlike the S-slack formulation, this algorithm constructs
the line intersections I over all the training samples and then 
estimates the best parameter varidation. The process of this 
algorithm which extracts 1-best translation is similar to that 
of Algorithm 2 extracting the most violated constraint.  In 
case of the 1-slack formulation with margin-rescaling, the 
Obj function (line 21) is constructed as Eq. 7.

5.  Experimental Results

5.1.  Systems

Experiments were conducted using a standard phrase-based 
statistical MT system called Moses [16] to generate K -best 
lists (K =1000).  Moses employs standard real-valued fea- 
tures:

• N -gram language model: P r(e) to calculate the flu- 
ency of the target side.

• Lexical translation model: t(ei |fj ) , t(fj |ei ) to calcu- 
late the word translation probability.

• Phrase translation model: φ(e|f ) , φ(f |e) to calculate 
the phrase translation probability.
















5.3.  Results

5.3.1.  Tuning Hyperparamers

Figure 2 shows the effect of the hyperprameter λ which em- 
phasizes the convex regularizer on SVM objective function. 
When we set λ high, the curve is like a quadratic function 
and the best parameter to optimize the objective function is 
close to 0. On the other hand, if we set it low, the shape of 
the function is almost the same MERT’s objective function. 
Hence it is very important for obtaining the good translation 
results to appropriately set the hyperparameters.
We tuned hyperparameters λ and Q in the SVM-based
method by the cross-validation method. We divided dev2006 
into two and the first estimation was performed on one (an- 
other was used for development set) and the second did on 
another.  In our cross-validation experiments for tuning the 
hyperparameters, we noticed that the higher Q is, the better










2.5

2


/
'
4
6
 	58/ ǳ=0.01
 	58/ ǳ=1.0
 	58/ ǳ=100



1.5

1

0.5

0

-0.5

-1

-1.5

-2    
-4	-3	-2	-1	0	1	2	3	4
2CTCOGVGT


Figure 2: This shows the shapes of BLEU and 1-slack SVM objective function for one parameter. These lines were calculated 
by 800 development sentences randomly selected from dev06 for development data when the hyperparameter Q is fixed 1000.0.





31.5

31

30.5

30



development 
test1(1-slack-
SVM) 
development 
test2(1-slack-
SVM)   	 development test1(S-slack-SVM) development test2(S-slack-SVM)   	


SVM 
objecti
ve 
functi
on 
while 
we 
evalua
ted the 
BLEU 
scores 
at the 
corpus 
level.


Table 
2:  
BLEU 
scores 
on the 
test08 
and 
news0
8 test 
data 
obtain
ed by 
model
s 
traine
d by 
MER
T and 
SVM.



29.5

29

28.5

28



27.5
10e0



10e1



10e2



10e3
HyperParameter Q



10e4



10e5



10e6



  Table 
2 shows 
the 
translati
on 
accurac
y in 
both in-
domain 
and out-
of-
domain 
test set.  
The 
”untune
” means 
the 
result


Figure 3:  Tuninig test for hyperparameter Q of structural
SVM (fixed λ=1.0) by increasing it.



BLEU scores on the development test set are (Figure 3); so 
in the open test experiments, we always fixed Q=100000.0 , 
λ=1.0 for S-slack SVM and Q=10000.0 , λ=1.0 for 1-slack 
SVM.

5.3.2.  Test Experiment

We compared our proposed SVM-based method to MERT 
using the whole dev2006 for development data on 4-gram 
BLEU scores of two open test sets.   Table 2 shows that
1-slack-SVM outperformed MERT. On the other hand, S- 
slack-SVM was not more effective than MERT because we 
approximated the BLEU scores to calculate the S-slack-


from default parameters and we performed MERT and SVM 
training, starting from these parameters. Our two proposed 
method based on SVM achieve comparable performance of 
MERT in in-domain test set (test08) and slightly outperform 
MERT in out-of-domain test set (news08). This means that 
SVM has good effects on training the parameter of log-linear 
model especially in case of the out-of-domain translation.

5.3.3.  Experiments in Case of Data Sparseness and Out-of- 
domain Problems


We also conducted the experiment in case of small develop- 
ment data.  Figure 4 shows the performance of Moses with 
training MERT, S-slack SVM and 1-slack SVM by gradu- 
ally increasing the development data.  The BLEU scores of 
MERT on the test08 data is drastically degraded when the de- 
velopment data size is getting smaller, and the learning curve










32.2

32


untune   	 MERT
S-slack SVM   	
1-slack SVM   	


Table 
4: 
BLEU 
scores 
of two 
open 
test 
sets 
obtain
ed 
when 
trainin
g by 
MERT
, S-
slack-
SVM 
and 
1-
slack-
SVM 
using 
four 
develo
pment 
sets 
contai
ning 
400 
senten
ces 
rando
mly 
se- 
lectin
g from 
WMT-
08 
dev20
06.



31.8

31.6

31.4

31.2

31

30.8
100	200	300	400	500	600	700	800	900	1000
the size of development data



Figure 4:  BLEU scores as a function of development data 
size.



is unstable. On the other hand, S-slack and 1-slack SVM is 
more stable than MERT and degrade of the smaller devel- 
opment data is fewer than that of MERT. Especially, 1-slack 
SVM is most stable among these three.


Table 3:  The average improvements of BLEU scores on 
the test08 and news08 (out-of-domain) when we trained 
the paramenters using only 400 development sentences with 
MERT and SVM-based algorithms four times.

m
et
ho
d
tes
t0
8 	news08
unt
un
e
s
m
o
ot
h
e
d-
M
E
R
T 
(
O
c
h, 
0
3) 
M
E
R
T
S-
sla
ck-
SV
M
1-
sla
ck-
SV
M
3
0.
8
4	13.75
+0
.2
2	−0.08
+0
.4
0	+0.03
+0
.4
5	+0.21
+0
.9
2	+0.40


  Table 3 shows the average improvements of BLEU scores 
when we train the parameters using only 400 sentences ran- 
domly selected from the development data.   We repeated 
the experiment four times and averaged the improvements 
of BLEU scores. Table 4 show these results in more details. 
This indicates that two SVM methods reduce the overfitting 
problem when assuming that only few development data are 
available or test set is out-of-domain.

6.  Related Work

Crammer et al. (2006) proposed Margin infused relaxed al- 
gorithm (MIRA) , which was the online large-margin train- 
ing algorithm for structured classification, and Watanabe et 
al.  (2007) applied it to a discriminative training algorithm 
for statistical machine translation to estimate a large num- 
ber of parameters. In some points, our proposed method is


diffrent from MIRA. First, the proposed algorithm is a batch 
style algorithm and using 1-slack formulation of structural 
SVM proposed by Joachims (2009) we try to use corpus-wise 
BLEU for the objective function without approximating the 
BLEU scores.  Secondly, we directly apply the line-search 
algorithm to SVM optimization problem to estimate a small 
number of paramenters.
  Cer and Manning (2008) proposed the other approach to 
regularize the objective function of the MERT. This regular- 
ization was not to search the current best optima but to con- 
sider the adjacent evaluation scores with fixed window size 
during line-search because the objective function had a very 
deep and narrow optima. This approach was different from 
our proposed method, but it statistically achieves significant 
gains when combined with line-search.


7.  Conclusion

We presented a new method to regularize the MERT objec- 
tive function using structural SVM. This function has 1 ||w||2
as a smooth convex regularizer and a factor maximizing the 
score margin between a reference and an incorrect transla- 
tion.  We also tried to apply the corpus-wise BLEU score 
to the objective function without approximating the BLEU 
scores for each sentence. To optimize a small number of real- 
valued parameters with this function, we directly used Och’s 
line-search algorithm. The experimental results show that a 
SVM-based methods are more stable than MERT in various 
conditions. They outperform MERT when only small devel- 
opment data is available or these are mismatch between the 
training and test conditions.
  In the future, we plan to experiment on a decoder that has 
a large number of features because SVM-based algorithm is 
expected to work more effectively on the sparse vector space 
[22]. We also think that a gradient based algorithm such as 
Pegasos [21] and a software package SV M struct [13] for 
SVM dual formulation allowing the use of kernels are more 
appropriate methods optimizing the parameter on such a de- 
coder than Och’s line-search algorithm.








8.  Acknowledgements

I would like to thank Yoshinobu Tonomura, Jun Suzuki, Sei- 
ichi Yamamoto and Shigeru Katagiri because of providing 
me the chance of this internship program, and Masafumi 
Nishida and Katsuhito Sudo for very helpful and detailed 
comments.

9.  References

[1] Och, F. J. and Ney, H., “Discriminative training and max- 
imum entropy models for statistical machine translation”, 
In Proc. ACL, pp. 295-302, 2002.

[2] Och, F. J., “Minimum error rate training in statistical ma- 
chine translation”, In Proc. ACL, pp. 160-167, 2003.

[3] Papineni, K. A., Roukos, S., Ward, T. and Zhu, W-J., 
“Bleu:  a method for automatic evaluation of machine 
translation”, In Proc. ACL, pp. 311-318, 2001.

[4] Chiang, D., “A hierachical phrase-based model for sta- 
tistical machine translation”, In Proc. ACL, pp. 263-270,
2005.

[5] Quirk,  C.,  Menezes,  A.  and  Cherry,  C.,  “Depen- 
dency Treelet Translation: Syntactically Informed Phrasal 
SMT”, In Proc. ACL, pp. 271-279, 2005.

[6] Joachims, T., “Text Categorization with Support Vector 
Machines:  Learning with Many Relevant Features”, In 
Proc. ECML, 1998.

[7] Kudo, T. and Matsumoto, Y., “Japanese Dependency 
Structure Analysis Based on Support Vector Machines”, 
In Proc. EMNLP/VLC, 2000.

[8] Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, 
S. and Singer, Y., “Online passive agressive algorithms”, 
JMLR, Vol. 7, pp. 551-585, 2006.

[9] Watanabe,  T.,  Suzuki,  J.,  Tsukada,  H.  and  Isozaki, 
H., “Online Large-Margin Training for Statistical Ma- 
chine Translation”, In Proc. EMNLP/CoNLL, pp. 764-
773, 2007.

[10] McDonald, R., Crammer, K. and Pereira, F., “Online 
large-margin training of dependency parsers”, In Proc. 
ACL, pp. 91-98, 2005.

[11] Shimizu,  N.  and  Haas,  A.,   “Exact  decoding  for 
jointly labeling and chunking sequneces”, In Proc. COL- 
ING/ACL, pp. 763-770, 2006.

[12] Tsochantaridis, I., Joachims, T., Hofmann, T. and Al- 
tun, Y. “Large margin methods for structured and interde- 
pendent output variables”, JMLR, Vol. 6, pp. 1453-1484,
2005.


[13] Joachims, T., Finley, T., and Chun-Nam Yu. “Cutting- 
Plane Training of Structural SVMs”, Machine Learning, 
to appear, 2009.

[14] Venugopal, A. and Vogel, S. “Considerations in maxi- 
mum mutual information and minimum classiffication er- 
ror training for statistical machine translation”, In Proc. 
EAMT, 2005.

[15] Watanabe, T., Suzuki, J., Tsukada, H., and Isozaki, H. 
“NTT Statistical Machine Translation for IWSLT 2006”, 
In Proc. IWSLT, pp. 95-102, 2006.

[16] Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., 
Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, 
C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and 
Herbst, E. “Moses: Open source toolkit for statistical ma- 
chine translation”, In Proc. ACL, pp.177-180, 2007.

[17] Koehn, P., Axelrod, A., Mayne, A-B., Callison-Burch, 
C., Osborne, M., and Talbot, D. “Edinburgh System De- 
scription for 2005 IWSLT Speech Translation Evalua- 
tion”, In Proc. IWSLT, 2005.

[18] Och, F. J. and Ney, H. “A systematic comparison of 
various statistical alignment models”, Computational Lin- 
guistics, Vol. 29, No. 1, pp. 19-51, 2003.

[19] Stockle, A. “SRILM - an extensible language modeling 
toolkit”, In Proc. ICSLP, 2002

[20] Koehn, P., “Europarl: A Parallel Corpus for Statistical
Machine Translation”, MT Summit, 2005.

[21] Shalev-Shwartz, S., Singer, Y., and Srebro, N., “Pega- 
sos:  primal estimated subgradient solver for SVM”, In 
proc. ICML, pp. 807-814, 2007.

[22] Chiang, D., Knight, K. and Wang, W., “11,001 New 
Features for Statistical Machine Translation”, In proc. 
NAACL, 2009.

[23] Cer, D., Jurafsky, D., and Manning, C. D., “Regulariza- 
tion and search for minimum error rate training”, In proc. 
The Third Workshop on Statistical Machine Translation, 
pp.26-34, 2008.

[24] Macherey, W., Och, F. J., Thayer, I. and Uskoreit, J., 
“Lattice-based minimum error rate training for statistical 
machine translation”, In proc. EMNLP, 2008.

[25] Moore, R. C. and Quirk, C., “Random restarts in min- 
imum error rate training for statistical machine transla- 
tion”, In proc. COLING, pp. 585-592, 2008.

[26] Brown, P. F., Pietra, S. A. D., Pietra, V. D. J., and Mer- 
cer, R. L., “The mathematics of statistical machine trans- 
lation: Parameter estimation”, Computational Linguistics, 
Vol. 19, No. 2, pp. 263-312, 1993.








