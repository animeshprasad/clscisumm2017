<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Minimum error rate training (MERT) is a widely used learn- ful tools for many tasks in natural language processing [6][7].</S>
		<S sid ="2" ssid = "2">The core of the form consists of a smooth convex regularizering method for statistical machine translation.</S>
		<S sid ="3" ssid = "3">In this pa- such as 1 ||w||2 and the empirical risk term of hinge loss.</S>
		<S sid ="4" ssid = "4">In per, we present a SVM-based training method to enhance generalization ability.</S>
		<S sid ="5" ssid = "5">We extend MERT optimization by maximizing the margin between the reference and incorrect translations under the L2-norm prior to avoid overfit- ting problem.</S>
		<S sid ="6" ssid = "6">Translation accuracy obtained by our proposed methods is more stable in various conditions than that obtained by MERT.</S>
		<S sid ="7" ssid = "7">Our experimental results on the French- English WMT08 shared task show that degrade of our proposed methods is smaller than that of MERT in case of small training data or out-of-domain test data.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">The state of the art statistical machine translation systems have been modeled by the log-linear approach which is a generalization of the noizy-channel approach.</S>
			<S sid ="9" ssid = "9">This approach has achieved a lot of great advances because it has provided a natural extention to integrate many useful components [1].</S>
			<S sid ="10" ssid = "10">To estimate the weights toward these components according to their performance, minimum error rate training (MERT) [2] was introduced by Och (2003).</S>
			<S sid ="11" ssid = "11">MERT improves statistical machine translation quality by optimizing the parameter of the log-linear function by using such automatic translation evaluation metrics as the BLEU scores [3].</S>
			<S sid ="12" ssid = "12">To train a small number of real-valued features used on a standard phrase-based statistical machine translation system like Moses [16], MERT with BLEU-based objective function is very effective due to line-search algorithm proposed by Och (2003).</S>
			<S sid ="13" ssid = "13">However, MERT tends to overfit to training data because its objective function consists of no regularizer.</S>
			<S sid ="14" ssid = "14">To enhance generalization ability, we would like to use other state-of-the-art machine learning techniques for machine translation.Support vector machines (SVM) have proven to be power †This research was conducted as an internship program 2008 of NTT.</S>
			<S sid ="15" ssid = "15">∗Taro Watanabe now belongs to National Institute of Information and Communications Technology (NICT) in Japan.this paper we present an approach to optimize the parame ter of the log-liner model using the primal form of structural SVM [12].</S>
			<S sid ="16" ssid = "16">We expect the convex regularizer or the factor of enlarging the margin (between the reference and the incorrect translation) of SVM to reduce the overfitting problem and enhance generalization ability.</S>
			<S sid ="17" ssid = "17">Using the BLEU scores to define the hinge loss, our proposed method also inherits the advantages of MERT, which enhance the BLEU scores of translations.</S>
			<S sid ="18" ssid = "18">In this paper we carried out on a French-English task and exprimentally compared our proposed method with MERT.</S>
			<S sid ="19" ssid = "19">We achieved more significant improvements than these obtained by conventional MERT, especially in case of small training data or out-of-domain test data.</S>
			<S sid ="20" ssid = "20">The remainder of this paper is organized as follows.</S>
			<S sid ="21" ssid = "21">Section 2 briefly reviews the framework for the MERT and Section 3 performs the formulations of the structural SVM and the cutting-plane algorithms to introduce our proposed method in Section 4.</S>
			<S sid ="22" ssid = "22">Section 4 describes the integration of the line-search algorithm with S-slack and 1-slack structural SVM.</S>
			<S sid ="23" ssid = "23">We experimentally compare structural SVM to MERT on the WMT08 French-English task in Section 5.</S>
			<S sid ="24" ssid = "24">In section 6, we conclude with summary and future work.</S>
	</SECTION>
	<SECTION title="Minimum Error Rate Training. " number = "2">
			<S sid ="25" ssid = "1">2.1.</S>
			<S sid ="26" ssid = "2">Log-Linear Approach.</S>
			<S sid ="27" ssid = "3">To translate source sentence f into another target language e, the log-linear approach seeks a maximum solution: ˆe = argmax(w, h(e, f )), (1) e where h(e, f ) is a feature vector and w is a weight vector that scales the contributions from all features.</S>
			<S sid ="28" ssid = "4">This approach has the advantage that addtional models or feature functions can be easily integrated into the overall system.</S>
			<S sid ="29" ssid = "5">However, it must appropriately optimize a weight vector to obtain the 2.3.</S>
			<S sid ="30" ssid = "6">Evaluation Metrics BLEU.</S>
			<S sid ="31" ssid = "7">The BLEU score [3] used in MERT is defined as follows: BLEU({ˆe}S ; {r} 1 )= BP ·exp N ) log pn ({ˆe}S , {r}S ) 1 1 N 1 1 n=1where pn (·) is the n-gram precision of hypothesezed trans lations {e}S given the reference {r}S and BP is a brevity 1 1 penalty.</S>
			<S sid ="32" ssid = "8">The BLEU scores are calculated in terms of a whole corpus.</S>
			<S sid ="33" ssid = "9">Figure 1: each line of the 6-best translations and BLEU scores with 1-best translation selected by the current parameter α good translation results.</S>
	</SECTION>
	<SECTION title="Structural Support Vector Machines. " number = "3">
			<S sid ="34" ssid = "1">In recent years, Support Vector Machines (SVM) for conventional binary classification have been generalized for multi- classification and structured output problems [12][13].</S>
			<S sid ="35" ssid = "2">The generalized SVM is called a structural SVM [12].</S>
			<S sid ="36" ssid = "3">In the generalized margin maximization principle we maximize the separation margin, which is the score difference between the reference rs and incorrect translation ˆes scores.</S>
			<S sid ="37" ssid = "4">To allow errors in training data, we introduce slack variables ξs and optimize with soft-margin criteria: 2.2.</S>
			<S sid ="38" ssid = "5">Minimum Error Rate Training.</S>
			<S sid ="39" ssid = "6">Minimum error rate training (MERT) obtains w that max argmin w,ξ≥0 λ 1w12 + 2 S ) ξs (4) S s=1 imizes BLEU scores on S-size training data {(rs , fs )}S and a set of K different candidate translations Cs = {ˆes,1 , ··· , ˆes,K } for each input sentence fs : s.t. ∀ˆe1 ∈ C1 \ r1 : .</S>
			<S sid ="40" ssid = "7">(w, δh1 )≥ 1 − ξ1 ( argmax BLEU( w S rs , argmax(w, h(ˆes , fs )) ˆes ∈Cs 1 ).</S>
			<S sid ="41" ssid = "8">(2) s.t. ∀ˆe S ∈ CS \ rS : (w, δh S )≥ 1 − ξS , where δhs is hs (rs , fs ) − hs (ˆes , fs ).</S>
			<S sid ="42" ssid = "9">The constraints state If we use the argmin function, we need to calculate the objective function as 1.0 − BLEU.</S>
			<S sid ="43" ssid = "10">Och’s line-search, one of the efficient algorithms for optimizing an objective function, maximizes the function through a sequence of line maximizations along vector directions d. To compute the most probable sentence ˆes,best from hypotheses Cs , the optimization problem is defined with α as follows: ˆes,best = argmax(w + αd, h(ˆes , ˆfs )) that for each training example (rs , fs ), the score w · h(rs , fs ) of the correct structure rs must be greater than the score w · h(ˆes , fs ) of all incorrect structures ˆe by the margin 1.</S>
			<S sid ="44" ssid = "11">The standard structural SVM optimization problem has also been generalized in several ways [12][13].</S>
			<S sid ="45" ssid = "12">Tsochantaridis et al.</S>
			<S sid ="46" ssid = "13">(2005) introduced two different ways of using a hinge loss to the convex upper bound of the loss, namely, ”margin-rescaling” and ”slack-rescaling”.</S>
			<S sid ="47" ssid = "14">In this paper, we use only a margin-rescaling.</S>
			<S sid ="48" ssid = "15">The margin-rescaling is for thespecial case of the Hamming loss.</S>
			<S sid ="49" ssid = "16">Each margin rescaling ˆes ∈Cs ⎧ ⎪⎨ ⎫ .</S>
			<S sid ="50" ssid = "17">(3) ⎬ constraint has the following form: = argmax (w, h(ˆes , ˆf )) +α (d, h(ˆe , ˆf )) s.t. ∀ˆes ∈ Cs \ rs : (w, δhs )≥ .6(rs , ˆes ) − ξs s s s ˆes ∈Cs ⎪⎩ inte r cept _ sl o pe _⎪⎭ where this rescaling can penalize ˆe /= r with high loss Each translation in the hypotheses corresponds to a line with the slope and the intercept in the argmax of Eq.3.</S>
			<S sid ="51" ssid = "18">Figure.1 shows the example of lines for the 6-best translations.</S>
			<S sid ="52" ssid = "19">For any particular choice of α, the decoder seeks the translation that yields the largest score.</S>
			<S sid ="53" ssid = "20">Och’s algorithm shifts α from −∞ to ∞ to obtain the best parameter w while recording the points where two or more lines intersect and selects α which is able to determine the translation obtaining the highest evaluation scores.</S>
			<S sid ="54" ssid = "21">Δ(r, ˆe) more severely than that with small loss.</S>
			<S sid ="55" ssid = "22">3.1.</S>
			<S sid ="56" ssid = "23">Cutting-Plane Algorithm.</S>
			<S sid ="57" ssid = "24">The optimization problem in Eq.4 with margin-rescaling hasO(S|C|) constraints.</S>
			<S sid ="58" ssid = "25">In general, |C| is extremely large or infinite.</S>
			<S sid ="59" ssid = "26">To cut a large number of these constraints, the cutting plane algorithm (Algorithm 1) iteratively finds the most violated constraint through the training data: Algorithm 1 Cutting-plane training for S-slack Formulation with margin-rescaling Algorithm 2 Cutting-plane training for 1-slack Formulation with margin-rescaling 1: Input : w, CS , {rs , fs }S ,E 1: Input : w, CS , {rs , fs S 1 1 2: 1 = {} 2: W 1 1 = {} 3: repeat 3: repeat λ 2 1 S 4: for s = 1 ...S do 4: w = argminw,ξ≥0 2 1w1 + S Ls=1 ξs 5: ˆes = argmaxˆes ∈Cs {Δ(rs , ˆes ) − (w, δhs )} s.t. ∀(ˆe1 , ··· , es ) ∈ W : λ Ls=1 (w, δhs ) ≥ 6: if Δ(rs , ˆes ) − (w, δhs )&gt;ξs + E then 1 LS.</S>
			<S sid ="60" ssid = "27">S s=1 .6(rs , S ˆes ) − ξs 7: add(Ws , ˆes ) λ 2 1 S 5: for s = 1 ...S do 8: w = argminw,ξ≥0 2 1w1 s=1 ξs 6: ˆes = argmaxˆes ∈Cs {Δ(rs , ˆes ) − (w, δhs )} s.t. ∀ˆe1 ∈ W1 : (w, δh1 )≥ .6(r1 , ˆe1 ) − ξ1 . ∀ˆeS ∈ WS : (w, δhS )≥ .6(rS , ˆeS ) − ξS 7: end for 8: add(W , (ˆe1 , ··· , ˆeS )) 9: until 9: end if 10: end for 11: until This algorithm iteratively constructs a working set W = W1 ∪· · ·∪ WS of constraints.</S>
			<S sid ="61" ssid = "28">The most violated constraint for margin-rescaling (line 5) is defined as follows: It iteratively constructs a working set W of constrains.</S>
			<S sid ="62" ssid = "29">In each iteration, this algorithm optimizes the parameter over the current working set W , find the most violated constrait, and add it to the working set.</S>
			<S sid ="63" ssid = "30">Unlike the S-slack algorithm, only a single costraint is added in each iteration.</S>
	</SECTION>
	<SECTION title="Minimum Error Rate Training based on. " number = "4">
			<S sid ="64" ssid = "1">Structural SVM ξs = max {.6(rs , ˆes ) − (w, δhs )}.</S>
			<S sid ="65" ssid = "2">(5) ˆes ∈Cs \rs If this constraint is violated by more than the desired precision E (line 6), the constraint is added to the working set W and the QP is solved over the W (line 8).</S>
			<S sid ="66" ssid = "3">3.2.</S>
			<S sid ="67" ssid = "4">1-Slack Formulation Joachims et al.</S>
			<S sid ="68" ssid = "5">(2009) proposed to replace the S cutting- plane models of the hinge loss with a single cutting-plane model for the sum of the hinge losses.</S>
			<S sid ="69" ssid = "6">This model has only one slack variable ξ that is shared across all constraints: argmin λ 1w12 + ξ.</S>
			<S sid ="70" ssid = "7">(6) w,ξ≥0 2 The constraint for 1-slack formulation with margin-rescaling is s.t. ∀(ˆe1 , ··· , ˆeS ) ∈ CS : In this section we describe how to apply the structural SVM to the MERT objective function.</S>
			<S sid ="71" ssid = "8">The first subsection addresses how to calculate the feature score of the ”correct” sentence.</S>
			<S sid ="72" ssid = "9">In the second subsection we define loss function Δ(rs , ˆes ) using the BLEU scores, and the third subsection extends Och’s line-search algorithm as the optimization algorithm for the SVM-based objective function.</S>
			<S sid ="73" ssid = "10">4.1.</S>
			<S sid ="74" ssid = "11">Selecting the Configuration in the K -best list.</S>
			<S sid ="75" ssid = "12">For structural SVM, we need to label a correct candidate translation for each source sentence from its K -best list of candidates, which is called a configuration.</S>
			<S sid ="76" ssid = "13">Since the BLEU scores are not cumulative, we cannot efficiently select the best configuration from the K -best list.</S>
			<S sid ="77" ssid = "14">So we approximate it by a greedy search algorithm [14].</S>
			<S sid ="78" ssid = "15">This algorithm considers the impact on the training set score when selecting an alternative translation by subtracting the statistics for the current configuration choice from the accumulated statistics and adding those for the alternative and selects the translation which results in the highest score.</S>
			<S sid ="79" ssid = "16">Repeat this process and continue untill there are no configura 1 ) S s=1 1 (w, δhs )≥ S S ) s=1 Δ(rs , ˆes ) − ξ.</S>
			<S sid ="80" ssid = "17">tion changes.</S>
			<S sid ="81" ssid = "18">The configuration obtained by this algorithm specifies the correct candidate for each K -best list, and the BLEU scores are the upper bound for the BLEU scores on the training set.</S>
			<S sid ="82" ssid = "19">It is proven that the S-slack and 1-slack formulations are equivalent in some points [13].</S>
			<S sid ="83" ssid = "20">In the next section we use this formulation to calculate the whole corpus-wise BLEU scores.</S>
			<S sid ="84" ssid = "21">Cutting-plane training for 1-slack formulation is showed in Algoritm 2: 4.2.</S>
			<S sid ="85" ssid = "22">Loss Function for Rescaling.</S>
			<S sid ="86" ssid = "23">4.2.1.</S>
			<S sid ="87" ssid = "24">Approximation for the Sentence-wise BLEU The BLEU scores are defined in terms of a whole corpus and not over individual sentences.</S>
			<S sid ="88" ssid = "25">However we need to calcu late the sentence-wise BLEU scores to define the loss function Δ(rs , ˆes ).</S>
			<S sid ="89" ssid = "26">To calculate sentence-wise BLEU we use the approximated BLEU like the one proposed by Watanabe et Algorithm 3 extended Och’s line-search algorithm for S- slack formulation 1: Input : w, d, CS , {rs , ˆe∗, fs S al.</S>
			<S sid ="90" ssid = "27">(2006).</S>
			<S sid ="91" ssid = "28">Given the configurations for S input sentences 2: I = {} 1 s 1 {ˆe∗ ...</S>
			<S sid ="92" ssid = "29">ˆe∗ }, this approximated BLEU scores on translation 3: for s = 1 ...S do 1 S candidate ˆes for s-th input sentence are calculated by substituting ˆe∗ with ˆes . 4: for all ˆes ∈ Cs \ ˆe∗ do 5: ˆe .m = (d, δh ) s s s 6: ˆes .b = Δ(ˆe∗, ˆes ) − (w, δhs )4.2.2.</S>
			<S sid ="93" ssid = "30">Loss Functions based on Sentence-wise and Corpus wise BLEUThe sentence-wise loss function Δ(rs , ˆes ) is defined as fol 7: end for 8: i =0 9: li = argminˆes ∈Cs \ˆe∗ ˆes .m lows by using this approximated BLEU scores: 10: xi = −∞ Q × {BLEU({rs , ˆe∗} ) − apBLEU({rs , ˆes } )}, 11: repeat 12: i = i +1 s 1 1 li .m−li−1 .m } &gt; xi−1 then where Q is a constant for scaling the BLEU loss function.</S>
			<S sid ="94" ssid = "31">If 14: li = argminˆe C ˆe∗ { ˆe l .m } li 1 .b−ˆes .b we set the parameter Q on high, we regard the loss function s ∈ s \ s s .m− i−1 as important.</S>
			<S sid ="95" ssid = "32">We use the sentence-wise BLEU loss function li 1 .b−li .b li .m−li−1 .m } Δ(rs , ˆes ) to calculate the objective function of the S-slack SVM formulation.</S>
			<S sid ="96" ssid = "33">Since we believe that the average sentence-wise BLEU 16: end if 17: until no more intersections 18: add(I , xi ) add(I , max(I )+ 2E) scores are less reliable than the whole corpus-wise BLEU scores, we tried to apply the corpus-wise BLEU to the objective function using a 1-slack formulation SVM assum 19: 20: xbest = argminx∈I Obj(w + (x − E)d, Cs ∗ s ing 1 LS Δ(rs , es ) ∝ corpus wise BLEU loss func 1 , { r j , ˆ e j , f j } j = 1 ) 21: w+ = (xbest − E) S s=1 tion Δ({rs , es S ).</S>
			<S sid ="97" ssid = "34">The corpus-wise BLEU loss function }1 S 22: delete(I , max(I )+ 2E) 23: end for Δ({ˆe∗, ˆes } ) is as follows: 24: return w s 1 Q × {BLEU({rs , ˆe∗ S ) − BLEU({rs , ˆes S )}.</S>
			<S sid ="98" ssid = "35">s 1 11-slack formulation with margin-rescaling is constructed us S ing the corpus-wise BLEU loss function Δ({ˆe∗, ˆes } ): In case of S-slack formulation (Algorithm 3), the max s 1 function in Eq. 5 corresponds to the argmax function in Eq. argmin λ 1w12 + ξ.</S>
			<S sid ="99" ssid = "36">(7)3, so we can find a translation that has the most violated con w,ξ≥0 2 straint ξs by a line-search algorithm with the following slope s.t. ∀(ˆe1 , ··· , ˆeS ) ∈ CS : and intercept (line 5,6) : 1 ) S (w, δhs )≥ Δ({ˆe∗, ˆes } ) − ξ.</S>
			<S sid ="100" ssid = "37">s 1 s=1 ⎧ ⎫ Unlike the margin infused relaxed algorithm (MIRA) [9] and ⎪⎨ ⎪⎬ argmax Δ(e∗ s , es ) − (w, δhs ) +α (d, δhs ) . S-slack formulation, we can directly apply the corpus-wise BLEU to the SVM objective function without approximating the BLEU scores.</S>
			<S sid ="101" ssid = "38">ˆes ∈Cs ⎪⎩ intercept _ slope _⎪⎭ 4.3.</S>
			<S sid ="102" ssid = "39">Optimization Algorithm.</S>
			<S sid ="103" ssid = "40">4.3.1.</S>
			<S sid ="104" ssid = "41">Line-search Algorithm Next we describe extended Och’s line-search algorithms for S-slack and 1-slack formulation of the Structural SVM.</S>
			<S sid ="105" ssid = "42">These pseudocodes for the line-search to optimize parameter w are given by Algorithm 3,4.</S>
			<S sid ="106" ssid = "43">In Algorithm 3,4 we can find the range of values along the direction vector d to which each candidate translation is assigned the best score.</S>
			<S sid ="107" ssid = "44">Algorithm 3 is the line-search algorithm for S-slack formulation of the structural SVM: Algorithm 3 iteratively constructs the line intersections I through the training examples and estimates the parameter variation (x − E) which is able to select the translation min imizing the objective function.</S>
			<S sid ="108" ssid = "45">The process of Algorithm 3 which extracts the most violated translation is similar to the cutting-plane training in Algorithm 1.</S>
			<S sid ="109" ssid = "46">In case of the S- slack formulation with margin-rescaling, the Obj function (line 20) is constructed by each of the most violated translations, as in Eq. 4, using margin-rescaling constraints.</S>
			<S sid ="110" ssid = "47">Algorithm 4 is the line-search algorithm for 1-slack formulation of the structural SVM:Algorithm 4 extended Och’s line-search algorithm for 1 slack formulation 1: Input : w, d, CS , {rs , ˆe∗, fs S • Three orientation types reordering model[17]: p(m|f , e) , p(s|f , e) , p(d|f , e) to capture the lexicalized information.</S>
			<S sid ="111" ssid = "48">2: I = {} 1 s 1 • Word , Phrase penalty: To control the target length and 3: for s = 1 ...S do 4: for all ˆes ∈ Cs \ ˆe∗ do the average length of the phrases.</S>
			<S sid ="112" ssid = "49">5: ˆes .m = (d, hs (ˆes , fs )) In this paper, we trained these small number of features and 6: ˆes .b = (w, hs (ˆes , fs )) 7: end for 8: i =0 9: li = argminˆes ∈Cs \ˆe∗ ˆes .m 10: xi = −∞ 11: repeat 12: i = i +1 li .m−li−1 .m } &gt; xi−1 then li−1 .b−ˆes .b phrases were extracted using a typical approach [16] that ran GIZA++ [18].</S>
			<S sid ="113" ssid = "50">We used a Katz smoothing 5-gram language model that was created using the SRILM toolkit [19].</S>
			<S sid ="114" ssid = "51">5.</S>
			<S sid ="115" ssid = "52">2. D at a Se t For experiments we used the French-English data provided for the Europarl-based WMT08 shared task.</S>
			<S sid ="116" ssid = "53">Europarl corpus was collected from the proceedings of European Parliament 14: li = argminˆes ∈Cs \ˆe∗ { ˆe .m−l s s li 1 .b−li .b li .m−li−1 .m } 16: end if i−1 .m } [20].</S>
			<S sid ="117" ssid = "54">This training corpus contains about 1.3 M sentences.</S>
			<S sid ="118" ssid = "55">Parameters were tuned over the provided development set (dev2006) that consisted of 2000 sentences with one refer 17: until no more intersections 18: add(I , xi ) 19: end for 20: add(I , max(I )+ 2E) 21: xbest = argminx∈I Obj(w + (x − S ence.</S>
			<S sid ="119" ssid = "56">We used two open test sets: Europarl test 2008, consisting of 2000 sentences with one reference, and News news- test 2008 (out-of-domain), consisting of 1500 sentences.</S>
			<S sid ="120" ssid = "57">Table 1 shows these contents in more detail.</S>
			<S sid ="121" ssid = "58">E)d, CS , {rs , ˆe∗, fs } ) 1 s 1 22: return w + (xbest − E)d Table 1: The Data statistics For the 1-slack formulation we should calculate slopes m and intercepts b the same as the S-slack formulation, but, to avoid bias toward the line-search procedure by sentence- wise BLEU, we computed them the same as MERT (line 5 6).</S>
			<S sid ="122" ssid = "59">Unlike the S-slack formulation, this algorithm constructs the line intersections I over all the training samples and then estimates the best parameter varidation.</S>
			<S sid ="123" ssid = "60">The process of this algorithm which extracts 1-best translation is similar to that of Algorithm 2 extracting the most violated constraint.</S>
			<S sid ="124" ssid = "61">In case of the 1-slack formulation with margin-rescaling, the Obj function (line 21) is constructed as Eq. 7.</S>
	</SECTION>
	<SECTION title="Experimental Results. " number = "5">
			<S sid ="125" ssid = "1">5.1.</S>
			<S sid ="126" ssid = "2">Systems.</S>
			<S sid ="127" ssid = "3">Experiments were conducted using a standard phrase-based statistical MT system called Moses [16] to generate K -best lists (K =1000).</S>
			<S sid ="128" ssid = "4">Moses employs standard real-valued features: • N -gram language model: P r(e) to calculate the fluency of the target side.</S>
			<S sid ="129" ssid = "5">• Lexical translation model: t(ei |fj ) , t(fj |ei ) to calculate the word translation probability.</S>
			<S sid ="130" ssid = "6">• Phrase translation model: φ(e|f ) , φ(f |e) to calculate the phrase translation probability.</S>
			<S sid ="131" ssid = "7">5.3.</S>
			<S sid ="132" ssid = "8">Results.</S>
			<S sid ="133" ssid = "9">5.3.1.</S>
			<S sid ="134" ssid = "10">Tuning Hyperparamers Figure 2 shows the effect of the hyperprameter λ which emphasizes the convex regularizer on SVM objective function.</S>
			<S sid ="135" ssid = "11">When we set λ high, the curve is like a quadratic function and the best parameter to optimize the objective function is close to 0.</S>
			<S sid ="136" ssid = "12">On the other hand, if we set it low, the shape of the function is almost the same MERT’s objective function.</S>
			<S sid ="137" ssid = "13">Hence it is very important for obtaining the good translation results to appropriately set the hyperparameters.</S>
			<S sid ="138" ssid = "14">We tuned hyperparameters λ and Q in the SVM-based method by the cross-validation method.</S>
			<S sid ="139" ssid = "15">We divided dev2006 into two and the first estimation was performed on one (another was used for development set) and the second did on another.</S>
			<S sid ="140" ssid = "16">In our cross-validation experiments for tuning the hyperparameters, we noticed that the higher Q is, the better 2.5 2 / &apos; 4 6 58/ ǳ=0.01 58/ ǳ=1.0 58/ ǳ=100 1.5 1 0.5 00.51 -1.52 -43 -21 0 1 2 3 4 2CTCOGVGT Figure 2: This shows the shapes of BLEU and 1-slack SVM objective function for one parameter.</S>
			<S sid ="141" ssid = "17">These lines were calculated by 800 development sentences randomly selected from dev06 for development data when the hyperparameter Q is fixed 1000.0.</S>
			<S sid ="142" ssid = "18">31.5 31 30.5 30 development test1(1-slack SVM) development test2(1-slack SVM) development test1(S-slack-SVM) development test2(S-slack-SVM) SVM objecti ve functi on while we evalua ted the BLEU scores at the corpus level.</S>
			<S sid ="143" ssid = "19">Table 2: BLEU scores on the test08 and news0 8 test data obtain ed by model s traine d by MER T and SVM.</S>
			<S sid ="144" ssid = "20">29.5 29 28.5 28 27.5 10e0 10e1 10e2 10e3 HyperParameter Q 10e4 10e5 10e6 Table 2 shows the translati on accurac y in both in domain and outof domain test set.</S>
			<S sid ="145" ssid = "21">The ”untune ” means the result Figure 3: Tuninig test for hyperparameter Q of structural SVM (fixed λ=1.0) by increasing it.</S>
			<S sid ="146" ssid = "22">BLEU scores on the development test set are (Figure 3); so in the open test experiments, we always fixed Q=100000.0 , λ=1.0 for S-slack SVM and Q=10000.0 , λ=1.0 for 1-slack SVM.</S>
			<S sid ="147" ssid = "23">5.3.2.</S>
			<S sid ="148" ssid = "24">Test Experiment We compared our proposed SVM-based method to MERT using the whole dev2006 for development data on 4-gram BLEU scores of two open test sets.</S>
			<S sid ="149" ssid = "25">Table 2 shows that 1-slack-SVM outperformed MERT.</S>
			<S sid ="150" ssid = "26">On the other hand, S- slack-SVM was not more effective than MERT because we approximated the BLEU scores to calculate the S-slack from default parameters and we performed MERT and SVM training, starting from these parameters.</S>
			<S sid ="151" ssid = "27">Our two proposed method based on SVM achieve comparable performance of MERT in in-domain test set (test08) and slightly outperform MERT in out-of-domain test set (news08).</S>
			<S sid ="152" ssid = "28">This means that SVM has good effects on training the parameter of log-linear model especially in case of the out-of-domain translation.</S>
			<S sid ="153" ssid = "29">5.3.3.</S>
			<S sid ="154" ssid = "30">Experiments in Case of Data Sparseness and Out-of- domain Problems We also conducted the experiment in case of small development data.</S>
			<S sid ="155" ssid = "31">Figure 4 shows the performance of Moses with training MERT, S-slack SVM and 1-slack SVM by gradually increasing the development data.</S>
			<S sid ="156" ssid = "32">The BLEU scores of MERT on the test08 data is drastically degraded when the development data size is getting smaller, and the learning curve 32.2 32 untune MERT S-slack SVM 1-slack SVM Table 4: BLEU scores of two open test sets obtain ed when trainin g by MERT, Sslack SVM and 1slack SVM using four develo pment sets contai ning 400 senten ces rando mly selectin g from WMT 08 dev20 06.</S>
			<S sid ="157" ssid = "33">31.8 31.6 31.4 31.2 31 30.8 100 200 300 400 500 600 700 800 900 1000 the size of development data Figure 4: BLEU scores as a function of development data size.</S>
			<S sid ="158" ssid = "34">is unstable.</S>
			<S sid ="159" ssid = "35">On the other hand, S-slack and 1-slack SVM is more stable than MERT and degrade of the smaller development data is fewer than that of MERT.</S>
			<S sid ="160" ssid = "36">Especially, 1-slack SVM is most stable among these three.</S>
			<S sid ="161" ssid = "37">Table 3: The average improvements of BLEU scores on the test08 and news08 (out-of-domain) when we trained the paramenters using only 400 development sentences with MERT and SVM-based algorithms four times.</S>
			<S sid ="162" ssid = "38">m et ho d tes t0 8 news08 unt un e s m o ot h ed M E R T ( O c h, 0 3) M E R TS slack SV M1 slack SV M 3 0.</S>
			<S sid ="163" ssid = "39">8 4 13.75 +0 .2 2 −0.08 +0 .4 0 +0.03 +0 .4 5 +0.21 +0 .9 2 +0.40 Table 3 shows the average improvements of BLEU scores when we train the parameters using only 400 sentences randomly selected from the development data.</S>
			<S sid ="164" ssid = "40">We repeated the experiment four times and averaged the improvements of BLEU scores.</S>
			<S sid ="165" ssid = "41">Table 4 show these results in more details.</S>
			<S sid ="166" ssid = "42">This indicates that two SVM methods reduce the overfitting problem when assuming that only few development data are available or test set is out-of-domain.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "6">
			<S sid ="167" ssid = "1">Crammer et al.</S>
			<S sid ="168" ssid = "2">(2006) proposed Margin infused relaxed algorithm (MIRA) , which was the online large-margin training algorithm for structured classification, and Watanabe et al.</S>
			<S sid ="169" ssid = "3">(2007) applied it to a discriminative training algorithm for statistical machine translation to estimate a large number of parameters.</S>
			<S sid ="170" ssid = "4">In some points, our proposed method is diffrent from MIRA.</S>
			<S sid ="171" ssid = "5">First, the proposed algorithm is a batch style algorithm and using 1-slack formulation of structural SVM proposed by Joachims (2009) we try to use corpus-wise BLEU for the objective function without approximating the BLEU scores.</S>
			<S sid ="172" ssid = "6">Secondly, we directly apply the line-search algorithm to SVM optimization problem to estimate a small number of paramenters.</S>
			<S sid ="173" ssid = "7">Cer and Manning (2008) proposed the other approach to regularize the objective function of the MERT.</S>
			<S sid ="174" ssid = "8">This regular- ization was not to search the current best optima but to consider the adjacent evaluation scores with fixed window size during line-search because the objective function had a very deep and narrow optima.</S>
			<S sid ="175" ssid = "9">This approach was different from our proposed method, but it statistically achieves significant gains when combined with line-search.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "7">
			<S sid ="176" ssid = "1">We presented a new method to regularize the MERT objective function using structural SVM.</S>
			<S sid ="177" ssid = "2">This function has 1 ||w||2 as a smooth convex regularizer and a factor maximizing the score margin between a reference and an incorrect translation.</S>
			<S sid ="178" ssid = "3">We also tried to apply the corpus-wise BLEU score to the objective function without approximating the BLEU scores for each sentence.</S>
			<S sid ="179" ssid = "4">To optimize a small number of real- valued parameters with this function, we directly used Och’s line-search algorithm.</S>
			<S sid ="180" ssid = "5">The experimental results show that a SVM-based methods are more stable than MERT in various conditions.</S>
			<S sid ="181" ssid = "6">They outperform MERT when only small development data is available or these are mismatch between the training and test conditions.</S>
			<S sid ="182" ssid = "7">In the future, we plan to experiment on a decoder that has a large number of features because SVM-based algorithm is expected to work more effectively on the sparse vector space [22].</S>
			<S sid ="183" ssid = "8">We also think that a gradient based algorithm such as Pegasos [21] and a software package SV M struct [13] for SVM dual formulation allowing the use of kernels are more appropriate methods optimizing the parameter on such a decoder than Och’s line-search algorithm.</S>
	</SECTION>
	<SECTION title="Acknowledgements. " number = "8">
			<S sid ="184" ssid = "1">I would like to thank Yoshinobu Tonomura, Jun Suzuki, Seiichi Yamamoto and Shigeru Katagiri because of providing me the chance of this internship program, and Masafumi Nishida and Katsuhito Sudo for very helpful and detailed comments.</S>
	</SECTION>
	<SECTION title="References. " number = "9">
</PAPER>
