<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Linear text segmentation aims at dividing a long text into several topical segments.</S>
		<S sid ="2" ssid = "2">It is beneficial to many natural language processing tasks, such as information retrieval and document summarization.</S>
		<S sid ="3" ssid = "3">In this article, an efficient linear text segmentation algorithm based on hierarchical agglomerative clustering is presented.</S>
		<S sid ="4" ssid = "4">The proposed linear text segmentation algorithm is implemented without auxiliary knowledge base, parameter setting, and user involvement.</S>
		<S sid ="5" ssid = "5">Experimental results show that the proposed linear text segmentation algorithm not only provides linear time computational complexity, but also provides comparable segmentation accuracy with several well- known linear text segmentation algorithms.</S>
		<S sid ="6" ssid = "6">Keywords-text segmentation; hierarchical agglomerative clustering, computational intelligence, NLP application</S>
	</ABSTRACT>
	<SECTION title="INTRODUCTION" number = "1">
			<S sid ="7" ssid = "7">The purpose of linear text segmentation is to divide a long text into several segments, each of which corresponds to a topic and consists of consecutive sentences or paragraphs.</S>
			<S sid ="8" ssid = "8">In other words, the task of linear text segmentation is to identify topic boundaries within a long text.</S>
			<S sid ="9" ssid = "9">Linear text segmentation algorithms are widely used as an essential step in many natural language processing tasks, such as information retrieval and document summarization.</S>
			<S sid ="10" ssid = "10">In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the user’s needs are retrieved [1].</S>
			<S sid ="11" ssid = "11">It not only provides more accurate information to the user, but also reduces the user’s burden to read the whole document.</S>
			<S sid ="12" ssid = "12">In document summarization, a long document is often divided into topics and then each topic is summarized independently [2].</S>
			<S sid ="13" ssid = "13">A text segmentation algorithm is usually applied as the first step of these tasks.</S>
			<S sid ="14" ssid = "14">Segmentation accuracy and computational complexity are two critical issues in linear text segmentation algorithm design.</S>
			<S sid ="15" ssid = "15">In the past, many well-known linear text segmentation algorithms have been proposed [3, 4, 5, 6].</S>
			<S sid ="16" ssid = "16">9780-76954584-4/11 $26.00 © 2011 IEEE DOI 10.1109/CIS.2011.240 1081 Although it has been proven that these algorithms either improve the segmentation accuracy or provide efficient processing of linear text segmentation, they can hardly deal with both issues at the same time.</S>
			<S sid ="17" ssid = "17">Some algorithms provide better segmentation accuracy, but suffer from expensive computational complexity; some algorithms provide efficient processing of linear text segmentation, but suffer from lower segmentation accuracy [7].</S>
			<S sid ="18" ssid = "18">In recent years, some algorithms, such as TSF [7], are proposed to provide high segmentation accuracy in a reasonable processing time.</S>
			<S sid ="19" ssid = "19">However, most of the algorithms rely heavily on a training phase or manpower to designate parameters used in the algorithms.</S>
			<S sid ="20" ssid = "20">The training phase is time consuming, while to designate parameters by manpower increases user’s burden.</S>
			<S sid ="21" ssid = "21">Moreover, it is difficult to determine manually suitable parameters.</S>
			<S sid ="22" ssid = "22">To tackle the problems mentioned above, a novel efficient linear text segmentation algorithm based on Hierarchical Agglomerative Clustering (HAC) is presented in this article.</S>
			<S sid ="23" ssid = "23">The proposed linear text segmentation algorithm dynamically designates parameters according to the metadata of a text, and hence neither training data nor user involvement is needed.</S>
			<S sid ="24" ssid = "24">In other words, the algorithm requires only the given text, no other information or preprocessing is needed.</S>
			<S sid ="25" ssid = "25">It not only reduces the user’s burden, but also increases the utility of the proposed linear text segmentation algorithm.</S>
			<S sid ="26" ssid = "26">Segmentation accuracy of the proposed linear text segmentation algorithm is evaluated with a most commonly used test collection, created by Choi [3].</S>
			<S sid ="27" ssid = "27">Also, the computational complexity of the proposed algorithm is analyzed.</S>
			<S sid ="28" ssid = "28">Experimental results show that the proposed algorithm not only provides comparable segmentation accuracy with several well-known linear text segmentation algorithms, but also provides a linear time computational complexity without any auxiliary knowledge base, parameter setting, or user involvement.</S>
			<S sid ="29" ssid = "29">Existing text segmentation algorithms can be divided into two major categories [3].</S>
			<S sid ="30" ssid = "30">The first category is lexical cohesion methods.</S>
			<S sid ="31" ssid = "31">The second category is multi-source methods.</S>
			<S sid ="32" ssid = "32">Lexical cohesion methods detect cohesion using word stem repetition, context vectors, entity repetition, semantic similarity, word distance model, and word frequency model.</S>
			<S sid ="33" ssid = "33">Multi-source methods combine lexical cohesion with other indicators of topic shift, including cue phrase, prosodic features, reference, syntax and lexical attraction using decision trees and probabilistic models.</S>
			<S sid ="34" ssid = "34">In the literatures, some efficient linear text segmentation algorithms are proposed, such as TextTiling [5].</S>
			<S sid ="35" ssid = "35">TextTiling is a well-known linear time text segmentation algorithm proposed by Hearst.</S>
			<S sid ="36" ssid = "36">It uses a sliding window approach to segment a text.</S>
			<S sid ="37" ssid = "37">The similarities between adjacent blocks within the text are computed to detect topic changes.</S>
			<S sid ="38" ssid = "38">The computed similarities are smoothed, and used to identify topic boundaries by a cutoff function.</S>
			<S sid ="39" ssid = "39">Although TextTiling is an efficient linear text segmentation method (the complexity is linear time), it suffers from lower segmentation accuracy.</S>
			<S sid ="40" ssid = "40">There are some more complex linear text segmentation algorithms, such as C99 [3].</S>
			<S sid ="41" ssid = "41">Sentence-similarity matrix, consists of similarities between all sentences within a text, is frequently adopted in these algorithms.</S>
			<S sid ="42" ssid = "42">For example, C99 proposed by Choi [3] segments a text by combining a rank matrix, transformed from the sentence-similarity matrix, and divisive clustering.</S>
			<S sid ="43" ssid = "43">Some algorithms apply the similarity matrix to detect the optimal topic boundaries by using dynamic programming [4, 6].</S>
			<S sid ="44" ssid = "44">Owing to the cost of constructing the sentence-similarity matrix is O(n2), where n represents the number of sentences in a text, such algorithms suffer from higher computational complexity [7].</S>
			<S sid ="45" ssid = "45">To tackle the problems observed above, Kern and Granitzer proposed an efficient linear text segmentation algorithm, called TSF [7].</S>
			<S sid ="46" ssid = "46">Similar to TextTiling, TSF identifies topic boundaries using a sliding window.</S>
			<S sid ="47" ssid = "47">The differences between TSF and TextTiling are: 1) in TSF, a term vector is built per sentence, instead of merging all terms within a block into one term vector in TextTiling; 2) both the inner similarityʳ of a block of sentences and the outer similarity of adjacent blocks of sentences are considered when evaluating a potential segmentation position.3) In TSFʳ the smoothing happens implicitly by using the average of the sentence similarities, instead of smoothing the similarity between adjacent blocks in TextTiling.</S>
			<S sid ="48" ssid = "48">It has been proven that TSF has an O(n) computational complexity, and provides a comparable accuracy when compared with several higher computational complexity algorithms.</S>
			<S sid ="49" ssid = "49">However, two parameters need to be provided by the user, say the size of block and the threshold to identify candidate topic boundaries.</S>
			<S sid ="50" ssid = "50">It may increase the user’s burden and the parameters provided may not always be suitable to reflect the linear time complexity, i.e., O(n).</S>
			<S sid ="51" ssid = "51">Especially, no parameter setting is required.</S>
			<S sid ="52" ssid = "52">III.</S>
			<S sid ="53" ssid = "53">TEXT SEGMENTATION BASED ON HIERARCHICAL AGGLOMERATIVE CLUSTERING (TSHAC) In this section, an efficient linear text segmentation algorithm (called TSHAC), which considers both computational complexity and segmentation accuracy, is proposed.</S>
			<S sid ="54" ssid = "54">The process of TSHAC consists of 4 steps.</S>
			<S sid ="55" ssid = "55">At first, a long text is preprocessed; tokenization, stopword removal, and stemming are conducted to construct the vocabulary of the text.</S>
			<S sid ="56" ssid = "56">After text preprocessing, the text can be represented as vectors, each of which represents a sentence within the text.</S>
			<S sid ="57" ssid = "57">A part of sentence similarities are then computed to construct the sentence-similarity matrix.</S>
			<S sid ="58" ssid = "58">Finally, the optimal topic boundaries are identified by the proposed algorithm.</S>
			<S sid ="59" ssid = "59">A. Text Preprocessing In general, the process of text preprocessing can be divided into 3 stages: 1) tokenization; 2) stopword removal; 3) stemming.</S>
			<S sid ="60" ssid = "60">Punctuation is firstly removed from a long text, and the sentences within the text are then converted into a stream of words.</S>
			<S sid ="61" ssid = "61">Subsequently, generic stopwords are removed.</S>
			<S sid ="62" ssid = "62">The rest of words are stemmed and regarded as the vocabulary of the long text.</S>
			<S sid ="63" ssid = "63">In 2003, Ji et al. [6] devise a new idea of document- dependent stopword removal.</S>
			<S sid ="64" ssid = "64">In contrast to the fixed set of generic stopwords, document-dependent stopwords are the words that are useful in discriminating among several different documents but are rather harmful in detecting subtopics in a document.</S>
			<S sid ="65" ssid = "65">Both generic stopword removal and document-dependent stopword removal are adopted in our proposed linear text segmentation algorithm.</S>
			<S sid ="66" ssid = "66">B. Text Representation Given a long text T, a sentence sj contained in T can be represented as a vector in order: s j = {w1 j , w2 j ,..., wij ,...wkj } where wij represents the word weight of the ith word in sj; k represents the total number of words in the vocabulary V constructed from the long text.</S>
			<S sid ="67" ssid = "67">In our proposed algorithm, wij can be computed by any formula, such as TFxIDF, that the word weight can be suitably represented.</S>
			<S sid ="68" ssid = "68">As the core contribution of the proposed algorithm is the process of hierarchical agglomerative clustering for linear text segmentation, here we use the same formula as in TSF for comparison purpose.</S>
			<S sid ="69" ssid = "69">The word weight wij is computed by a variant of TFxIDF [7]: real metadata.</S>
			<S sid ="70" ssid = "70">Based on the above observations, an efficient linear text segmentation algorithm based on Hierarchical wij = termFreqij tokenCount j (log( docCount docFreqi + 1 ) + 1) , Agglomerative Clustering (HAC) [8] will be presented in the next section.</S>
			<S sid ="71" ssid = "71">The proposed linear text segmentation where termFreqij represents the occurrences of word i in sentence j; tokenContj represents the total number of words from a corpus; docFreqi represents the total number of documents contain word i. C. Sentence-Similarity Matrix Construction In previous researches, the sentence-similarity matrix is usually constructed by computing the similarities of all pairs of sentences within a text.</S>
			<S sid ="72" ssid = "72">The similarity of two sentences is usually computed by cosine measurement: ¦ wik w jk , Co sin e(s , s ) = k Segmentation Hierarchical Agglomerative Clustering (HAC) [8] is a bottom-up hierarchical clustering method.</S>
			<S sid ="73" ssid = "73">It has been successfully applied to many applications.</S>
			<S sid ="74" ssid = "74">In TSHAC, a sentence is used as the basic processing unit.</S>
			<S sid ="75" ssid = "75">For more efficient process of linear text segmentation, in the first step, a text is roughly divided into several blocks of sentences by the method mentioned in the last subsection.</S>
			<S sid ="76" ssid = "76">Each block is then regarded as an independent cluster.</S>
			<S sid ="77" ssid = "77">In each merging stage, the similarities between a cluster and its neighbors are measured by the following formula: i j 2 2 ik jk k k 2 × ¦ ¦ sim(s m , s n ) sm ∈ci sn ∈c j , (1) where si represents the ith sentence and sj represents the jth sentence in text T respectively.</S>
			<S sid ="78" ssid = "78">As mentioned in Section II, the construction of such a sentence-similarity matrix costs Sim(ci , c j ) = (| ci | + | c j |) − STDEV O(n2), where n represents the number of sentences.</S>
			<S sid ="79" ssid = "79">To reduce the cost, TSHAC computes only the similarities alongside the main diagonal according to the block size, instead of the complete sentence-similarity matrix.</S>
			<S sid ="80" ssid = "80">At first, the similarities between each sentence and its neighbors are computed.</S>
			<S sid ="81" ssid = "81">If the similarity between two adjacent sentences is equal to 0, the sentences are divided into different blocks.</S>
			<S sid ="82" ssid = "82">Hence, text T is roughly divided into several blocks, each of which is composed of consecutive sentences.</S>
			<S sid ="83" ssid = "83">Subsequently, the number of sentences in the largest block is regarded as the block size.</S>
			<S sid ="84" ssid = "84">The similarities alongside the diagonal of the sentence-similarity matrix are then computed according to the block size.</S>
			<S sid ="85" ssid = "85">where Sim(ci, cj) represents the similarities of cluster i and cluster j. The first part of (1) represents the lexical cohesion.</S>
			<S sid ="86" ssid = "86">The numerator is the total similarities of sentences from ci with the sentences from cj.</S>
			<S sid ="87" ssid = "87">The denominator is used to normalize the lexical cohesion to the range [0, 1].</S>
			<S sid ="88" ssid = "88">The second part of (1) represents the standard deviation of the cluster size (i.e., number of sentences in a cluster) after ci and cj are merged into a group.</S>
			<S sid ="89" ssid = "89">In general cases, the number of sentences in each cluster is similar.</S>
			<S sid ="90" ssid = "90">Based on this observation, smaller standard deviation of the cluster size is preferred [9].</S>
			<S sid ="91" ssid = "91">The standard deviation of the cluster size is computed by the following equation: For example, assume that the block size is 2, only the elements (i, j) under i = 1, 2, …, 7 and j = i + 1, i + 2, i + 3 (i.e., 2 ͪ block size - 1), where j &lt; 7, are computed.</S>
			<S sid ="92" ssid = "92">Each element (i, j) in the sentence-similarity represents the STDEV = #cluster ¦( clusterj − μ ) j =1 , # cluster similarity between sentences i and j (i.e., Cosine(si, sj)).Because all the elements in the diagonal of the sentence similarity matrix are equal to 1 and the sentence-similarity matrix is symmetric, only the upper triangle of the sentence- similarity matrix needs to be computed.</S>
			<S sid ="93" ssid = "93">An example of the sentence-similarity matrix is shown as Fig.</S>
			<S sid ="94" ssid = "94">1.</S>
			<S sid ="95" ssid = "95">The construction of the sentence-similarity matrix is similar to TSF [7].</S>
			<S sid ="96" ssid = "96">However, in the proposed algorithm, the block size is dynamically determined according to the metadata of a text, instead of statically provided by the user.</S>
			<S sid ="97" ssid = "97">where μ represents the average number of sentences within clusters.</S>
			<S sid ="98" ssid = "98">Because the sentences within a text are located in order, only adjacent clusters need to be considered in each merging stage (i.e., only Sim(ci1, ci) and Sim(ci, ci+1) need to be computed).</S>
			<S sid ="99" ssid = "99">The nearest pair of adjacent clusters is then merged into a new cluster.</S>
			<S sid ="100" ssid = "100">The merging process is repeated until all clusters are merged into one universal cluster.</S>
			<S sid ="101" ssid = "101">A hierarchical cluster tree is then constructed as shown in Fig.</S>
			<S sid ="102" ssid = "102">2.</S>
			<S sid ="103" ssid = "103">1 2 3 4 5 6 7 1 2 3 4 5 6 7 0.1 0.2 0.4 0.6 0.7 0.8 c1 c2 c3 c4 c5 c6 c7 Figure 2.</S>
			<S sid ="104" ssid = "104">An example of a hierarchical cluster tree.</S>
			<S sid ="105" ssid = "105">Figure 1.</S>
			<S sid ="106" ssid = "106">Upper triangle of the sentence-similarity matrix.</S>
			<S sid ="107" ssid = "107">In order to divide the text into several topical segments, after the hierarchical cluster tree is constructed, the input data objects can be divided into several clusters, each of which represents a topic, by cutting the hierarchical cluster tree at a feasible height.</S>
			<S sid ="108" ssid = "108">A fitness function is proposed to evaluate each of the potential height hi of the hierarchical cluster tree constructed as follow: #seg −1 ¦ dissimilarity j contain less than or equal to 2 sentences is also considered.</S>
			<S sid ="109" ssid = "109">A lower proportion of short segments is preferred in our fitness function.</S>
			<S sid ="110" ssid = "110">E. The Proposed TSHAC Algorithm The complete process of TSHAC is summarized as Fig.</S>
			<S sid ="111" ssid = "111">3.</S>
			<S sid ="112" ssid = "112">IV.</S>
			<S sid ="113" ssid = "113">PERFORMANCE EVALUATIONS To evaluate the performance of TSHAC, a publicly available test corpus is adopted.</S>
			<S sid ="114" ssid = "114">The test corpus was created f (hi ) = j =1 # seg − STDEV (i) − # seg len≤ 2 (2) # s e g by Choi [3] and has been commonly used in previous researches.</S>
			<S sid ="115" ssid = "115">The test corpus consists of 700 samples.</S>
			<S sid ="116" ssid = "116">A sample is a concatenation of ten text segments.</S>
			<S sid ="117" ssid = "117">A segment is As mention in previous researches [9], when dividing a long text into several topical segments, the sentences within a topical segment should cover the same subtopic.</S>
			<S sid ="118" ssid = "118">Moreover, sentences among different segments should belong to different subtopics.</S>
			<S sid ="119" ssid = "119">Therefore, both the average sentence similarity within each segment (inner similarity) and the average sentence similarity between two consecutive segments (outer similarity) are considered in the fitness function.</S>
			<S sid ="120" ssid = "120">The inner similarity and the outer similarity are then combined as dissimilarity [7] as follows: the first n sentences of a randomly selected document from the Brown corpus.</S>
			<S sid ="121" ssid = "121">The 700 samples are divided into 4 sets according to the range of the number of sentences.</S>
			<S sid ="122" ssid = "122">Table I shows the statistics of the test corpus, n represents the number of sentences.</S>
			<S sid ="123" ssid = "123">To compare with several well-known text segmentation algorithms, including TextTiling [5], C99 [3], U00 [10], TopSeg02 [11], AniDiffDynProg03 [6], and TSF [7], the commonly used segmentation metric proposed by Beeferman [12] is adopted: sim inner − sim outer p(error|ref, hyp, k ) = dissimilarity j = j j sim inner , where p(miss|ref, hyp, diffe rent ref s egments, k ) p(diff ref s egments|re f, k ) + p( false alar m|ref, hyp , same ref segment, k ) p(same ref s egment|ref , k ) μ ( B pre , B pre ) + μ ( B post , B post ) The aim of the segmentation metric is to compute the error probability of a randomly chosen pair of words at distance k sim inner = j j j 2 j and words apart that is inconsistently classified.</S>
			<S sid ="124" ssid = "124">ref represents the tru e se g men tation and hy p repr e se n t s t h e prop o sed sim j = μ B j B j , The proposed TSHAC algorithm Input: A long text.</S>
			<S sid ="125" ssid = "125">outer ( pre , post ) Output: Several topical segments.</S>
			<S sid ="126" ssid = "126">Step 1.</S>
			<S sid ="127" ssid = "127">Preprocess a text by tokenization, generic stopword removal, where μ represents average pairwise sentence similarities of document-dependent stopword removal, and stemming.</S>
			<S sid ="128" ssid = "128">two blocks; pre i represents the block of sentences that Step 2.</S>
			<S sid ="129" ssid = "129">Deter mine block size by the method mentioned in Section III-C.</S>
			<S sid ="130" ssid = "130">precede the potential boundary; B post represents the block Step 3.</S>
			<S sid ="131" ssid = "131">Compute the similarities of sentences alongside the diagonal of sentences that succeed the potential boundary.</S>
			<S sid ="132" ssid = "132">The higher dissimilarityj implies higher feasibility of the potential boundary.</S>
			<S sid ="133" ssid = "133">On the other hand, the standard deviation of the segment length (as shown in (3)) is also considered in the fitness function.</S>
			<S sid ="134" ssid = "134">Because the number of sentences in different segment is usually similar [9], a lower standard deviation value of segment size is preferred when we design the fitness function.</S>
			<S sid ="135" ssid = "135"># seg ¦( seg j − μ) of the sentence-similarity matrix with the block size.</S>
			<S sid ="136" ssid = "136">Step 4.</S>
			<S sid ="137" ssid = "137">Segment the text by the following steps: Step 4.1.</S>
			<S sid ="138" ssid = "138">The text is roughly divided into several groups by the method mentioned in Section III-C and each group is regarded as an independent cluster.</S>
			<S sid ="139" ssid = "139">Step 4.2.</S>
			<S sid ="140" ssid = "140">Compute the similarities between adjacent clusters by (1).</S>
			<S sid ="141" ssid = "141">Step 4.3.</S>
			<S sid ="142" ssid = "142">Merge the pair of clusters to achieve the greatest similarity.</S>
			<S sid ="143" ssid = "143">Step 4.4.</S>
			<S sid ="144" ssid = "144">Repeat Step 4.2 and Step 4.3 until only one universal cluster left.</S>
			<S sid ="145" ssid = "145">Step 4.5.</S>
			<S sid ="146" ssid = "146">Cut the hierarchical cluster tree by the predesigned fitness function (shown as (2)) to achieve the highest fitness value.</S>
			<S sid ="147" ssid = "147">STDEV(i) = j =1 # seg (3) Figure 3.</S>
			<S sid ="148" ssid = "148">The porposed TSHAC algorithm.</S>
			<S sid ="149" ssid = "149">Finally, a topic segment usually contains several sentences that could well describe a topic.</S>
			<S sid ="150" ssid = "150">To avoid topic segments to be too short, the number of segments which TABLE I. STATISTICS OF THE TEST CORPUS segmentation.</S>
			<S sid ="151" ssid = "151">The error probability is composed of the miss and the false alarm probabilities.</S>
			<S sid ="152" ssid = "152">The miss probability is a conditional probability that the randomly chosen pair of words spans a segment boundary for the true segmentation, but lies in the same segment for the proposed segmentation.</S>
			<S sid ="153" ssid = "153">The false alarm probability is a conditional probability that the randomly chosen pair of words lies in the same segment for the true segmentation, but spans a segment boundary for the proposed segmentation.</S>
			<S sid ="154" ssid = "154">The lower error probability implies higher segmentation accuracy.</S>
			<S sid ="155" ssid = "155">A. Computational Complexity Analysis As mentioned above, the construction of the complete similarity matrix cost O(n2), where n is the number of sentences in a text.</S>
			<S sid ="156" ssid = "156">To reduce the time complexity, in the proposed algorithm, only the similarities alongside the diagonal are computed with the block size.</S>
			<S sid ="157" ssid = "157">Therefore, the time complexity is O(nk), where n represents the number of sentences and k represents the block size.</S>
			<S sid ="158" ssid = "158">In this article, the proposed linear text segmentation algorithm is based on HAC.</S>
			<S sid ="159" ssid = "159">Unlike the general process of HAC, only the similarities between each cluster and its neighbors are computed at each merging stage.</S>
			<S sid ="160" ssid = "160">Therefore, in contrast to the general HAC, which takes O(n2), the proposed TSHAC algorithm compute only the similarities between the newly merged cluster and its two neighbors in each merging stage, and hence it takes only O(n).</S>
			<S sid ="161" ssid = "161">Moreover, the roughly grouping of sentences into blocks will further improve the efficiency of the proposed linear text segmentation algorithm.</S>
			<S sid ="162" ssid = "162">The actual complexity of the HAC process will be less than or equal to O(n).</S>
			<S sid ="163" ssid = "163">According to the above analysis, TSHAC is a linear time algorithm of linear text segmentation.</S>
			<S sid ="164" ssid = "164">B. Experimental Results Table II shows the error probability of TSHAC and several well-known text segmentation algorithms.</S>
			<S sid ="165" ssid = "165">Among these algorithms, TextTiling, TSF and TSHAC are linear time algorithms, and the others are non-linear time algorithms [7].</S>
			<S sid ="166" ssid = "166">The error probability of the well-known linear text segmentation algorithms are collected from the literatures [3, 7].</S>
			<S sid ="167" ssid = "167">The experimental results in Table II are reported when the number of segments is unknown in advance.</S>
			<S sid ="168" ssid = "168">From Table II, the proposed linear text segmentation algorithm (i.e., TSHAC) outperforms the linear time algorithm, TextTiling, and the more complex algorithms, C99.</S>
			<S sid ="169" ssid = "169">TSHAC also provides comparable results with other algorithms.</S>
			<S sid ="170" ssid = "170">Moreover, unlike TSF, which requires manpower to designate parameters used, TSHAC provides a fully automatic process for linear text segmentation without auxiliary knowledge base, parameter setting, or user involvement.</S>
			<S sid ="171" ssid = "171">V. CONCLUSION AND FUTURE WORKS The contributions of this article include: 1) Propose a high performance linear text segmentation algorithm based on HAC; 2) The proposed linear text segmentation algorithm TABLE II.</S>
			<S sid ="172" ssid = "172">EXPERIMENTAL RESULTS AND COMPARISON 3 1 1 3 5 6 8 9 1 1 T e x t T i l i n g 4 6 % 4 4 % 4 3 % 4 8 % C 9 9 1 3 % 1 8 % 1 0 % 1 0 % U 0 0 1 1 % 1 3 % 6 % 6 % T o p S e g 10.</S>
			<S sid ="173" ssid = "173">74 % 7.</S>
			<S sid ="174" ssid = "174">44 % 7.</S>
			<S sid ="175" ssid = "175">95 % 6.</S>
			<S sid ="176" ssid = "176">75 % An iDi ffD yn Pro g03 6.</S>
			<S sid ="177" ssid = "177">0 % 7.</S>
			<S sid ="178" ssid = "178">1 % 5.</S>
			<S sid ="179" ssid = "179">3 % 4.</S>
			<S sid ="180" ssid = "180">3 % T S F 9.</S>
			<S sid ="181" ssid = "181">0 % 9.</S>
			<S sid ="182" ssid = "182">3 % 6.</S>
			<S sid ="183" ssid = "183">8 % 9.</S>
			<S sid ="184" ssid = "184">2 % T S H A C 9.</S>
			<S sid ="185" ssid = "185">84 % 9.</S>
			<S sid ="186" ssid = "186">26 % 8.</S>
			<S sid ="187" ssid = "187">23 % 7.</S>
			<S sid ="188" ssid = "188">54 % achieves comparable segmentation accuracy with efficient processing time; 3) The parameters used in the process of linear text segmentation are automatically designated without auxiliary knowledge base, parameter setting, or user involvement.</S>
			<S sid ="189" ssid = "189">Therefore, the proposed linear text segmentation algorithm is feasible for segmenting long texts in real time, especially when user involvement is unavailable.</S>
			<S sid ="190" ssid = "190">Furthermore, the proposed algorithm not only provides topical segments, but also provides hierarchical discourse structure of the text, if necessary.</S>
			<S sid ="191" ssid = "191">In the future, we plan to implement the proposed linear text segmentation algorithm in some real world applications.</S>
	</SECTION>
</PAPER>
