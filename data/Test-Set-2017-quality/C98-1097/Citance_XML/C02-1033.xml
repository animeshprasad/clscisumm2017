<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present in this paper a method for achieving in an integrated way two tasks of topic analysis: segmentation and link detection.</S>
		<S sid ="2" ssid = "2">This method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches.</S>
		<S sid ="3" ssid = "3">We report an evaluation of our method for segmentation on two corpora, one in French and one in English, and we propose an evaluation measure that specifically suits that kind of systems.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Topic analysis, which aims at identifying the topics of a text, delimiting their extend and finding the relations between the resulting segments, has recently raised an important interest.</S>
			<S sid ="5" ssid = "5">The largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the TDT (Topic Detection and Tracking) initiative (Fiscus et al., 1999), which addresses all the tasks we have mentioned but from a domain-dependent viewpoint and not necessarily in an integrated way.</S>
			<S sid ="6" ssid = "6">Systems that implement this work can be categorized according to what kind of knowledge they use.</S>
			<S sid ="7" ssid = "7">Most of those that achieve text segmentation only rely on the intrinsic characteristics of texts: word distribution, as in (Hearst, 1997), (Choi, 2000) and (Utiyama and Isahara, 2001), or linguistic cues as in (Passonneau and Litman, 1997).</S>
			<S sid ="8" ssid = "8">They can be applied without restriction about domains but have low results when a text doesn’t characterize its topical structure by surface clues.</S>
			<S sid ="9" ssid = "9">Some systems exploit domain-independent knowl edge about lexical cohesion: a network of words built from a dictionary in (Kozima, 1993); a large set of collocations collected from a corpus in (Fer ret, 1998), (Kaufmann, 1999) and (Choi, 2001).</S>
			<S sid ="10" ssid = "10">To some extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains.</S>
			<S sid ="11" ssid = "11">The last main type of systems relies on knowledge about the topics they may encounter in the texts they process.</S>
			<S sid ="12" ssid = "12">This is typically the kind of approach developed in TDT where this knowledge is automatically built from a set of reference texts.</S>
			<S sid ="13" ssid = "13">The work of Bigi (Bigi et al., 1998) stands in the same perspective but focuses on much larger topics than TDT.</S>
			<S sid ="14" ssid = "14">These systems have a limited scope due to their topic representations but they are also more precise for the same reason.</S>
			<S sid ="15" ssid = "15">Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.</S>
			<S sid ="16" ssid = "16">The topic analysis we propose implements such a hybrid approach: it relies on a general language resource, a collocation network, but exploits it together with word recurrence in texts.</S>
			<S sid ="17" ssid = "17">Moreover, it simultaneously achieves topic segmentation and link detection, i.e. determining whether two segments discuss the same topic.</S>
			<S sid ="18" ssid = "18">We detail in this paper the implementation of this analysis by the TOPICOLL system, we reportevaluations of its capabilities concerning segmen tation for two languages, French and English, and finally, we propose an evaluation measure that integrates both segmentation and link detection.</S>
	</SECTION>
	<SECTION title="Overview of TOPICOLL. " number = "2">
			<S sid ="19" ssid = "1">In accordance with much work about discourse analysis, TOPICOLL processes texts linearly: it detects topic shifts and finds links between segments without delaying its decision, i.e., by only taking into account the part of text that has been already analyzed.</S>
			<S sid ="20" ssid = "2">A window that delimits the current focus of the analysis is moved over each text to be processed.</S>
			<S sid ="21" ssid = "3">This window contains the lemmatized content words of the text, resulting from its pre-processing.</S>
			<S sid ="22" ssid = "4">A topic context is associated to this focus window.</S>
			<S sid ="23" ssid = "5">It is made up of both the words of the window and the words that are selected from a collocation network1 as strongly linked to the words of the window.</S>
			<S sid ="24" ssid = "6">The current segment is also given a topic context.</S>
			<S sid ="25" ssid = "7">This context results from the fusion of the contexts associated to the focus window when this window was in the segment space.</S>
			<S sid ="26" ssid = "8">A topic shift is then detected when the context of the focus window and the context of the current segment are not similar any more for several successive positions of the focus window.</S>
			<S sid ="27" ssid = "9">This process also performs link detection by comparing the topic context of each new segment to the context of the already delimited segments.</S>
			<S sid ="28" ssid = "10">The use of a collocation network permits TOPICOLL to find relations beyond word recurrence and to associate a richer topical representa tion to segments, which facilitates tasks such as link detection or topic identification.</S>
			<S sid ="29" ssid = "11">But work such as (Kozima, 1993), (Ferret, 1998) or (Kaufmann, 1999) showed that using a domain- independent source of knowledge for text segmentation doesn’t necessarily lead to get better results than work that is only based on word distribution in texts.</S>
			<S sid ="30" ssid = "12">One of the reasons of this fact is that these methods don’t precisely control the relations they select or don’t take into account the sparseness of their knowledge.</S>
			<S sid ="31" ssid = "13">Hence, while they discard some incorrect topic shifts found by methods based on word recurrence, they also find incorrect shifts when the relevant relations are not present in their knowledge or don’t find some correct shifts because of the selection of non relevant relations from a topical viewpoint.</S>
			<S sid ="32" ssid = "14">By combining word recurrence and relations selected from a collocation network, TOPICOLL aims at exploiting a domain-independent source of knowledge for text segmentation in a more accurate way.</S>
	</SECTION>
	<SECTION title="Collocation networks. " number = "3">
			<S sid ="33" ssid = "1">TOPICOLL depends on a resource, a collocation network, that is language-dependent.</S>
			<S sid ="34" ssid = "2">Two collocation networks were built for it: one for French, 1 A collocation network is a set of collocations between words.</S>
			<S sid ="35" ssid = "3">This set can be viewed as a network whose nodes are words and edges are collocations.</S>
			<S sid ="36" ssid = "4">from the Le Monde newspaper (24 months between 1990 and 1994), and one for English, from the L.A. Times newspaper (2 years, part of the TREC corpus).</S>
			<S sid ="37" ssid = "5">The size of each corpus was around 40 million words.</S>
			<S sid ="38" ssid = "6">The building process was the same for the two networks.</S>
			<S sid ="39" ssid = "7">First, the initial corpus was pre- processed in order to characterize texts by their topically significant words.</S>
			<S sid ="40" ssid = "8">Thus, we retained only the lemmatized form of plain words, that is, nouns, verbs and adjectives.</S>
			<S sid ="41" ssid = "9">Collocations were extracted according to the method described in (Church and Hanks, 1990) by moving a window on texts.</S>
			<S sid ="42" ssid = "10">Parameters were chosen in order to catch topical relations: the window was rather large, 20-word wide, and took into account the boundaries of texts; moreover, collocations were indifferent to word order.</S>
			<S sid ="43" ssid = "11">We also adopted an evaluation of mutual information as a cohesion measure of each collocation.</S>
			<S sid ="44" ssid = "12">This measure was normalized according to the maximal mutual information relative to the considered corpus.</S>
			<S sid ="45" ssid = "13">After filtering the less significant collocations (collocations with less than 10 occurrences and cohesion lower than 0.1), we got a network with approximately 23,000 words and 5.2 million collocations for French, 30,000 words and 4.8 million collocations for English.</S>
	</SECTION>
	<SECTION title="Description of TOPICOLL. " number = "4">
			<S sid ="46" ssid = "1">TOPICOLL is based on the creation, the update and the use of a topical representation of both the segments it delimits and the content of its focus window at each position of a text.</S>
			<S sid ="47" ssid = "2">These topical representations are called topic contexts.</S>
			<S sid ="48" ssid = "3">Topic shifts are found by detecting that the topic context of the focus window is not similar anymore to the topic context of the current segment.</S>
			<S sid ="49" ssid = "4">Link detection is performed by comparing the context of a new segment to the context of the previous segments.</S>
			<S sid ="50" ssid = "5">4.1 Topic contexts.</S>
			<S sid ="51" ssid = "6">A topic context characterizes the topical dimension of the entity it is associated to by two vectors of weighted words.</S>
			<S sid ="52" ssid = "7">One of these vectors, called text vector, is made up of words coming from the text that is analyzed.</S>
			<S sid ="53" ssid = "8">The other one, called collocation vector, contains words selected from a collocation network and strongly linked to the words of the processed text.</S>
			<S sid ="54" ssid = "9">For both vectors, the weight of a word expresses its importance with regard to the other words of the vector.</S>
			<S sid ="55" ssid = "10">4.1.1 Topic context of the focus window The text vector of the context associated to the focus window is made up of the content words of the window.</S>
			<S sid ="56" ssid = "11">Their weight is given by: lected if it is linked to at least wst (3 in our experiments) words of the window.</S>
			<S sid ="57" ssid = "12">A collocation vector may also contain some words of the window as they are generally part of the collocation network and may be selected as its other words.</S>
			<S sid ="58" ssid = "13">Each selected word from the network is then assigned a weight.</S>
			<S sid ="59" ssid = "14">This weight is equal to the sum of the contributions of the window words to which it wght txt (w) occNb(w) signif (w) (1) is linked to.</S>
			<S sid ="60" ssid = "15">The contribution of a word of the window to the weight of a selected word is equal where occNb(w) is the number of occurrences of the word w in the window and signif(w) is the significance of w. The weight given by (1) combines the importance of w in the part of text delimited by the window and its general significance.</S>
			<S sid ="61" ssid = "16">This significance is defined as in (Kozima, 1993) as its normalized information in a reference corpus2: to its weight in the window, given by (1), modulated by the cohesion measure between these two words in the network (see Figure 1).</S>
			<S sid ="62" ssid = "17">More precisely, the combination of these two factors is achieved by a geometric mean: signif (w) log 2 ( f w S c ) (2) wghtcoll (w) wgthtxt (wi ) i coh(w, wi ) (3) log 2 (1 S c ) where fw is the number of occurrences of the word w in the corpus and Sc, the size of the corpus.</S>
			<S sid ="63" ssid = "18">0.48 = pw3x0.18+p w4x0.13 where coh(w,wi) is the measure of the cohesion between w and wi in the collocation network.</S>
			<S sid ="64" ssid = "19">4.1.2 Topic context of a segment The topic context of a segment results from the fusion of the contexts associated to the focus win n1 n2 0.21 0.10 +pw5x0.17 0.17 dow when it was inside the segment.</S>
			<S sid ="65" ssid = "20">The fusion is achieved as the segment is extended: the context associated to each new position of the segment is 0.14 0.18 0.13 combined with the current context of the segment.</S>
			<S sid ="66" ssid = "21">This combination, which is done separately for 1.0 1.0 1.0 1.0 1.0 w1 0.48 w2 w3 w4 w5 text vectors and collocation vectors, consists in merging two lists of weighted words.</S>
			<S sid ="67" ssid = "22">First, the 1.0 selected word from the collocati on network (with its weight) word from text (with p wi, its weight in the window, equal to 1.0 for all words of the window in this example ) wor ds of the win dow cont ext that are not in the seg men t cont ext are adde d to it.</S>
			<S sid ="68" ssid = "23">The n, the weig ht of each wor d of the resul ting list is com pute d ac 0.21 link in the collocation network (with its cohesion value) Figure 1 – Selection and weighting of words from cording to its weight in the window context and its previous weight in the segment context: the collocation network wght x (w, Cs, t ) wght x (w, Cs, t 1) (4) The building of the collocation vector for the window’s context comes from the procedure presented (signif (w) wght x (w, Cw, t ))in (Ferret, 1998) for evaluating the lexical cohe sion of a text.</S>
			<S sid ="69" ssid = "24">It consists in selecting words of the collocation network that are topically close to those in the window.</S>
			<S sid ="70" ssid = "25">We assume that this closeness is related to the number of links that exist between a word of the network and the words of the window.</S>
			<S sid ="71" ssid = "26">Thus, a word of the network is se 2 In our case, this is the corpus used for building the.</S>
			<S sid ="72" ssid = "27">collocation network.with Cw, the context of the window, Cs, the con text of the segment and wghtx(w,C{s,w},t), the weight of the word w in the vector x (txt or coll) of the context C{s,w} for the position t. For the words from the window context that are not part of the segment context, wghtx(w,Cs,t-1) is equal to 0.The revaluation of the weight of a word in a seg ment context given by (4) is a solution halfway between a fast and a slow evolution of the content of segment contexts.</S>
			<S sid ="73" ssid = "28">The context of a segment has to be stable because if it follows too narrowly the topical evolution of the window context, topic shifts could not be detected.</S>
			<S sid ="74" ssid = "29">However, it must also adapt itself to small variations in the way a topic is expressed when progressing in the text in order not to detect false topic shifts.</S>
			<S sid ="75" ssid = "30">4.1.3 Similarity between contexts In order to determine if the content of the focus window is topically coherent or not with the current segment, the topic context of the window is compared to the topic context of the segment.</S>
			<S sid ="76" ssid = "31">This comparison is performed in two stages: first, a similarity measure is computed between the vectors of the two contexts; then, the resulting values are exploited by a decision procedure that states if the two contexts are similar.</S>
			<S sid ="77" ssid = "32">As (Choi, 2000) or (Kaufmann, 1999), we use the cosine measure for evaluating the similarity be tween a vector of the context window (Vw) and the equivalent vector in the segment context (Vs): sim(Vs x ,Vwx ) tive vote in favor of the similarity of the two contexts is decided if the value exceeds this threshold.</S>
			<S sid ="78" ssid = "33">Then, the global similarity of the two contexts is rejected only if the votes for the two vectors are negative.</S>
			<S sid ="79" ssid = "34">4.2 Topic segmentation.</S>
			<S sid ="80" ssid = "35">The algorithm for detecting topic shifts is taken from (Ferret and Grau, 2000) and basically relies on the following principle: at each text position, if the similarity between the topic context of the focus window and the topic context of the current segment is rejected (see 4.1.3), a topic shift is assumed and a new segment is opened.</S>
			<S sid ="81" ssid = "36">Otherwise, the active segment is extended up to the current position.</S>
			<S sid ="82" ssid = "37">This algorithm assumes that the transition between two segments is punctual.</S>
			<S sid ="83" ssid = "38">As TOPICOLL only oper ates at word level, its precision is limited.</S>
			<S sid ="84" ssid = "39">This imprecision makes necessary to set a short delay before deciding that the active segment really ends and similarly, before deciding that a new segment with a stable topic begins.</S>
			<S sid ="85" ssid = "40">Hence, the algorithm wg x (wi , Cs) i 2 wg x (wi , Cw) 2 (5) for detecting topic shifts distinguishes four states: – the NewTopicDetection state takes place when a wg x (wi , Cs) i wg x (wi , Cw) i new segment is going to be opened.</S>
			<S sid ="86" ssid = "41">This opening is then confirmed provided that the content of the where wgx(wi,C{s,w}) is the weight of the word wi in the vector x (txt or coll) of the context C{s,w}.</S>
			<S sid ="87" ssid = "42">As we assume that the most significant words of a segment context are the most recurrent ones, the similarity measure takes into account only the words of a segment context whose the recurrence3 is above a fixed threshold.</S>
			<S sid ="88" ssid = "43">This one is higher for text vectors than for collocation vectors.</S>
			<S sid ="89" ssid = "44">This filtering is applied only when the context of a segment is considered as stable (see 4.2).</S>
			<S sid ="90" ssid = "45">The decision stage takes root in work about combining results of several systems that achieve thesame task.</S>
			<S sid ="91" ssid = "46">In our case, the evaluation of the simi larity between Cs and Cw at each position is based on a vote that synthesizes the viewpoint of the text vector and the viewpoint of the collocation vector.</S>
			<S sid ="92" ssid = "47">First, the value of the similarity measure for each vector is compared to a fixed threshold and a posi 3 The recurrence of a word in a segment context is gi-.</S>
			<S sid ="93" ssid = "48">ven by the ratio between the number of window contexts in which the word was present and the number of window contexts gathered by the segment context.</S>
			<S sid ="94" ssid = "49">focus window context doesn’t change for several positions.</S>
			<S sid ="95" ssid = "50">Moreover, the core of the segment con text is defined when TOPICOLL is in this state; – the InTopic state is active when the focus window is inside a segment with a stable topic; – the EndTopicDetection state occurs when the focus window is inside a segment but a difference between the context of the window and the context of the current segment suggests that this segment could end soon.</S>
			<S sid ="96" ssid = "51">As for the NewTopicDetection state, this difference has to be confirmed for several positions before a change of state is decided; – the OutOfTopic state is active between two segments.</S>
			<S sid ="97" ssid = "52">Generally, TOPICOLL stays in this state no longer than 1 or 2 positions but when neither the words from text nor the words selected from the collocation network are recurrent, i.e. no stable topic can be detected according to these features, this number of positions may be equal to the size of a segment.</S>
			<S sid ="98" ssid = "53">The transition from one state to another follows the automaton of Figure 2 according to three parameters: – its current state; – the similarity between the context of the focus window and the context of the current segment: Sim or no Sim; – the number of successive positions of the focus window for which the current state doesn’t change: confirmNb.</S>
			<S sid ="99" ssid = "54">It must exceed the Tconfirm threshold (equal to 3 in our experiments) for leaving the NewTopicDetection or the EndTopicDe tection state.</S>
			<S sid ="100" ssid = "55">4.3 Link detection.</S>
			<S sid ="101" ssid = "56">The algorithm of TOPICOLL for detecting identity links between segments is closely associated to its algorithm for delimiting segments.</S>
			<S sid ="102" ssid = "57">When TO- PICOLL goes from the NewTopicDetection state to the InTopic state, it first checks whether the current context of the new segment is similar to one of the contexts of the previous segments.</S>
			<S sid ="103" ssid = "58">In this case, the similarity between contexts only relies on the similarity measure (see (5)) between their collocation vectors.</S>
			<S sid ="104" ssid = "59">A specific threshold is used for Sim &amp; confirmNb = Tconfirm InTopic Sim Sim no Sim no Sim &amp; confirmNb &lt; Tconfirm the decision.</S>
			<S sid ="105" ssid = "60">If the similarity value exceeds this threshold, the new segment is linked to the corresponding segment and takes the context of this one as its own context.</S>
			<S sid ="106" ssid = "61">In this way, TOPICOLL assumes NewTopic Detection Sim &amp; confirmNb &lt; Tconfirm Sim no Sim OutOfTopic no Sim EndTopic Detection no Sim &amp; confirmNb = Tconfirm that the new segment continues to develop a previous topic.</S>
			<S sid ="107" ssid = "62">When several segments fulfills the condition for link detection, TOPICOLL selects the one with the highest similarity value.</S>
			<S sid ="108" ssid = "63">5 Experime nts Figure 2 – Automaton for topic shift detection The processing of a segment starts with the OutOfTopic state, after the end of the previous segment or at the beginning of the text.</S>
			<S sid ="109" ssid = "64">As soon as the context of the focus window is stable enough between two successive positions, TOPICOLL enters into the NewTopicDetection state.</S>
			<S sid ="110" ssid = "65">The InTopic state can then be reached only if the window context is found stable for the next confirmNb1 positions.</S>
			<S sid ="111" ssid = "66">Otherwise, TOPICOLL assumes that it is a false alarm and returns to the OutOfTopic state.</S>
			<S sid ="112" ssid = "67">The detection of the end of a segment is symmetrical to the detection of its beginning.</S>
			<S sid ="113" ssid = "68">TOPICOLL goes into the EndTopicDetection state as soon as the content of the window context begins to change significantly between two successive positions but the transition towards the OutOfTopic state is done only if this change is confirmed for the next confirmNb1 next positions.</S>
			<S sid ="114" ssid = "69">This algorithm is completed by a specific mechanism related to the OutOfTopic state.</S>
			<S sid ="115" ssid = "70">When TOPICOLL stays in this state for a too long time (this time is defined as 10 positions of the focus window in our experiments), it assumes that the topic of the current part of text is difficult to characterize by using word recurrence or selection from a collocation network and it creates a new segment that covers all the concerned positions.</S>
			<SUBSECTION>5.1 Topic segmentation.</SUBSECTION>
			<S sid ="116" ssid = "71">For evaluating TOPICOLL about segmentation, we applied it to the “classical” task of discovering boundaries between concatenated texts.</S>
			<S sid ="117" ssid = "72">TOPICOLL was adapted for aligning boundaries with ends of sentences.</S>
			<S sid ="118" ssid = "73">We used the probabilistic error metric Pk proposed in (Beeferman et al., 1999) for measuring segmentation accuracy4.</S>
			<S sid ="119" ssid = "74">Recall and precision was computed for the Le Monde corpus to compare TOPICOLL with older systems5.</S>
			<S sid ="120" ssid = "75">In this case, the match between a boundary from TOPICOLL and a document break was accepted if the boundary was not farther than 9 plain words.</S>
			<SUBSECTION>5.1.1 Le Monde corpus The evaluation corpus for French was made up of 49 texts, 133 words long on average, from the Le 4 Pk evaluates the probability that a randomly chosen.</SUBSECTION>
			<S sid ="121" ssid = "76">pair of words, separated by k words, is wrongly classified, i.e. they are found in the same segment by TOPICOLL while they are actually in different ones (miss of a document break) or they are found in different segments by TOPICOLL while they are actually in the same one (false alarm).</S>
			<S sid ="122" ssid = "77">D the number of document breaks, Nb the number of boundaries found by TOPICOLL and Nt the number of boundaries that are document breaks.</S>
			<S sid ="123" ssid = "78">Monde newspaper.</S>
			<S sid ="124" ssid = "79">Results in Tables 1 and 2 are average values computed from 10 different sequences of them.</S>
			<S sid ="125" ssid = "80">The baseline procedure consisted in randomly choosing a fixed number of sentence ends as boundaries.</S>
			<S sid ="126" ssid = "81">Its results in Tables 1 and 2 are average values from 1,000 draws.</S>
			<S sid ="127" ssid = "82">Systems Recall Precision F1-measure baseline 0.51 0.28 0.36 SEGCOHLEX SEGAPSITH TextTiling 0.68 0.92 0.72 0.37 0.52 0.81 0.48 0.67 0.76 TOPICOLL1 TOPICOLL2 TOPICOLL3 0.86 0.86 0.66 0.74 0.78 0.60 0.80 0.81 0.63 Table 1 – Precision/recall for Le Monde corpus TOPICOLL1 is the system described in section 4.</S>
			<S sid ="128" ssid = "83">TOPICOLL2 is the same system but without its link detection part.</S>
			<S sid ="129" ssid = "84">The results of these two variants show that the search for links between segments doesn’t significantly debase TOPICOLL’s capabilities for segmentation.</S>
			<S sid ="130" ssid = "85">TOPICOLL3 is a version of TOPICOLL that only relies on word recurrence.</S>
			<S sid ="131" ssid = "86">SEGCOHLEX and SEGAPSITH are the systems described in (Ferret, 1998) and (Ferret and Grau, 2000).</S>
			<S sid ="132" ssid = "87">TextTiling is our implementation of Hearst’s algorithm with its standard parameters.</S>
			<S sid ="133" ssid = "88">Systems Miss False alarm Error baseline 0.46 0.55 0.50 TOPICOLL1 TOPICOLL2 0.17 0.17 0.24 0.22 0.21 0.20 Table 2 – Pk for Le Monde corpus First, Table 1 shows that TOPICOLL is more accurate when its uses both word recurrence and collocations.</S>
			<S sid ="134" ssid = "89">Furthermore, it shows that TOPICOLL gets better results than a system that only relies on a collocation network such as SEGCOHLEX.</S>
			<S sid ="135" ssid = "90">It also gets better results than a system such as TextTiling that is based on word recurrence and as TOPICOLL, works with a local context.</S>
			<S sid ="136" ssid = "91">Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.</S>
			<S sid ="137" ssid = "92">Moreover, TOPICOLL is more accurate than a system such as SEGAPSITH that depends on topic rep resentations.</S>
			<S sid ="138" ssid = "93">Its accuracy is also slightly higher than the one reported in (Bigi et al., 1998) for a system that uses topic representations in a probabilistic way: 0.75 as precision, 0.80 as recall and 0.77 as f1-measure got on a corpus made of Le Monde’s articles too.</S>
			<SUBSECTION>5.1.2 C99 corpus For English, we used the artificial corpus built by Choi (Choi, 2000) for comparing several segmentation systems.</SUBSECTION>
			<S sid ="139" ssid = "94">This corpus is made up of 700 samples defined as follows: “A sample is a concatenation of ten text segments.</S>
			<S sid ="140" ssid = "95">A segment is the first n sentences of a randomly selected document for the Brown corpus”.</S>
			<S sid ="141" ssid = "96">Each column of Table 3 states for an interval of values for n. Systems 311 35 68 911 baseline 0.45 0.38 0.39 0.36 CWM U00 C99 DotPlot Segmenter TextTiling 0.09 0.10 0.12 0.18 0.36 0.46 0.10 0.09 0.11 0.20 0.23 0.44 0.07 0.07 0.09 0.15 0.33 0.43 0.05 0.05 0.09 0.12 0.43 0.48 TOPICOLL1 TOPICOLL2 0.30 0.31 0.28 0.28 0.27 0.28 0.34 0.34 Table 3 – Pk for C99 corpus The first seven lines of Table 3 results from Choi’s experiments (Choi, 2001).</S>
			<S sid ="142" ssid = "97">The baseline is a procedure that partitions a document into 10 segments of equal length.</S>
			<S sid ="143" ssid = "98">CWM is described in (Choi, 2001), U00 in (Utiyama and Isahara, 2001), C99 in (Choi, 2000), DotPlot in (Reynar, 1998) and Segmenter in (Kan et al., 1998).</S>
			<S sid ="144" ssid = "99">Table 3 confirms first that the link detection part of TOPICOLL doesn’t debase its segmentation capabilities.</S>
			<S sid ="145" ssid = "100">It also shows that TOPICOLL’s results on this corpus are significantly lower than its results on the Le Monde corpus.</S>
			<S sid ="146" ssid = "101">This is partially due to our collocation network for English: its density, i.e. the ratio between the size of its vocabulary and its number of collocations, is 30% lower than the density of the network for French, which has certainly a significant effect.</S>
			<S sid ="147" ssid = "102">Table 3 also shows that TOPICOLL has worse results than systems such as CWM, U00, C99 or DotPlot.</S>
			<S sid ="148" ssid = "103">This can be explained by the fact that TOPICOLL only works with a local context whereas these systems rely on the whole text they process.</S>
			<S sid ="149" ssid = "104">As a consequence, they have a global view on texts but are more costly than TOPICOLL from an algorithmic viewpoint.</S>
			<S sid ="150" ssid = "105">Moreover, link detection makes TOPICOLL functionally richer than they are.</S>
			<SUBSECTION>5.2 Global evaluation.</SUBSECTION>
			<S sid ="151" ssid = "106">The global evaluation of a system such as TOPICOLL faces a problem: a reference for link detection is relative to a reference for segmentation.</S>
			<S sid ="152" ssid = "107">Hence, mapping it onto the segments delimited by a system to evaluate is not straightforward.</S>
			<S sid ="153" ssid = "108">To bypass this problem, we chose an approach close the one adopted in TDT for the link detection task: we evaluated the probability of an error in classifying each couple of positions in a text as being part of the same topic (Cpsame) or belonging to different topics (Cpdiff).</S>
			<S sid ="154" ssid = "109">A miss is detected if a couple is found about different topics while they are about the same topic and a false alarm corresponds to the complementary case.</S>
			<S sid ="155" ssid = "110">Systems Miss False alarm Error baseline TOPICOLL 0.85 0.73 0.06 0.01 0.45 0.37 Table 4 – Error rates for Le Monde corpus As the number of Cpdiff couples is generally much larger than the number of Cpsame couples, we randomly selected a number of Cpdiff couples equal to the number of Cpsame couples in order to have a large range of possible values.</S>
			<S sid ="156" ssid = "111">Table 4 shows the results of TOPICOLL for the considered measure and compares them to a baseline procedure that randomly set a fixed number of boundaries and a fixed number of links between the delimited segments.</S>
			<S sid ="157" ssid = "112">This measure is a first proposition that should certainly be improved, especially for balancing more soundly misses and false alarms.</S>
			<S sid ="158" ssid = "113">6 Conclusion.</S>
			<S sid ="159" ssid = "114">We have proposed a method for achieving both topic segmentation and link detection by using collocations together with word recurrence in texts.</S>
			<S sid ="160" ssid = "115">Its evaluation showed the soundness of this approach for working with a local context.</S>
			<S sid ="161" ssid = "116">We plan to extend it to methods that rely on the whole text they process.</S>
			<S sid ="162" ssid = "117">We also aim at extending the evaluation part of this work by improving the global measure we have proposed and by comparing our results to human judgments.</S>
	</SECTION>
</PAPER>
