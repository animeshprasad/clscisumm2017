<PAPER>
	<ABSTRACT>
	<SECTION title="Compositionality of MWEs. " number = "1">
			<S sid ="1" ssid = "1">Multiword expressions (hereafter MWEs) are combinations of words which are lexically, syntac­ tically, semantically or statistically idiosyncratic (Sag et al., 2002; Baldwin and Kim, 2009).</S>
			<S sid ="2" ssid = "2">Much research has been carried out on the extraction and identification of MWEs1 in English (Schone and Jurafsky, 2001; Pecina, 2008; Fazly et al., 2009) and other languages (Dias, 2003; Evert and Krenn, 2005; Salehi et al., 2012).</S>
			<S sid ="3" ssid = "3">However, considerably less work has addressed the task of predicting the meaning of MWEs, especially in non-English lan­ guages.</S>
			<S sid ="4" ssid = "4">As a step in this direction, the focus of this study is on predicting the compositionality of MWEs.</S>
			<S sid ="5" ssid = "5">An MWE is fully compositional if its meaning is predictable from its component words, and it is non-compositional (or idiomatic) if not.</S>
			<S sid ="6" ssid = "6">For ex­ ample, stand up &quot;rise to one&apos;s feet&quot; is composi 1In this paper, we follow Baldwin and Kim (2009) in considering MWE &quot;identification&quot; to be a token-level disam­ biguation task, and MWE &quot;extraction&quot; to be a type-level lex· icon induction task.</S>
			<S sid ="7" ssid = "7">tional, because its meaning is clear from the mean­ ing of the components stand and up.</S>
			<S sid ="8" ssid = "8">However, the meaning of strike up &quot;to start playing&quot; is largely unpredictable from the component words strike and up.</S>
			<S sid ="9" ssid = "9">In this study, following McCarthy et al.</S>
			<S sid ="10" ssid = "10">(2003) and Reddy et al.</S>
			<S sid ="11" ssid = "11">(2011), we consider composition­ ality to be graded, and aim to predict the degree of compositionality.</S>
			<S sid ="12" ssid = "12">For example, in the dataset of Reddy et al.</S>
			<S sid ="13" ssid = "13">(2011), climate change is judged to be 99% compositional, while silver screen is 48% compositional and ivory tower is 9% com­ positional.</S>
			<S sid ="14" ssid = "14">Formally, we model compositionality prediction as a regression task.</S>
			<S sid ="15" ssid = "15">An explicit handling of MWEs has been shown to be useful in NLP applications (Rarnisch, 2012).</S>
			<S sid ="16" ssid = "16">As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti­ cal machine translation.</S>
			<S sid ="17" ssid = "17">They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional­ ity ofMWEs into their system, they could improve translation quality.</S>
			<S sid ="18" ssid = "18">Acosta et al.</S>
			<S sid ="19" ssid = "19">(2011) showed that treating non-compositional MWEs as a sin­ gle unit in information retrieval improves retrieval effectiveness.</S>
			<S sid ="20" ssid = "20">For example, while searching for documents related to ivory tower, we are almost certainly not interested in documents relating to elephant tusks.</S>
			<S sid ="21" ssid = "21">Our approach is to use a large-scale multi-way translation lexicon to source translations of MWEs and their component words, and then model the relative similarity between each of the component words and the MWE, using distributional similar­ ity based on monolingual corpora for the source language and each of the target languages.</S>
			<S sid ="22" ssid = "22">Our hypothesis is that using distributional similarity in more than one language will improve the pre­ diction of compositionality.</S>
			<S sid ="23" ssid = "23">hnportantly, in order to make the method as language-independent and 472 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 472--481, Gothenburg, Sweden, Apri12630 2014.</S>
			<S sid ="24" ssid = "24">@2014 Association for Computational Linguistics broadly-applicable as possible, we make no use of corpus preprocessing such as lemmatisation, and rely only on the availability of a translation dictio­ nary and monolingual corpora.</S>
			<S sid ="25" ssid = "25">Our results confirm our hypothesis that distri­ butional similarity over the source language in ad­ dition to multiple target languages improves the quality of compositionality prediction.</S>
			<S sid ="26" ssid = "26">We also show that our method can be complemented with string similarity (Salehi and Cook, 2013) to further improve compositionality prediction.</S>
			<S sid ="27" ssid = "27">We achieve state-of-the-art results over two datasets.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="28" ssid = "1">Most recent work on predicting the composi­ tionality of MWEs can be divided into two categories: language/construction-specific and general-purpose.</S>
			<S sid ="29" ssid = "2">This can be at either the token­ level (over token occurrences of an MWE in a cor­ pus) or type-level (over the MWE string, indepen­ dent of usage).</S>
			<S sid ="30" ssid = "3">The bulk of work on composition­ ality has been language/construction-specific and operated at the token-level, using dedicated meth­ ods to identify instances of a given MWE, and specific properties of the MWE in that language to predict compositionality (Lin, 1999; Kim and Baldwin, 2007; Fazly et al., 2009).</S>
			<S sid ="31" ssid = "4">General-purpose token-level approaches such as distributional similarity have been commonly applied to infer the semantics of a word/MWE (Schone and Jurafsky, 2001; Baldwin et al., 2003; Reddy et al., 2011).</S>
			<S sid ="32" ssid = "5">These techniques are based on the assumption that the meaning of a word is predictable from its context of use, via the neigh­ bouring words of token-level occurrences of the MWE.</S>
			<S sid ="33" ssid = "6">In order to predict the compositionality of a given MWE using distributional similarity, the different contexts of the MWE are compared with the contexts of its components, and the MWE is considered to be compositional if the MWE and component words occur in similar contexts.</S>
			<S sid ="34" ssid = "7">Identifying token instances of MWEs is not al­ ways easy, especially when the component words do not occur sequentially.</S>
			<S sid ="35" ssid = "8">For example consider put on in put your jacket on, and put your jacket on the chair.</S>
			<S sid ="36" ssid = "9">In the first example put on is an MWE while in the second example, put on is a simple verb with prepositional phrase and not an instance of an MWE.</S>
			<S sid ="37" ssid = "10">Moreover, if we adopt a con­ servative identification method, the number of to­ ken occurrences will be limited and the distribu tional scores may not be reliable.</S>
			<S sid ="38" ssid = "11">Additionally, for morphologically-rich languages, it can be dif­ ficult to predict the different word forms a given MWE type will occur across, posing a challenge for our requirement of no language-specific pre­ processing.</S>
			<S sid ="39" ssid = "12">Pichotta and DeNero (2013) proposed a token­ based method for identifying English phrasal verbs based on parallel corpora for 50 languages.</S>
			<S sid ="40" ssid = "13">They show that they can identify phrasal verbs bet­ ter when they combine information from multiple languages, in addition to the information they get from a monolingual corpus.</S>
			<S sid ="41" ssid = "14">This finding lends weight to our hypothesis that using translation data and distributional similarity from each of a range of target languages, can improve compositionality prediction.</S>
			<S sid ="42" ssid = "15">Having said that, the general applica­ bility of the method is questionable -there are many parallel corpora involving English, but for other languages, this tends not to be the case.</S>
			<S sid ="43" ssid = "16">Salehi and Cook (2013) proposed a general­ purpose type-based approach using translation data from multiple languages, and string similar­ ity between the MWE and each of the compo­ nent words.</S>
			<S sid ="44" ssid = "17">They use training data to identify the best-10 languages for a given family ofMWEs, on which to base the string similarity, and once again find that translation data improves their results substantially.</S>
			<S sid ="45" ssid = "18">Among the four string similarity measures they experimented with, longest com­ mon substring was found to perform best.</S>
			<S sid ="46" ssid = "19">Their proposed method is general and applicable to dif­ ferent families of MWEs in different languages.</S>
			<S sid ="47" ssid = "20">In this paper, we reimplement the method of Salehi and Cook (2013) using longest common substring (LCS), and both benchmark against this method and combine it with our distributional similarity­ based method.</S>
	</SECTION>
	<SECTION title="Our Approach. " number = "3">
			<S sid ="48" ssid = "1">To predict the compositionality of a given MWE, we first measure the semantic similarity between the MWE and each of its component words2 using distributional similarity based on a monolingual corpus in the source language.</S>
			<S sid ="49" ssid = "2">We then repeat the process for translations of the MWE and its com­ ponent words into each of a range of target lan­ guages, calculating distributional similarity using 2Note that we will always assume that there are two component words, but the method is easily generalisable to MWEs with more than two components.</S>
			<S sid ="50" ssid = "3">MWE component1 component2 Score1foreachlanguage Score2foreachlangua ge Translate (using Panlex) Translate (using Panlex) Translate (using Panlex) csmethod csmethod DS DS (using Wikiepdia) (using Wikiepdia) j j score1 score2 Figure 1: Outline of our approach to computing the distributional similarity (DS) of translations of an MWE with each of its component words, for a given target language.</S>
			<S sid ="51" ssid = "4">score1 and score2 are the similarity for the first and second compo­ nents, respectively.</S>
			<S sid ="52" ssid = "5">We obtain translations from Panlex, and use Wikipedia as our corpus for each language.</S>
			<S sid ="53" ssid = "6">a monolingual corpus in the target language (Fig­ ure 1).</S>
			<S sid ="54" ssid = "7">We additionally use supervised learning to identify which target languages (or what weights for each language) optimise the prediction of com­ positionality (Figure 2).</S>
			<S sid ="55" ssid = "8">We hypothesise that by using multiple translations -rather than only in­ formation from the source language -we will be able to better predict compositionality.</S>
			<S sid ="56" ssid = "9">We optionally combine our proposed approach with string similarity, calculated based on the method of Salehi and Cook (2013), using LCS.</S>
			<S sid ="57" ssid = "10">Below, we detail our method for calculating dis­ tributional similarity in a given language, the dif­ ferent methods for combining distributional simi­ larity scores into a single estimate of composition­ ality, and finally the method for selecting the target languages to use in calculating compositionality.</S>
			<S sid ="58" ssid = "11">3.1 Calculating Distributional Similarity.</S>
			<S sid ="59" ssid = "12">In order to be consistent across all languages and be as language-independent as possible, we calcu Compositionality score Figure 2: Outline of the method for combin­ ing distributional similarity scores from multiple languages, across the components of the MWE.</S>
			<S sid ="60" ssid = "13">CSmethod refers to one of the methods described in Section 3.2 for calculating compositionality.</S>
			<S sid ="61" ssid = "14">late distributional similarity in the following man­ ner for a given language.</S>
			<S sid ="62" ssid = "15">Tokenisation is based on whitespace delimiters and punctuation; no lemmatisation or case-folding is carried out.</S>
			<S sid ="63" ssid = "16">Token instances of a given MWE or component word are identified by full-token n­ gram matching over the token stream.</S>
			<S sid ="64" ssid = "17">We assume that all full stops and equivalent characters for other orthographies are sentence boundaries, and chunk the corpora into (pseudo-)sentences on the basis of them.</S>
			<S sid ="65" ssid = "18">For each language, we identify the 51st1050th most frequent words, and consider them to be content-bearing words, in the manner of Schtitze (1997).</S>
			<S sid ="66" ssid = "19">This is based on the assump­ tion that the top-50 most frequent words are stop words, and not a good choice of word for calculat­ ing distributional similarity over.</S>
			<S sid ="67" ssid = "20">That is not to say that we can&apos;t calculate the distributional similarity for stop words, however (as we will for the verb particle construction dataset-see Section 4.3.2) they are simply not used as the dimensions in our calculation of distributional similarity.</S>
			<S sid ="68" ssid = "21">We form a vector of content-bearing words across all token occurrences of the target word, on the basis of these content-bearing words.</S>
			<S sid ="69" ssid = "22">Dis­ tributional similarity is calculated over these con­ text vectors using cosine similarity.</S>
			<S sid ="70" ssid = "23">Accord­ ing to Weeds (2003), using dependency rela­ tions with the neighbouring words of the target word can better predict the meaning of the target word.</S>
			<S sid ="71" ssid = "24">However, in line with our assumption of no language-specific preprocessing, we just use word co-occurrence.</S>
			<S sid ="72" ssid = "25">3.2 Calculating Compositionality.</S>
			<S sid ="73" ssid = "26">First, we need to calculate a combined composi­ tionality score from the individual distributional similarities between each component word and the MWE.</S>
			<S sid ="74" ssid = "27">Following Reddy et al.</S>
			<S sid ="75" ssid = "28">(2011), we combine the component scores using the weighted mean (as shown in Figure 2): comp = as1 + (1- a)s2 (1) where s1 and s2 are the scores for the first and the second component, respectively.</S>
			<S sid ="76" ssid = "29">We use dif­ ferent a settings for each dataset, as detailed in Section 4.3.</S>
			<S sid ="77" ssid = "30">We experiment with a range of methods for cal­ culating compositionality, as follows: CS Lt : calculate distributional similarity using only distributional similarity in the source language corpus (This is the approach used by Reddy et al.</S>
			<S sid ="78" ssid = "31">(2011), as discussed in Sec­ tion 2).</S>
			<S sid ="79" ssid = "32">CS L2N: exclude the source language, and com­ pute the mean of the distributional similarity scores for the best-N target languages.</S>
			<S sid ="80" ssid = "33">The value of N is selected according to training data, as detailed in Section 3.3.</S>
			<S sid ="81" ssid = "34">CS Lt +L2N: calculate distributional similarity over both the source language ( CS Lt) and the mean of the best-N languages ( CS L2N ), and combine via the arithmetic mean.3 This is to examine the hypothesis that using multiple target languages is better than just using the source language.</S>
			<S sid ="82" ssid = "35">CSsvR(Lt+L2): train a support vector regressor (SVR: Smola and SchOlkopf (2004)) over the distributional similarities for all 52 languages (source and target languages).</S>
			<S sid ="83" ssid = "36">3We also experimented with taking the mean over all the languages -target and source -but found it best to com­ bine the scores for the target languages first, to give more weight to the source language.</S>
			<S sid ="84" ssid = "37">CS string: calculate string similarity using the LCS-based method of Salehi and Cook (2013).4 CS string+Lt: calculate the mean of the string similarity (CS string) and distributional sim­ ilarity in the source language (Salehi and Cook, 2013).</S>
			<S sid ="85" ssid = "38">CS all: calculate the mean of the string similarity ( CS string) and distributional similarity scores ( CS L1 and CS L2N ).</S>
			<S sid ="86" ssid = "39">3.3 Selecting Target Languages.</S>
			<S sid ="87" ssid = "40">We experiment with two approaches for combin­ ing the compositionality scores from multiple tar­ get languages.</S>
			<S sid ="88" ssid = "41">First, in CS L2N (and CS Lt +L2N and CS all that build off it), we use training data to rank the target languages according to Pearson&apos;s correlation be­ tween the predicted compositionality scores and the gold-standard compositionality judgements.</S>
			<S sid ="89" ssid = "42">Based on this ranking, we take the best-N lan­ guages, and combine the individual composition­ ality scores by taking the arithmetic mean.</S>
			<S sid ="90" ssid = "43">We se­ lect N by determining the value that optimises the correlation over the training data.</S>
			<S sid ="91" ssid = "44">In other words, the selection of Nand accordingly the best-N lan­ guages are based on nested cross-validation over training data, independently of the test data for that iteration of cross-validation.</S>
			<S sid ="92" ssid = "45">Second in CSsvR(Lt+L2), we combine the compositionality scores from the source and all 51 target languages into a feature vector, and train an SVR over the data using LIBSVM.5</S>
	</SECTION>
	<SECTION title="Resources. " number = "4">
			<S sid ="93" ssid = "1">In this section, we describe the resources required by our method, and also the datasets used to eval­ uate our method.</S>
			<S sid ="94" ssid = "2">4.1 Monolingual Corpora for Different.</S>
			<S sid ="95" ssid = "3">Languages We collected monolingual corpora for each of 52 languages (51 target languages + 1 source lan­ guage) from XML dumps of Wikipedia.</S>
			<S sid ="96" ssid = "4">These languages are based on the 54 target languages 4Due to differences in our random partitioning, our re­ ported results over the two English datasets differ slightly over the results of Salehi and Cook (2013) using the same method.</S>
			<S sid ="97" ssid = "5">5http://www.csie.ntu.edu.tw/-cjlin/libsvm used by Salehi and Cook (2013), excluding Span­ ish because we happened not to have a dump of Spanish Wikipedia, and also Chinese and Japanese because of the need for a language-specific word tokeniser.</S>
			<S sid ="98" ssid = "6">The raw corpora were preprocessed us­ ing the WP2TXT toolbox6 to eliminate XML tags, HTML tags and hyperlinks, and then tokenisa­ tion based on whitespace and punctuation was per­ formed.</S>
			<S sid ="99" ssid = "7">The corpora vary in size from roughly 150M tokens for English, to roughly 640K tokens for Marathi.</S>
			<S sid ="100" ssid = "8">4.2 Multilingual Dictionary.</S>
			<S sid ="101" ssid = "9">To translate the MWEs and their components, we follow Salehi and Cook (2013) in using Pan­ lex (Baldwin et al., 2010).</S>
			<S sid ="102" ssid = "10">This online dictio­ nary is massively multilingual, covering more than 1353 languages.</S>
			<S sid ="103" ssid = "11">For each MWE dataset (see Sec­ tion 4.3), we translate the MWE and component words from the source language into each of the 51 languages.</S>
			<S sid ="104" ssid = "12">In instances where there is no direct translation in a given language for a term, we use a pivot lan­ guage to find translation(s) in the target language.</S>
			<S sid ="105" ssid = "13">For example, the English noun compound silver screen has direct translations in only 13languages in Panlex, including Vietnamese (m2m bac) but not French.</S>
			<S sid ="106" ssid = "14">There is, however, a translation of man bac into French (cinema), allowing us to infer an indirect translation between silver screen and cinema.</S>
			<S sid ="107" ssid = "15">In this way, if there are no direct translations into a particular target language, we search for a single-pivot translation via each of our other target languages, and combine them all to­ gether as our set of translations for the target lan­ guage of interest.</S>
			<S sid ="108" ssid = "16">In the case that no translation (direct or indirect) can be found for a given source language term into a particular target language, the compositionality score for that target language is set to the average across all target languages for which scores can be calculated for the given term.</S>
			<S sid ="109" ssid = "17">If no translations are available for any target language (e.g. the term is not in Panlex) the compositionality score for each target language is set to the average score for that target language across all other source language terms.</S>
			<S sid ="110" ssid = "18">6http://wp2txt.rubyforge.org/ 4.3 Datasets.</S>
			<S sid ="111" ssid = "19">We evaluate our proposed method over three datasets (two English, one German), as described below.</S>
			<S sid ="112" ssid = "20">4.3.1 English Noun Compounds (ENC) Our first dataset is made up of 90 binary English noun compounds, from the work of Reddy et al.</S>
			<S sid ="113" ssid = "21">(2011).</S>
			<S sid ="114" ssid = "22">Each noun compound was annotated by multiple annotators using the integer scale 0 (fully non-compositional) to 5 (fully compositional).</S>
			<S sid ="115" ssid = "23">A final compositionality score was then calculated as the mean of the scores from the annotators.</S>
			<S sid ="116" ssid = "24">If we simplistically consider 2.5 as the threshold for compositionality, the dataset is relatively well balanced, containing 48% compositional and 52% non-compositional noun compounds.</S>
			<S sid ="117" ssid = "25">Following Reddy et al.</S>
			<S sid ="118" ssid = "26">(2011), in combining the component­ wise distributional similarities for this dataset, we weight the first component in Equation 1 higher than the second (a= 0.7).</S>
			<S sid ="119" ssid = "27">4.3.2 English Verb Particle Constructions (EVPC) The second dataset contains 160 English verb par­ ticle constructions (VPCs), from the work of Ban­ nard (2006).</S>
			<S sid ="120" ssid = "28">In this dataset, a verb particle con­ struction consists of a verb (the head) and a prepo­ sitional particle (e.g. hand in, look up or battle on).</S>
			<S sid ="121" ssid = "29">For each component word (the verb and parti­ cle, respectively), multiple annotators were asked whether the VPC entails the component word.</S>
			<S sid ="122" ssid = "30">In order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that VPC.</S>
			<S sid ="123" ssid = "31">That is, following Bannard et al.</S>
			<S sid ="124" ssid = "32">(2003), we only consider the compositionality of the verb component in our experiments (and as such a = 1 in Equation 1).</S>
			<S sid ="125" ssid = "33">One area of particular interest with this dataset will be the robustness of the method to function words (the particles), both under translation and in terms of calculating distributional similarity, al­ though the findings of Baldwin (2006) for English prepositions are at least encouraging in this re­ spect.</S>
			<S sid ="126" ssid = "34">Additionally, English VPCs can occur in &quot;split&quot; form (e.g. put your jacket on, from our earlier example), which will complicate identifi­ cation, and the verb component will often be in­ flected and thus not match under our identification strategy (for both VPCs and the component verbs).</S>
			<S sid ="127" ssid = "35">Dataset Language Frequency Family Italian 100 Romance French 99 Romance each N is selected over 100 folds on ENC, EVPC and GNC datasets, respectively.</S>
			<S sid ="128" ssid = "36">From the his­ ENC EVPC GNC German 86 Germanic Vietnamese 83 VietMuong Portuguese 62 Romance Bulgarian 100 Slavic Breton 100 Celtic Occitan 100 Romance Indonesian 100 Indonesian Slovenian 100 Slavic Polish 100 Slavic Lithuanian 99 Baltic Finnish 74 Uralic Bulgarian 72 Slavic Czech 40 Slavic tograms, N = 6, N = 15 and N = 2 are the most commonl y selected settings for ENC, EVPC and GNC, respective ly.</S>
			<S sid ="129" ssid = "37">That is, multiple languages are generally used, but more languages are used for English VPCs than either of the compoun d noun datasets.</S>
			<S sid ="130" ssid = "38">The 5 most selected languages for ENC, EVPC and GNC are shown in Table 1.</S>
			<S sid ="131" ssid = "39">As we can see, there are some languages which are al­ ways selected for a given dataset, but equally the commonl y-selected languages vary considera bly between datasets.</S>
			<S sid ="132" ssid = "40">Further analysis reveals that 32 (63%) target Table 1: The 5 best languages for the ENC, EVPC and GNC datasets.</S>
			<S sid ="133" ssid = "41">The language family is based on Voegelin and Voegelin (1977).</S>
			<S sid ="134" ssid = "42">4.3.3 German Noun Compounds (GNC) Our final dataset is made up of 246 German noun compounds (von der Heide and Borgwaldt, 2009; Schulte im Walde et al., 2013).</S>
			<S sid ="135" ssid = "43">Multiple anno­ tators were asked to rate the compositionality of each German noun compound on an integer scale of 1 (non-compositional) to 7 (compositional).</S>
			<S sid ="136" ssid = "44">The overall compositionality score is then calcu­ lated as the mean across the annotators.</S>
			<S sid ="137" ssid = "45">Note that the component words are provided as part of the dataset, and that there is no need to perform de­ compounding.</S>
			<S sid ="138" ssid = "46">Following Schulte im Walde et al.</S>
			<S sid ="139" ssid = "47">(2013), we weight the first component higher in Equation 1 (a= 0.8) when calculating the overall compositionality score.</S>
			<S sid ="140" ssid = "48">This dataset is significant in being non-English, and also in that German has relatively rich mor­ phology, which we expect to impact on the iden­ tification of both the MWE and the component words.</S>
	</SECTION>
	<SECTION title="Results. " number = "5">
			<S sid ="141" ssid = "1">All experiments are carried out using 10 iterations of 10-fold cross validation, randomly partitioning the data independently on each of the 10 iterations, and averaging across all 100 test partitions in our presented results.</S>
			<S sid ="142" ssid = "2">In the case of CSL2N and other methods that make use of it (i.e. CSL1+L2N and CSau), the languages selected for a given training fold are then used to compute the compositionality scores for the instances in the test set.</S>
			<S sid ="143" ssid = "3">Figures 3a, 3b and 3c are histograms of the number of times languages for ENC, 25 (49%) target languages for EVPC, and only 5 (10%) target languages for GNC have a correlation of r 2: 0.1 with gold­ standard compositionality judgements.</S>
			<S sid ="144" ssid = "4">On the other hand, 8 (16%) target languages for ENC, 2 (4%) target languages for EVPC, and no target lan­ guages for GNC have a correlation of r :::; -0.1.</S>
			<S sid ="145" ssid = "5">5.1 ENC Results.</S>
			<S sid ="146" ssid = "6">English noun compounds are relatively easy to identify in a corpus,7 because the components oc­ cur sequentially, and the only morphological vari­ ation is in noun number (singular vs. plural).</S>
			<S sid ="147" ssid = "7">In other words, the precision for our token match­ ing method is very high, and the recall is also acceptably high.</S>
			<S sid ="148" ssid = "8">Partly as a result of the ease of identification, we get a high correlation of r = 0.700 for CSL1 (using only source language data).</S>
			<S sid ="149" ssid = "9">Using only target languages (CSL2N), the results drop tor = 0.434, but when we combine the two (CSL1+L2N), the correlation is higher than using only source or target language data, at r = 0.725.</S>
			<S sid ="150" ssid = "10">When we combine all languages us­ ing SVR, the results rise slightly higher again to r = 0.744, which is slightly above the correla­ tion of the state-of-the-art method of Salehi and Cook (2013), which combines their method with the method of Reddy et al.</S>
			<S sid ="151" ssid = "11">(2011) (CS string+LJ ).</S>
			<S sid ="152" ssid = "12">These last two results support our hypothesis that using translation data can improve the prediction of compositionality.</S>
			<S sid ="153" ssid = "13">The results for string similar­ ity on its own (CSstring• r = 0.644) are slightly lower than those using only source language dis­ tributional similarity, but when combined with 7Although see Lapata and Lascarides (2003) for discus­ sion of the difficulty of reliably identifying low-frequency English noun compounds.</S>
			<S sid ="154" ssid = "14">(a) ENC (b)EVPC &apos;&quot; 10 8 bestN (c) GNC Figure 3: Histograms displaying how many times a given N is selected as the best number of languages over each dataset.</S>
			<S sid ="155" ssid = "15">For example, according to the GNC chart, there is a peak for N = 2, which shows that over 100 folds, the best-2languages achieved the highest correlation on 18 folds.</S>
			<S sid ="156" ssid = "16">M eth od Su m ma ry of the M eth od E N C E VP C G N C CS LJ So urc e lan gu ag e 0.7 00 0.</S>
			<S sid ="157" ssid = "17">17 7 0.1 41 CS L2 N Best N tar get lan gu ag es 0.4 34 0.</S>
			<S sid ="158" ssid = "18">39 8 0.1 13 CS LJ + L2 N So urc e + best N tar get lan gu ag es 0.7 25 0.</S>
			<S sid ="159" ssid = "19">31 2 0.1 78 C S SV R( Ll + L2 ) SV R (S ou rc e+ all 51 tar get lan gu ag es) 0.7 44 0.</S>
			<S sid ="160" ssid = "20">38 9 0.0 85 CS str in g Str ing Si mi lar ity (S ale hi an d Co ok, 20 13 ) 0.6 44 0.</S>
			<S sid ="161" ssid = "21">38 5 0.3 72 C S str ing +L l C S stri ng + C S Ll (S ale hi an d Co ok , 20 13 ) 0.7 39 0.</S>
			<S sid ="162" ssid = "22">36 0 0.3 53 C Sa u C S Ll + C S L2 N + C S stri ng 0.7 32 0.</S>
			<S sid ="163" ssid = "23">41 7 0.3 64 Table 2: Pearson&apos;s correlation on the ENC, EVPC and GNC datasets CS Ll +L2N (i.e. CS au) there is a slight rise in cor­ relation (from r = 0.725 to r = 0.732).</S>
			<S sid ="164" ssid = "24">5.2 EVPC Results.</S>
			<S sid ="165" ssid = "25">English VPCs are hard to identify.</S>
			<S sid ="166" ssid = "26">As discussed in Section 2, VPC components may not occur se­ quentially, and even when they do occur sequen­ tially, they may not be a VPC.</S>
			<S sid ="167" ssid = "27">As such, our sim­ plistic identification method has low precision and recall (hand analysis of 927 identified VPC in­ stances would suggest a precision of around 74%).</S>
			<S sid ="168" ssid = "28">There is no question that this is a contributor to the low correlation for the source language method ( CS LJ; r = 0.177).</S>
			<S sid ="169" ssid = "29">When we use target lan­ guages instead of the source language ( CS L2N ), the correlation jumps substantially to r = 0.398.When we combine English and the target Ian guages ( CS Ll +L2N ), the results are actually lower than just using the target languages, because of the high weight on the target language, which is not desirable for VPCs, based on the source lan­ guage results.</S>
			<S sid ="170" ssid = "30">Even for CS SVR(LJ+£ 2 ), the re­ sults (r = 0.389) are slightly below the target language-only results.</S>
			<S sid ="171" ssid = "31">This suggests that when predicting the compositionality of MWEs which are hard to identify in the source language, it may actually be better to use target languages only.</S>
			<S sid ="172" ssid = "32">The results for string similarity (CS string: r = 0.385) are similar to those for CS L2N . However, as with the ENC dataset, when we combine string simi­ larity and distributional similarity ( CS au), the re­ sults improve, and we achieve the state-of-the-art for the dataset.In Table 3, we present classification-based eval M eth od Pre cis ion Re cal lF sco re ({3 = 1) Ac cur ac y Ba nn ard et al.</S>
			<S sid ="173" ssid = "33">(2 00 3) 6 0 . 8 6 6.</S>
			<S sid ="174" ssid = "34">6 6 3 . 6 6 0 . 0 Sal ehi an d Co ok (2 01 3) 8 6 . 2 7 1.</S>
			<S sid ="175" ssid = "35">8 7 7 . 4 6 9 . 3 CSau 79.5 89.3 82.0 74.5 Table 3: Results(%) for the binary compositionality prediction task on the EVPC dataset uation over a subset of EVPC, binarising the com­ positionality judgements in the manner of Bannard et al.</S>
			<S sid ="176" ssid = "36">(2003).</S>
			<S sid ="177" ssid = "37">Our method achieves state-of-the-art results in terms of overall F-score and accuracy.</S>
			<S sid ="178" ssid = "38">5.3 GNC Results.</S>
			<S sid ="179" ssid = "39">German is a morphologically-rich language, with marking of number and case on nouns.</S>
			<S sid ="180" ssid = "40">Given that we do not perform any lemmatization or other language-specific preprocessing, we inevitably achieve low recall for the identification of noun compound tokens, although the precision should be nearly 100%.</S>
			<S sid ="181" ssid = "41">Partly because of the resultant sparseness in the distributional similarity method, the results for CS L1 are low (r = 0.141), al­ though they are lower again when using target lan­ guages (r = 0.113).</S>
			<S sid ="182" ssid = "42">However, when we combine the source and target languages ( CS L1+L2N) the results improve to r = 0.178.</S>
			<S sid ="183" ssid = "43">The results for CSSVR(L1+L2)• on the other hand, are very low (r = 0.085).</S>
			<S sid ="184" ssid = "44">Ultimately, simple string similar­ ity achieves the best results for the dataset (r = 0.372), and this result actually drops slightly when combined with the distributional similarities.</S>
			<S sid ="185" ssid = "45">To better understand the reason for the lacklus­ tre results using SVR, we carried out error analysis and found that, unlike the other two datasets, about half of the target languages return scores which correlate negatively with the human judgements.</S>
			<S sid ="186" ssid = "46">When we filter these languages from the data, the score for SVR improves appreciably.</S>
			<S sid ="187" ssid = "47">For example, over the best-3 languages overall, we get a corre­ lation score of r = 0.179, which is slightly higher than CS L1+L2N.</S>
			<S sid ="188" ssid = "48">We further investigated the reason for getting very low and sometimes negative correlations with many of our target languages.</S>
			<S sid ="189" ssid = "49">We noted that about 24% of the German noun compounds in the dataset do not have entries in Panlex.</S>
			<S sid ="190" ssid = "50">This contrasts with ENC where only one instance does not have an entry in Panlex, and EVPC where all VPCs have translations in at least one language in Panlex.</S>
			<S sid ="191" ssid = "51">We experimented with using string sim­ ilarity scores in the case of such missing transla tions, as opposed to the strategy described in Sec­ tion 4.2.</S>
			<S sid ="192" ssid = "52">The results for CS SVR(Ll +L2) rose to r = 0.269, although this is still below the correla­ tion for just using string similarity.</S>
			<S sid ="193" ssid = "53">Our results on the GNC dataset using string similarity are competitive with the state-of-the-art results (r = 0.45) using a window-based distribu­ tional similarity approach over monolingual Ger­ man data (Schulte im Walde et al., 2013).</S>
			<S sid ="194" ssid = "54">Note, however, that their method used part-of-speech in­ formation and lemmatisation, where ours does not, in keeping with the language-independent philos­ ophy of this research.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number = "6">
			<S sid ="195" ssid = "1">In this study, we proposed a method to predict the compositionality of MWEs based on monolingual distributional similarity between the MWE and each of its component words, under translation into multiple target languages.</S>
			<S sid ="196" ssid = "2">We showed that using translation and multiple target languages en­ hances compositionality modelling, and also that there is strong complementarity between our ap­ proach and an approach based on string similarity.</S>
			<S sid ="197" ssid = "3">In future work, we hope to address the ques­ tion of translation sparseness, as observed for the GNC dataset.</S>
			<S sid ="198" ssid = "4">We also plan to experiment with un­ supervised morphological analysis methods to im­ prove identification recall, and explore the impact of tokenization.</S>
			<S sid ="199" ssid = "5">Furthermore, we would like to in­ vestigate the optimal number of stop words and content-bearing words for each language, and to look into the development of general unsupervised methods for compositionality prediction.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="200" ssid = "6">We thank the anonymous reviewers for their insightful comments and valuable suggestions.</S>
			<S sid ="201" ssid = "7">NICTA is funded by the Australian government as represented by Department of Broadband, Com­ munication and Digital Economy, and the Aus­ tralian Research Council through the ICT Centre of Excellence programme.</S>
	</SECTION>
</PAPER>
