<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">None</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="2" ssid = "2">Current technologies are making increasingly vast amounts of information available to common internet users.</S>
			<S sid ="3" ssid = "3">Though the availability of data is ever growing, finding information relevant to a specific need is not always straightforward.</S>
			<S sid ="4" ssid = "4">Information Retrieval techniques deal with this problem and continue to receive a lot of attention in today’s data-driven world.</S>
			<S sid ="5" ssid = "5">Information Retrieval (IR) and Natural Language Processing (NLP) are deeply related because most search engines are based on a document corpus written in natural language text.</S>
			<S sid ="6" ssid = "6">Traditionally, Information Retrieval systems have been aimed at shallow levels of parsing of input queries and most studies in NLP for Information Retrieval have been directed towards parsing the English language.</S>
			<S sid ="7" ssid = "7">However, there is an increasing need to expand the scope of this type of systems due to more demanding users seeking precise answers, and to the growing nature of (non-English) online content.</S>
			<S sid ="8" ssid = "8">This paper discusses two main problems with non-traditional IR systems: multilingual information retrieval and understanding the logical meaning of queries to find more precise answers.</S>
			<S sid ="9" ssid = "9">We analyze the challenges posed by both issues and the techniques that are being applied to solve them.</S>
			<S sid ="10" ssid = "10">The paper is organized in four main parts.</S>
			<S sid ="11" ssid = "11">Section two will discuss linguistic features of languages that pose challenges for information retrieval.</S>
			<S sid ="12" ssid = "12">In particular, we analyze some features that are not present in the English language but that are specific to two languages which were studied in contrast to English, namely Arabic and Russian.</S>
			<S sid ="13" ssid = "13">Section three will discuss the issue of resource availability to support NLP and the study of languages.</S>
			<S sid ="14" ssid = "14">In particular, we discuss resources from the three languages we cover in this paper.</S>
			<S sid ="15" ssid = "15">Section four presents the different language processing levels used by IR systems, highlighting the challenges of each level.</S>
			<S sid ="16" ssid = "16">We discuss how some of these difficulties have been overcome and which additional difficulties are still open or arise when considering different languages.</S>
			<S sid ="17" ssid = "17">Finally we discuss two new trends in the retrieval of information: open domain information extraction and question answering.</S>
			<S sid ="18" ssid = "18">These techniques require deep parsing strategies to correctly interpret the logical meaning of natural language in order to either convert natural language into a structured logical form, or into a query which can be executed on a database of structured information.</S>
	</SECTION>
	<SECTION title="Language Properties  as Challenges to IR and IE. " number = "2">
			<S sid ="19" ssid = "1">This section discusses different linguistic features which hinder the processing of natural language queries and their mapping to relevant information.</S>
			<S sid ="20" ssid = "2">We do this as an introduction for the reader to understand why some particular challenges arise when trying to process a natural language input.</S>
			<S sid ="21" ssid = "3">We classify these features according to their nature and explain the challenges inherent to each property.</S>
			<S sid ="22" ssid = "4">Many of the properties we present in this chapter are not present in the English language, but are specific to the two languages that we study in the context of multilingualism in IR: Arabic and Russian.</S>
			<S sid ="23" ssid = "5">Because these two languages are so different from English, they are interesting subjects to understand the challenges posed by multilingualism.</S>
			<S sid ="24" ssid = "6">We wish to provide the reader with the notion that IR techniques developed for English language search engines, may not be efficient for search engines in other languages.</S>
			<S sid ="25" ssid = "7">Furthermore, Russian and Arabic were a natural choice of languages because they are native to two of the authors of this paper.</S>
			<S sid ="26" ssid = "8">We will begin this chapter by briefly describing the two studied languages and continue to present language properties.</S>
			<S sid ="27" ssid = "9">Arabic Arabic is a Semitic language which was first formalized in the 6th century with the birth of Islam.</S>
			<S sid ="28" ssid = "10">It is written using the Arabic script, which is an abjad 1 consisting of 28 letters and is written from right to left.</S>
			<S sid ="29" ssid = "11">It is the native language of almost 300 million people and is one of the six official languages of the United Nations.</S>
			<S sid ="30" ssid = "12">Many dialects of Arabic exist across the Arab World, with important phonological, morphological, lexical and syntactic differences compared sometimes to those among the Romance languages [22].</S>
			<S sid ="31" ssid = "13">The effects of these differences on natural language processing tasks will be discussed later in this section.</S>
			<S sid ="32" ssid = "14">Throughout this report, the HabashSoudiBuckwalter transliteration system for Arabic will be used [43].</S>
			<S sid ="33" ssid = "15">We included the transliteration table in appendix A - HabashSoudiBuckwalter Arabic Transliteration Scheme.</S>
			<S sid ="34" ssid = "16">It should be noted that while most English glosses contain the lowercase letters (a, i, u), these are only added to enhance readability and are usually not present in the original Arabic words 1 An abjad is a writing system where most symbols correspond to consonants while vowels can usually be added by the means of diacritics.</S>
			<S sid ="35" ssid = "17">due to the lack of vocalization (see Section 2.3.2).</S>
			<S sid ="36" ssid = "18">Russian Russian is a Slavic language that belongs to the IndoEuropean family of languages and is natively spoken by 150 million people.</S>
			<S sid ="37" ssid = "19">Along with Ukrainian and Belorussian, it was derived from the Old Russian language in 14th15th century.</S>
			<S sid ="38" ssid = "20">Russian is the eighth most spoken language in the world by the number of native speakers and one of the six official languages of the United Nations.</S>
			<S sid ="39" ssid = "21">Russian uses the Cyrillic alphabet and is written left to right, similar to English.</S>
			<S sid ="40" ssid = "22">The Cyrillic alphabet consists of 33 letters and can be easily transliterated using the Latin alphabet.</S>
			<S sid ="41" ssid = "23">The Russian language is a morphologically rich and highly inflective language that makes natural language processing more challenging in comparison to English (see Section 2.1.1).</S>
			<S sid ="42" ssid = "24">The remainder of this section will discuss linguistic features that were found across the languages, highlighting the problematic nature of each language and paving the way for a detailed discussion on how they are addressed in section 4.</S>
			<S sid ="43" ssid = "25">2.1 Morphological Properties.</S>
			<S sid ="44" ssid = "26">2.1.1 Inflectional Morphology In linguistics, inflection is the modification of the word to express different grammatical categories such as tense, mood, gender, number, person and voice.</S>
			<S sid ="45" ssid = "27">Different language families show different degrees of inflection in their morphology.</S>
			<S sid ="46" ssid = "28">For example, modern English is considered a weakly inflected language as there are only two forms of nouns (singular and plural) and four forms of verbs 2 . On the other hand, both Russian and Arabic are highly inflected languages with a complex agreement system between nouns and several forms for verbs.</S>
			<S sid ="47" ssid = "29">In the following subsections, we will briefly discuss the inflection for both languages in nouns, adjectives and verbs.</S>
			<S sid ="48" ssid = "30">Inflection in Arabic Arabic verbal morphology is very regular with only a handful of exceptions.</S>
			<S sid ="49" ssid = "31">Almost all verbs follow a set of predefined patterns in conjugation and inflection.</S>
			<S sid ="50" ssid = "32">Nonetheless, they are still highly inflected as they inflect for subject (person, number and gender), aspect, mood and voice [42].</S>
			<S sid ="51" ssid = "33">As an example, consider the phrase in Table 1: swryA dum~irat wa+lam yafσalwA šayã Syria destoyed and+not do anything &quot;Syria was destroyed and they didn’t do anything&quot; Table 1: Arabic Verbal Inflection In the example in Table 1, there are two verbs that show inflectional features: • dum~irat: meaning &quot;to destroy&quot;, inflects for its subject, Syria, on person (other), number (singular) and gender (feminine).</S>
			<S sid ="52" ssid = "34">It also inflects for aspect (perfective), mood (indicative) and voice (passive).</S>
			<S sid ="53" ssid = "35">• yafσalwA: meaning &quot;to do&quot;, inflects for its dropped subject on person (other), number (plural) and gender (masculine).</S>
			<S sid ="54" ssid = "36">It also inflects for aspect (perfective), mood (jussive) and voice (active).</S>
			<S sid ="55" ssid = "37">Compared to verbs, Arabic nominal morphology is far more complicated and has many exceptions and special cases.</S>
			<S sid ="56" ssid = "38">In general, Arabic nouns inflect for gender (masculine and feminine), number (singular, dual and plural), case (nominative, accusative and genitive) and state (definite and indefinite).</S>
			<S sid ="57" ssid = "39">A list of the rules for a regular noun inflection can be found in [42].</S>
			<S sid ="58" ssid = "40">It is worth mentioning that singular nouns inflect for case only by changing the vocalization of the last letter.</S>
			<S sid ="59" ssid = "41">However, there are many special cases in obtaining the inflected forms of nouns, the most common of which is the &quot;broken plural&quot;.</S>
			<S sid ="60" ssid = "42">Broken plurals account for almost 50% of all plurals in Arabic [42] and are formed by atypical stem and suffix 2 as described in Brinton, Laurel J.</S>
			<S sid ="61" ssid = "43">(2000).</S>
			<S sid ="62" ssid = "44">The structure of modern English: a linguistic introduction.</S>
			<S sid ="63" ssid = "45">Amsterdam, Philadelphia: John Benjamins.</S>
			<S sid ="64" ssid = "46">p. 104.</S>
			<S sid ="65" ssid = "47">alternations which do not conform to a predefined rule.</S>
			<S sid ="66" ssid = "48">For example, Table 2 includes words that have the same ending when in singular but that inflect in different ways when transformed into their broken plural.</S>
			<S sid ="67" ssid = "49">This makes it more difficult for language processing tools to identify the number of the inflected word, as there is no specific rule to form the broken plural.</S>
			<S sid ="68" ssid = "50">Singular Plural Translation rafyq γurf madr~as xalyf &apos; rafyqAt γuraf madAris xulafA´ Friend (fem.)</S>
			<S sid ="69" ssid = "51">Room (fem.)</S>
			<S sid ="70" ssid = "52">School (fem.)</S>
			<S sid ="71" ssid = "53">Caliph (masc.)</S>
			<S sid ="72" ssid = "54">Table 2: Arabic Nominal Inflection - Broken Plural It can also be noted that, while all the above words have the same ending when they are in singular form, some are feminine and others are masculine, which makes it hard to determine the gender of the word.</S>
			<S sid ="73" ssid = "55">Many other special cases of Arabic nominal inflection are detailed in [42].</S>
			<S sid ="74" ssid = "56">Inflection in Russian Russian is a language with a rich morphology and hence, it is highly inflective.</S>
			<S sid ="75" ssid = "57">This property makes morphological analysis more challenging than for a morhologically poor language like English.</S>
			<S sid ="76" ssid = "58">As previously discussed, Russian is a synthetic and fusional or inflectional language.</S>
			<S sid ="77" ssid = "59">In this section we define inflection as the phenomenon of the same lexeme having many inflected forms for many different combinations of grammatical features.</S>
			<S sid ="78" ssid = "60">Grammatical Inflection Russian inflectional grammar is very complex.</S>
			<S sid ="79" ssid = "61">Since the context of this research is to evaluate language processing in the specific application of information retrieval, we will focus on nouns and adjectives.</S>
			<S sid ="80" ssid = "62">Research shows that verbs are less important to retrieving relevant documents, because nouns and adjectives are the parts of speech that carry the biggest semantic lead in a document [28].</S>
			<S sid ="81" ssid = "63">In what follows we present a list of grammatical inflection in Russian nouns and adjectives: • Nouns can have one of the three possible genders: masculine (masc), feminine (fem), and neuter (neut).</S>
			<S sid ="82" ssid = "64">• Nouns decline according to number: singular (sg) or plural (pl), and six grammatical cases: nominative (nom), genetive (gen), dative (dat), accusative (acc), instrumental (inst), and locative (loc), which is also called prepositional (prep).</S>
			<S sid ="83" ssid = "65">• Nouns form gender-case combination which, in turn, each have a set of characteristics (hard-stem, soft-stem, etc.) • Each gender-case combination does not require a distinct suffix.</S>
			<S sid ="84" ssid = "66">• Inflectional suffixes can also be present in particles, numerals, and adjectives.</S>
			<S sid ="85" ssid = "67">• Adjectives agree in gender, number and case with the noun that they modify.</S>
			<S sid ="86" ssid = "68">• Full adjectives inflect in case, gender and number; short adjectives - in gender and number.</S>
			<S sid ="87" ssid = "69">• Adverbs, prepositions, and conjunctions are uninflected.</S>
			<S sid ="88" ssid = "70">Since full adjectives inflect in case, number, and gender they have more inflected forms than nouns that do not inflect in gender.</S>
			<S sid ="89" ssid = "71">Table 3 presents all possible inflections for a full adjective новый/ ‘novyj’/ new.</S>
			<S sid ="90" ssid = "72">Note that the ‘masc.in’ stands for masculine inanimate gender (ex: house) and ‘masc.an’ represents masculine animate/human gender (ex: man).</S>
			<S sid ="91" ssid = "73">case/num masc.in masc.an neut fem case/num masc.in, fem, neut masc.an nom/sg gen/sg dat/sg acc/sg inst/sg prep/sg нов-ый нов-ого нов-ому нов-ый нов-ым нов-ом -ый -ого -ому -ого -ым -ом -ое -ого -ому -ое -ым -ом -ая -ой -ой -ую -ой -ой nom/pl gen/pl dat/pl acc/pl inst/sg prep/sg -ые -ых -ым -ые -ыми -ых -ые -ых -ым -ых -ыми -ых Table 3: Adjective Full Inflection As shown in Table 3, the adjective new has fourty-eight possible gender-case-number combinations.</S>
			<S sid ="92" ssid = "74">However, in the given example only twelve forms are distinct.</S>
			<S sid ="93" ssid = "75">This example is good to demonstrate both the property of inflection, and the syncretism (reference section 2.1.2).</S>
			<S sid ="94" ssid = "76">Agreement in a Sentence It is important to note that in a sentence, the main verb agrees in person, number, and gender with the subject.</S>
			<S sid ="95" ssid = "77">Adjectives agree in number, case, and gender with the nouns they modify [49].</S>
			<S sid ="96" ssid = "78">Stem Alternation Another important notion in the Russian language inflection is stem alternation because of its importance for tagging, morphological analysis and generation.</S>
			<S sid ="97" ssid = "79">Russian, much like Spanish, has various stem alternations for nouns and verbs.</S>
			<S sid ="98" ssid = "80">According to [39], nouns with alternations in Russian have two stems (Table 4) and verbs with alternations have up to four, as demonstrated in Table 5 for the verb ‘to eat’ conjugated in Present Tense.</S>
			<S sid ="99" ssid = "81">молоток- молотк-а molotok (masc.sg.nom) molotka (masc.sg.gen) &quot;hammer&quot; &quot;of hammer&quot; Table 4: Noun Stem Alternation The notion shown in Table 4 is sometimes referred to as the fleeting vowel.</S>
			<S sid ="100" ssid = "82">Usually, the vowel ‘о’ is fleeting in genitive plural (or singular as in the example 4), but letter ‘е’ follows the same fleeting pattern in dative singular.</S>
			<S sid ="101" ssid = "83">For instance, ‘отец’ (father) becomes ‘отцу’ (to father), without an ‘e’.</S>
			<S sid ="102" ssid = "84">Ест-ь est’ to eatЯ ем Ты ешь- Он ест- Мы ед-им Вы ед-ите Они ед-ят Ja em Ty esh’ On est My edim Vy edite Oni edjat I eat You eat He eats We eat You eat They eat Table 5: Verb Stem Alternation Derivation Stem alternation is not only typical for nominal inflection, it is also present in derivation, i.e., when a suffix is added to a stem to create a new word, which usually belongs to a different part-of-speech [28].</S>
			<S sid ="103" ssid = "85">The new words are generally composed of a prefix, a stem, and a suffix.</S>
			<S sid ="104" ssid = "86">cпутник кровавый с + пут-ь + ник кров-авый prefix(с) + stem(пут-ь) + suffix(ник) stem(кров-ь) + suffix(авый) sputnik krovaviy with + path + ’nik’ blood + ’aviy’ &quot;satelite&quot; &quot;bloody&quot; Table 6: Derivation by Means of Adding a Suffix Table 6 shows two examples of word derivation.</S>
			<S sid ="105" ssid = "87">A noun satel lite is formed from a noun path, and an adjective bloody is derived from a noun blood.</S>
			<S sid ="106" ssid = "88">One example uses a prefix, when the other does not.</S>
			<S sid ="107" ssid = "89">Synthetic Languages Both Russian and Arabic belong to the synthetic group of languages.</S>
			<S sid ="108" ssid = "90">Synthetic languages have a high interpretation-per-word ratio and usually represent morphologically rich languages.</S>
			<S sid ="109" ssid = "91">There exist two types of morphological arrangement: syncretism and agglutination [40].</S>
			<S sid ="110" ssid = "92">2.1.2 Syncretism Languages with the property of syncretism have a tendency to express several combination sets of grammatical categories by one flexion, and have unpredictable stem alternations.</S>
			<S sid ="111" ssid = "93">Syncretism is typical for Slavic languages such as Russian or Czech as well as Spanish and Portuguese [40].</S>
			<S sid ="112" ssid = "94">This property can often be mistaken for inflection.</S>
			<S sid ="113" ssid = "95">For the purpose of this report, we define syncretism as a phenomenon of different inflected forms that are realized by the same surface form.</S>
			<S sid ="114" ssid = "96">In Table 7 we can observe that the same surface form can correspond to several grammatical categories.</S>
			<S sid ="115" ssid = "97">The word ‘structures’ is shown in genitive singular, accusative plural, and nominative plural forms.</S>
			<S sid ="116" ssid = "98">The three inflected forms result in the same surface form - структуры.</S>
			<S sid ="117" ssid = "99">анализ структуры в эти структуры эти структуры привлечены analiz struktury analysis structure (gen, sg) &quot;analysis of the structure&quot; v jeti struktury in these structure (acc, pl) &quot;into these structures&quot; jeti struktury privlecheny these structure (nom, pl) involve &quot;these structures are involved&quot; Table 7: Syncretism Example 1 Table 8 presents an even more interesting example of syncretism: the two surface forms represent two different parts of speech: a noun in genetive case and a verb in the past tense.</S>
			<S sid ="118" ssid = "100">стали стали stali(N.gen.sg.fem) stali(V.past.pl) steel to become &quot;of steel&quot; &quot;became&quot; Table 8: Syncretism Example 2 Syncretism differs from agglutination by the number of surface forms per grammatical, syntactic or semantic change.</S>
			<S sid ="119" ssid = "101">A syncretic language has many meanings in one form, which creates ambiguity in the natural processing.</S>
			<S sid ="120" ssid = "102">In contrast, agglunative languages (section 2.1.3) often have one meaning in one surface form.</S>
			<S sid ="121" ssid = "103">2.1.3 Agglutination Agglutinative languages are languages whose morphology is mainly constructed by agglutinating phonetically unchangeable affixes to a stem.</S>
			<S sid ="122" ssid = "104">Such languages either have no stem alternations or the alternations are predictable.</S>
			<S sid ="123" ssid = "105">A specific type of agglutination exists in Arabic where it is a common feature of the language to append affixes to the stem.</S>
			<S sid ="124" ssid = "106">Unlike affixes in English, Arabic has a specific type of affixes called clitics.</S>
			<S sid ="125" ssid = "107">A clitic is a morpheme that has syntactic characteristics of a word but is phonologically dependent on a neighboring word 3 [64].</S>
			<S sid ="126" ssid = "108">Take the example in Table 9: katab+nA wrote+we &quot;we wrote&quot; Table 9: Arabic Clitics - Example 1 3 http://www01.sil.org/linguistics/GlossaryOfLinguisticTerms/WhatIsACliticGrammar.htm The previous word consists of two main parts: ktb which is the verb &quot;to write&quot; and nA which is the pronominal clitic indicating that the subject of the verb is &quot;we&quot;.</S>
			<S sid ="127" ssid = "109">More precisely, Arabic clitics can be divided into 5 groups and attach to base words in the following strict order [42]: [ QST + [ CNJ + [ PRT + [ DET + BASE + PRO ] ] ] ] • QST: the interrogative particle Â • CNJ: the conjunctions w and f which mean &quot;and&quot; and &quot;then&quot; respectively.</S>
			<S sid ="128" ssid = "110">They attach to both verbs and nouns • PRT: particle proclitics, some of which only attach to verbs and others only to nouns • DET: the determiner Al which only attaches to nouns • PRO: pronominal clitics which can attach to nouns (as possessives) or to verbs (as objects) However, only up to 4 clitics can be attached to a single Arabic word (the determiner and the pronominal clitics never coexist on the same word).</S>
			<S sid ="129" ssid = "111">This is shown in Table 10: Â+wa+bi+Al+qalami sa+yu+HAriba+hum QST +and+with+the+pen will+he+fight+them &quot;Will he fight them with the pen?&quot;</S>
			<S sid ="130" ssid = "112">Table 10: Arabic Clitics - Example 2 2.1.4 Compounding Compounding is a way to form words by combining two or more lexemes into one unit that holds a specific meaning [24].</S>
			<S sid ="131" ssid = "113">Such behavior is typical for German, Dutch, and Finnish, and is also possible, though not regular, in Russian and English.</S>
			<S sid ="132" ssid = "114">In Russian, compounds are often encountered in specialized fields, such as biology or physics.</S>
			<S sid ="133" ssid = "115">ветрогенератор электромагнитный ветер+генератор электро+магнит vetrogenerator = veter+generator jelektromagnitnyj = jelektro+magnit wind+generator electro+magnet &quot;wind generator&quot; &quot;electromagnetic&quot; Table 11: Compounding in Russian 2.2 Syntactic Properties.</S>
			<S sid ="134" ssid = "116">2.2.1 Pronoun Dropping A pro-drop language (pronoun-dropping) is a language in which certain classes of pronouns may be dropped from the sentence under certain conditions.</S>
			<S sid ="135" ssid = "117">Arabic can be classified under a subcategory of pro-drop languages called Null Subject Languages 4 . These languages allow subject pronouns to be freely dropped as explained in [33].</S>
			<S sid ="136" ssid = "118">ÂnA qarÂtu Al+SaHyf I read the+newspaper &quot;I read the newspaper&quot; Table 12: Arabic Pronoun Dropping 4 as defined in Martin Haspelmath, The European linguistic area: Standard Average European, in Martin Haspelmath, et al., Language Typology and Language Universals, vol.</S>
			<S sid ="137" ssid = "119">2, 2001, pp.</S>
			<S sid ="138" ssid = "120">14921510 In the sentence in Table 12, the personal pronoun Âna is almost always dropped in naturally occurring Arabic text.</S>
			<S sid ="139" ssid = "121">The pronoun can be deduced from the conjugation of the verb &quot;to read&quot;, the final &quot;t&quot; in the gloss (the &quot;u&quot; is only included to enhance readability while it is not present in the original Arabic word due to lack of vocalization, see section 2.3.2).</S>
			<S sid ="140" ssid = "122">This property is not exclusive to Arabic as it exists in several other languages such as Spanish, Italian, Japanese, and Russian.</S>
			<S sid ="141" ssid = "123">It introduces ambiguity when determining the subject of a verb because it cannot always be determined from the conjugation of the verb.</S>
			<S sid ="142" ssid = "124">Moreover, this pro-drop feature is further complicated if the language is an order-free language; a sequence of Noun-Verb can either be Subject-Verb or Object-Verb.</S>
			<S sid ="143" ssid = "125">2.2.2 Zero-Copula and Ellipsis In linguistics, a copula is a word in a sentence that is used to link the subject with the predicate.</S>
			<S sid ="144" ssid = "126">An example of a copula in English is the verb ‘to be’ [10].</S>
			<S sid ="145" ssid = "127">Copulative sentences in Russian have a special zero-form in the present tense, where the subject is directly linked to the predicate without a special indication of such relation, i.e. with the absence of the copula [14].</S>
			<S sid ="146" ssid = "128">Он - учитель On - uchitel’ He - teacher He is a teacher Table 13: Zero-Copula in Russian The same phenomenon exists in Arabic.</S>
			<S sid ="147" ssid = "129">Although called equational sentences by Arab linguists, it is identical to zero-copula constructions in Russian.</S>
			<S sid ="148" ssid = "130">Table 14 shows two examples of equational sentences in Arabic taken from [42].</S>
			<S sid ="149" ssid = "131">u J Al+kitAbu jadydu˜ Al+rajulu fy Al+bayt the+book new the+man in the+house &quot;The book is new&quot; &quot;The man is in the house&quot; Table 14: Arabic Equational Sentences Elliptical constructions or ellipsis is a property that is present in the Russian language.</S>
			<S sid ="150" ssid = "132">Ellipsis is omitting of one or more words from a clause that are understood from the context of the remaining elements [10].</S>
			<S sid ="151" ssid = "133">In Table 15, the verb ’bought’ is omitted from the second part of the sentence because it is clear that the action for the entire clause is ’to buy’ an object.</S>
			<S sid ="152" ssid = "134">Elliptical constructions are one of the biggest challenges in the formalization of natural language syntax in many languages [14].</S>
			<S sid ="153" ssid = "135">Я купил рубашку, а он галстук Ja kupil rubashku, a on galstuk I bought a shirt, and he a necktie I bought a shirt, and he bought a necktie Table 15: Challenge in Elliptical Constructions 2.2.3 Order-Free Languages A free word order language is a language that doesn’t exhibit strict syntactic rules regarding the word order in a sentence.</S>
			<S sid ="154" ssid = "136">In Arabic, the primary word order is Verb-Subject-Object (VSO).</S>
			<S sid ="155" ssid = "137">However, both Subject-Verb-Object (SVO), and less commonly, Object-Verb-Subject (OVS) can be used in certain contexts [33].</S>
			<S sid ="156" ssid = "138">This property is demonstrated by the example in Table 16 where the first sentence follows the VSO order while the second the SVO order.</S>
			<S sid ="157" ssid = "139">dam~arati Al+Harbu swryA Al+Harbu dam~arat swrya destroy the+war Syria the+war destroy Syria &quot;The war destroyed Syria&quot; Table 16: Arabic Order-Free Structure Russian, as well as Arabic, is a relatively order-free language, which complicates identifying syntactic relationships between the words in the sentence [73].</S>
			<S sid ="158" ssid = "140">The example in Table 17 illustrates that the words in a sentence can appear in any order.</S>
			<S sid ="159" ssid = "141">Phrase Variations Word Order 1.Борис навестил Ивана SVO Boris navestil Ivana Boris (nom) visited Ivan (acc) &quot;Boris visited Ivan&quot; 2.Борис Ивана навестил SOV 3.Ивана навестил Борис OVS 4.Ивана Борис навестил OSV 5.Навестил Борис Ивана VSO 6.Навестил Ивана Борис VOS Table 17: Six Accepted Word Orders in Russian Even though all six orders are accepted, the study conducted in [54] shows that Russian native speakers have a preference for some word orders over the others.</S>
			<S sid ="160" ssid = "142">For instance, SVO, OVS and SOV are used more frequently and are considered more grammatically correct.</S>
			<S sid ="161" ssid = "143">Parsing of order-free languages is more challenging than languages with strict word order such as English.</S>
			<S sid ="162" ssid = "144">In particular, differentiating between subjects and objects can be specifically hard in Russian and Arabic as they can be put in any order around the verb that connects them.</S>
			<S sid ="163" ssid = "145">2.2.4 Idafa Construction The idafa construction is a phenomenon specific to the Arabic language.</S>
			<S sid ="164" ssid = "146">We use the definition given in [42] where they define idafa as &quot;a possessive/genetive construction relating two nouns: the first noun grammatically heads and semantically possesses the second&quot;.</S>
			<S sid ="165" ssid = "147">It is an important construct that is widely used in Arabic.</S>
			<S sid ="166" ssid = "148">Table 18 gives an example of idafa.</S>
			<S sid ="167" ssid = "149">madyna u Al+yAsamyn city the+jasmine &quot;The city of jasmine&quot; Table 18: Arabic idafa Construct This construction poses a problem for computational linguistics as it requires special handling because the nouns are connected without the existence of any concrete connectors.</S>
			<S sid ="168" ssid = "150">Moreover, while the first noun does not contain the definite article &quot;Al&quot;, it should not be considered as indefinite because the second noun clearly defines it.</S>
			<S sid ="169" ssid = "151">2.3 Phonological Properties.</S>
			<S sid ="170" ssid = "152">2.3.1 Phonetic Stress In Russian language, all words have a phonetic stress and sometimes, the same surface forms of a word can have two meanings depending on where the stress goes.The stress is implicit and is not reflected in writing, unlike it is in Spanish, for example.</S>
			<S sid ="171" ssid = "153">Words with the same spelling but different stress are referred to as homographs [10].</S>
			<S sid ="172" ssid = "154">сн´ега снег´а snega (gen.sg.masc) snega(nom.pl.masc) &quot;of snow&quot; &quot;(a lot of) snow&quot; Table 19: Phonetic Stress in Russian Table 19 shows an example of homographs, two words with the same spelling, but a different meaning.</S>
			<S sid ="173" ssid = "155">Russian also has fake homographs.</S>
			<S sid ="174" ssid = "156">They are occurs because of the letter ‘ё’, which always has to be stressed in the word that uses the letter.</S>
			<S sid ="175" ssid = "157">However, in modern literature, the letter is often replaced by ‘е’.</S>
			<S sid ="176" ssid = "158">In these cases, the phonetic stress is still there but it is not reflected in writing, sometimes generating two words with identical spellings but different meanings (refer to section 2.4.2 for word sense ambiguity).</S>
			<S sid ="177" ssid = "159">The example in Table 20 not only demonstrates the importance of the stress of letter ’ё’ but also that two homographs can belong to different parts of speech.</S>
			<S sid ="178" ssid = "160">бер´ёг(verb) б´ерег(noun) berjog (past tense.sg) bereg(nom.sg.masc) &quot;kept&quot; &quot;shore&quot; Table 20: Phonetic Stress in Russian: Fake Homograph 2.3.2 Vocalization One very interesting problem with Arabic text is the lack of vowels.</S>
			<S sid ="179" ssid = "161">In traditional Arabic, there are 3 long vowels that are written as separate letters while their 3 short variants are written as diacritics over the preceding letter.</S>
			<S sid ="180" ssid = "162">However, these diacritics are dropped from most modern Arabic texts and hence the text is left almost vowel-less.</S>
			<S sid ="181" ssid = "163">A statistical study done in [64] shows that only 0.5% of words in newspaper articles have some kind of diacritics.</S>
			<S sid ="182" ssid = "164">One special diacritic, the &quot;Shadda&quot; (which is the consonant gemination marker), accounts for more than 60% of these diacritics.</S>
			<S sid ="183" ssid = "165">It should be noted however that in some contexts, such as any extracts from the Quran or children’s books, the text is usually fully diacritized.</S>
			<S sid ="184" ssid = "166">This lack of vocalization in general-purpose Arabic texts clearly leads to an ambiguity problem; one written morpheme can actually be interpreted in many different ways, each with its own pronunciation, meaning and grammatical category.</S>
			<S sid ="185" ssid = "167">Take for example the sequence &quot;ktb&quot;, [64] lists at least 5 different forms in which this sequence can be interpreted, depending on the pronunciation and the missing short vowels.</S>
			<S sid ="186" ssid = "168">Table 21 lists those forms.</S>
			<S sid ="187" ssid = "169">ktb kataba he wrote kutiba it was written kattaba he made him write kuttiba he was made to write kutub books Table 21: Arabic Vocalization Problem However, this problem, which primarily affects part-of-speech tagging, is made easier by another property of the Arabic language.</S>
			<S sid ="188" ssid = "170">As some clitics only attach to a specific grammatical category (e.g. nouns and not verbs), we can disambiguate the word by the clitics attached to it.</S>
			<S sid ="189" ssid = "171">To continue with the same example, the sequence &quot;Al+ktb&quot; can only be interpreted as &quot;the books&quot; since the definite article &quot;Al&quot; does not attach to verbs.</S>
			<S sid ="190" ssid = "172">2.3.3 Diglossia Arabic exhibits a phenomenon called Diglossia.</S>
			<S sid ="191" ssid = "173">Diglossia is defined by [33] as a phenomenon in which two or more varieties of the same language exist side by side, each being used for specific purposes and in distinct situations.</S>
			<S sid ="192" ssid = "174">In Arabic, at least two such varieties can be distinguished; Modern Standard Arabic (MSA) and the many regional dialects.</S>
			<S sid ="193" ssid = "175">While the dialects are acquired by speakers natively at home and used in everyday life, MSA is learned primarily through formal education and used in more formal situations such as university lectures, news broadcasts and parliamentary debates.</S>
			<S sid ="194" ssid = "176">A more formal definition of diglossia can be found in [64].</S>
			<S sid ="195" ssid = "177">This is different from languages that have different dialects, such as English, where the dialects and the standard languages are only slightly different but are generally governed by the same linguistic rules.</S>
			<S sid ="196" ssid = "178">To further demonstrate, consider Table 22 where the first sentence is written in MSA while the second is in the Levantine Palestinian Dialect (LD) of Arabic [22]: J 11 J 11 J - J lA yu ibu Al+rijAl haðA Al+σamal Al+rijAl bi+ ibw+š Al+šuγil hadA not like the+men this the+work the+men like+not the+work this &quot;Men don’t like this work&quot; Table 22: Arabic Diglossia As simple as the example in Table 22 is, the following differences can be noted: • lexically, the word for &quot;work&quot; is Al+σamal in MSA but Al+šuγil in LD • the negation function word is lA in MSA but it transforms into the clitic š in LD • syntactically, while Arabic is a free-order language, sentences in MSA tend to follow the VSO order while in most dialects, subject-initial sentences (SVO) are more common • the demonstrative determiner for &quot;this&quot; precedes the noun in MSA while it follows it in LD This diglossic situation clearly hampers efforts for automatic processing of Arabic.</S>
			<S sid ="197" ssid = "179">The difference between MSA and the dialects makes it extremely difficult to develop one NLP tool that can be used on all of them.</S>
			<S sid ="198" ssid = "180">Moreover, even the dialects among themselves show significant differences that can be sometimes compared to those among Romance languages.</S>
			<S sid ="199" ssid = "181">2.4 Semantic Properties.</S>
			<S sid ="200" ssid = "182">Semantics is the branch of linguistics concerned with meaning.</S>
			<S sid ="201" ssid = "183">Because the human brain is capable of mapping natural language to meaning with little effort, it may result surprising how difficult it is to automate this task.</S>
			<S sid ="202" ssid = "184">We attempt to illustrate some of the reasons behind this.</S>
			<S sid ="203" ssid = "185">In this section we will not particularly focus on russian and arabic but rather on challenges present in the english language, since state-of-the-art semantic parsing in english is still under work and facing these challenges.</S>
			<S sid ="204" ssid = "186">2.4.1 Multiword expressions A multiword expression (MWE) is a lexeme, composed by a set of lexemes that has properties which cannot be predicted from those of its individual components.</S>
			<S sid ="205" ssid = "187">It is estimated that MWEs occur as frequently in open text as single word expressions[5].</S>
			<S sid ="206" ssid = "188">Because of this, recognizing MWEs is important for improving results obtained when retrieving information.</S>
			<S sid ="207" ssid = "189">Indexing and searching for the individual components of an MWE will most likely result in irrelevant answers, e.g. a search on pop star retrieving results relevant to pop and star separately is most likely not what the user was looking for[1].</S>
			<S sid ="208" ssid = "190">Under the same logic, recognizing MWEs is crucial to obtaining the correct logic representation of a phrase when parsing it.</S>
			<S sid ="209" ssid = "191">In the example he took his clothes off, the correct semantic meaning would be he undressed, however identifying this logic correctly is not possible unless we are aware of the existence of the MWE take off [clothes].</S>
			<S sid ="210" ssid = "192">Despite the interest in identifying MWEs and interpreting the meaning of them, these tasks continue to be a great challenge, since MWEs are structurally very diverse.</S>
			<S sid ="211" ssid = "193">One of the challenges in handling MWEs is that they vary greatly between languages, and even regionally within a same language.</S>
			<S sid ="212" ssid = "194">For example if we wish to search for restaurants that prepare meals intended to be eaten else where, we would search for take out in American English, but for take away in British English, and carry out in Scottish English.</S>
			<S sid ="213" ssid = "195">Another challenge in MWE recognition is that it is common for new MWEs to be created as languages evolve.</S>
			<S sid ="214" ssid = "196">For example, the expression carbon footprint has only become popular since global warming has started concerning humankind.</S>
			<S sid ="215" ssid = "197">This evolution of MWEs make it difficult to keep MWE lexicons up to date.</S>
			<S sid ="216" ssid = "198">A major challenge in MWE recognition is that the components of MWEs must not necessarily appear contiguously in a phrase.</S>
			<S sid ="217" ssid = "199">For example, if we wish to know &quot;Why did Guns n’ Roses kick Steven Adler out?&quot; we are not referring to the literal meaning of the word kick.</S>
			<S sid ="218" ssid = "200">Techniques for identifying MWEs vary for different types of MWEs constructions.</S>
			<S sid ="219" ssid = "201">Some MWE constructions such as noun compounds e.g. air conditioner, are relatively easy to detect, whereas others can be very difficult.</S>
			<S sid ="220" ssid = "202">One type of MWEs which lead to important parsing difficulties are light verb constructions (LVC).</S>
			<S sid ="221" ssid = "203">These are constructions made up of a verb and noun complement, in which the contribution of the verb to the meaning of the expression is relatively small in comparison to that of the complement.</S>
			<S sid ="222" ssid = "204">In some cases the contribution of the verb is so light that the LVC can be paraphrased with the verbal form of the noun complement.</S>
			<S sid ="223" ssid = "205">For example, if we search the phrase &quot;How to give a kiss?&quot; the verb give is not actually the subject of interest, but rather the action of kissing.</S>
			<S sid ="224" ssid = "206">[5].</S>
			<S sid ="225" ssid = "207">The state-of-the-art studies in information extraction and question answering that will be discussed in the following sections do not emphasize or provide details on how MWEs recognition is dealt with.</S>
			<S sid ="226" ssid = "208">Still, it is worthwhile to discuss the challenge of MWE recognition since it has a strong effect on precision of the remaining parsing stages and is still an open research area in Semantic Parsing.</S>
			<S sid ="227" ssid = "209">2.4.2 Ambiguity Ambiguity refers to a word or sentence having more than one possible meaning.</S>
			<S sid ="228" ssid = "210">As mentioned in previous sections, phonological properties such as phonetic stress and vocalization can lead to ambiguity (section 2.3).</S>
			<S sid ="229" ssid = "211">Solving ambiguities is deeply related to semantic parsing in that it requires finding the true meaning of text when there is more than one possible meaning.</S>
			<S sid ="230" ssid = "212">Polysemes Polysemes are words that are particularly difficult to dissambiguate because they are written the same and have different, but related meanings.</S>
			<S sid ="231" ssid = "213">For example, in the question &quot;What country was Napolean born in?&quot;, if we only consider the relation born in we could either be talking about the place of birth or the year of birth.</S>
			<S sid ="232" ssid = "214">In order to disambiguate we need to look at other parts of the sentence.</S>
			<S sid ="233" ssid = "215">Passive Verb Forms In most English sentences with an action verb, the subject performs the action denoted by the verb.</S>
			<S sid ="234" ssid = "216">These sentences are said to be in the active voice.</S>
			<S sid ="235" ssid = "217">For example, in the sentence &quot;John won the gold medal&quot;, the subject, John is the person who performed the action won.</S>
			<S sid ="236" ssid = "218">However, it is possible to change normal order of any active sentence with a direct object so that the subject is no longer active, but is instead, being acted upon by the verb or passive.</S>
			<S sid ="237" ssid = "219">In the phrase &quot;The medal was won by John&quot;, the subject is the medal but is not performing the action.</S>
			<S sid ="238" ssid = "220">It is necessary to recognize these passive verb forms in order to correctly map the meaning of a phrase.</S>
			<S sid ="239" ssid = "221">This is specially true when both the action maker and the object receiving the action are capable of performing the action.</S>
			<S sid ="240" ssid = "222">For example, if we consider the active voice &quot;Ben loves Amanda&quot; and the passive voice &quot;Ben is loved by Amanda&quot;, though they use similar sets of lexemes in a similar order, they actually have quite different meanings.</S>
	</SECTION>
	<SECTION title="Language Resources and Knowledge Bases. " number = "3">
			<S sid ="241" ssid = "1">This chapter is dedicated to available language resources in English, Russian and Arabic.</S>
			<S sid ="242" ssid = "2">By language resources we refer to datasets of language specific information which support language processing.</S>
			<S sid ="243" ssid = "3">The simplest form of a language resource is a dictionary, however, as we will discuss, there are much more complex forms.</S>
			<S sid ="244" ssid = "4">As previously mentioned, most research in NLP has been focused towards the English language.</S>
			<S sid ="245" ssid = "5">However, due to the varying properties of world languages, the techniques used to parse English phrases do not always adapt to other languages.</S>
			<S sid ="246" ssid = "6">However, developing NLP techniques for different languages requires resources which are often unnavailable and costly to generate.</S>
			<S sid ="247" ssid = "7">As an alternative, researchers sometimes attempt to use resources of a similar language for processing.</S>
			<S sid ="248" ssid = "8">Language similarity between two different languages or two dialects of a given language could either benefit or hurt when it comes to the different NLP parsing levels.</S>
			<S sid ="249" ssid = "9">For instance, research shows that some Russian NLP researchers use the Czech language resources to improve their results.</S>
			<S sid ="250" ssid = "10">On the opposite end, Arabic dialects are so divergent from the standard language that they require additional efforts in NLP development.</S>
			<S sid ="251" ssid = "11">3.1 Russian Language Resources.</S>
			<S sid ="252" ssid = "12">Challenges Russian is a language spoken by millions of people, however, the research in NLP is not as advanced as one might think.</S>
			<S sid ="253" ssid = "13">There are several reasons that could account for a slow progress for NLP in the Russian language.</S>
			<S sid ="254" ssid = "14">Research is done by independent groups separately and there is not much communication between such groups.</S>
			<S sid ="255" ssid = "15">According to [16], the groups that inherited the Soviet tradition and the modern commercial laboratories work independently instead of defining a common standard or evaluating the state-of-art of the Russian parsers’ performance.</S>
			<S sid ="256" ssid = "16">The first initiative to increase communication among researchers in Russia and to evaluate the methodology used for Information Retrieval of Cyrillic texts (ROMIP) was launched in 2002.</S>
			<S sid ="257" ssid = "17">Partially due to the ROMIP’s initiative and yearly seminars that finally brought together the researchers, the first Gold Standard Corpus for the Russian language was developed in 2004.</S>
			<S sid ="258" ssid = "18">The initiative continues its work and involves students from the Moscow State University to further unite the research groups and improve the methodology for Russian text IR.</S>
			<S sid ="259" ssid = "19">In 2013, the following challenges have been identified as a part of a ROMIP seminar [27]: • Absence of publicly available Russian text collections • Low interest in creating Russian text collections • Low participation rate of researches in the evaluation initiatives The poverty of resources and more importantly the lack of interest in NLP research for Russian are the main drawbacks for the future of IR for Cyrillic texts.</S>
			<S sid ="260" ssid = "20">Currently the students are being involved in the initiatives to promote the interest, meanwhile, an interesting attempt of sharing resources among similar languages is taking place.</S>
			<S sid ="261" ssid = "21">Sharing Resources Russian and Czech are both Slavic languages and are similar in many grammatical aspects.</S>
			<S sid ="262" ssid = "22">The Czech language is much more developed in terms of the NLP resources freely available for use.</S>
			<S sid ="263" ssid = "23">Recently, it has become common practice for Russian researchers to build parsers for Russian based on Czech resources.</S>
			<S sid ="264" ssid = "24">Such methods will be further discussed in follwing sections of this report, specifically sections 4.2, 4.4, and 4.5.</S>
			<S sid ="265" ssid = "25">Hana et.al. [49] make use of Czech resources for tagging Russian and give the following arguments in terms of why Czech and Russian can use shared resources: • both languages are Slavic (Czech - West Slavonic, Russian - East Slavonic) • both languages have a rich morphology • in both languages the main verb agrees in person and number with the subject; past tense verbs also agree in gender.</S>
			<S sid ="266" ssid = "26">• in both, adjectives and nouns agree in gender, number, and case • both are order free languages • in both, the word order is defined by the discourse Regarding the Russian morphology in comparison with Czech, Russian usually uses less morphological categories because it has six cases, when Czech has seven.</S>
			<S sid ="267" ssid = "27">In addition, Czech uses formal and colloquial forms of the same word.</S>
			<S sid ="268" ssid = "28">Table 23 shows the number of different tags used for tagging Russian text using Czech resources (further described in section 4.2).</S>
			<S sid ="269" ssid = "29">Note that the table has more cases for both languages than those usually described in the language grammar, but those were artificially introduced during research for better automatic morphological analysis.</S>
			<S sid ="270" ssid = "30">For a full table, refer to [49] Description Abbr.</S>
			<S sid ="271" ssid = "31">#values(CZ) #values(RU) POS Detailed POS Gender Number Case P s g n c 12 75 11 6 9 12 32 5 4 8 Table 23: A snippet from Russian and Czech tag comparison 3.2 Arabic Language Resources.</S>
			<S sid ="272" ssid = "32">Since the early 2000s, Arabic has seen increasing interest within the field of NLP.</S>
			<S sid ="273" ssid = "33">Perhaps the first major project to try to improve the quality and quantity of Arabic resources is the Penn Arabic Treebank (PATB) developed by the University of Pennsylvania5 . The first version of this project was released in 2003 and included morphological and syntactic-level annotations for Arabic text.</S>
			<S sid ="274" ssid = "34">Another project was launched around the same time at Charles University in Prague, the Prague Arabic Dependency Treebank (PADT) 6 . While the annotations at the morphological level are the same as in PATB, PADT uses dependency grammar instead of constituency to represent Arabic parsing trees.</S>
			<S sid ="275" ssid = "35">Moreover, they are planning to add an extra level of annotations, called the tectogrammatical level, which tries to capture the underlying syntax reflecting the linguistic meaning of a sentence [48].</S>
			<S sid ="276" ssid = "36">Other treebank projects include the Columbia Arabic Treebank (CATiB), developed by Columbia University7 while Bibliotheca Alexandrina in Egypt maintains a large corpus of Arabic documents without any treebank information8 . Although resources for Arabic are abundant, the diglossic situation of the language creates a problem regarding resources.</S>
			<S sid ="277" ssid = "37">The dialects are hardly ever written and hence there’s a considerable lack of resources when attempting to study them from a computational point of view.</S>
			<S sid ="278" ssid = "38">In most studies done so far on Arabic dialects, transcribed telephone conversations and TV interviews have been used as the main data source [22] [70].</S>
			<S sid ="279" ssid = "39">3.3 Knowledge Bases.</S>
			<S sid ="280" ssid = "40">At a broad level, a knowledge base (KB) is a technology used to store complex structured and unstructured information used by a computer system.</S>
			<S sid ="281" ssid = "41">Knowledge bases are meant to represent facts about the world, and in particular, they are interesting for language processing because they can help understand the meaning of words and relations between known entities.</S>
			<S sid ="282" ssid = "42">In this section we present some of the better known and used KBs in the study of IR.</S>
			<S sid ="283" ssid = "43">• WordNet WordNet is a large freely and publicly available lexical database.</S>
			<S sid ="284" ssid = "44">Information is organized and represented in sets of synonymous words called synsets, each representing one base concept or meaning.</S>
			<S sid ="285" ssid = "45">There are about 117,000 linked synsets9 . The idea behind WordNet is shown in figure 1, where we have two verbs slow and speed and each of these verbs creates a synset with its associated synonyms.</S>
			<S sid ="286" ssid = "46">The two generated synsets are then linked together with the use of an antonym relation (i.e. the two synsets have the opposite meaning).</S>
			<S sid ="287" ssid = "47">Figure 1: WordNet representation 5 http://www.ircs.upenn.edu/arabic/ 6 http://ufal.mff.cuni.cz/padt/PADT_1.0/docs/index.html 7 http://www.ccls.columbia.edu/project/catib 8 http://www.bibalex.org/unl/frontend/Project.aspx?id=9 9 http://wordnet.princeton.edu/ WordNet for other languages was also developed.</S>
			<S sid ="288" ssid = "48">For instance, EuroWordNet includes wordNets for English, Italian, Spanish, German, Dutch, Estonian, French, and Czech [74].</S>
			<S sid ="289" ssid = "49">WordNet can be used for word sense disambiguation (i.e. the use of WordNet to archive disambiguation) as well as for automatic query expansion, where the WordNet semantic relations can be used to expand queries so that document searches are not confined to pattern-matching of query terms but also cover synonyms [74].</S>
			<S sid ="290" ssid = "50">• DBPedia DBPedia 10 is a knowledge base that has been constructed by the automatic extraction of structured information from Wikipedia.</S>
			<S sid ="291" ssid = "51">This is a very powerful resource since Wikipedia has grown into one of the central knowledge sources of mankind, maintained by thousands of contributors collaborating together.</S>
			<S sid ="292" ssid = "52">DBPedia is publicly availbale and allows you to make sophisticated queries against Wikipedia.</S>
			<S sid ="293" ssid = "53">The important advantages that DBPedia offers over other knowledge bases is the large coverage it has over many different domains and the fact that it represents knowledge in many languages.</S>
			<S sid ="294" ssid = "54">DBpedia uses the subject-property-object triples (SPO triples) as a flexible data model for representing extracted information.</S>
			<S sid ="295" ssid = "55">Data from Wikipedia articles is transformed into RDF triples.</S>
			<S sid ="296" ssid = "56">This data is queried using SPARQL, a language with a syntax similar to that of SQL.</S>
			<S sid ="297" ssid = "57">The English version of the DBpedia knowledge base currently describes 4.0 million things, out of which 3.22 million are classified in a consistent ontology, including 832,000 persons, 639,000 places, 372,000 creative works (music albums, films, video games) and 209,000 organizations (companies, educational institutions).</S>
			<S sid ="298" ssid = "58">The full DBpedia data set features labels and abstracts for 12.6 million unique things in 119 different languages.</S>
			<S sid ="299" ssid = "59">The dataset consists of 2.46 billion RDF triples, out of which 470 million were extracted from the English edition of Wikipedia, 1.98 billion were extracted from other language editions, and about 45 million are links to external datasets such as YAGO.</S>
			<S sid ="300" ssid = "60">• Freebase Freebase 11 is a collaborative, freely available knowledge base, licensed by Creative Commons.</S>
			<S sid ="301" ssid = "61">Freebase is structured as a graph; instead of using tables and keys like in conventional databases to define data structures, Freebase defines its data structure as a set of nodes and links that establish relationships between the nodes.</S>
			<S sid ="302" ssid = "62">Because its data structure is non-hierarchical, Freebase can model much more complex relationships between individual elements than a conventional database, and is open for users to enter new objects and relationships into the underlying graph.</S>
			<S sid ="303" ssid = "63">Freebase has over 39 million topics about real-world entities like people, places and things.</S>
			<S sid ="304" ssid = "64">These topics correspond to the nodes in the graph.</S>
			<S sid ="305" ssid = "65">Some topics are notable because they hold a lot of data (e.g., Wal-Mart), and some are notable because they link to many other topics, potentially in different domains of information.</S>
			<S sid ="306" ssid = "66">For example, abstract topics like love and poverty don’t have many properties associated with them but they appear often as subjects to other entities like books or films.</S>
			<S sid ="307" ssid = "67">Any given topic can be seen from many different perspectives, for example, Bob Dylan can be seen as a song writer, singer, performer, book author, and film actor.</S>
			<S sid ="308" ssid = "68">In order to capture this multifaceted nature of topics, Freebase introduces the concept of types.</S>
			<S sid ="309" ssid = "69">Properties define the unique qualities of a given Type.</S>
			<S sid ="310" ssid = "70">Just as properties and topics are grouped into types, types themselves are grouped into domains.</S>
			<S sid ="311" ssid = "71">Domains follow the same logic as sections a newspaper like Business, Life Style, Politics, etc 12 . An example of a Freebase schema representing the fact that William Shakespeare wrote the play Hamlet is written as: William Shakespeare is a type Author has a property WorksWritten with a value Hamlet • Yago2 YAGO is a huge semantic knowledge base, derived from Wikipedia, WordNet and GeoNames.</S>
			<S sid ="312" ssid = "72">10 http://dbpedia.org/About 11 https://www.freebase.com/ 12 https://developers.google.com/freebase/guide/basic_concepts YAGO currently contains more than 10 million entities (persons, organizations, cities, etc.) and more than 120 million facts about these entities 13 . YAGO tries to infer class memberships from rich Wikipedia category names, and has integrated this information with the taxonomic backbone of WordNet so far assigning entities to more than 350,000 classes[51] Like the RDF model YAGO represents facts as triples of subject (S), predicate (P), and object (O) (SPO).</S>
			<S sid ="313" ssid = "73">YAGO2 is the current version of YAGO that combines the entity-relationship-oriented facts with the spatial and temporal dimensions.</S>
			<S sid ="314" ssid = "74">YAGO2 uses the enhanced representation model SPOTL for SPO, Time and Location.</S>
			<S sid ="315" ssid = "75">This model which can coexist with SPO triples, but provides a much more convenient way of browsing and querying the knowledge base.</S>
			<S sid ="316" ssid = "76">This model is further improved by the inclusion of context forming a full representation model SPOTLX tuples (SPO + Time + Location + context)[51] Using the time and space dimensions, as entities, combined with context data users can for example can ask to identify actors who were born in the vicinity (i.e. at most 10 km away from) Berlin after the “German reunification”: Given by the following query: ?p isA actor . ?p wasBornIn ?l nearby Berlin 10.0 . ?p wasBornOnDate ?d after &quot;German reunification&quot; . In this example “wasBornIn” represents the space dimension entity while “wasBornOnDate” represent the time entity.</S>
			<S sid ="317" ssid = "77">Having introduced the main language resources we procede to introduce different language processing levels in the following section.</S>
	</SECTION>
	<SECTION title="Language Processing Levels. " number = "4">
			<S sid ="318" ssid = "1">Natural Language Processing(NLP) techniques are often used in the Information Retrieval (IR) field.</S>
			<S sid ="319" ssid = "2">The benefit of using NLP in IR is nevertheless argued among the researchers who consider that document retrieval is not the perfect application for NLP [18].</S>
			<S sid ="320" ssid = "3">In discussing different NLP techniques such as tokenization, part-of-speech tagging, morphological analysis, named entity recognition, and syntactic parsing we wish to present the reader with ideas of how these processing levels can support document retrieval.</S>
			<S sid ="321" ssid = "4">The following section illustrates the different levels of processing present in NLP and the current state of the art in English language processing.</S>
			<S sid ="322" ssid = "5">We analyze the additional challenges that are present when processing Arabic and Russian.</S>
			<S sid ="323" ssid = "6">4.1 Tokenization.</S>
			<S sid ="324" ssid = "7">Tokenization is a common term in IR and NLP that refers to splitting streams of characters into tokens that are often also called terms [21].</S>
			<S sid ="325" ssid = "8">It is the basis of most, if not all, NLP processes as an early step of processing the input document.</S>
			<S sid ="326" ssid = "9">Tokenization is crucial in bag-of-words approaches to IR since these depend on term matching between queries and documents.</S>
			<S sid ="327" ssid = "10">If a token in the query matches a token in the document, the document is more likely to be relevant [53].</S>
			<S sid ="328" ssid = "11">Normalizing generated tokens into one canonical form is also beneficial for IR systems, despite the risk of decreased precision.</S>
			<S sid ="329" ssid = "12">The normalization can be achieved through stemming or lemmatization that both achieve the task of normalizing tokens that look similar and hold the same meaning into one canonical form [53] It is especially beneficial for inflectional languages like Russian and Arabic, but also morphologically poor languages like English.</S>
			<S sid ="330" ssid = "13">Stemming usually refers to a crude heuristic process that chops off the ends of words and often includes the removal of derivational affixes.</S>
			<S sid ="331" ssid = "14">one of the most popular algorithms for English stemming is Porter’s algorithm, which removes endings based on the longest-match principle [61].</S>
			<S sid ="332" ssid = "15">Lemmatization refers to token normalization with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings and return to the base dictionary form of a word.[61] 13 as defined in :http://www.mpiinf.mpg.de/yagonaga/yago/ 4.1.1 Tokenization in Russian For both English and Russian, as well as most languages that use white spaces to separate words in a sentence, the process of tokenization is rather straightforward.</S>
			<S sid ="333" ssid = "16">The white spaces are used as delimiters to identify the tokens or separate words.</S>
			<S sid ="334" ssid = "17">A slightly more sophisticated approach is to use all non-alphanumerical characters as delimiters during the tokenization process [21].</S>
			<S sid ="335" ssid = "18">Having said that, to- kenization in English and Russian can still be tricky in the case of compounds or words separated by a hyphen, for instance.</S>
			<S sid ="336" ssid = "19">Compounds are a challenge for automatic NLP systems because they often are not listed in lexical sources, therefore recognizing and splitting them can benefit information retrieval along with other tasks [24].</S>
			<S sid ="337" ssid = "20">Even though many languages have compound words, the approach of NLP systems to deal with the issue tends to be language specific.</S>
			<S sid ="338" ssid = "21">Table 11 shows an example of a compound word in Russian (ветрогенератор), that is formed by ’ветер’ and ’генератор’.</S>
			<S sid ="339" ssid = "22">The problem of simply segmenting the compound word into two is that the two obtained tokens would be ’ветрo’ and ’генератор’, where a commonly recognized token, ’ветер’, is different than ’ветро’.</S>
			<S sid ="340" ssid = "23">Noun stem alternation discussed in 2.1.1 makes tokenization of compounds in Russian a challenge.</S>
			<S sid ="341" ssid = "24">An interesting approach of combining statistical features as well as morphological features of a language in order to tokenize compounds was taken by [24].</S>
			<S sid ="342" ssid = "25">A baseline experiment of just splitting while matching tokens to a dictionary was enriched with the usage of the corpus, similarity measures, and language-specific rule sets.</S>
			<S sid ="343" ssid = "26">A list of twelve transformation rules for Russian compounds was created with rules specific for the Russian language morphology.</S>
			<S sid ="344" ssid = "27">The use of the corpus combined with Levenshtein distance and a large rule set that included inflection treatment resulted in 92.24% of splitting precision of Russian compounds.</S>
			<S sid ="345" ssid = "28">Table 24 demonstrates how by the combination of recognizing the word from the corpus (магнитный), then applying one of the inflection rules in the rule-set, and comparing the obtained token with an existing token ’магнит’, we obtain the correct result after splitting a compound in Russian.</S>
			<S sid ="346" ssid = "29">электромагнитный → электро + магнитный rule 11 магнитный → магнитн similarity (магнитн, магнит) = 0.86 result: электромагнитный → электро + магнит Table 24: Splitting Compounds in Russian Stemming in Russian Token normalization by applying stemming has proved to have a positive effect on Information Retrieval results.</S>
			<S sid ="347" ssid = "30">As described in section 2.1.1, Russian inflection is expressed through attaching different inflectional suffixes to stems.</S>
			<S sid ="348" ssid = "31">The approaches to stemming of morphologically complex languages differs from the English language stemmers.</S>
			<S sid ="349" ssid = "32">They require either a corresponding language-specific lexical stemmer, or an algorithmic stemmer.</S>
			<S sid ="350" ssid = "33">Research suggested by [28] describes two new stemming approaches for the Russian language that enhance the result of information retrieval systems.</S>
			<S sid ="351" ssid = "34">The results are then compared to one of the most popular stemmers, Snowball.</S>
			<S sid ="352" ssid = "35">The two approaches are called &quot;light&quot; and &quot;aggressive&quot;.</S>
			<S sid ="353" ssid = "36">The &quot;light&quot; approach used a set of 40 rules to address the properties described in section 2.1.1 and a list of stop words.</S>
			<S sid ="354" ssid = "37">The &quot;aggressive&quot; approach used the same set of rules and the most common derivational suffixes.</S>
			<S sid ="355" ssid = "38">Both stemmers were tested and compared with other approaches, such as Snowball, 4-gram, and no stemmer at all.</S>
			<S sid ="356" ssid = "39">All performed better and improved IR results compared to using no stemmer at all.</S>
			<S sid ="357" ssid = "40">The &quot;light&quot; approach performed best among other stemming strategies even though the difference is not statistically significant [28].</S>
			<S sid ="358" ssid = "41">Moreover, the results of the &quot;light&quot; stemmer increased retrieval performance by 90% in Russian compared to using no stemmer at all, which proves once again the importance of stemming Russian texts as part of the NLP for IR.</S>
			<S sid ="359" ssid = "42">4.1.2 Tokenization in Arabic The main problem in tokenizing Arabic text is the agglutinative nature of the language (see Section 2.1.3).</S>
			<S sid ="360" ssid = "43">While splitting on whitespaces is enough to obtain all tokens in English, the existence of clitics in Arabic requires further treatment of the text in order to obtain all tokens.</S>
			<S sid ="361" ssid = "44">Depending on the application, different schemes of tokenization in Arabic can be used.</S>
			<S sid ="362" ssid = "45">As [42] specifies, a tokenization scheme is defined by the outcome of the tokenization process.</S>
			<S sid ="363" ssid = "46">They range from the most basic scheme where no decliticization is performed, to the most advanced where all of the groups of clitics are split off.</S>
			<S sid ="364" ssid = "47">Table 25 illustrates the 4 most common tokenization schemes and the outcome of each (a full list of all schemes can be found in [42]).</S>
			<S sid ="365" ssid = "48">u u wsynhy Alryˆys jwlth bzyAr A˘lý trkyA and+will+finish the+president tour+his with+visit to Turkey &quot;The president will finish his tour with a visit to Turkey&quot; D0 wsynhy Alryˆys jwlth bzyAr D1 w+synhy Alryˆys jwlth bzyAr D2 w+s+ynhy Alryˆys jwlth b+zyAr A˘lý trkyA A˘lý trkyA A˘lý trkyA D3 w+s+ynhy Al+ryˆys jwlt+h b+zyAr A˘lý trkyA Table 25: Arabic Tokenization Schemes We believe that in IR systems the most common tokenization scheme should be D3 because it splits all clitics and hence gives as much information about the text as possible.</S>
			<S sid ="366" ssid = "49">Another level of tokenization is introduced in [64].</S>
			<S sid ="367" ssid = "50">The author calls it segmentation and distinguishes it from tokenization in that it also includes the splitting of inflectional affixes.</S>
			<S sid ="368" ssid = "51">For example, consider Table 26.</S>
			<S sid ="369" ssid = "52">The At suffix, which denotes feminine plural, is split off in segmentation but is left as part of the base word in the D3 tokenization scheme.</S>
			<S sid ="370" ssid = "53">wkmhndsAtnA and + like + engineers(fem.)</S>
			<S sid ="371" ssid = "54">+ our &quot;and like our female engineers&quot; D3 w+k+mhndsAt+nA Seg.</S>
			<S sid ="372" ssid = "55">w+k+mhnds+At+nA Table 26: Arabic Tokenization Schemes While this segmentation might not have a clear benefit for IR, [64] argues that it is crucial for lemmatization which helps in grouping different forms of the same word under one form, which in turn is beneficial for languages with inflectional morphology such as Arabic.</S>
			<S sid ="373" ssid = "56">E. Mohamed also conducted several experiments in [64] on a corpus extracted from the PATB.</S>
			<S sid ="374" ssid = "57">Instead of directly doing a D3 tokenization, he performed a full segmentation of the words and then removed inflectional segment boundaries to revert words back to their D3 form.</S>
			<S sid ="375" ssid = "58">He achieved accuracy levels of 99.36% but with a marked decrease on unknown words.</S>
			<S sid ="376" ssid = "59">One noteworthy observation in [64] is the effect of automatically vocalizing the text before segmenting it.</S>
			<S sid ="377" ssid = "60">While his vocalization experiments yielded 93.3% accuracy, segmenting the vocalized text saw a drop of accuracy of more than 1 percentage points.</S>
			<S sid ="378" ssid = "61">This indicates that while vocalization might reduce the ambiguity of text, automatic vocalization does not necessarily help other NLP tasks.</S>
			<S sid ="379" ssid = "62">The best results obtained in [64] are marginally better than the results reached a few years earlier by N. Habash in [44].</S>
			<S sid ="380" ssid = "63">While using the same PATB corpora, Habash’s tokenization experiments used a machine learning algorithm (SVM) with a vector of 10 features extracted from a morphological analyzer and reached an accuracy of 99.3%.</S>
			<S sid ="381" ssid = "64">4.2 Part-of-Speech Tagging.</S>
			<S sid ="382" ssid = "65">Part-of-Speech (POS) tagging refers to assigning a part-of-speech to each word in a sentence.</S>
			<S sid ="383" ssid = "66">Some common tags are N (noun), V(verb), ADV (adverb), ADJ (adjective), DET (determinor), etc. However, there are different tagsets that are used among the linguists.</S>
			<S sid ="384" ssid = "67">POS tagging is particularly helpful in IR because it brings the knowledge of different senses in which a word is used.</S>
			<S sid ="385" ssid = "68">For instance, a surface form &quot;object&quot; has two meanings in English: object (N) and object (V).</S>
			<S sid ="386" ssid = "69">If &quot;object&quot; is used in a query or found in a document, the IR system will be able to retrieve more relevant documents knowing which part-of-speech the word belongs to, and, therefore, which meaning it holds [75].</S>
			<S sid ="387" ssid = "70">Part-of-speech challenges are language specific because different languages, based of their properties, have more or less structural ambiguity.</S>
			<S sid ="388" ssid = "71">English, for instance, uses articles, which make noun identification much easier, whereas other languages might not have this advantage [66].</S>
			<S sid ="389" ssid = "72">Despite the obvious effect that the language structure has on POS tagging, currently a lot of work is done in developing POS taggers for multiple languages.</S>
			<S sid ="390" ssid = "73">The results for supervised POS tagging in English have reached approximately 97.3% accuracy and now the focus is on unsupervised techniques as well as multilingual and cross-lingual tagging.</S>
			<S sid ="391" ssid = "74">In order to accomplish such a goal, much research is devoted to developing a universal tagset that can be used for many different languages and achieve state-of-the-art results [69].</S>
			<S sid ="392" ssid = "75">The research done by [69] (2012) suggests that having a universal set of tags does not hurt the accuracy of the results specific to the processing level of part of speech tagging.</S>
			<S sid ="393" ssid = "76">The researchers developed a set of 12 universal tags and mapped them to 25 different treebanks of 22 different languages (for some languages several treebanks were used).</S>
			<S sid ="394" ssid = "77">Mapping some tags was easier than others and there were many ambiguities in the tags across 25 treebanks, however, the universal tagset simplified and combined many categories into one.</S>
			<S sid ="395" ssid = "78">Moreover, when testing the performance of the tagset in a dependency parser, the results outperform all studied previously suggested universal tagsets.</S>
			<S sid ="396" ssid = "79">Table 27 shows the languages studied in this report and their performance with the original tagset from the corresponding treebank, and the performance with the newly developed universal tagset (note, that [69] provides data for all 22 languages).</S>
			<S sid ="397" ssid = "80">Language Source # Tags Original(O) Universal(U) O/U Arabic English Russian PADT/CoNLL07 (Hajic et al., 2004) PennTreebank (Marcus et al., 1993) SynTagRusRNC (Boguslavsky et al., 2002) 21 45 11 96.1 96.7 96.8 96.9 96.8 96.8 97.0 97.7 96.8 Table 27: Performance comparison between original and universal tagsets The ’Original’ tab displays results for training and testing using the original tagset, similarly, the ’Universal’ shows the results for training and testing using the Universal tagset.</S>
			<S sid ="398" ssid = "81">The last column of the table is for the results where training was performed using the original tagset, and testing - using the universal tagset.</S>
			<S sid ="399" ssid = "82">As you can see, the performance for Arabic improves by 0.08%, for Russian - remains the same, and for English only drops 0.06 percent.</S>
			<S sid ="400" ssid = "83">The minor decrease in the English language performance seems even less sufficient considering that the tagset reduced from original 45 to universal 12.</S>
			<S sid ="401" ssid = "84">This comparison was conducted by training a supervised tagger based on the trigram Markov model.</S>
			<S sid ="402" ssid = "85">for the 25 treebanks [69].</S>
			<S sid ="403" ssid = "86">It is important to note that having a universal tagset is a positive initiative, however simplifying the tagset makes tagging a simpler task, hence the improved tagging perfromance.</S>
			<S sid ="404" ssid = "87">Moreover, more fine-grained tagsets are more useful for more advanced processing levels.</S>
			<S sid ="405" ssid = "88">4.2.1 POS Tagging in Russian The challenges for part-of-speech tagging in Russian come from the linguistic properties fusion and syncretism (see section 2.1.2 and 2.3.1).</S>
			<S sid ="406" ssid = "89">These challenges are similar to the ones identified for English, however, the approach to treat them is language-specific: more complex for a language with a complex morphology.</S>
			<S sid ="407" ssid = "90">The benefit of successful POS tagging in the IR field is obvious, since identifying the correct part- of-speech of the word could change the meaning of a user-query dramatically.</S>
			<S sid ="408" ssid = "91">In Russian, if the tagset can only distinguish between the most basic parts of speech without any additional morphological information, there would not be a way to tell apart polysemes, such as the word ’стали’ as a noun formed from ’сталь’ (steel) or as a verb in the past tense, ’стать’ (to become).</S>
			<S sid ="409" ssid = "92">Two sets of completely different documents could be extracted by IR systems depending on the POS tag.</S>
			<S sid ="410" ssid = "93">As a solution to the described problem, Russian taggers have to use some morpho-syntactic features (tense, case, number) in order to identify the part of the speech correctly.</S>
			<S sid ="411" ssid = "94">This approach is referred to as morpho-syntactic tagging, or extended POS tagging.</S>
			<S sid ="412" ssid = "95">According to [34], morpho-syntactic tagging is contextually disambiguous morphological analysis.</S>
			<S sid ="413" ssid = "96">Due to a significant similarity in the process of morpho-syntactic and morphological analysis, both are described together in section 4.4.</S>
			<S sid ="414" ssid = "97">Developing a Russian Tagset One of the biggest challenges of the Russian POS tagging is the resource unavailability as described in section 3, and particularly, the absence of a publicly available extensive tagset and a large annotated corpus.</S>
			<S sid ="415" ssid = "98">Multiple studies have been done using Czech tagsets and annotated corpora before the first syntactically tagged Russian corpus, SynTagRus was developed in 2002 [52], and the first Gold Standard annotated corpus in 2004.</S>
			<S sid ="416" ssid = "99">Research done by [73] proposed an extensive tagset for Russian consisting of 600 tags and resulting in 95 percent accuracy on the disambiguated part of the Russian National Corpus (RNC).</S>
			<S sid ="417" ssid = "100">One of the objectives of this project was to create an appropriate tagset for Russian to avoid disambiguity in part of speech tagging, but also to be able to use the tagset for other similar Slavonic languages.</S>
			<S sid ="418" ssid = "101">That is why the morpho-syntactic features were selected using the MULTEXT-East framework that is suitable for many Slavonic languages.</S>
			<S sid ="419" ssid = "102">Two Russian morphologically annotated corpora had been taken into account when designing the tagset: RNC (137 labels) and HANCO (147 labels).</S>
			<S sid ="420" ssid = "103">The resulting tagset consists of 12 main categories with zero to ten attributes in each, resulting in 156 value-pairs (for more details see [73]).</S>
			<S sid ="421" ssid = "104">Interesting results were obtained after tagging Russian text while using the designed tagset.</S>
			<S sid ="422" ssid = "105">A disambiguated part of RNC which contains around 5 million words was used to train three statistical taggers, while 10% of the corpus were left apart for testing purposes.</S>
			<S sid ="423" ssid = "106">The TnT tagger (an implementation of the Viterbi algorithm for second-order Markov models) performed the best with the state-of-the-art result of 95.28 percent.</S>
			<S sid ="424" ssid = "107">Moreover, some conclusions were made about the tagger’s performance separately for verbs, nouns, and adjectives as well as different tasks, such as POS, gender, number, etc. This experiment proves that nouns and adjectives are the most difficult parts of speech to tag and are particularly challenging in gender and case [73].</S>
			<S sid ="425" ssid = "108">Resource-light Approach to Tagging Russian The taggers using Czech resources result in more moderate results, as described in [49], but they are considered resource-light approaches.</S>
			<S sid ="426" ssid = "109">Resource-light approaches for Russian are interesting since there is a lack of freely available Russian annotated corpora of a large size.</S>
			<S sid ="427" ssid = "110">Instead of using such an extensive resource, a preexisting annotated Czech corpus and an unannotated Russian corpus are used by [49].</S>
			<S sid ="428" ssid = "111">For the evaluation purposes, a small Russian corpus (1,758 words) was manually annotated and used in training and testing of the tagger.</S>
			<S sid ="429" ssid = "112">As a result, only a difference of 6 percent overall was noted between labeling Russian text by the use of a Czech morphological analyzer (73.5 percent) and when training a statistical analyzer directly on a small sample of manually annotated Russian corpus (79.7 percent).</S>
			<S sid ="430" ssid = "113">4.2.2 POS Tagging in Arabic The first question to be considered when developing a part of speech tagger is the tagset to be used.</S>
			<S sid ="431" ssid = "114">Several tagsets have been developed for Arabic including, but not limited to, Buckwalter Tagset, PABT Tagset, PADT Tagset and the Khoja Tagset.</S>
			<S sid ="432" ssid = "115">They mainly differ in the number of categories they include and hence each is suitable for a different type of application.</S>
			<S sid ="433" ssid = "116">[42] discusses in detail the most commonly used tagsets.</S>
			<S sid ="434" ssid = "117">To demonstrate the effects of the tagset, Table 28 lists a few examples of different studies using different tagsets and the results they obtained.</S>
			<S sid ="435" ssid = "118">It should be noted that although it is a common understanding that a smaller tagset improves accuracy, the results in Table 28 may not be directly comparable due to the different techniques and corpora used.</S>
			<S sid ="436" ssid = "119">Study Approach Corpus (# tokens) Tagset # Tags Results Aliwy, 2013 [4] Mohamed, 2010 [65] El Hadj, 2009 [29] Kulick, 2010 [57] Habash, 2005 [44] Master - Slaves Memory-Based Hybrid (Statistical / HMM) Regular Expressions SVM 29 k 500 k 20 k 340 k 120 k N/A HabashRambow N/A Reduced PATB Reduced PATB 3,000 15 13 40 15 90.05% 96.41% 96% 95.1% 98.1% Table 28: Arabic POS Studies with Different Tagsets There have been many attempts to implement part-of-speech taggers for Arabic using traditional methods.</S>
			<S sid ="437" ssid = "120">[65] implemented a Memory-Based POS tagger (MBT) using the PATB tagset and training data and ran experiments on both whole words and automatically segmented ones.</S>
			<S sid ="438" ssid = "121">The results were similar to what the author reached in [64] for tokenization; performing automatic segmentation of words does not improve the accuracy of tagging.</S>
			<S sid ="439" ssid = "122">One interesting observation was that nouns, adjectives and proper nouns were the most confused tags and this confusion resulted in almost 25% of erroneous tags.</S>
			<S sid ="440" ssid = "123">This is due to the nature of the Arabic language in which most proper nouns are directly derived from common nouns and where nouns and adjectives are often interchangeable [65].</S>
			<S sid ="441" ssid = "124">Similarly, [9] uses an approach based on genetic algorithms to tag Arabic text.</S>
			<S sid ="442" ssid = "125">They obtain mediocre results (accuracy of 92%) on a training corpus of over 1 million tokens.</S>
			<S sid ="443" ssid = "126">Other approaches such as a statistical POS tagger ([29]) and a rule-based tagger ([71]) also failed to give results comparable to English POS taggers.</S>
			<S sid ="444" ssid = "127">Hybrid approaches have also been used to tackle the problem of Arabic POS tagging.</S>
			<S sid ="445" ssid = "128">[4] implements a master-slave architecture that uses the output of a Maximum Match tagger (slave) to modify the probabilities of an HMM tagger (master).</S>
			<S sid ="446" ssid = "129">The architecture is also extended to include a Brill tagger as another slave and obtains slightly better result using a self-designed tagset (95.7%).</S>
			<S sid ="447" ssid = "130">In another study, [47] uses a pipeline of a rule-based tagger followed by an HMM tagger where the erroneous output of the former is passed to the latter for correction.</S>
			<S sid ="448" ssid = "131">While they obtained a 97.6% accuracy, their experiment was conducted on Quranic scripts using a limited tagset of 33 tags specifically designed for Classical Arabic (CA).</S>
			<S sid ="449" ssid = "132">To date, the most successful approaches to Arabic POS tagging have been those that perform the task simultaneously with tokenization.</S>
			<S sid ="450" ssid = "133">The approach was implemented by N. Habash in [44] where an SVM classifier was implemented on a set of 10 morphological features to help choose the &quot;most appropriate&quot; tag from a set obtained from a morphological analyzer.</S>
			<S sid ="451" ssid = "134">They obtained a score of 97.5% on a corpus extracted from PATB.</S>
			<S sid ="452" ssid = "135">[35] further developed the previous system by substituting a Conditional Random Fields (CRF) classifier for the SVM used in [44].</S>
			<S sid ="453" ssid = "136">Their aim was to decrease processing time since their version of the system was used as a pre-processing step for a machine translation system.</S>
			<S sid ="454" ssid = "137">We believe that this consideration (processing time) can also be taken into account for IR systems.</S>
			<S sid ="455" ssid = "138">A similar approach was implemented in [57] who attempted to exploit the benefits of performing tokenization and POS tagging together without the use of a morphological analyzer.</S>
			<S sid ="456" ssid = "139">Instead, they used regular expressions to encode text and POS tag possibilities and used two types of matching, text-match and pos-match, on test data.</S>
			<S sid ="457" ssid = "140">They obtained an accuracy of 95.1% and also faced erroneous results when disambiguating the noun and adjective categories.</S>
			<S sid ="458" ssid = "141">Nowadays, two of the most commonly used tools for Arabic tokenization and part of speech tagging are MADA developed at Columbia University [46] and AMIRA developed at Stanford University [26].</S>
			<S sid ="459" ssid = "142">The main differences between the two tools are those of design.</S>
			<S sid ="460" ssid = "143">While MADA performs tokenization and POS tagging together based on the output of a morphological analyzer (similar to [44]), AMIRA incorporates much less linguistic knowledge and does tokenization and POS tagging sequentially based on the forms of the words.</S>
			<S sid ="461" ssid = "144">A detailed description of the tools is available in [46] and [26] respectively while [42] includes a thorough comparison.</S>
			<S sid ="462" ssid = "145">4.3 Named Entity Recognition.</S>
			<S sid ="463" ssid = "146">Named Entity Recognition (NER) is one of the major NLP tasks in IR systems as most user queries include proper names that refer to real-world entities.</S>
			<S sid ="464" ssid = "147">In most western languages, specifically English, NER has been deeply researched and developed.</S>
			<S sid ="465" ssid = "148">The current state-of-the-art in English is focused on refining the categorization of the discovered named entities beyond the 4 main categories; person, location, organization and miscellaneous.</S>
			<S sid ="466" ssid = "149">This is due to the nature of the English language where proper nouns can be relatively easily identified.</S>
			<S sid ="467" ssid = "150">In other languages however, the task can be troublesome as will be discussed in this chapter.</S>
			<S sid ="468" ssid = "151">4.3.1 NER in Russian NER in Russian has not received much attention in the recent years.</S>
			<S sid ="469" ssid = "152">The first attempt to address the issue of NER in Russian was developed as an extension to GATE, a general-purpose NLP platform initially created for English and then extend to a number of other languages.</S>
			<S sid ="470" ssid = "153">It uses six classes: date, person, organization, location, percent, and money.</S>
			<S sid ="471" ssid = "154">The performance reached by GATE was an F-measure of 71 percent, F-measure being a statistical feature that combines precision and recall.</S>
			<S sid ="472" ssid = "155">However, there is no system developed that only focuses on NER recognition in Russian.</S>
			<S sid ="473" ssid = "156">An initiative by [37] introduced two baseline approaches: • a knowledge-based approach with a set of dictionaries and manually-compiled rules; • a statistical approach using a machine-learning system (linear-chain conditional random field or CRF); The knowledge-based approach resulted in the F-measure of 62.17 percent and the statistical approach - 75.05 (above the state-of-art result).</S>
			<S sid ="474" ssid = "157">NER in Russian is definitely less studied than in English, however, considering the results from [37], the approaches used for English could be also relevant to Russian NER.</S>
			<S sid ="475" ssid = "158">This due to the fact that the statistical result, which has not used any specific to Russian grammar rules, performed better than the knowledge-based approach.</S>
			<S sid ="476" ssid = "159">Despite the relatively low results in Russian NER, it is still less challenging than Arabic, where proper names are not capitalized in text.</S>
			<S sid ="477" ssid = "160">4.3.2 NER in Arabic As for NER in Arabic, it faces several problems due mainly to language-specific properties: the absence of capital letters and the inflectional nature of Arabic (which also applies to proper nouns) [8].</S>
			<S sid ="478" ssid = "161">Moreover, given that Arabic is written with its own script, many transliterations of the same foreign noun can be found in texts [72].</S>
			<S sid ="479" ssid = "162">Table 29 shows 4 different ways to write &quot;Los Angeles&quot; in Arabic, all extracted from online newswire articles [72].</S>
			<S sid ="480" ssid = "163">Los Angeles lws Ânjlys lws Ânjlws lws Ânjyls lws Ânjylys Table 29: Different Arabic Transliterations of &quot;Los Angeles&quot; To the best of our knowledge, the most advanced system to tackle the problem of NER in Arabic is ANERSys which was developed in the Universidad Politécnica de Valencia in 2007.</S>
			<S sid ="481" ssid = "164">The first version [8] used an approach based on Maximum Entropy and included the building of a corpus (ANERcorp) and several gazetteers (ANERgazet) compiled from several online resources.</S>
			<S sid ="482" ssid = "165">With a precision of 63.21% and a recall of 49%, the first experiments on the system were not satisfactory.</S>
			<S sid ="483" ssid = "166">The same team followed up with a second version in the same year [6].</S>
			<S sid ="484" ssid = "167">They recognized the main source of errors of the first version as being multi-word NEs and proposed to tackle this problem by performing NER in two steps.</S>
			<S sid ="485" ssid = "168">The first step included recognizing the beginning and ending boundaries of the NEs separately from each other and using POS information to merge the tokens of the multi-word NE.</S>
			<S sid ="486" ssid = "169">After completing that, the second step, which is based on Maximum Entropy as well, would classify the detected NE into one of the main categories (person, location, organization and other).</S>
			<S sid ="487" ssid = "170">The results in these experiments [6] showed a noticeable improvement reaching an F-measure of 65.9%, more than 10 points greater than the first version.</S>
			<S sid ="488" ssid = "171">Further improvement on the system was done in [7] when the Maximum Entropy model was replaced by a Conditional Random Fields (CRF) model.</S>
			<S sid ="489" ssid = "172">The experiments, conducted on ANERcorp (which by then included more than 150,000 annotated tokens), showed further improvement with the F-measure reaching 79.2%.</S>
			<S sid ="490" ssid = "173">The most relevant feature in training the model was found to be the POS tag, followed by the Base-Phrase Chunk (BPC).</S>
			<S sid ="491" ssid = "174">Another system that we would like to overview here is NERA, developed in 2009 by the British University in Dubai [72].</S>
			<S sid ="492" ssid = "175">The system follows a rule-based approach using NE indicators (triggers) to compensate for the lack of capital letters in Arabic script.</S>
			<S sid ="493" ssid = "176">It also includes a gazetteer of NEs, &quot;whitelist&quot;, and a &quot;blacklist&quot; to filter matches that appear after NE indicators but that are not valid NEs.</S>
			<S sid ="494" ssid = "177">Moreover, the system was developed for the aim of being used in information extraction and hence it divides NEs into 10 different categories (e.g. date, phone number, price) [72].</S>
			<S sid ="495" ssid = "178">Their experiments reached F-measures between 85% and 98%, depending on the NE category with the weakest categories detected being company, location and person.</S>
			<S sid ="496" ssid = "179">They followed up their research in 2012 by including a Machine Learning (ML) component into the system [68].</S>
			<S sid ="497" ssid = "180">In this new version, the output of the rule-based component is now fed to an ML component for refinement.</S>
			<S sid ="498" ssid = "181">This ML component included different sets of features for different NE categories with the most common features used being POS tag, the decision of the rule-based component, and several morphological features extracted from MADA [46].</S>
			<S sid ="499" ssid = "182">Three classifiers were trained in order to make a comparison, Decision Trees, SVM and Logistic Regression.</S>
			<S sid ="500" ssid = "183">The experiments were done on several corpora, including ANERCorp developed in [8].</S>
			<S sid ="501" ssid = "184">The obtained results greatly surpassed those of the experiments conducted in [7], reaching an F-measure of 94.4% for the person category.</S>
			<S sid ="502" ssid = "185">Future research is planned on increasing the size of the gazetteers, refining the rules of the rule-based system and experimenting with other classifiers [68].</S>
			<S sid ="503" ssid = "186">4.4 Morphological Analysis.</S>
			<S sid ="504" ssid = "187">Morphological Analysis is done as part of the pipeline of NLP tasks in several applications such as information retrieval and machine translation.</S>
			<S sid ="505" ssid = "188">In an IR context, the most commonly used morphological analysis is lemmatization of words in order to extract roots.</S>
			<S sid ="506" ssid = "189">Moreover, the output of a morphological analyzer can be used to facilitate other NLP tasks such as POS tagging, as explained in Section 4.2.2.</S>
			<S sid ="507" ssid = "190">While this task is fairly complicated in inflectionally poor languages such as English, it is even more so in inflectional and agglutinative languages.</S>
			<S sid ="508" ssid = "191">More precisely, inflectional languages tend to have several stem alternations while the main problem in agglutinative languages is splitting the agglutinated form into its components and lemmatizing each of them.</S>
			<S sid ="509" ssid = "192">4.4.1 Morphological Analysis in Russian Russian NLP is at the stage of developing automatic systems for morphological analysis and detailed POS tagging.</S>
			<S sid ="510" ssid = "193">Developing such systems is difficult due to the rich morphology, high inflection, wordform ambiguity, and stem alternations.</S>
			<S sid ="511" ssid = "194">Due to above listed reason morpho-syntactic tagging uses a tagset of about 600900 tags (different number in different studies) in order to express appropriate morphological features, compared to the 50 tags that are enough to describe English morphology [34].</S>
			<S sid ="512" ssid = "195">Full morphological analysis requires even more resources than morpho-syntactic features because of the stem alternations: many rules have to be developed in order to be able to predict all possible stem alternations for a word.</S>
			<S sid ="513" ssid = "196">Such rules often have poor documented linguistic interpretation, which leaves a lot of ambiguities in a language’s grammatical paradigms [39].</S>
			<S sid ="514" ssid = "197">These rule sets are necessary for a direct approach to morphological analysis.</S>
			<S sid ="515" ssid = "198">The rules are artificial and non-intuitive for native speakers and create a new class for every paradigm in the language: 1000 in Russian and 1500 in Czech [40].</S>
			<S sid ="516" ssid = "199">Due to the large rule set necessary for traditional or direct morphological analysis, a new type of analysis through generation was proposed by [39].</S>
			<S sid ="517" ssid = "200">Morphological analysis through generation is a simpler approach because it uses the morphological models that already exist as part of natural language grammar.</S>
			<S sid ="518" ssid = "201">The number of such for Russian is 40 classes, compared to 1000 artificial classes (a class is refered to a set of flexions that uniquely identify a stem).</S>
			<S sid ="519" ssid = "202">A static method of processing stem morphemes is used in the method.</S>
			<S sid ="520" ssid = "203">Even though stem alternation is a challenge, only about 30 percent of Russian words have alternation, and the maximum alternations a word could have is four.</S>
			<S sid ="521" ssid = "204">Hence, the static approach of saving all possible stem alternations in a dictionary is not going to increase its size significantly [40].</S>
			<S sid ="522" ssid = "205">Analysis through generation is much simpler because instead of coming across with молотк- and having to come up with молоток- it does the inverse.</S>
			<S sid ="523" ssid = "206">In order to identify that молотк- is a form of молоток- several complex rules need to be applied, which are highly unintuitive to the native speaker, and have many exceptions.</S>
			<S sid ="524" ssid = "207">On the other hand, when we have молоток-, by applying a single rule, we know that in genitive singular it changes the stem to молотк-, which makes the task rather straightforward.</S>
			<S sid ="525" ssid = "208">The method works this way: • an input wordform is detached from all the known suffixes and flexions; • the remaining part of the word is considered a stem and checked against the dictionary; • analysis is successful if the stem is found in the dictionary and the detached suffix(es) correspond to the saved grammatical information.</S>
			<S sid ="526" ssid = "209">Since the number of morphological classes is low, the algorithm is much easier to implement than the direct morphological approach [39].</S>
			<S sid ="527" ssid = "210">Detailed POS tagging and Morphological Analysis are important steps for processing of any morphologically rich language, such as Russian.</S>
			<S sid ="528" ssid = "211">These NLP levels also proved to be beneficial for information retrieval.</S>
			<S sid ="529" ssid = "212">Along with stemming, POS tagging and morphological analysis are used for a more efficient way to index documents.</S>
			<S sid ="530" ssid = "213">It is currently difficult to find the state-of-the-art results for morphological analysis effectiveness in IR, however, some results were found for a domain specific study for indexing medical texts [23].</S>
			<S sid ="531" ssid = "214">Medical terminology in Russian is said to be morphologically complex, hence stemming techniques discussed in 4.1.1 are not sufficient.</S>
			<S sid ="532" ssid = "215">Morphological analysis is used to analyze the terms and helps the indexing process for IR.</S>
			<S sid ="533" ssid = "216">The study [23] suggests a 10% improvement in mean average precision of the IR systems that use morphological analysis.</S>
			<S sid ="534" ssid = "217">The study suggests that the improvement in precision depends on the domain of the language, the query length, and the document length, however, can bring great results for specific terminology texts.</S>
			<S sid ="535" ssid = "218">Similar to how the POS tagging is often combined with morphological analysis due to the rich morphology of the Russian language, some researchers propose combining the two with syntactic parsing.</S>
			<S sid ="536" ssid = "219">Such combination of the three NLP levels demonstrates a better performance than the state-of-the-art results for all studied languages, including Russian [15].</S>
			<S sid ="537" ssid = "220">4.4.2 Morphological Analysis in Arabic For many years, morphological analysis in Arabic has been extensively studied since Arabic is a morphologically rich language with a high degree of inflection.</S>
			<S sid ="538" ssid = "221">Several approaches have been used since the early 2000s to build morphological analyzers for Arabic.</S>
			<S sid ="539" ssid = "222">A comprehensive survey of these approaches, categorized by the main technique used, is done in [2].</S>
			<S sid ="540" ssid = "223">In this section, we will review three of the most used morphological analyzers for Arabic, highlighting their key implementation details as well as their strong points and drawbacks.</S>
			<S sid ="541" ssid = "224">It should be noted that while these systems also have morphological generation capabilities, we will not mention them here as it is of little interest to IR systems.</S>
			<S sid ="542" ssid = "225">1.</S>
			<S sid ="543" ssid = "226">BAMA: Buckwalter Arabic Morphological Analyzer.</S>
			<S sid ="544" ssid = "227">BAMA was developed in 2002 by Tim Buckwalter 14 . While the two first releases are publicly available, the third is available through the Linguistic Data Consortium (LDC) [42].</S>
			<S sid ="545" ssid = "228">BAMA is used in both PATB and PADT and is still to date one of the most popular morphological analyzers.</S>
			<S sid ="546" ssid = "229">BAMA is based on a simple assumption that Arabic words are composed of three parts: a) a prefix (04 characters), b) a stem (1-infinite characters), and c) a suffix (06 characters) It uses this assumption to generate all possible segmentations of an input token.</S>
			<S sid ="547" ssid = "230">Afterwards, it uses dictionary lookup to check if the segmented prefix, stem and suffix are valid entries.</S>
			<S sid ="548" ssid = "231">If all three are found to be valid, their morphological categories are used to check if they are compatible together or not.</S>
			<S sid ="549" ssid = "232">BAMA uses a form-based approach, as it depends on the form of the word in the analysis.</S>
			<S sid ="550" ssid = "233">This approach also falls under the Table Lookup Approaches in the categorization proposed by [2].</S>
			<S sid ="551" ssid = "234">Its main advantage is its simplicity of use but it has several drawbacks.</S>
			<S sid ="552" ssid = "235">Most notably, [3] notes that BAMA does not recognize the Arabic broken plural (instead categorizing the words as singular feminine) and confuses the starting &quot;Al&quot; in some words with the Arabic definite article (e.g. AltzAm i ).</S>
			<S sid ="553" ssid = "236">BAMA also only performs stemming without extracting roots as its lookup dictionary contains many forms of the same lemma.</S>
			<S sid ="554" ssid = "237">2.</S>
			<S sid ="555" ssid = "238">MAGEAD: Morphological Analysis and Generation for Arabic and its Dialects.</S>
			<S sid ="556" ssid = "239">N. Habash and O. Rambow proposed in [45] MAGEAD, the first morphological analyzer and generator that addresses the Arabic dialects explicitly as well as the first system to use a linguistic knowledge representation to process multiple variants of the same language family.</S>
			<S sid ="557" ssid = "240">The system was implemented using FSA technology and as such can be used for both analysis and generation.</S>
			<S sid ="558" ssid = "241">In order to achieve this goal, a dictionary lookup cannot be used since including the lexicon of all dialects is virtually impossible.</S>
			<S sid ="559" ssid = "242">Moreover, there is not standard writing convention for the dialects and hence the system cannot depend on the surface form of the words.</S>
			<S sid ="560" ssid = "243">Hence, an abstract lexeme-and-features representation was used in the system.</S>
			<S sid ="561" ssid = "244">In general, the system includes five tiers of representation: a) pattern and affixes, b) root, c) vocalization, d) phonological representation, and e) orthographic representation.</S>
			<S sid ="562" ssid = "245">When analyzing a surface form token, the orthographic representation is converted to a phonological one using a set 14 http://www.qamus.org/morphology.htm of rules (53 for MSA).</S>
			<S sid ="563" ssid = "246">This phonological representation is then vocalized with all possible combinations (a very limited set since Arabic only has 3 short vowels).</S>
			<S sid ="564" ssid = "247">The root is extracted by using another set of rules (69 for MSA) and finally the pattern and affixes are extracted.</S>
			<S sid ="565" ssid = "248">As can be concluded from the above description, adapting this system for use with different Arabic dialects can be achieved easily.</S>
			<S sid ="566" ssid = "249">As a matter of fact, the authors adapted the system for use with Levantine Arabic (LA) to demonstrate its capabilities.</S>
			<S sid ="567" ssid = "250">The main necessary changes were rewriting some (only 2) conversion rules between phonological and orthographic representations and adding two features to the lexeme-and-features representation.</S>
			<S sid ="568" ssid = "251">As reported in [45], the whole conversion was done by a native speaker linguist in 6 hours.</S>
			<S sid ="569" ssid = "252">3.</S>
			<S sid ="570" ssid = "253">Sakhr:.</S>
			<S sid ="571" ssid = "254">This morphological analyzer and generator is developed by Sakhr software company 15 . It performs single word analysis extracting the root, affixes, morphological features and part of speech tag of the surface word [3].</S>
			<S sid ="572" ssid = "255">While it is very popular for use in Arabic NLP solutions, no details about the implementation of their software have been published.</S>
			<S sid ="573" ssid = "256">This product was only included for purposes of demonstrating the increased interest in Arabic NLP.</S>
			<S sid ="574" ssid = "257">4.5 Syntactic Parsing.</S>
			<S sid ="575" ssid = "258">While syntactic parsing is not traditionally thought of as an important NLP task for IR systems, it is increasingly becoming so with the rapid development of parsing engines.</S>
			<S sid ="576" ssid = "259">Syntactic parsing can help improve the accuracy of an IR system by giving information not present in the previously mentioned NLP tasks because understanding the structure of a sentence is the first step towards understanding its meaning.</S>
			<S sid ="577" ssid = "260">Syntactic parsing often combines multiple NLP tasks, such as tokenization, POS taggings, and morphological analysis and provides information about how the words are linked within a sentence.</S>
			<S sid ="578" ssid = "261">As with other sub-fields of NLP, syntactic parsing is generally more advanced and researched in English than in other languages.</S>
			<S sid ="579" ssid = "262">In this section we will discuss the language-specific problems of syntactic parsing and the proposed solutions to overcome them.</S>
			<S sid ="580" ssid = "263">4.5.1 Syntactic Parsing in Russian Syntactic parsing has been addressed by many researchers in the last twenty years.</S>
			<S sid ="581" ssid = "264">Most of the approaches were statistical and when applied multilingually performed rather poorly for morphologically rich languages with a relatively free word order (section 2.2.3), such as Russian.</S>
			<S sid ="582" ssid = "265">In 2006 and 2007, CoNLL shared tasks on multilingual dependency parsing were organized, where the group of morphologically rich languages showed the lowest parsing accuracy among all [15].</S>
			<S sid ="583" ssid = "266">Ever since, much attention has been devoted to studying the reasons of such poor performance and finding ways to improve parsing accuracy.</S>
			<S sid ="584" ssid = "267">According to [15], one possible reason for low parsing accuracy on highly inflective languages is the separation of morphological analysis and syntactic parsing, hence the authors proposed a joint method for morphological analysis and dependency parsing to evaluate parsing accuracy.</S>
			<S sid ="585" ssid = "268">References to other papers that tried similar approaches can be found in [15], like the joint method for part-of-speech tagging and dependency parsing or the morpho-syntactic joint method used in constituency-based statistical parsing.</S>
			<S sid ="586" ssid = "269">The results show that the joint morphological analysis and syntactic parsing, a rule-based morphological analyzer, and word clusters all contribute to an increase of parsing accuracy of all studied languages, including Russian, Hungarian and Finnish (the last two being agglutinative languages like Arabic) [15].</S>
			<S sid ="587" ssid = "270">Russian, specifically, reached an accuracy above the state-of-art at that time in POS tagging, morphological analysis, and lemmatization: 98.9, 95.7, and 96.6 percent respectively [15].</S>
			<S sid ="588" ssid = "271">An evaluation of Russian syntactic parsers took place as a part of RUEVAL forum in 2012 where eight teams (companies or educational institutions) submitted their results for evaluation.</S>
			<S sid ="589" ssid = "272">The evaluation committee identified the following as the main features in Russian parsers [38]: • they are mostly based on dependency tree representations; • they are rule-based; • they have no uniform annotation scheme between the systems; 15 http://www.sakhr.com/index.php/en/ • many formalisms are used to recognize semantic-syntactic relations (in order to reduce the free word order issue); As the result of the evaluation, the two best performing parsers were developed using a manual rule-based approach and a semantic component developed by a team of linguists: ABBYY (precision of 0.952) and ETAP3 (precision of 0.933).</S>
			<S sid ="590" ssid = "273">The average of all eight parsers was 0.888, which is a relatively high result considering the complexity of the Russian language.</S>
			<S sid ="591" ssid = "274">Importantly, the main challenge of parsing the Russian language: relatively free word order, has been overcome by a combinations of including semantic components and integrating statistical approaches into the rule-based systems [38].</S>
			<S sid ="592" ssid = "275">4.5.2 Syntactic Parsing in Arabic Syntactic parsing in Arabic has seen increased interest from the NLP community in the last few years.</S>
			<S sid ="593" ssid = "276">As with other NLP tasks, it suffers from major performance shortages due to the lack of sufficient resources as well as the nature of the Arabic language.</S>
			<S sid ="594" ssid = "277">More precisely, the main problems in parsing Arabic are [41]: • lack of vocalization, • difficulty in distinguishing between nouns and adjectives, • the idafa construct (see Section 2.2.4), • the length of Arabic sentences, which usually exceeds that of English and other European languages.</S>
			<S sid ="595" ssid = "278">[42] names three parsers as the &quot;state-of-art-parsers in parsing Arabic&quot;.</S>
			<S sid ="596" ssid = "279">We will review each of them next.</S>
			<S sid ="597" ssid = "280">1.</S>
			<S sid ="598" ssid = "281">Stanford Arabic Parser 16.</S>
			<S sid ="599" ssid = "282">Stanford parser was implemented in 2002 at the Department of Computer Science at Stanford University.</S>
			<S sid ="600" ssid = "283">The principal model was proposed by D. Klein and C.D. Manning in [55] and is based on the A* parsing algorithm.</S>
			<S sid ="601" ssid = "284">While the first model was based on the constituency grammar developed for the English version of the parser, this proved to be insufficient for Arabic and gave poor results.</S>
			<S sid ="602" ssid = "285">S. Green and C.D. Manning try to improve the performance of the Arabic parser in [41].</S>
			<S sid ="603" ssid = "286">They added language-specific features of Arabic to the parser in order to better parse constructs that do not exist in English.</S>
			<S sid ="604" ssid = "287">Namely, they handled equational sentences and the idafa construct by explicitly tagging sentences as such.</S>
			<S sid ="605" ssid = "288">They ran their experiments on PATB and while their results did not surpass those of Berkeley’s parser, they saw an improvement of more than 5 percentage points over the previous version [41].</S>
			<S sid ="606" ssid = "289">2.</S>
			<S sid ="607" ssid = "290">Bikel’s Parser.</S>
			<S sid ="608" ssid = "291">Daniel Bikel presented this parser in his doctoral dissertation in 2004 [13].</S>
			<S sid ="609" ssid = "292">His main motivation behind building a new parser was that most existing parsers were too dependent on English and on PTB.</S>
			<S sid ="610" ssid = "293">His model is based on constituency grammar with each phrase being divided into a head and a body and offers complete language independence with &quot;language packages&quot; that include language-specific dependency rules.</S>
			<S sid ="611" ssid = "294">His dissertation included experiments on Chinese, Arabic and Portuguese and his parsing of Arabic achieved an accuracy of 72.5% [13].</S>
			<S sid ="612" ssid = "295">His work was followed in 2006 by S. Kulick et.</S>
			<S sid ="613" ssid = "296">al. [58].</S>
			<S sid ="614" ssid = "297">They ran experiments on English using a training set of a similar size to the PATB and still obtained better results; eliminating the argument that the unavailability of Arabic resources was damaging the parser’s performance.</S>
			<S sid ="615" ssid = "298">Moreover, they tried to improve Bikel’s results by handling linguistic phenomena specific to Arabic by improving the tagset mapping from English and handling the idafa construct and equational sentences.</S>
			<S sid ="616" ssid = "299">They managed to improve the results obtained in [13] by almost 6 percentage points using the PATB corpus.</S>
			<S sid ="617" ssid = "300">3.</S>
			<S sid ="618" ssid = "301">MaltParser 17.</S>
			<S sid ="619" ssid = "302">Another approach to language-independent parsing came in 2006 by J. Nivre et.</S>
			<S sid ="620" ssid = "303">al. [67].</S>
			<S sid ="621" ssid = "304">Unlike the first two parsers, this parser is based on dependency grammar.</S>
			<S sid ="622" ssid = "305">It implements a data-driven approach in which the system uses a set of lexical and morphological features to learn a grammar from a treebank and does not need any pre-written grammar rules.</S>
			<S sid ="623" ssid = "306">While the first experiments with MaltParser did not include Arabic, it has since been used in several studies on Arabic parsing.</S>
			<S sid ="624" ssid = "307">[64] implemented a pipeline for processing Arabic text starting from tokenization and ending with a dependency parsing using MaltParser.</S>
			<S sid ="625" ssid = "308">He obtained a parsing accuracy of 73% noting that this can be improved using gold-standard POS tagging.</S>
			<S sid ="626" ssid = "309">It is worth mentioning that [64] names the advantages of using dependency rather than constituency parsing to be that dependency relations are closer to semantic relations and are more suitable to languages with variable word orders such as Arabic.</S>
			<S sid ="627" ssid = "310">Y. Marton et.</S>
			<S sid ="628" ssid = "311">al. also conducted an extensive experiment with MaltParser in [62].</S>
			<S sid ="629" ssid = "312">They configured the parser to use the Columbia Arabic Tree Bank (CATiB) as the treebank source, SVM as the learning algorithm and a set of lexical and morphological features extracted from MADA as the feature model.</S>
			<S sid ="630" ssid = "313">The best obtained results in [62] reached an accuracy of 80.45% and the most beneficial features were determined to be the determiner, number, gender and person features.</S>
			<S sid ="631" ssid = "314">It also found that while the case feature was the most important on gold-standard POS tagging, it actually harmed the performance of the parser with predicted POS tags (since singular nouns inflect for case by changing vocalization only.</S>
			<S sid ="632" ssid = "315">See Section 2.1.1) It should be noted that while MaltParser’s ability to operate on treebanks without handwritten grammars is generally considered as an advantage, it would render it virtually impossible to extend its use on the rarely-written Arabic dialects which suffer from a severe lack of resources.</S>
			<S sid ="633" ssid = "316">4.6 Semantic Parsing.</S>
			<S sid ="634" ssid = "317">Semantic parsing is understanding the logical meaning of natural language phrases.</S>
			<S sid ="635" ssid = "318">This subject is becoming increasingly interesting in the context of question answering and information extraction.</S>
			<S sid ="636" ssid = "319">For these applications, a surface level, bag-of-words oriented NLP, does not suffice.</S>
			<S sid ="637" ssid = "320">It is necessary to derive enough meaning from natural language input in order to obtain precise answers [36].</S>
			<S sid ="638" ssid = "321">First approaches of semantic parsing used highly supervised learning techniques over specific domains.</S>
			<S sid ="639" ssid = "322">In these approaches, large amounts of domain specific labeled data are used to train models capable of parsing input related to the same domain.</S>
			<S sid ="640" ssid = "323">However, these models suffer from two shortcomings: (1) labeled data for training are costly to obtain and (2) modern applications require extracting information from open domains and vast data repositories [31].</S>
			<S sid ="641" ssid = "324">Because of this, research in semantic parsing is now evolving towards less supervised models, capable of parsing open domain phrases.</S>
			<S sid ="642" ssid = "325">4.6.1 Shallow vs. Deep Semantic Parsing In semantic parsing we try to understand the meaning of free text.</S>
			<S sid ="643" ssid = "326">Though syntactic levels of processing help to understand the surface of a phrase, it is only through semantic analysis that we can fully interpret what is being stated or asked for.</S>
			<S sid ="644" ssid = "327">Shallow semantic parsing involves a surface level interpretation of meaning, mostly relating meaning to words or sets of words.</S>
			<S sid ="645" ssid = "328">There are two main tasks associated to semantic parsing: • Text chunking consists of dividing a text in syntactically correlated groups of words.</S>
			<S sid ="646" ssid = "329">Though this task is more related to syntactic parsing, we will discuss it in this section, as it is generally a step towards full semantic parsing.</S>
			<S sid ="647" ssid = "330">The idea is to subdivide sentences into clusters called chunks according to prosodic patterns and pauses in reading.</S>
			<S sid ="648" ssid = "331">The following step is to construct a parse tree where the content words are usually the major heads of phrases.</S>
			<S sid ="649" ssid = "332">The structure of chunks can best be described by a context-free grammar.</S>
			<S sid ="650" ssid = "333">In figure 2 we see an example of chunking of the phrase i.e. [the bold man] [was sitting] [on his suitcase].</S>
			<S sid ="651" ssid = "334">• Semantic Role Labeling aims at assigning a semantic role to constituents of a sentences.</S>
			<S sid ="652" ssid = "335">It involves the detection of the semantic arguments associated with the predicate or verb of a sentence and 17 http://www.maltparser.org/ Figure 2: Shallow parsing: chunking (Extracted from: http://kontext.fraunhofer.de) their classification into their specific roles.</S>
			<S sid ="653" ssid = "336">Recognition of semantic roles for the English language, based on PropBank [50] predicate-argument structures, where given a sentence we analyse the propositions expressed by some target verbs of the sentence.</S>
			<S sid ="654" ssid = "337">Example Semantic Role Labeling “The cat ate the mouse.” The meaning representation of the phrase is given as Ate (cat mouse) Result Ate: verb (action) Cat: Agent (entity performing the action) Mouse: Patient (entity affected by action) In a novel approach to shallow parsing, Collobert et.</S>
			<S sid ="655" ssid = "338">al. propose a technique which combines four NLP tasks that had previously only been considered separately: POS, chunking, Named Entity Recognition and Semantic Role Labeling [25].</S>
			<S sid ="656" ssid = "339">One of the main drawbacks in using parsing techniques in real time search engines is that they tend to be computationally expensive, affecting response times.</S>
			<S sid ="657" ssid = "340">Instead of first extracting a rich set of features from a sentence and then feeding them to a classical shallow classification algorithm, they propose a deep neural network architecture which receives an input sentence and extracts features for words in a first layer and then features for windows of words.</S>
			<S sid ="658" ssid = "341">The features in the layers of the network are automatically trained by backpropagation.</S>
			<S sid ="659" ssid = "342">In their approach, Collobert et.</S>
			<S sid ="660" ssid = "343">al. compare the performance of their so called &quot;almost from scratch&quot; NLP tagger with state-of-the-art techniques for each of the four tagging processes, finding satisfying performance with respect to each of them.</S>
			<S sid ="661" ssid = "344">Deep parsing seeks to provide meaning to a whole sentence, rather than only to parts of it.</S>
			<S sid ="662" ssid = "345">This task is more computationally expensive than shallow parsing, since the second can generally be approached using statistical methods.</S>
			<S sid ="663" ssid = "346">Deep parsing generally makes use of linguistic grammars or sets of syntactic rules which govern the composition of lexical items (words or phrases).</S>
			<S sid ="664" ssid = "347">Linguistic grammars rely basically on two main methods: the unification-based approaches and the lambda-calculus approaches [79].</S>
			<S sid ="665" ssid = "348">4.6.2 Unification-based parsing approaches These approaches rely on representations called feature structures that express valid phrase structural rules between syntactic categories and use lexical entries as their content.</S>
			<S sid ="666" ssid = "349">These rules guarantee the correct unification of all the semantic features with instances that respect the syntactic features.</S>
			<S sid ="667" ssid = "350">Phrase structure rules commonly operate according to the constituency relation, and hence govern constituency grammars, however this is not always the case.</S>
			<S sid ="668" ssid = "351">Unification takes in two feature structures as arguments and returns a merged feature structure if they are compatible, otherwise the return a failure [74].</S>
			<S sid ="669" ssid = "352">Example Unification operator Simple unification operator for performing an equality check.</S>
			<S sid ="670" ssid = "353">[NUMBER PL] u [NUMBER PL] = [NUMBER PL] Success due to same value for NUMBER feature [NUMBER PL] u [NUMBER [] ] = [NUMBER PL] Success due to compatibility of the structures (i.e. unspecified value in one structure, can successfully be matched with any value in the corresponding feature in another structure).</S>
			<S sid ="671" ssid = "354">[NUMBER PL] u [NUMBER SG] = [NUMBER PL] Failure due the NUMBER features of the first and the second structures having incompatible values.</S>
			<S sid ="672" ssid = "355">A particular and commonly used grammar within these approaches is Head-Driven Phrase Structure Grammar (HPSG) in which the formalism is based on lexicalism.</S>
			<S sid ="673" ssid = "356">That means the lexicon is more than just a list of entries; it is in itself richly structured and individual entries are marked with types.</S>
			<S sid ="674" ssid = "357">The IBM Watson project team developed a parser which combines shallow and deep parsing using an HPSG-style grammar.</S>
			<S sid ="675" ssid = "358">This project has received a lot of attention because they have successfully built a parser capable of playing and beating the best human players at Jeopardy!, a quiz competition in which contestants are presented with general knowledge clues in the form of answers, and must phrase their responses in question form.</S>
			<S sid ="676" ssid = "359">This problem requires deep parsing and understanding the subject of the input clues which may not even be obvious for a human reader.</S>
			<S sid ="677" ssid = "360">An example of a Jeopardy!</S>
			<S sid ="678" ssid = "361">clue is &quot;Chandeliers look great but nowadays do not usually use these items from which their name is derived&quot;[63].</S>
			<S sid ="679" ssid = "362">For deep semantic parsing in Watson, a Slot Grammar was used, in which a lexicon associates lexemes with so called complement slot frames which can be complements or adjuncts to the lexeme [63].</S>
			<S sid ="680" ssid = "363">When parsing an input sentence, each word is associated to a list of complement slots, filled in by other words of the sentence.</S>
			<S sid ="681" ssid = "364">This association generates a dependency structure which is interesting since HPSG is generally used to define constituency relations.</S>
			<S sid ="682" ssid = "365">In particular, for verbs, the first slot is filled by the logical subject of the verb, that is, the subject if the verb were in the active form, even if the input grammar is in the passive form.</S>
			<S sid ="683" ssid = "366">This means Watson is able to overcome this aforementioned challenge (see section 2.4.2).</S>
			<S sid ="684" ssid = "367">4.6.3 Lambda-calculus based parsing These approaches use lambda-calculus as the glue to combine semantic representations.</S>
			<S sid ="685" ssid = "368">Lambda-calculus consists of a single transformation rule (variable substitution) and a single function definition scheme.</S>
			<S sid ="686" ssid = "369">The advantage of using lambda-calculus lies in its generality.</S>
			<S sid ="687" ssid = "370">The meanings of individual words and phrases can be arbitrary lambda-expressions, while the final meaning for a sentence can take different forms.</S>
			<S sid ="688" ssid = "371">One of the most frequent grammars used to map natural language to lambda-calculus is Combinatory Categorical Grammar (CCG).</S>
			<S sid ="689" ssid = "372">A CCG grammar includes a lexicon Λ with entries like the following: N ewY ork \- N P ny borders \- (S\N P )/N P : λxλy.next_to(y, x) V ermont \- N P vt where each lexical entry w \- X : h has words w, a syntactic category X , and a logical form h expressed as a lambda-calculus expression.</S>
			<S sid ="690" ssid = "373">CCG combines categories using a set of combinatory rules, for example forward(&gt;) and backward(&lt;) application rules: X/Y : f Y : g → X : f (g) (&gt;) Y : g X \Y : f → X : f (g) (&lt;) These rules provide a relaxed notion of constituency.</S>
			<S sid ="691" ssid = "374">Each step in the parse applies a combinatory rule, until one logical representation of the input is created.</S>
			<S sid ="692" ssid = "375">Figure 3 is an example of a CCG parse using the previous examples for lexicon and combinatory rules [60].</S>
			<S sid ="693" ssid = "376">Figure 3: An example CCG parse obtained from [60] Due to the generality of lambda-calculus approaches, parsing tends to over-generate logical representations given one input sentence.</S>
			<S sid ="694" ssid = "377">Once a set of logical expressions is generated, feature models can be used to select the most accurate representation.</S>
			<S sid ="695" ssid = "378">The parameters of these optimization functions are usually learned through labeled training data.</S>
			<S sid ="696" ssid = "379">There is increasing convergence in methods and objectives between deep, grammar-based parsing and shallow, statistics oriented parsing.</S>
			<S sid ="697" ssid = "380">On the one hand, as data-driven, statistical methods become more sophisticated, they become capable of learning structured representations and multiple layers of annotations more accurately [25].</S>
			<S sid ="698" ssid = "381">On the other hand, many of the current deep systems have statistical components: either as pre- or post-processing to control ambiguity, or as means of acquiring and extending lexical resources, or they even use machine learning techniques to acquire deep grammars automatically[19].</S>
	</SECTION>
	<SECTION title="Open Domain  Information Extraction. " number = "5">
			<S sid ="699" ssid = "1">In this section we will cover state-of-the-art Information Extraction techniques and their relation to Semantic Parsing.</S>
			<S sid ="700" ssid = "2">We describe Information Extraction as the automatic extraction of structured information from unstructured machine-readable documents.</S>
			<S sid ="701" ssid = "3">It is different from Information Retrieval in that the latter retrieves unstructured data satisfying a given need.</S>
			<S sid ="702" ssid = "4">The following citation borrowed from [31] intends to ilustrate the motivation behind Information Extraction: “Ever since its invention, text has been the fundamental repository of human knowledge and understanding.</S>
			<S sid ="703" ssid = "5">With the invention of the printing press, the computer, and the explosive growth of the Web, the amount of readily accessible text has long surpassed the ability of humans to read it.” We have vast amounts of textual information available, but it is in the form of natural language.</S>
			<S sid ="704" ssid = "6">In order to take advantage of this data, we need to find a way to automatically structure it so that we can actually use it to answer precise queries.</S>
			<S sid ="705" ssid = "7">For example, if a search engine is requested to answer a question of the form: &quot;Who was the winner of the 1998 Worldcup&quot;, we need to know that a World Cup is a yearly event, that winners of a World Cup are teams represented by countries, and that France was the country of the 1998 World Cup winning team.</S>
			<S sid ="706" ssid = "8">Typically, Information Extraction (IE) systems are trained through labeled examples which map open text to predefined relations.</S>
			<S sid ="707" ssid = "9">However, this approach does not scale to corpora where the number of target relations is very large and cannot be specified in advance.</S>
			<S sid ="708" ssid = "10">Open Information Extraction (OIE) aims at developing unlexicalized, domain-independant extractors, capable of scaling to Web corpus.</S>
			<S sid ="709" ssid = "11">In a nutshell, OIE systems should be capable of extracting relational tuples of the form (Arg1, P red, Arg2), without requiring any relation-specific training data.</S>
			<S sid ="710" ssid = "12">For example, given the sentence “McCain fought hard against Obama, but finally lost the election”, an Open IE system should extract two tuples, (McCain, fought against, Obama), and (McCain, lost, the election)[31].</S>
			<S sid ="711" ssid = "13">The result of OIE systems are knowledge bases or repositories of structured information such as those presented in section 3.3.</S>
			<S sid ="712" ssid = "14">5.1 Three approaches to Open Information Extraction.</S>
			<S sid ="713" ssid = "15">In this section we will present three state-of-the-art approaches to Open Information Extraction and discuss their contributions and shortcomings.</S>
			<S sid ="714" ssid = "16">5.1.1 Reverb: a constraint based approach to identifying relations [32] Reverb is the name of a program, developed at the University of Washington, that automatically identifies and extracts binary relationships from English sentences 18 . In particular, Reverb extracts triples of the form (Arg1, P red, Arg2) At the moment Reverb was developed, previous OIE systems developed at the University of Washington, TEXTRUNNER[30] and WOE, proceded in three steps: 1.</S>
			<S sid ="715" ssid = "17">Labeling: Heuristics or distant supervision was used over small corpus to learn a classifier which.</S>
			<S sid ="716" ssid = "18">labeled extractions as &quot;trustworthy&quot; or not.</S>
			<S sid ="717" ssid = "19">2.</S>
			<S sid ="718" ssid = "20">Identifying relations: A relation phrase extractor was learned to generate one or more candidate.</S>
			<S sid ="719" ssid = "21">tuples from each sentence, and send each candidate to the classifier, retaining the ones labeled as trustworthy.</S>
			<S sid ="720" ssid = "22">The extractor uses most probable POS, and noun phrase chunking as a pre-processing step.</S>
			<S sid ="721" ssid = "23">Relations are found by examining the text between the noun phrases and heuristically eliminating nonessential phrases, such as prepositional phrases that overspecify an entity (e.g.“Scientists from many universities are studying ...” is analyzed as “Scientists are studying...”).</S>
			<S sid ="722" ssid = "24">3.</S>
			<S sid ="723" ssid = "25">Extracting: The system takes a sentence, identifies a pair of NP arguments (Arg1, Arg2) and.</S>
			<S sid ="724" ssid = "26">uses the learned Extractor to label the words between the two arguments as part of the relation phrase (or not).</S>
			<S sid ="725" ssid = "27">Though these systems performed well, they suffered from incoherent extractions and uninformative extractions.</S>
			<S sid ="726" ssid = "28">• Incoherent extractions: are cases where the extracted relation phrase has no meaningful interpretation.</S>
			<S sid ="727" ssid = "29">In an example from [32] the sentence &quot;The guide contains dead links and omits site&quot; lead to extracting the incoherent relation &quot;contains omits&quot;.</S>
			<S sid ="728" ssid = "30">Incoherent extractions occur because the learned extractor makes a sequence of decisions about whether to include each word in the relation phrase, often resulting in uncomprehensible relations.</S>
			<S sid ="729" ssid = "31">• Uninformative extractions: these are extractions that omit critical information.</S>
			<S sid ="730" ssid = "32">In another example from [32], the sentence “Faust made a deal with the devil” was returning the uninformative relation (F aust, made, a deal).</S>
			<S sid ="731" ssid = "33">This type of error is caused by improper handling of relation phrases that are expressed by a combination of a verb with a noun, such as light verb constructions (see section 2.4.1).</S>
			<S sid ="732" ssid = "34">To overcome these problems, Fader et.</S>
			<S sid ="733" ssid = "35">al. [32] introduced a syntactic constraint.</S>
			<S sid ="734" ssid = "36">They defined a POS tag pattern which the relation phrase is required to match.</S>
			<S sid ="735" ssid = "37">The pattern limits relation phrases to be either a verb (e.g., created), a verb followed immediately by a preposition (e.g., made in), or a verb followed by nouns, adjectives, or adverbs ending in a preposition (e.g., has atomic weight of).</S>
			<S sid ="736" ssid = "38">If there are multiple possible matches in a sentence for a single verb, the longest possible match is chosen.</S>
			<S sid ="737" ssid = "39">If the pattern matches multiple adjacent sequences, we merge them into a single relation phrase (e.g., wants to extend).</S>
			<S sid ="738" ssid = "40">This refinement enabled the model to handle relation phrases containing multiple verbs, but requires relation phrases to be a contiguous sequence of words in the sentence.</S>
			<S sid ="739" ssid = "41">The syntactic constraint implemented in REVERB proved to sometimes match relation phrases which were so specific that they had only a few possible instances.</S>
			<S sid ="740" ssid = "42">One of the examples presented by the authors of REVERB is the sentence &quot;The Obama administration is offering only modest greenhouse gas reduction targets at the conference&quot; in which the relation extracted was &quot;offering only modest greenhouse gas reduction targets at&quot;.</S>
			<S sid ="741" ssid = "43">To avoid extraction of over specific relations, a lexical constraint was introduced to separate valid relation phrases from over-specified relation phrases.</S>
			<S sid ="742" ssid = "44">Lexical constraints were determined statistically and enforced using a large dictionary of relation phrases.</S>
			<S sid ="743" ssid = "45">The intuition behind this constraint is that relations that appear often in a corpus with different arguments, are more likely to be correct relations than those that appear very unfrequently.</S>
			<S sid ="744" ssid = "46">They construct the dictionary of valid relations offline by finding all matches of the POS syntactic pattern in a 18 Downloadable at :http://reverb.cs.washington.edu/.</S>
			<S sid ="745" ssid = "47">corpus of 500 million Web sentences.</S>
			<S sid ="746" ssid = "48">For each relation phrase, its arguments were heuristically identified and finally, the dictionary was constructed as the set of all relation phrases that take at least k distinct argument pairs.</S>
			<S sid ="747" ssid = "49">In order to allow for minor variations in relation phrases, the extracted relations were normalized by removing inflection, auxiliary verbs, adjectives, and adverbs.</S>
			<S sid ="748" ssid = "50">The result was a dictionary of around 1.7 million distinct normalized relation phrases, which is stored in memory at extraction time.</S>
			<S sid ="749" ssid = "51">To summarize, REVERB uses POS tagging and NPchunking as a preprocesing step.</S>
			<S sid ="750" ssid = "52">It then uses semantic and lexical constraints to identify binary, verb-based relations composed of contiguous words inside of sentences.</S>
			<S sid ="751" ssid = "53">Finally it procedes to heuristically identifying the arguments related to the relations, by finding the nearest noun phrases.</S>
			<S sid ="752" ssid = "54">This type of techniques work well for English since it is not an order-free language (section 2.2.3).</S>
			<S sid ="753" ssid = "55">However, languages where subject, verb and object can appear in any order, these techniques which rely on sequences of POS tags may not be very efficient.</S>
			<S sid ="754" ssid = "56">Fader et.al. acknowledge that their constraints represent an idealized model of relation phrases.</S>
			<S sid ="755" ssid = "57">To analyze the type of relations that they fail to recognize using the described constraints, they used a test set of 300 sentences in which all verb-based relationships were manually identified.</S>
			<S sid ="756" ssid = "58">For each relation phrase they checked whether it satisfied the constraints defined by them, and found the results shown in figure 4.</S>
			<S sid ="757" ssid = "59">They found that 15% of the relations violated the constraints.</S>
			<S sid ="758" ssid = "60">Most of the failed cases involved long-range dependencies between words in a sentence.</S>
			<S sid ="759" ssid = "61">Those types of dependencies are not easily representable using patterns over POS tags and would require a deeper syntactic analysis.</S>
			<S sid ="760" ssid = "62">Fader et.</S>
			<S sid ="761" ssid = "63">al. also analyzed the incorrect extractions made by Reverb and noticed that 65% of incorrect extractions were due to incorrect argument identification, rather than relation extraction.</S>
			<S sid ="762" ssid = "64">One common mistake they found (16% of errors) was when trying to extract a relation phrase expressing an nary relationship via a ditransitive verb.</S>
			<S sid ="763" ssid = "65">For example, in the sentence &quot;I gave him 15 photographs&quot; REVERB would extract (I , gave, him).</S>
			<S sid ="764" ssid = "66">This is due to the fact that REVERB only models binary relations.</S>
			<S sid ="765" ssid = "67">Figure 4: Relation phrase compliance with semantic/lexical constraints [32] 5.1.2 DepOE: Using Dependencies to Identify Non-obvious Clauses In an attempt to overcome the shortcomings of the approach taken by Fader et.al. in their system REVERB, Gamallo et.al. propose a dependecy-based approach for extracting verb-based relations[36].</S>
			<S sid ="766" ssid = "68">Moreover, whereas Reverb is an Open Information Extractor for the English language, Gamallo et.al. claim their approach to be multilingual, provided there is access to a dependency grammar for parsing the desired language.</S>
			<S sid ="767" ssid = "69">To further illustrate the advantage of using dependency parsing we will present an example from [36].</S>
			<S sid ="768" ssid = "70">In the sentence “The first commercial airline flight was from St. Petersburg to Tampa in 1914” two or three different relational triples could be extracted, namely (the f irst commercial airline f light, was f rom, St.P etersburg) (the f irst commercial airline f light, was to, T ampa) (the f irst commercial airline f light, was in, 1914) Performing this multiple extraction is quite challenging if we cannot identify that there is some connection between non-contiguous elements in the sentence.</S>
			<S sid ="769" ssid = "71">The OEI method proposed by Gamallo et.al. consists of three steps: • Dependency parsing: This first step is done using DepPattern, a multilingual, robust and fast dependency parser which receives POS tags as input 19 . • Clause constituents: For each parsed sentence, the verb clauses are discovered, and then, for each clause, their participants are identified, as well as the participant functions, i.e. subject, direct object, attribute, and prepositional complements.</S>
			<S sid ="770" ssid = "72">The objective is to transform the dependency path built in the first step into a partial constituency tree, where only the constituents of the clause are selected.</S>
			<S sid ="771" ssid = "73">This is performed as follows: – Given a verb dependency (subj, dobj, vprep, attrib) the dependent lemma of the clause verb is selected, and then all dependent lemmas linked to the target lemma (as a head) are listed using the syntactic dependency path.</S>
			<S sid ="772" ssid = "74">– The verb phrase is built similarly, it contains all dependent lemmas of the verb that are not part of the clause constituents identified before.</S>
			<S sid ="773" ssid = "75">Figure 5: Dependency parsing and clause constituents The function of a constituent inherits the name of the dependent relation linking the clause verb to the head of the constituent.</S>
			<S sid ="774" ssid = "76">Figure 5 shows the dependency parse and constituent clauses obtained for the phrase &quot;The coach of Benefica has held a press conference in Lisbon&quot;.</S>
			<S sid ="775" ssid = "77">• Extraction: The final step of the method is to extract the target triples, by applying simple, grammar-based rules on the clause constituents.</S>
			<S sid ="776" ssid = "78">Figure 6 shows a summary of the main rules applied for extraction.</S>
			<S sid ="777" ssid = "79">The rules applied by Gamallo et.al. only consider verb-based clause triples and only extract one triple per clause.</S>
			<S sid ="778" ssid = "80">However, it is possible to write extraction rules to generate several triples from one clause with many arguments, or to extract triples from other patterns of constituents.</S>
			<S sid ="779" ssid = "81">19 Downloadable at: http://gramatica.usc.es/pln/tools/deppattern.html.</S>
			<S sid ="780" ssid = "82">Figure 6: Relation extraction rules used by Gamallo et.al.[36] When compared to Reverb, the method proposed by Gamallo et.al. did not extract as many triples.</S>
			<S sid ="781" ssid = "83">They attribute this to the fact that that the DepPattern grammars are not complete.</S>
			<S sid ="782" ssid = "84">In particular, the grammars do not consider all types of coordination and do not deal with significant linguistic clausal phenomena such as interrogative, conditional, causal, or adversative clauses.</S>
			<S sid ="783" ssid = "85">DepOE showed to have more precise extractions of the two arguments in triples, in particular of Arg1, since the parser is able to correctly identify the subject.</S>
			<S sid ="784" ssid = "86">Nevertheless, as in Reverb, it produces many truncated Arg2 arguments.</S>
			<S sid ="785" ssid = "87">An example of a truncated Arg2 is in the extracted triple (C ities and towns in Romania, can have, the status) from the sentence “Cities and towns in Romania can have the status either of municipiu or oras”.</S>
			<S sid ="786" ssid = "88">The authors of DepOE conclude that the improvement of the system depends on improving the grammars it is based on.</S>
			<S sid ="787" ssid = "89">Additionally, they found problems concerning the correct identification of arguments due to unefficient Named Entity Recognition.</S>
			<S sid ="788" ssid = "90">Unfortunately, improving grammars can be very labor-intensive and thus, a costly task.</S>
			<S sid ="789" ssid = "91">The same applies for the design of rules for the extraction phase.</S>
			<S sid ="790" ssid = "92">5.1.3 Mapping Natural Language to Knowledge Bases This third approach to OIE, takes advantage of existing knowledge bases to identify known relations or word entities in sentences.</S>
			<S sid ="791" ssid = "93">Bordes et.al.[17] use WordNet (see section 3.3), among other data sources, to map sentences to known WordNet relations and arguments.</S>
			<S sid ="792" ssid = "94">They generate tuples of the form (arg1, relation, arg2) where the arguments correspond to WordNet synsets and the relation corresponds to egdes between synsets in the WordNet graph.</S>
			<S sid ="793" ssid = "95">In a similar mapping approach, Weston et.al. [76] assume that a previous language processing step will recognize the arguments in sentences, and then map the possible &quot;relation mentions&quot; between the identified arguments, to relations from Freebase.</S>
			<S sid ="794" ssid = "96">Bordes et.al. use the efficient, statistics-based Semantic Role Labeling method introduced in section 4.6.1 which allows identifying the role of each set of words in an input sentence.</S>
			<S sid ="795" ssid = "97">They then procede to construct tuples of the form (subject, verb, direct_object) where each element is a lemma.</S>
			<S sid ="796" ssid = "98">Both approaches use an energy-based model which projects words in a low-dimensional vector space and can compare triples based on their similarity in this space.</S>
			<S sid ="797" ssid = "99">Bordes et.al. map their extracted tuples to the generated tuples of WordNet synsets.</S>
			<S sid ="798" ssid = "100">Weston et.al. map possible tuples, formed by conjunctions of words appearing in between two arguments, to known Freebase (see section 3.3) relations with the same two arguments.</S>
	</SECTION>
	<SECTION title="Question Answering. " number = "6">
			<S sid ="799" ssid = "1">In this section we will cover another interesting research topic related to semantic parsing, namely Question Answering (QA).</S>
			<S sid ="800" ssid = "2">We define Question Answering as the automatic search for the answer to a question posed in natural language.</S>
			<S sid ="801" ssid = "3">QA is becoming more and more interesting in our data-driven world, where query engines are now not only expected to propose relevant documents, but also answer specific questions.</S>
			<S sid ="802" ssid = "4">We can observe this tendency in the Google Search Engine, which now includes QA for general questions.</S>
			<S sid ="803" ssid = "5">For example, when querying &quot;who was the first person to climb Mount Everest?&quot;, Google provides an answer showing Mount Everest’s first ascenders, followed by the usual list of relevant URLs, retrieved using IR techniques.</S>
			<S sid ="804" ssid = "6">However, if this question is further specified to &quot;who was the first person to climb Mount Everest without oxygen?&quot;, the search engine is uncapable of mapping it to the correct answer, and only shows the list of related URLs.</S>
			<S sid ="805" ssid = "7">Similarly, if we change the verb climb with the verb ascend, even though they are synonyms, the Google QA engine is uncapable of finding the answer.</S>
			<S sid ="806" ssid = "8">QA is highly related to OIE since most approaches to QA involve applying NLP to an input and mapping it to some repository of structured information.</S>
			<S sid ="807" ssid = "9">As we saw in the previous section, these repositories of structured information are often constructed using Information Extraction techniques.</S>
			<S sid ="808" ssid = "10">Moreover, OIE methods which involve mapping natural language to relations in knowledge bases, like the one presented in section 5.1.3, can also be applied to question answering.</S>
			<S sid ="809" ssid = "11">If we are capable of mapping entities of an input question to a KB relation tuple, and identify a missing argument from the input, that missing argument could be the subject of the question being asked.</S>
			<S sid ="810" ssid = "12">6.1 Main challenges in Question Answering.</S>
			<S sid ="811" ssid = "13">Two main challenges arise in the problem of QA of open domain questions: 1.</S>
			<S sid ="812" ssid = "14">The first challenge is related to the very large number of ways that one relation can be refered to.</S>
			<S sid ="813" ssid = "15">in natural language.</S>
			<S sid ="814" ssid = "16">On one hand it is impossible to have every possible relation represented in a knowledge base, but on the other hand, mapping two relations that are logically the same but are expressed differently can be very challenging.</S>
			<S sid ="815" ssid = "17">For example in the question: &quot;Who starred in Pulp Fiction?&quot;, it is likely that a knowledge base does not contain the relation stars_in but rather acts_in.</S>
			<S sid ="816" ssid = "18">Since the end goal of QA systems is to be able to answer any type of questions made in natural language, solving this mapping is crucial.</S>
			<S sid ="817" ssid = "19">2.</S>
			<S sid ="818" ssid = "20">The second challenge has to do with the fact that users may often pose questions that cannot be an-.</S>
			<S sid ="819" ssid = "21">swered by a single relation in the knowledge base, but rather a composition or an aggregation of a set of relations.</S>
			<S sid ="820" ssid = "22">For example, in the specific question: &quot;Who was the coach when Brasil won the World Cup?&quot; the knowledge base will most likely not contain the relation coach_of _world_cup_winner but rather the relations coach_of and won_world_cup.</S>
			<S sid ="821" ssid = "23">6.2 Approaches to Question Answering.</S>
			<S sid ="822" ssid = "24">In this section we will cover state-of-the-art approaches to question answering, discussing their performance and shortcomings.</S>
			<S sid ="823" ssid = "25">We will address QA in two main trends, one based on semantic parsing and the other on knowledge base exploration.</S>
			<S sid ="824" ssid = "26">All the studied state-of-the-art QA methods are constructed using the knowledge base Freebase, presented in section 3.3.</S>
			<S sid ="825" ssid = "27">6.2.1 QA by Training of a Semantic Parser These methods use knowledge from Freebase, combined with large text corpus to train semantic parsers capable of mapping natural language to logical predicates from the knowledge base.</S>
			<S sid ="826" ssid = "28">There are two tendencies in representing natural language as logic: • CCG parsing CCG parsing approaches tackle the challenge of mapping a sentence to a composition of logical predicates in a knowledge base.</S>
			<S sid ="827" ssid = "29">Since CCGs represent language in a logical form, the mapping from a CCG to predicates in a Knowledge base is rather straightforward.</S>
			<S sid ="828" ssid = "30">1.</S>
			<S sid ="829" ssid = "31">The approach taken by Krishnamurthy et.al. [56] builds a parser using a knowledge base K B,.</S>
			<S sid ="830" ssid = "32">a corpus of dependency-parsed sentences S, a CCG lexicon capable of producing logical forms with predicates from K B and a procedure for identifying mentions of entities from K B in the corpus.</S>
			<S sid ="831" ssid = "33">In the parser, the CCG logical forms are constructed by combining categories, relations and entities from the knowledge base with logical connectives.</S>
			<S sid ="832" ssid = "34">The semantic parser is trained to learn parameters for the CCG that produce correct semantic parses for sentences in the corpus.</S>
			<S sid ="833" ssid = "35">To construct the CCG lexicon which allows producing logical forms with predicates from K B, Krishnamurthy et.al. use the corpus of dependency-parsed sentences and identify entities in those sentences.</S>
			<S sid ="834" ssid = "36">Tuples of the form (e1, e2, s) are extracted where e1 and e2 are entities and s is the sentence where they are found.</S>
			<S sid ="835" ssid = "37">A mapping is then done from the dependency path between e1 and e2, to a set of specified dependency parse patterns.</S>
			<S sid ="836" ssid = "38">Finally these patterns are matched to relations r(e1, e2) known to exist in the knowledge base.</S>
			<S sid ="837" ssid = "39">In this sence, this approach to semantic parsing is similar to the dependency based approach to OIE presented in section 5.1.2.</S>
			<S sid ="838" ssid = "40">The parser is trained to learn the parameters for the CCG using weak supervision.</S>
			<S sid ="839" ssid = "41">On one hand it is assumed that every relation instance r(e1 , e2 ) from K B is expressed by at least one sentence in the corpus.</S>
			<S sid ="840" ssid = "42">On the other hand it is also assumed that the correct semantic parse of a sentence contains a subset of the syntactic dependencies contained in a dependency parse of the corpus S. These two assumptions allow for training a parser capable of correctly mapping the logical meaning of an utterance, without ever having observed labeled data mapping input to logical form.</S>
			<S sid ="841" ssid = "43">Krishnamurthy et.al. were able to train a parser to accurately map the logical meaning of most binary relations and even around 50% of more complex logical forms, involving conjunctions of multiple relations.</S>
			<S sid ="842" ssid = "44">The main limitation of this approach is that they rely on hand-built dependency parse patterns which require costly, manual engineering.</S>
			<S sid ="843" ssid = "45">2.</S>
			<S sid ="844" ssid = "46">Cai &amp; Yates build the semantic parser FreeParser which takes an approach similar to that of.</S>
			<S sid ="845" ssid = "47">Krishnamurthy et.al. but overcome the need for manual specification of rules that construct CCG lexical entries from dependency parses [20].</S>
			<S sid ="846" ssid = "48">They automate the construction of the CCG lexical entries using a self-supervised architecture.</S>
			<S sid ="847" ssid = "49">The way this task is approached is through a sentence retrieval engine which constructs keyword queries to obtain sentences over a large text corpus, that are likely to express the same relationships as those of the object knowledge base (Freebase).</S>
			<S sid ="848" ssid = "50">The way these sentences are identified is by the mapping of entities in the sentence as well as in the Freebase relation.</S>
			<S sid ="849" ssid = "51">These sentences are then mapped to a simple logical form, using only the Freebase relation and the keyword query.</S>
			<S sid ="850" ssid = "52">From the retrieved sentences, those that are too long or that appear to be labeled incorrectly are filtered out, and the remaining are used to train the semantic parser.</S>
			<S sid ="851" ssid = "53">By mapping &quot;relation mentions&quot; to relations from Freebase, based on how often they appear with the same entities, they tackle the challenge of recognizing different representations of a same relations (section 6.1).</S>
			<S sid ="852" ssid = "54">This approach relates to the lexical constraint introduced in Reverb (section 5.1.1).</S>
			<S sid ="853" ssid = "55">3.</S>
			<S sid ="854" ssid = "56">Kwiatkowski et.al. [59] take a different approach than the previous two and train their parser.</S>
			<S sid ="855" ssid = "57">using question-answer pairs rather than weak supervision based on constraints.</S>
			<S sid ="856" ssid = "58">Their parser over-generates possible logical forms to a question and chooses the one with the highest score as the most correct one.</S>
			<S sid ="857" ssid = "59">The features for this score calculation are derived from the training set.</S>
			<S sid ="858" ssid = "60">Generating QA answer pairs requires manual labeling which can be costly.</S>
			<S sid ="859" ssid = "61">In their research, Kwiatkowski et.al. used a set of QA pairs mapping NL questions to Freebase relations which was created and made publicly available by Berant et.al. [11] in their QA studies.</S>
			<S sid ="860" ssid = "62">It was created using the Google Suggest API to obtain one million questions of which a random sample of one hundred thousand were submitted to Amazon Mechanical Turk.</S>
			<S sid ="861" ssid = "63">In a collaborative, manual effort, these questions were mapped to Freebase entities.</S>
			<S sid ="862" ssid = "64">Kwiatskowski et.al. try to overcome the challenge of the very large number of ways that a relation can be expressed by natural language, by introducing an ontology mapping stage in the parsing.</S>
			<S sid ="863" ssid = "65">The parser first constructs a linguistically motivated domain-independent meaning representation using CCG.</S>
			<S sid ="864" ssid = "66">It then uses a learned ontology matching model to transform this representation for the target domain.</S>
			<S sid ="865" ssid = "67">As opposed to the previous two approaches, Kwiatkowsi et.al. do not induce a CCG lexicon, the lexicon is open domain, using no symbols from the ontology.</S>
			<S sid ="866" ssid = "68">The burden of learning word meaning is shifted to the second, ontology matching, stage of parsing.</S>
			<S sid ="867" ssid = "69">The ontology mapping phase not only matches constants (e.g. stars in to acts in), but it also does structure matching.</S>
			<S sid ="868" ssid = "70">Structure matching is done by applying rules that collapse or expand literals to obtain new logical forms.</S>
			<S sid ="869" ssid = "71">An example of collapsing would be mapping z.P ublic(z) Library(z) Of (z, N ewY ork) to the more specific form P ublicLibraryOf N ewY ork.</S>
			<S sid ="870" ssid = "72">Performing collapses on the underspecified logical form allows non-contiguous phrases to be represented in a collapsed form.</S>
			<S sid ="871" ssid = "73">On the other hand, expanding a logical predicate can al low finding relations that can be represented as a conjunction of relations of the ontology (knowledge base).</S>
			<S sid ="872" ssid = "74">For example Freebase does not contain a relation ’daugther’ but this can be represented by expanding to the relations ’female’ and ’child’.</S>
			<S sid ="873" ssid = "75">• Dependency-based compositional semantics 1.</S>
			<S sid ="874" ssid = "76">Berant et.</S>
			<S sid ="875" ssid = "77">at.</S>
			<S sid ="876" ssid = "78">[11] develop SEMPRE20 , a semantic parser for Freebase which is trained using.</S>
			<S sid ="877" ssid = "79">question-answer pairs, like Kwiatkowsi et.al. [59].</S>
			<S sid ="878" ssid = "80">They build upon the idea of Cai &amp; Yates [20] to align sentences to relations based on appearance of same entities.</S>
			<S sid ="879" ssid = "81">In particular, they build a lexicon which maps natural language relations to Freebase relations.</S>
			<S sid ="880" ssid = "82">Instead of using CCG to map sentences to logical representations, they use their constructed lexicon to detect possible mappings of a sentence to a relation.</S>
			<S sid ="881" ssid = "83">To improve the quality of these mappings, they introduce a bridging operation which is capable of deriving new relations from found ones.</S>
			<S sid ="882" ssid = "84">This is important to cover cases where predicates are expressed weakly or implicitly (section 6.1).</S>
			<S sid ="883" ssid = "85">We will use the examples presented by the authors to illustrate this process.</S>
			<S sid ="884" ssid = "86">In the question &quot;What government does Chile have?&quot;, the predicate is expressed by the light verb have.</S>
			<S sid ="885" ssid = "87">In the question &quot;What actors are in Top Gun?&quot; the predicate is expressed in the highly ambiguous preposition in.</S>
			<S sid ="886" ssid = "88">The bridging relation takes mapped relations and is capable of creating logical expressions that may not appear in the lexicon, or that are defined as a conjunction of relations of the knowledge base.</S>
			<S sid ="887" ssid = "89">This operation greatly over-generates the possible representations for a given question, and like Krishnamurthy et.al.[56], they train their model using learned features to select the most appropriate representation.</S>
			<S sid ="888" ssid = "90">To guide the composition of predicates in the bridging stage, they use POS tag features.</S>
			<S sid ="889" ssid = "91">These features can penalize skipping of important elements such as proper nouns, and can promote the conjunction of elements likely to depend on each other.</S>
			<S sid ="890" ssid = "92">For example, the phrase “located” is aligned to the predicate ContainedBy.</S>
			<S sid ="891" ssid = "93">POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position.</S>
			<S sid ="892" ssid = "94">2.</S>
			<S sid ="893" ssid = "95">Berant &amp; Liang [12] develop PARASEMPRE, a system built using paraphrasing to generate.</S>
			<S sid ="894" ssid = "96">possible meanings for a given input.</S>
			<S sid ="895" ssid = "97">Given an input utterance, they first use a simple method to deterministically generate a set of candidate logical forms present in a knowledge base, with a canonical realization in natural language for each.</S>
			<S sid ="896" ssid = "98">Then, a paraphrase model is used to choose the realization that best paraphrases the input.</S>
			<S sid ="897" ssid = "99">Paraphrases are generated using an association model and a vector space model and the models are trained using QA pairs.</S>
			<S sid ="898" ssid = "100">Figure 7 shows a high level comparison between the semantic parsing QA models we have discussed so far.</S>
			<S sid ="899" ssid = "101">Whereas Krishnamurthy et.al. [56], Cai &amp; Yates [20] and Berant et.al. [11] take a direct approach to mapping utterances to logical predicates, Kwiatkowski et.al. [59] and Berant &amp; Liang [12] rely on an intermediate layer to improve mappings between natural language associations and relations explicitly existing in a knowledge base.</S>
			<S sid ="900" ssid = "102">Figure 7: A comparison between QA semantic parsing approaches[12] To construct candidate logical forms for a given utterance the strategy is to find an entity from the knowledge base in the utterence and grow the logical form from that entity.</S>
			<S sid ="901" ssid = "103">This is done following a simple set of templates.</S>
			<S sid ="902" ssid = "104">Then each candidate logical form is mapped to 20 Source code can be downloaded at: http://nlp.stanford.edu/software/sempre/.</S>
			<S sid ="903" ssid = "105">a canonical representation in natural language.</S>
			<S sid ="904" ssid = "106">The mapping of logical predicate to natural language turned out to be fairly simple using the alignment lexicon released by Berant et.al [11].</S>
			<S sid ="905" ssid = "107">The paraphrasing step aims at selecting the canonical representation, generated from the logical forms, which best paraphrases the input utterance.</S>
			<S sid ="906" ssid = "108">The association model aims at determining whether two natural language phrases contain subphrases that are likely to be paraphrases.</S>
			<S sid ="907" ssid = "109">For each pair of NL phrases, each pair of subsets of words is a possible association.</S>
			<S sid ="908" ssid = "110">Associations are primarily detected if they can be found in the PARALEX corpus, a resource containing 18 million pairs of question paraphrases from wikianswers.com, which were tagged as having the same meaning by users.</S>
			<S sid ="909" ssid = "111">Candidate associations were also generated between words sharing the same lemma, the same POS tag, or that are linked through a derivation link on WordNet (see section 3.3).</S>
			<S sid ="910" ssid = "112">The association model relies on having a good set of candidate associations, but mining associations suffers from coverage issues.</S>
			<S sid ="911" ssid = "113">The vector space model tries to overcome this by assigning a vector representation for each utterance, and learning a scoring function that ranks paraphrase candidates.</S>
			<S sid ="912" ssid = "114">They start by constructing vector representations of words.</S>
			<S sid ="913" ssid = "115">Next, they construct a vector for each utterance by averaging the vectors of all content words (nouns, verbs, and adjectives).</S>
			<S sid ="914" ssid = "116">A paraphrase score is the weighted combination of the components of two phrase vectors.</S>
			<S sid ="915" ssid = "117">This approach of using similarity between utterances through representations that are based on the word components relates to the OIE approach proposed by Weston et.al.</S>
			<S sid ="916" ssid = "118">(see section ??)</S>
			<S sid ="917" ssid = "119">in which an energy based model compares relations by representing them with their component words in a low dimensional space.</S>
			<S sid ="918" ssid = "120">The vector space model can identify correct paraphrases in cases where it is hard to directly associate phrases from one phrase to another.</S>
			<S sid ="919" ssid = "121">For example, the answer to “Where is made Kia car?”, is given by the canonical utterance “What city is Kia motors a headquarters of?”.</S>
			<S sid ="920" ssid = "122">The association model does not associate “made” and “headquarters”, but the vector space model is able to determine that these utterances are semantically related[12].</S>
			<S sid ="921" ssid = "123">This paraphrasing approach tackles both challenges presented in section 6.1 since the natural language utterances are produced from the logical form, instead of the other way around.</S>
			<S sid ="922" ssid = "124">6.2.2 QA by Exploration of a Knowledge Base Graph This approach differs from the one presented in the previous section, in that it’s goal is not to understand the whole logic of a given question, but simply to find the answer.</S>
			<S sid ="923" ssid = "125">It views a KB as an interlinked collection of topics.</S>
			<S sid ="924" ssid = "126">For a given question regarding one or several topics, a view over the knowledge base is considered, concerning only the involved topics.</S>
			<S sid ="925" ssid = "127">This view is then explored to inspect all elements within a few hops from the topics detected in the input [78].</S>
			<S sid ="926" ssid = "128">In a comparison done between both approaches [77], it was shown that that they behaved similarly in terms of accurate question answering.</S>
			<S sid ="927" ssid = "129">This approach does not completely ignore parsing.</S>
			<S sid ="928" ssid = "130">They train a dependency parser using QA pairs, that learns to identify a question word, a question focus a question verb, and a question topic.</S>
			<S sid ="929" ssid = "131">Figure 8a shows the parsing of the question &quot;What is the name of Justin Bieber’s brother?&quot;</S>
			<S sid ="930" ssid = "132">In their approach, Yao &amp; Durme [78] aim at finding the most likely relation a question maps to, i.e. P (relation|question).</S>
			<S sid ="931" ssid = "133">This probability is calculated using Naive Bayes, under the assumption of conditional independence between words.</S>
			<S sid ="932" ssid = "134">Although this is a strong assumption which normally does not hold, the approach proved to behave as well as other state-of-the-art involving language processing.</S>
			<S sid ="933" ssid = "135">Figure 8b shows a view over Freebase entities associated to Justin Bieber, to answer the same question regarding his brothers name.</S>
			<S sid ="934" ssid = "136">This approach simplifies language related challenges mentioned in section 6.1 since it only deals with parsing the structure of an input question.</S>
			<S sid ="935" ssid = "137">They assume that input questions for a search engine will generally have a simple, predictable structure and so will not require deep parsing.</S>
			<S sid ="936" ssid = "138">Figure 8: a) Parsing of input sentence[78] b) Freebase graph[78] representation</S>
	</SECTION>
	<SECTION title="Conclusion and Final  Remarks. " number = "7">
			<S sid ="937" ssid = "1">Most research in IR has been oriented towards bag-of-words approaches.</S>
			<S sid ="938" ssid = "2">These approaches are efficient for English content because the English language is morphologically poor.</S>
			<S sid ="939" ssid = "3">However, non-English content is becoming more frequent online, which brings the need for multilingual NLP.</S>
			<S sid ="940" ssid = "4">Developing language-specific systems poses new challenges not faced when dealing with English, since different languages have very different linguistic properties.</S>
			<S sid ="941" ssid = "5">Nonetheless, it is interesting to notice that languages that appear very different, such as Arabic and Russian, can sometimes take advantage of the same types of natural language processing tehniques.</S>
			<S sid ="942" ssid = "6">Moreover, IR systems are tending towards QA systems, that is, users do not want relevant documents but answers to questions.</S>
			<S sid ="943" ssid = "7">The approaches taken to construct QA engines have so far been focused on English, due to the availability of knowledge bases in this language.</S>
			<S sid ="944" ssid = "8">The most popular knowledge base currently used in QA (Freebase) is manually constructed.</S>
			<S sid ="945" ssid = "9">Another approach would be using semantic parsing to automatically extract structured information from the Web; this will make it possible to construct rich knowledge bases for languages other than English.</S>
			<S sid ="946" ssid = "10">While at shallow levels of NLP processing a rich morphology might be an additional challenge, at a semantic level of language processing it can actually help understand the meaning of a sentence.</S>
			<S sid ="947" ssid = "11">For example, the use of cases in Russian can help clearly identify the role of a noun in a sentence, which is an important part of information extraction.</S>
	</SECTION>
</PAPER>
