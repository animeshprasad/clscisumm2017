Entity Extraction via Ensemble Semantics




Marco Pennacchiotti 
Yahoo! Labs 
Sunnyvale, CA, 94089
pennac@yahoo-inc.com


Patrick Pantel 
Yahoo! Labs 
Sunnyvale, CA, 
94089
ppantel@yahoo-inc.com








Abstract

Combining information extraction  sys- 
tems yields significantly  higher quality re- 
sources than each system in isolation. In 
this paper, we generalize such a mixing of 
sources and features in a framework  called 
Ensemble Semantics. We show very large 
gains in entity extraction by combining 
state-of-the-art distributional  and pattern- 
based  systems  with a  large set  of fea- 
tures from a webcrawl,   query logs, and 
Wikipedia.  Experimental results on a web- 
scale extraction of actors, athletes and mu- 
sicians show significantly higher mean av- 
erage precision  scores (29% gain) com- 
pared with the current state of the art.

1   Introduction

Mounting evidence shows that combining infor- 
mation sources and information  extraction algo- 
rithms leads to improvements  in several  tasks 
such as fact extraction  (Pas¸ca et al., 2006), open- 
domain IE (Talukdar et al., 2008), and entailment 
rule acquisition (Mirkin et al., 2006). In this paper, 
we show large gains in entity extraction by com- 
bining state-of-the-art distributional  and pattern- 
based systems  with a large  set of features from 
a 600 million document webcrawl,  one year of 
query logs, and a snapshot of Wikipedia. Further, 
we generalize  such a mixing of sources and fea- 
tures in a framework  called Ensemble Semantics.
  Distributional and pattern-based extraction  al- 
gorithms  capture aspects of paradigmatic and syn- 
tagmatic dimensions of semantics, respectively, 
and are believed to be quite complementary.  Pas¸ca 
et al. (2006) showed that filtering facts, extracted 
by a pattern-based system, according  to their ar- 
guments’ distributional similarity with seed facts 
yielded large precision gains. Mirkin et al. (2006) 
showed similar gains on the task of acquiring lex- 
ical entailment rules by exploring   a supervised


combination of distributional and pattern-based al- 
gorithms using an ML-based SVM classifier.
  This paper builds on the above work, by study- 
ing the impact of various sources of features exter- 
nal to distributional and pattern-based algorithms, 
on the task of entity extraction. Mirkin et al.’s re- 
sults are corroborated  on this task and large and 
significant  gains over this baseline are obtained 
by incorporating 402 features from a webcrawl, 
query logs and Wikipedia.  We extracted candidate 
entities for the classes Actors,  Athletes and Mu- 
sicians from a webcrawl  using a variant  of Pas¸ca 
et al.’s (2006) pattern-based engine and Pantel et 
al.’s  (2009) distributional  extraction system. A 
gradient boosted decision tree is used to learn a re- 
gression function  over the feature space for rank- 
ing the candidate entities. Experimental  results 
show 29% gains (19% nominal) in mean average 
precision over Mirkin et al.’s  method and 34% 
gains (22% nominal) in mean average precision 
over an unsupervised baseline similar to Pas¸ca et 
al.’s method.  Below we summarize the contribu- 
tions of this paper:
• We explore the hypothesis that although dis- 
tributional  and pattern-based algorithms  are 
complementary, they do not exhaust the se- 
mantic space; other sources of evidence can
be leveraged to better combine them;
• We model the mixing of knowledge sources
and features in a novel and general informa-
tion extraction framework  called Ensemble
Semantics; and
• Experiments over an entity extraction task
show that our model achieves large and sig-
nificant gains over state-of-the-art extractors. 
A detailed analysis of feature correlations 
and interactions shows that query log and we- 
bcrawl features yield the highest gains, but 
easily accessible Wikipedia  features also im- 
prove over current state-of-the-art systems.




238

Proceedings of the 2009 Conference on Empirical  Methods in Natural  Language Processing, pages 238–247, 
Singapore, 6-7 August 2009. Qc 2009 ACL and AFNLP


 
Figure 1: The Ensemble Semantics framework  for information extraction.




  The remainder of this paper is organized  as fol- 
lows. In the next section, we present our Ensemble 
Semantics framework and outline how various in- 
formation  extraction  systems can be cast into the 
framework.  Section 3 then presents our entity ex- 
traction  system as an instance of Ensemble Se- 
mantics, comparing and contrasting it with previ- 
ous information  extraction  systems.  Our experi- 
mental methodology  and analysis is described in 
Section 4 and shows empirical  evidence that our 
extractor significantly  outperforms prior art. Fi- 
nally, Section 5 concludes with a discussion  and 
future work.

2   Ensemble Semantics

Ensemble Semantics (ES) is a general framework 
for modeling information extraction  algorithms 
that combine multiple  sources of information and 
multiple extractors. The ES framework allows to:
• Represent multiple sources of knowledge and
multiple extractors of that knowledge;
• Represent multiple  sources of features;
• Integrate both rule-based   and ML-based
knowledge ranking algorithms; and
• Model previous information  extraction sys-
tems (i.e., backwards compatibility).

2.1   ES Framework
ES can be instantiated to extract various types of 
knowledge  such as entities,  facts, and lexical en- 
tailment rules. It can also be used to better under- 
stand the commonalities  and differences between 
existing information extraction systems.
  After presenting the framework in the next sec- 
tion, Section 2.2 shows how previous information 
extraction algorithms can be cast into ES. In Sec- 
tion 3 we describe our novel entity extraction al- 
gorithm  based on ES.
  

The ES framework is illustrated in Figure 1. It 
decomposes the process of information extraction 
into the following components:

Sources (S):   textual repositories of information, 
either  structured  (e.g., a database such as DBpe- 
dia), semi-structured (e.g., Wikipedia Infoboxes or 
HTML tables) or unstructured (e.g., news articles 
or a webcrawl).

Knowledge  Extractors (KE):   algorithms re- 
sponsible for extracting  candidate instances such 
as entities  or facts.  Examples include fact extrac- 
tion systems such as (Cafarella  et al., 2005) and 
entity extraction  systems such as (Pas¸ca, 2007).

Feature Generators (FG):   methods that extract 
evidence (features) of knowledge in order to de- 
cide which candidate  instances  extracted  from 
KEs are correct. Examples include capitalization 
features for named entity extractors,  and the dis- 
tributional similarity matrix used in (Pas¸ca et al.,
2006) for filtering facts.

Aggregator (A).   A module collecting  and as- 
sembling the instances coming from the different 
extractors.   This module  keeps the footprint of 
each instance, i.e. the number and the type of the 
KEs that extracted the instance. This information 
can be used by the Ranker module to build a rank- 
ing strategy, as described below.

Ranker (R):   a module  for ranking the knowl- 
edge instances returned from KEs using the fea- 
tures generated by FGs. Ranking algorithms may 
be rule-based (e.g., the one using a threshold  on 
distributional similarity in (Pas¸ca et al., 2006)) or 
ML-based (e.g., the SVM model in (Mirkin et al.,
2006) for combining  pattern-based and distribu- 
tional features).


  The Ranker is composed of two sub-modules: 
the Modeler and the Decoder. The Modeler is re- 
sponsible for creating the model which ranks the 
candidate instances. The Decoder collects the can- 
didate instances from the Aggregator, and applies 
the model to produce the final ranking.
  In rule-based systems, the Modeler  corresponds 
to a set of manually crafted or automatically in- 
duced rules operating on the features (e.g. a com- 
bination of thresholds). In ML-based systems, it is 
an actual machine learning algorithm,  that takes as 
input a set of labeled training instances, and builds 
the model according to their features. Training in- 
stances can be obtained  as a subset of those col- 
lected by the Aggregator,  or from some  exter- 
nal resource. In many  cases, training instances 
are manually  labeled by human experts, through 
a long and costly editorial process.

  Information  sources (S) serve as inputs  to the 
system. Some sources will serve  as sources for 
knowledge extractors to generate candidate in- 
stances, some will serve as sources for feature gen- 
erators to generate features or evidence of knowl- 
edge, and some will serve as both.

2.2   Related Work

To date, most information  extraction systems rely 
on a model composed of a single source S, a single 
extractor K E and a single feature generator F G. 
For example, many classic relation extraction sys- 
tems (Hearst, 1992; Riloff and Jones, 1999; Pan- 
tel and Pennacchiotti,  2006; Pas¸ca et al., 2006) 
are based on a single pattern-based extractor K E, 
which is seeded with a set of patterns or instances 
for a given relation (e.g. the pattern ‘X starred in 
Y’ for the act-in relation). The extractor then itera- 
tively extracts new instances until a stop condition 
is met. The resulting extractor scores are proposed 
by F G as a feature.  The Ranker  simply consists 
of a sorting function on the feature from F G.
  Systems  such as the above that do not consist 
of multiple  sources, knowledge extractors or fea- 
ture generators are not considered Ensemble Se- 
mantics models, even though they can be cast into 
the framework. Recently,  some researchers have 
explored more complex systems, having multiple 
sources, extractors and feature generators. Below 
we show examples and describe how they map as 
Ensemble Semantics systems. We use this charac- 
terization to clearly outline how our proposed en- 
tity extraction system, proposed in Section 3, dif-


fers from previous work.
  Talukdar et  al.  (2008) present a   weakly- 
supervised  system  for  extracting  large sets  of 
class-instance pairs using two knowledge extrac- 
tors: a pattern-based extractor supported by distri- 
butional evidence, which harvests candidate pairs 
from a Web corpus; and a table extractor that har- 
vests candidates  from Web tables. The Ranker 
uses graph random walks  to combine the informa- 
tion of the two extractors and output the final list. 
The authors show large improvements in coverage 
with little precision loss.
  Mirkin et al. (2006) introduce a machine learn- 
ing system for extracting lists  of lexical entail-
ments (e.g. ‘government’ → ‘organization’). They
rely on two knowledge extractors, operating on a
same large textual source: a pattern-based extrac- 
tor, leveraging the Hearst (1992) is-a patterns; and 
a distributional  extractor applied to a set of entail- 
ment seeds. Candidate  instances are passed to an 
SVM Ranker, which  uses features stemming  from 
the two extractors, to decide which instances are 
output in the final list. The authors report a +9% 
increase in F-measure over a rule-based system 
that takes the union of the instances extracted by 
the two modules.
  Other examples   include the system for 
taxonomic-relation   extraction   by  Cimiano et 
al.  (2005), using a   pool  of  feature genera- 
tors  based    on   pattern-based, distributional 
and WordNet techniques;   and Pas¸ca  and Van 
Durme’s  (2008) system that uses  a Web corpus 
and query logs to extract  semantic  classes and 
their attributes.
  Similarly to these methods, our proposed entity 
extractor (Section 3) utilizes  multiple  sources and 
extractors. A key difference of our method lies in 
the Feature Generator module. We propose sev- 
eral generators resulting in 402 features extracted 
from Web pages, query logs and Wikipedia  arti- 
cles. The use of these features results in dramatic 
performance improvements, reported in Section 4.

3   ES for Entity Extraction

Entity extraction is a fundamental   task in NLP 
responsible for extracting  instances of semantic 
classes (e.g., ‘Brad Pitt’ and ‘Tom Hanks’ are in- 
stances of the class Actors).    It forms a  build- 
ing block for various NLP tasks such as  on- 
tology learning (Cimiano and Staab,  2004) and 
co-reference  resolution  (Mc Carthy and Lehn-



Fa
mi
ly
Ty
pe
Fe
at
ur
es
We
b 
(w)
F
r
e
q
u
e
n
c
y	(wF ) Pattern 	(wP )

Di
st
ri
b
ut
io
n
al	(wD) Termness	(wT )
ter
m 
fre
que
ncy
;  
doc
um
ent 
fre
que
ncy
;  
ter
m 
fre
que
ncy  
as 
nou
n 
phr
as
e
co
nfi
d
e
nc
e 
sc
or
e 
re
tu
rn
e
d 
b
y 
K 
E
pa
t ; 
p
m
i 
wi
th 
th
e 
1
0
0 
m
os
t 
re
lia
bl
e 
p
at
te
rn
s 
us
ed 
b
y 
K 
E
pa
t
di
str
ib
uti
on
al 
si
mi
la
rit
y 
wi
th 
th
e 
ce
ntr
oi
d 
in 
K 
E
di
s ; 
di
str
ib
uti
on
al 
si
mi
la
riti
es 
wi
th 
ea
ch 
se
ed 
in 
S
ra
tio 
be
tw
ee
n 
te
r
m 
fr
eq
ue
nc
y  
as 
no
un 
ph
ra
se 
an
d 
te
r
m 
fr
eq
ue
nc
y;  
p
m
i 
b
et
w
e
e
n 
int
er
n
al 
to
ke
ns 
of 
th
e 
in
st
a
nc
e; 
ca
pit
ali
za
tio
n 
ra
ti
o
Qu
ery 
log 
(q)
Fr
eq
ue
nc
y	(qF )

C
o
-
o
c
c
u
r
r
e
n
c
e	(qC ) Pattern 	(qP )
D
i
s
tr
i
b
u
ti
o
n
a
l	(qD) Termness	(qT )
nu
mb
er 
of 
qu
eri
es 
ma
tchi
ng 
the 
inst
anc
e; 
nu
mb
er 
of 
qu
eri
es 
con
tain
ing 
the 
in-
sta
nc
e
qu
ery 
log 
pm
i 
wit
h 
any 
see
d in 
S
pm
i 
wit
h a 
set 
of 
trig
ger 
wo
rds 
T 
(i.e
., 
the 
10 
wor
ds 
in 
the 
qu
ery 
log
s 
wit
h
hig
hes
t 
pm
i 
wit
h 
S)
dis
trib
uti
on
al 
si
mil
arit
y 
wit
h S 
(ve
cto
r 
coo
rdi
nat
es 
con
sist 
of 
the 
inst
anc
e’s 
pm
i
wit
h 
the 
wor
ds 
in 
T )
rati
o 
bet
we
en 
the 
tw
o 
fre
qu
enc
y 
fea
tur
es 
F
We
b 
tab
le 
(t)
Fr
eq
ue
nc
y	(tF )
Co
-
oc
cur
re
nc
e	(tC )
tabl
e 
fre
qu
en
cy
tabl
e 
pm
i 
wit
h 
S; 
tabl
e 
pm
i 
wit
h 
any 
see
d in 
S
Wi
kip
edi
a 
(k)
Fr
eq
ue
nc
y	(kF )
C
o
-
o
c
c
u
rr
e
n
c
e	(kC ) Distributional	(kD)
ter
m 
fre
qu
en
cy
pm
i 
wit
h 
any 
see
d in 
S
dist
rib
uti
on
al 
si
mil
arit
y 
wit
h S
Table 1: Feature space describing  each candidate instance (S indicates the set of seeds for a given class).




ert, 2005). Search engines such as Yahoo,  Live, 
and Google  collect large sets of entities (Pas¸ca,
2007; Chaudhuri et al., 2009) to better interpret 
queries (Tan and Peng, 2006),  to improve query 
suggestions (Cao et al., 2008) and to understand 
query intents (Hu et al., 2009). Entity extraction 
differs from the similar task of named entity ex- 
traction, in that classes are more fine-grained  and 
possibly overlapping.
  Below, we propose a new method for entity ex- 
traction built on the ES framework  (Section 3.1). 
Then, we comment on related work in entity ex- 
traction (Section 3.2).

3.1   ES Entity Extraction Model
In this section,  we propose  a  novel entity ex- 
traction model following the Ensemble Semantics 
framework  presented in Section 2. The sources of 
our systems can come from any textual corpus. In 
our experiments (described in Section 4.1), we ex- 
tracted entities from a large crawl of the Web, and 
generated features from this crawl as well as query 
logs and Wikipedia.

3.1.1  Knowledge extractors
Our system relies on two knowledge extractors:
one pattern-based and the other distributional.

Pattern-based extractor (K Epat).   We reimple- 
mented Pas¸ca et al.’s (2006) state-of-the-art web- 
scale fact extractor,  which,  given seed instances of 
a binary relation,  finds instances of that relation. 
We extract entities of a class, such as Actors,  by 
instantiating typical relations involving that class


such as act-in(Actor, Movie). We instantiate such 
relations instead of the classical is-a patterns since 
these have been shown to bring in too many false 
positives,  see (Pantel and Pennacchiotti, 2006) for 
a discussion  of such generic patterns. The extrac- 
tor’s confidence score for each instance is used by 
the Ranker to score the entities being extracted. 
Section 4.1 lists the system parameters we used in 
our experiments.
Distributional extractor (K Edis).   We use Pan- 
tel et al.’s  (2009) distributional entity extractor. 
For each noun in our source corpus, we build a 
context vector consisting of the noun chunks pre- 
ceding and following the target noun, scored us- 
ing pointwise mutual information (pmi).  Given
a small set of seed entities  S of a class,  the ex-
tractor computes the centroid of the seeds’ context
vectors as a geometric  mean, and then returns all 
nouns whose similarity with the centroid exceeds a 
threshold τ (using the cosine measure between the 
context vectors). Full algorithmic  details are pre- 
sented in (Pantel et al., 2009). Section 4.1 lists the 
threshold and text preprocessing algorithms  used 
in our experiments.
  The Aggregator simply takes a union of the 
en- tities discovered by the two extractors.

3.1.2  Feature generators
Our  model includes four  feature generators, 
which compute  a total of 402 features (full set 
described in Table 1).   Each generator  extracts 
from  a specific source a feature family,  as follows:
• Web (w): a body  of 600 million 
documents


crawled from the Web at Yahoo! in 2008;
• Query logs  (q):  one year of web search
queries issued to the Yahoo!  search engine;
• Web tables: all HTML inner tables extracted
from the above Web source; and
• Wikipedia:  an official Wikipedia dump from
February 2008, consisting of about 2 million
articles.

  Feature families are further subclassified into 
five  types: frequency (F) (frequency-based  fea- 
tures); co-occurrence (C) (features capturing first 
order co-occurrences  between  an instance  and 
class seeds); distributional (D) (features based on 
the distributional similarity between an instance 
and class  seeds);  pattern (P) (features  indicat- 
ing class-specific  lexical pattern  matches);  and 
termness (T) (features  used to distinguish well- 
formed  terms such as ‘Brad Pitt’ from ill-formed
ones such as ‘with Brad Pitt’). The seeds S used
in many of the feature families are the same seeds
used by the K Epat  extractor,  described in Sec- 
tion 3.1.1.
  The different seed families are designed to cap- 
ture different semantic aspects: paradigmatic (D), 
syntagmatic (C and P), popularity (F), and term 
cohesiveness (T).

3.1.3  ML-based Ranker
Our Modeler  adopts a supervised ML regression 
model. Specifically, we use a Gradient Boosted 
Decision Tree regression model - GBDT (Fried- 
man, 2001), which consists of an ensemble of de- 
cision trees, fitted in a forward  step-wise manner 
to current residuals. Friedman (2001) shows that 
by drastically easing the problem of overfitting on 
training data (which is common in boosting al- 
gorithms), GDBT competes with state-of-the-art 
machine learning algorithms  such as SVM (Fried- 
man, 2006) with much smaller resulting models 
and faster decoding time.  The model is trained 
on a manually  annotated random sample of enti- 
ties taken from the Aggregator, using the features 
generated by the four generators presented in Sec- 
tion 3.1.2. The Decoder then ranks each entity ac- 
cording to the trained model.

3.2   Related Work
Entity extraction  systems follow two main ap- 
proaches:  pattern-based and distributional. The 
pattern-based approach leverages lexico-syntactic 
patterns to extract instances of a given class. Most


commonly  used are is-a pattern families such as 
those first proposed by Hearst (1992) (e.g., ‘Y such 
as X’ for matching ‘actors  such as Brad Pitt’). 
Minimal supervision is used in the form of small 
sets of manually provided seed patterns or seed in- 
stances.  This approach is very common in both 
the NLP and Semantic Web communities  (Cimi- 
ano and Staab, 2004; Cafarella et al., 2005; Pantel 
and Pennacchiotti, 2006; Pas¸ca et al., 2006).
  The distributional approach uses contextual ev- 
idence to model the instances of a  given class, 
following the distributional hypothesis  (Harris,
1964). Weakly  supervised, these methods take a 
small set of seed instances (or the class label) and 
extract new instances from noun phrases that are 
most similar to the seeds (i.e., that share similar 
contexts). Following Lin (1998), example sys- 
tems include Fleischman and Hovy (2002), Cimi- 
ano and Volker (2005), Tanev and Magnini (2006), 
and Pantel et al. (2009).

4   Experimental Evaluation

This section reports our experiments, showing the 
effectiveness of our entity extraction  system and 
the importance of our different feature families.

4.1   Experimental Setup

Evaluated classes.  We evaluate our system over 
three classes:  Actors  (movie, tv and stage  ac- 
tors); Athletes (professional  and amateur); Musi- 
cians (singers, musicians, composers, bands, and 
orchestras)

System setup. We instantiated our knowledge 
extractors, K Epat  and K Edis  from Section 3.1.1, 
over our Web crawl of 600 million documents (see 
Section 3.1.2). The documents were preprocessed 
using Brill’s POS-tagger (Brill, 1995) and the Ab- 
ney’s chunker (Abney, 1991). For K Edis, context 
vectors are extracted for noun phrases recognized 
as NP-chunks  with removed modifiers. The vec- 
tor space includes  the 250M most frequent noun 
chunks in the corpus.  K Edis  returns as instances 
all noun phrases having a similarity with the seeds’
centroid above τ  = 0.0051. The sets of seeds S
for K Edis  include 10, 24 and 10 manually chosen
instances for respectively the Actors, Athletes and
Musicians classes2. The sets of seeds P for K Epat

1 Experimentally set on an independent  development  set.
     2 The higher number of seeds for Athletes is chosen to 
cover different sports.



Syst
em
A
c
t
o
r
s
A
P
	Cov
A
t
h
l
e
t
e
s
A
P
	Cov
M
u
si
ci
a
n
s
A
P	Cov
B1
B2
B3
B4
0.
72
9	51.2%
0.
61
8	64.1%
0.
67
6	100%
0.
71
5	100%
0.6
16	66.1%
0.6
87	39.5%
0.6
64	100%
0.6
97	100%
0.5
70	88.1%
0.6
81	17.2%
0.5
76	100%
0.5
79	100%
ES-
all
0.8
60‡   
10
0%
0.9
15‡   
10
0%
0.78
8‡   
100
%



Table 2: Number of extracted instances and the 
sample sizes (P and N indicate positive and neg- 
ative annotations).


include 11, 8 and 9 pairs respectively for the Ac- 
tors (relation acts-in), Athletes (relation plays-for) 
and Musicians (relation part-of-band) classes. Ta- 
ble 6 lists all seeds for both K Edis  and K Epat. 
The GBDT ranker uses an ensemble of 300 trees.3

Goldset Preparation. The number of instances 
extracted  by K Epat   and K Edis   for each class 
is reported in Table 2.  For each class,  we ex-
tract a random  sample R of 500 instances from 
K Epat ∪ K Edis. A pool of 10 paid expert editors 
annotated the instances of each class in R as posi-
tive or negative. Inter-annotator overlap was 0.88. 
Uncertain instances were manually adjudicated by 
a separate paid expert editor, yielding  a gold stan- 
dard dataset for each class.

Evaluation Metrics. Entity extraction perfor- 
mance is evaluated using the average precision 
(AP) statistic, a  standard   information retrieval 
measure  for evaluating  ranking algorithms,  de- 
fined as:
 |L|



Table 3:  Average precision  (AP) and coverage

(Cov) results for our proposed system ES-all and 
the baselines. ‡  indicates AP statistical signifi-
cance at the 0.95 level wrt all baselines.


ES-all. Our ES system, using K Epat and K Edis, 
the full set of feature families described in 
Section 3.1.2, and the GBDT ranker.
B1.		K Epat   alone, a state-of-the-art  pattern- 
based extractor reimplementing (Pas¸ca et al.,
2006), where the Ranker assigns scores to in- 
stances using the confidence score returned 
by K Epat.

B2. K Edis  alone, a state-of-the-art  distributional 
system implementing  (Pantel et al., 2009), 
where the Ranker assigns scores to instances 
using the similarity score returned by K Edis 
alone.
B3. A rule-based ES system, combining  B1 and 
B2. This system uses both K Epat and K Edis 
as  extractors,    and a  Ranker that assigns 
scores to instances according to the sum of 
their normalized confidence scores.
B4. A state-of-the-art machine learning system 
based  on (Mirkin  et al., 2006).  This ES


AP (L) =    i=1 P (i) · corr (i )
 |L|


(1)


system uses K Epat


and K Edis


as extractors.


i=1 corr (i)
where L is a ranked list produced by an extractor, 
P (i) is the precision of L at rank i, and corr(i) is 1 
if the instance at rank i is correct, and 0 otherwise.
AP is computed over R for each class.
We also evaluate the coverage, i.e. the percent-
age of instances extracted by a system  wrt those 
extracted by all systems.
  In order to accurately compute statistical signif- 
icance, our experiments are performed using 10- 
fold cross validation.

Baselines and comparisons.   We compare our 
proposed ES entity extractor, using different fea- 
ture configurations, with state-of-the-art  systems 
(referred to as baselines B* below):


The Ranker  is a  GBDT regression model, 
using the full sets of features derived from 
the two extractors,  i.e., wP  and wD  (see 
Table 1). GBDT parameters are set as for our 
proposed ES-all system.

4.2   Experimental  Results

Table 3 summarizes the average-precision (AP) 
and coverage results for our ES-all system and the 
baselines.  Figure 2 reports the precision at each 
rank for the Athletes class (the other two classes 
follow similar trends). Table 6 lists the top-10 en- 
tities discovered for each class on one test fold. 
ES-all outperforms all baselines in AP (all results 
are statistically  significant at the 0.95 level), offer- 
ing at the same time full coverage4.


3 GBDT model parameters were experimentally set on an	 	


independent  development  set as follows: trees=300, shrink- 
age=0.01, max nodes per tree=12, sample rate=0.5.
  

4 Recall that coverage is reported relative to all instances 
retrieved by extractors K Epat and K Edis .


 
Figure 2: Precision at rank for the different sys- 
tems on the Athletes class.



  Our simple rule-based combination  baseline, 
B3, leads to a large increase in coverage wrt the in- 
dividual extractors alone (B1 and B2) without sig- 
nificant impact on precision. The supervised ML- 
based combination  baseline (B4) consistently im- 
proves AP across classes wrt the rule-based com- 
bination (B3), but without statistical significance. 
These results corroborate those found in (Mirkin et 
al., 2006), where this ML-based combination was 
reported to be significantly  better than a rule-based 
one on the task of lexical entailment acquisition.
  The large set of features adopted in ES-all ac- 
counts for a dramatic improvement  in AP, indicat- 
ing that existing state-of-the-art systems for entity 
extraction (reflected by our baselines strategies) 
are not making use of enough semantic cues. The 
adoption of evidence other than distributional  and 
pattern-based, such as features coming  from web 
documents, HTML tables and query logs, is here 
demonstrated to be highly valuable.
  The above empirical claim can be grounded and 
corroborated by a deeper semantic analysis.  From 
a semantic perspective, the above results translate 
in the observation that distributional  and pattern- 
based evidence  do not completely capture all se- 
mantic aspects of entities. Other evidence, such as 
popularity, term cohesiveness and co-occurrences 
capture other aspects. For instance, in one of our 
Actors folds, the B3 system ranks the incorrect in- 
stance ‘Tom  Sellek’ (a misspelling of ‘Tom  Sel- 
leck’) in 9th  position (out of 142), while ES-all 
lowers it to the 33rd  position, by relying on table- 
based features (intuitively, tables contain much 
fewer misspelling than running text). Other than 
misspellings,  ES-all fixes errors that are either typ- 
ical of distributional  approaches, such as the in- 
clusion of instances  of other classes  (e.g.  the 
movie  ‘Someone Like You’ often appears in con- 
texts similar to those  of actors);  errors typical 
of pattern-based  approaches, such as incorrect in-






Table 4: Overall AP results of the different feature 
configurations,  compared to two baselines.  †  in-
dicates statistical significance at the 0.95 level wrt
B3. ‡ indicates statistical significance at 0.95 level
wrt both B3 and B4.


stances highly-associated  with an ambiguous pat- 
tern (e.g., the pattern ‘X of the rock band Y’ for 
finding Musicians matched an incorrect instance
‘song submission’); or errors typical of both, such 
as the inclusion  of common nouns (e.g. ‘country 
music hall’) or too generic last names (e.g. ‘John- 
son’). ES-all successfully recovers all these error 
by using termness, co-occurrence  and frequency 
features.
  We also compare ES-all with a state-of-the-art 
random walk system (RW ) presented by Talukdar 
et al. (2008)  (see Section 2.2 for a description). 
As we could not reimplement  the system, we re- 
port the following indirect  comparison.  RW was 
evaluated on five entity classes, one of which, NFL 
players, overlaps with our Athletes class. On this 
class, they report 0.95 precision on the top-100 
ranked entities.  Unfortunately,  they do not report 
coverage or recall statistics, making the interpre- 
tation of this analysis difficult.  In an attempt to 
compare RW with ES-all, we evaluated the preci- 
sion of our top-100 Athletes, obtaining 0.99. Us- 
ing a random sample of our extracted Athletes, we 
approximate the precision of the top-22,000 Ath-
letes to be 0.97 ± 0.01 (at the 0.95 level).

4.3   Feature Analysis

Feature family analysis: Table 4 reports the av- 
erage precision (AP) for our system using different 
feature family combinations  (see Table 1).  Col- 
umn 1 reports the family combinations; columns



2-4 report AP for each class; and column 5 reports 
the mean-average-precision (MAP)  across classes. 
In all configurations,  except the k family alone, 
and along all classes, our system significantly out- 
performs (at the 0.95 level) the baselines.
  Rows 3-6 report the performance of each fea- 
ture family alone. w and t are consistently  better 
than q and k, across all classes.  k is shown to be 
the least useful family. This is mostly due to data 
sparseness, e.g., in our experiments almost 40% 
of the test instances in the Actors sample do not 
have any occurrence in Wikipedia. However, with- 
out access to richer  resources such as a webcrawl 
or query logs, the features from k do indeed pro- 
vide large gains over current baselines (on average
+10.2% and +7.7% over B3 and B4).
  Rows 7-12 report results for combinations of 
two feature families.  All  combinations (except 
those with k) appear valuable,  substantially  in- 
creasing the single-family  results in rows 3-6, in- 
dicating that combining different feature families 
(as suggested by the ES paradigm) is helpful.  Sec- 
ond, it indicates that q, w and t convey comple- 
mentary information,  thus boosting the regression 
model when combined together. It is interesting to 
notice that q+t tends to be the best combination, 
surprising given that t alone did not show high per- 
formance (row 5). One would expect the combina- 
tion q+w to outperform q+t, but the good perfor- 
mance of q+t is mainly due to the fact that these 
two families are more complementary  than q+w. 
To verify this intuition,  we compute the Spearman 
correlation coefficient r among the rankings pro- 
duced by the different combinations. As expected, 
q and w have a higher correlation  (r = 0.82) than 
q and t (r = 0.67) and w and t (r = 0.66), suggest- 
ing that q and w tend to subsume each other (i.e. 
no added information for the regression model).
  Rows 13-15 report results for combinations of 
three feature families.   As expected,  the best 
combination is q+w+t with an average precision 
nearly identical to the full ES-all system.  If one 
has access to Web or query log sources, then the 
value of the Wikipedia  features tends to be sub- 
sumed by our web and query log features.

Feature by feature analysis: The feature fam- 
ilies analyzed in the previous  section consist of
402 features.  For each  trained GBDT model, 
one can inspect the resulting most important fea- 
tures (Friedman,  2001).  Consistently,  the two 
most important features  for ES-all are, as  ex-



Syst
em
A
P
Ac
tor
s 
At
hle
tes 
Mu
sici
ans
MA
P
B4
0.7
15	0.697	0.579
0.6
64
B4+
w
B
4
+
w
F
 
B
4
+
w
T
0.8
13	0.908	0.724
0.7
98	0.865	0.679
0.8
06	0.891	0.717
0.8
15
0.7
81
0.8
05
B4+
t
B
4
+
t
F
 
B
4
+
t
C
0.7
84	0.825	0.727
0.7
60	0.802	0.701
0.7
71	0.815	0.718
0.7
79
0.7
81
0.8
05
B
4
+
q
B
4
+
q
F
 
B
4
+
q
C
 
B
4
+
q
D
 
B
4
+
q
P
 
B
4
+
q
T
B4
+q
F+
qW
+q
T
0.8
15	0.905	0.743
0.7
86	0.890	0.693
0.7
15	0.738	0.581
0.7
35	0.709	0.644
0.7
79	0.796	0.648
0.7
80	0.868	0.725
0.8
16	0.906	0.743
0.8
21
0.7
90
0.6
78
0.6
96
0.7
41
0.7
91
0.8
22
ES-
all
0.8
60	0.915	0.788
0.8
54
Table 5: Ablation study of the web (w), query- 
log (q) and table (t) features (bold letters indicate 
whole feature families).


pected, the confidence  scores of  K Epat    and 
K Edis.     This suggests  that syntagmatic  and 
paradigmatic  information  are most important  in 
defining the semantics of entities. Also very im- 
portant, in third position, is a feature  from qT , 
namely the ratio between the number of queries 
matching the instance and the number of queries 
containing it as a substring.  This feature is a strong 
indicator of termness.
  Webcrawl  term frequencies and document fre- 
quencies (from the wF  set) are also important. 
Very frequent and infrequent instances were found 
to be often incorrect (e.g., respectively ‘song’ and
‘Brad Pitttt’). Table PMI (a feature in the qC fam- 
ily) also ranked high in importance: instances that 
co-occurr very frequently in the same column/row
with seeds S are often found to be correct (e.g.,
a  column containing the seeds  ‘Brad Pitt’  and
‘Tom  Hanks’ will  likely contains other actors). 
Other termness (T ), frequency-based (F ) and co- 
occurrence (C ) features also play some role in the 
model.
  Variable importance is only an intrinsic indi- 
cator of feature relevance.  In order to better as- 
sess  the actual impact of the single features on 
AP, we ran our system on each feature type: re- 
sults for the web (w), query log (q) and table (t) 
families are reported  in Table 5.  For reason of 
space constraints,  we here only focus on some 
high level observations.  The set of web termness 
features (wT ) and frequency  features (wF ) are 
alone able to provide a large improvement over B4 
(row 1), while their combination (row 2) does not 
improve much over the features taken individually.


Seed instances for K Edis
Act
ors
A
t
h
l
e
t
e
s
Mu
sici
ans
Jodi
e 
Fos
ter
Hu
m
ph
re
y 
B
og
art 
An
th
on
y 
H
op
ki
ns 
Ka
th
ari
ne 
H
ep
bu
rn 
Ch
ris
to
ph
er 
W
al
ke
n 
Ge
ne 
H
ac
k
m
an 
Di
an
e 
K
ea
to
n 
Ed
wa
rd 
N
ort
on 
Ro
be
rt 
D
uv
all 
Hil
ar
y 
S
w
an
k
Bob 
Gib
son	Jared Allen	Randy Moss
Don 
Dry
sdal
e	Andres Romero	Peyton Manning
Alb
ert 
Puj
ols	Kenny Perry	Jerry Rice
Yo
gi 
B
er
ra	Martin Kaymer	Robert Karlsson Dejan Bodiroga 	Alexander Ovechkin	Gheorghe Hagi Allen Iverson	Shea Weber	Marco Van Basten Yao Ming	Patrick Roy	Zinedine Zidane Tim Duncan	Alexei Kovalev	Roberto Baggio
Rise 
Agai
nst 
the 
Ma
chin
e
P
i
n
k
 
F
l
o
y
d
 
S
p
i
c
e
 
G
i
r
l
s
 
P
u
s
s
y
c
a
t
 
D
o
l
l
s
 
T
h
e
 
B
e
a
t
l
e
s
 
I
r
o
n
 
M
a
i
d
e
n
 
J
o
h
n
 
L
e
n
n
o
n
 
F
r
a
n
k
 
S
i
n
a
t
r
a
 
L
e
d
 
Z
e
p
p
e
l
i
n
Fred
die 
Mer
cur
y

Seed instances for K Epat
A
c
t
o
r
s
A
t
h
l
e
t
e
s
M
u
s
i
c
i
a
n
s
Den
nis 
Hop
per 
- 
The 
Goo
d 
Life
Tom 
Han
ks - 
The 
Ter
min
al
Julia 
Rob
erts 
- 
Mon
a 
Lisa 
Smi
le
Kevi
n 
Bac
on - 
Foo
tloo
se
Kea
nu 
Ree
ves 
- 
The 
Lak
e 
Hou
se
Marl
on 
Bra
ndo 
- 
Don 
Jau
n 
De
mar
co
Mor
gan 
Free
man 
- 
The 
Sha
wsh
ank 
Red
em
ptio
n
Nic
ole 
Kid
man 
- 
Eye
s 
Wid
e 
Shu
t
A
l
 
P
a
c
i
n
o
 
-
 
T
h
e
 
G
o
d
f
a
t
h
e
r
 
J
o
h
n
n
y
 
D
e
p
p
 
-
 
C
h
o
c
o
l
a
t
 
H
a
l
l
e
 
B
e
r
r
y
 
-
 
M
o
n
s
t
e
r
’
s
 
B
a
l
l
Dall
as 
cow
boy
s - 
Juli
us 
Cro
ssli
n
Ne
w 
yo
rk 
Gi
an
ts 
- 
Pl
axi
co 
B
ur
re
ss 
Ph
ila
del
phi
a 
Ea
gle
s - 
Da
nn
y 
A
m
en
do
la 
W
as
hi
ng
to
n 
Re
ds
kin
s - 
Ro
ck 
C
art
wr
ig
ht 
Ne
w 
En
gl
an
d 
Pa
tri
ot
s - 
La
ur
en
ce 
M
ar
on
ey 
B
uff
al
o 
Bil
ls 
- 
Xa
vi
er 
O
m
on
Mia
mi 
Dol
phin
s - 
Ern
est 
Wilf
ord
Ne
w 
Yor
k 
Jets 
- 
Cha
nsi 
Stuc
key
Kevi
n 
Bro
wn - 
Cor
nda
ddy
Barr
y 
Gib
b - 
The 
Bee 
Gee
s
Patt
y 
Smy
th - 
Sca
nda
l
Dav
e 
Matt
hew
s - 
Dav
e 
Mat
hew
s 
Ban
d
G
w
e
n
 
S
t
e
f
a
n
i
 
-
 
N
o
 
D
o
u
b
t
 
G
e
o
r
g
e
 
M
i
c
h
a
e
l
 
-
 
W
h
a
m
 
M
a
r
k
 
K
n
o
p
f
l
e
r
 
-
 
D
i
r
e
 
S
t
r
a
i
t
s
Bria
n 
Jone
s - 
The 
Rolli
ng 
Sto
nes
Pete 
Shel
ley - 
Buz
zco
cks

10-best ranked instances in one test fold

A
c
t
o
r
s
A
t
h
l
e
t
e
s
M
u
s
i
c
i
a
n
s
Gor
don 
Too
too
sis	Ron Randell
R
o
s
a
l
i
n
d
 
C
h
a
o
	
A
l
i
m
i
 
B
a
l
l
a
r
d
 
J
o
h
n
 
H
a
w
k
e
s
 
	
F
e
r
n
a
n
d
o
 
L
a
m
a
s
 
J
e
f
f
r
e
y
 
D
e
a
n
 
M
o
r
g
a
n
	
B
r
u
n
o
 
C
r
e
m
e
r
 
G
e
o
r
g
e
 
M
a
c
r
e
a
d
y
	
M
u
h
a
m
m
a
d
 
B
a
k
r
i
Ru
mea
l 
Rob
inso
n	Todd Warriner
J
e
f
f
 
M
c
i
n
n
i
s
	
H
o
n
g
-
c
h
i
h
 
K
u
o
 
A
h
m
a
d
 
N
i
v
i
n
s
	
L
e
o
n
 
C
l
a
r
k
e
 
C
a
r
l
o
s
 
M
a
r
c
h
e
n
a
	
J
o
s
h
 
D
o
l
l
a
r
d
 
C
h
a
d
 
K
r
e
u
t
e
r
	
R
o
b
b
i
e
 
A
l
o
m
a
r
Coli
n 
Ne
wm
an	Wu-tang Clan
Gho
st 
Circ
us	Tristan Prettyman
Ray 
Dor
set	Top Cats
Plas
tic 
Tre
e	*Roseanne
*Do
om
wat
ch	John Moen

Table 6: Listing of all seeds used for K Edis  and K Epat, as well as the top-10 entities discovered by
ES-all on one of our test folds.




This suggests that wT and wF capture very simi- 
lar information:  they are indeed highly correlated 
(r = 0.80). Rows 5-7 refer to web table features: 
the features tC outperform  and subsume the fre- 
quency features tF  (r = 0.92). For query log 
features (rows 8-14), only qF , qP and qT signif- 
icantly increase performance.  Distributional  and 
co-occurrence features (qD and qC ) have very low 
effect, as they are mostly subsumed by the others. 
The combination of qF , qP and qT (row 14) per- 
forms as well as the whole q (row 8).

Experiment conclusions:    From  our  experi- 
ments, we can draw the following conclusions:

1. Wikipedia features taken alone outperform 
the baselines, however,  web and query log 
features, if available, subsume Wikipedia fea- 
tures;

2. q, t and w are all important,  and should be 
used in combination, as they drive mostly  in- 
dependent information;

3. the syntagmatic and paradigmatic informa- 
tion conveyed by the two extractors are most 
relevant, and can be significantly  boosted by 
adding  frequency-  and termness-based fea- 
tures from other sources.


5   Conclusions and Future  Work


In this paper,  we presented  a general  informa- 
tion extraction framework,  called Ensemble Se- 
mantics, for combining multiple sources of knowl- 
edge, and we instantiated the framework  to build 
a novel ML-based  entity extraction system. The 
system  significantly outperforms  state-of-the-art 
ones by up to 22% in mean average precision. 
We provided an in-depth analysis of the impact of 
our proposed 402 features, their feature families 
(Web documents, HTML tables, query logs, and 
Wikipedia),  and feature types.
  There is ample directions for future work. On 
entity extraction, exploring  more knowledge ex- 
tractors from different sources (such as the HTML 
tables and query log sources used for our features) 
is promising.  Other feature types may potentially 
capture other aspects of the semantics of entities, 
such as WordNet  and search engine click logs. For 
the ranking  system, semi- or weakly-supervised 
algorithms may provide competing performance 
to our model with reduced manual labor. Finally, 
there are many opportunities for applying the gen- 
eral Ensemble Semantics framework  to other in- 
formation  extraction  tasks such as fact extraction 
and event extraction.


References

Steven  Abney.    1991.   Learning taxonomic rela- 
tions from heterogeneous sources of evidence.   In 
Principle-Based Parsing. Kluwer Academic Pub- 
lishers.

Eric Brill.   1995. Transformation-based error-driven 
learning and natural language processing:  A case 
study in part of speech tagging. Computational Lin- 
guistics, 21(4).

Michael J. Cafarella,  Doug Downey,  Stephen Soder- 
land, and Oren Etzioni. 2005. KnowItNow: Fast, 
scalable information extraction from the web.  In 
Proceedings of EMNLP-2005.

Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen 
Liao, Enhong Chen, and Hang Li.  2008. Context- 
aware query suggestion by mining click-through and 
session data.  In Proceedings of KDD-08, pages
875–883.

Surajit Chaudhuri,  Venkatesh Ganti, and Dong Xin.
2009. Exploiting web search to generate synonyms 
for entities. In Proceedings  of WWW-09, pages 151–
160.

Philipp Cimiano and Steffen Staab. 2004. Learning by 
googling. SIGKDD Explorations, 6(2):24–34.

Philipp Cimiano and Johanna  Volker.   2005.   To- 
wards large-scale, open-domain and ontology-based 
named entity classification.  In Proceedings of 
RANLP-2005, pages 166–172.

Philipp Cimiano, Aleksander Pivk, Lars Schmidt- 
Thieme, and Steffen Staab. 2005. Learning taxo- 
nomic relations from heterogeneous sources of ev- 
idence.	In Ontology Learning from Text:  Meth- 
ods, Evaluation and Applications, pages 59–73. IOS 
Press.

Michael Fleischman and Eduard Hovy. 2002. Classi- 
fication of named entities. In Proceedings of COL- 
ING 2002.

Jerome H. Friedman.  2001. Greedy function approx- 
imation: A gradient boosting machine. Annals of 
Statistics, 29(5):1189–1232.

Jerome H. Friedman. 2006. Recent advances in pre- 
dictive (machine) learning. Journal of Classifica- 
tion, 23(2):175–197.

Zellig Harris. 1964. Distributional structure. In Jer- 
rold J. Katz and Jerry A. Fodor, editors, The Philos- 
ophy of Linguistics, New York. Oxford University 
Press.

Marti A. Hearst. 1992. Automatic acquisition of hy- 
ponyms from large text corpora.  In Proceedings of 
COLING-92, pages 539–545.

Jian Hu, Gang Wang, Fred Lochovsky,  Jian tao Sun, 
and Zheng Chen. 2009. Understanding user’s query 
intent with Wikipedia. In Proceedings of WWW-09, 
pages 471–480.



Dekang Lin. 1998. Automatic retrieval and clustering 
of similar words. In Proceedings of COLING-ACL-
98.

Joseph F. Mc Carthy and Wendy G Lehnert. 2005. Us- 
ing decision trees for coreference resolution.  In Pro- 
ceedings of IJCAI-1995, pages 1050–1055.

Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional  similar- 
ity methods for lexical entailment acquisition. In 
Proceedings of ACL/COLING-06, pages 579–586.

Marius Pas¸ca  and Benjamin Van Durme.	2008.
Weakly-supervised  acquisition of  open-domain 
classes and class attributes  from  web documents  and 
query logs. In Proceedings  of ACL-08,  pages 19–27.

Marius Pas¸ca, Dekang  Lin, Jeffrey Bigham, Andrei 
Lifchits, and Alpa Jain.  2006.  Organizing and 
searching the world wide web of facts - step one: 
The one-million  fact extraction challenge. In Pro- 
ceedings of AAAI-06,  pages 1400–1405.

Marius  Pas¸ca. 2007. Weakly-supervised discovery of 
named entities using web search queries. In Pro- 
ceedings of CIKM-07, pages 683–690,  New York, 
NY, USA.

Patrick   Pantel  and Marco Pennacchiotti.	2006.
Espresso: A Bootstrapping Algorithm for Automat- 
ically Harvesting Semantic Relations. In Proceed- 
ings of ACL-2006, Sydney, Australia.

Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana- 
Maria Popescu, and Vishnu Vyas. 2009. Web-scale 
distributional similarity and entity set expansion. In 
Proceedings of EMNLP-09, Singapore.

Ellen Riloff and Rosie  Jones.   1999.  Learning dic- 
tionaries for information extraction by multi-level 
bootstrapping. In Proceedings of AAAI-99,  pages
474–479.

Partha  Pratim Talukdar, Joseph  Reisinger, Marius 
Pas¸ca, Deepak  Ravichandran,   Rahul Bhagat,  and 
Fernando Pereira.  2008. Weakly-supervised acqui- 
sition of labeled class instances using graph random 
walks. In Proceedings of EMNLP-08,  pages 582–
590.

Bin Tan and Fuchun  Peng.  2006.   Unsupervised 
query segmentation using generative language mod- 
els and wikipedia.   In Proceedings of WWW-06, 
pages 1400–1405.

Hristo Tanev and Bernardo Magnini. 2006. Weakly 
supervised approaches for ontology population. In 
Proceedings of EACL-2006.

