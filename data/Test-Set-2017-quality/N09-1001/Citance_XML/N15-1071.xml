<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Sentiment analysis has undergone a shift from document-level analysis, where labels expresses the sentiment of a whole document or whole sentence, to subsentential approaches, which assess the contribution of individual phrases, in particular including the composition of sentiment terms and phrases such as negators and intensifiers.</S>
		<S sid ="2" ssid = "2">Starting from a small sentiment treebank modeled after the Stanford Sentiment Treebank of Socher et al.</S>
		<S sid ="3" ssid = "3">(2013), we investigate suitable methods to perform compositional sentiment classification for German in a data-scarce setting, harnessing cross-lingual methods as well as existing general-domain lexical resources.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">In sentiment classification, we find a general tendency from document-level classification towards more fine-grained approaches that yield a more detailed appraisal of the judgement performed in the text - in particular, using composition over syntactic structure to get a more detailed approach over phrases.</S>
			<S sid ="5" ssid = "5">For English movie reviews, work using the Stanford Sentiment Treebank (SSTb) has shown that such subsentential sentiment information can yield approaches with both very high accuracy (Socher et al., 2013; Dong et al., 2014; Hall et al., 2014) and precise information about the role of each phrase – information which can subsequently used for extracting or summarizing the sentiment expressed in the text.</S>
			<S sid ="6" ssid = "6">The effort for creating a sentiment treebank such as the SSTb, however, seems prohibitive if we wanted to create such a resource for each pair of relevant domain and language: Compared to document-level annotations for sentiment, which are easy to come by (e.g., star ratings), annotating individual syntactic phrases requires considerable effort.</S>
			<S sid ="7" ssid = "7">The main focus of this paper is the question if and how it is possible to reach sensible performance for compositional sentiment classification when we only have limited resources to spend on an in-language, in-domain sentiment treebank.</S>
			<S sid ="8" ssid = "8">For this goal, we use a new resource, the Heidelberg Sentiment Treebank (HeiST), which is a German-language counterpart to the Stanford Sentiment Treebank in the sense that it makes explicit the composition of sentiment expression over syntactic phrases.</S>
			<S sid ="9" ssid = "9">Our experiments on HeiST provide a direct comparison of different techniques for harnessing cross-lingual, cross-domain, or cross-task information, and are the first of this kind to specifically target compositional sentiment analysis.</S>
			<S sid ="10" ssid = "10">Figure 1 (next page) shows a schematic overview of the experiments: beyond supervised baseline experiments using SVM classification and a supervised RNTN model (section 3), we evaluated cross- lingual projection (section 4), lexicon-based approaches (section 5), as well as semi-supervised approaches based on word clusters (section 6).</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="11" ssid = "1">The starting point for our research is the idea that the sentiment of larger stretches of text can be calculated through composition over smaller stretches of text, which was investigated in a learning framework by both Yessenalina and Cardie (2011) and Socher et al.</S>
			<S sid ="12" ssid = "2">(2011, 2012), both learning in a compositional 694 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 694–704, Denver, Colorado, May 31 – June 5, 2015.</S>
			<S sid ="13" ssid = "3">Qc 2015 Association for Computational Linguistics English EN model SSTb projection/ tree align [4] Crowdsourcing of HeiST [3] SVM [3.2] RNTN baseline [3.2] RNTN + clusters [6] German Google Translate HeiST (train) DE model clusters split [6] lexicon [5] evaluation H e i S T (t e s t) Figure 1: Plan to the experiments described in this paper fashion from datasets that only have document-level sentiment annotation.</S>
			<S sid ="14" ssid = "4">On the same dataset as Socher et al., Wang and Manning (2012) later demonstrated that unigram and bigram features in an SVM-based classification framework can reach a greater accuracy than the earlier recursive neural network approach of Socher et al.</S>
			<S sid ="15" ssid = "5">(2011, 2012), which calls into question the assumption that sentiment composition can be learned purely from sentence-level annotations.</S>
			<S sid ="16" ssid = "6">Compositionality through Tensors In subsequent work, Socher et al.</S>
			<S sid ="17" ssid = "7">(2013) introduce the Stanford Sentiment Treebank, which contains detailed annotations of sentiment values for individual syntactic phrases in a binarized tree, and an approach based on recursive neural tensor networks (RNTN) which yields significant improvements over the earlier approaches using token-level features.</S>
			<S sid ="18" ssid = "8">The RNTN represents the contribution of individual nodes as vectors of reals and achieves its preci sion by using a tensor V [1:d] ∈ R2d×2d×d as wellas a matrix W ∈ R2d×d to capture second-order de pendencies between the two children of a node in the tree (with vectors a, b), yielding first a vector h by sentiment label of a node is then gained by multiplying these hidden vectors by a matrix Ws, yielding a five-dimensional vector with the classification.</S>
			<S sid ="19" ssid = "9">Using hidden vectors for each node and capturing second-order interaction between the two child vectors a and b, the RNTN model achieves descriptive power greater than that of TreeCRFs (Nakagawa et al., 2010), and similar to latent-variable models that have been very successful in syntactic parsing (Petrov et al., 2006).</S>
			<S sid ="20" ssid = "10">In later work, Zhu et al.</S>
			<S sid ="21" ssid = "11">(2014) show that the RNTN’s lexicalized modeling of negators and their behaviour leads to increased descriptive power of the model, which results in an improved treatment of negation.</S>
			<S sid ="22" ssid = "12">Dong et al.</S>
			<S sid ="23" ssid = "13">(2014) introduce an approach that chooses between multiple composition tensors (AdaMCRNTN), which yields further gains with respect to RNTN performance.</S>
			<S sid ="24" ssid = "14">In contrast to the lexicalized and high- dimensional RNTN model, there are several lines of work that attempt to work in a more data-scarce setting.</S>
			<S sid ="25" ssid = "15">Lexicon-based approaches The classical approach for performing sentiment classification in a hi = r l b V [i] r a b l + Wi r l b setting where training data is sparse can be seen in the SO-CAL approach of Taboada et al.</S>
			<S sid ="26" ssid = "16">(2011): Using a manually curated dictionary with senti then using a monotonic nonlinear function on h (here: tanh) to yield the vector for this node.</S>
			<S sid ="27" ssid = "17">The ment values for multiple parts of speech, and a set of heuristics that predict how intensifiers, nega tors/shifters as well as nonveridical moods affect the sentiment of a phrase, they show that it is possible to reach good results across different domains.</S>
			<S sid ="28" ssid = "18">Choi and Cardie (2009) show that it is possible to adapt an existing general-domain sentiment lexicon to a specific domain using an approach that optimizes a joint objective of classification loss, sparsity of the changes made to the lexicon, and ambiguity of lexicon entries.</S>
			<S sid ="29" ssid = "19">Their approach yields appreciable gains over the general-domain lexicon, both with CRF-based machine learning classification and with a simpler “vote &amp; flip” algorithm that is based on majority voting and negators.</S>
			<S sid ="30" ssid = "20">0 Was what- 0 mir me- 0 schwer heavy auf on- 0 die the 0 geht 0 goes 0 Nerven nerves Crosslingual Sentiment Analysis involves the usage of a dataset in one language to perform sentiment analysis in another language; in their work, Banea et al.</S>
			<S sid ="31" ssid = "21">(2013) show that translating text in the target language to the source language and applying a well-tuned sentiment classification system works better than either translating the training corpus or the lexicon used by the system.</S>
			<S sid ="32" ssid = "22">In research by other groups, Wan (2009) advocates a bootstrapping approach that combines source-side and target-side features in one classifier; Duh et al.</S>
			<S sid ="33" ssid = "23">(2011) note that crosslingual sentiment analysis techniques always incur a loss due to the shift in language from the source language texts to the target language even though the general domain is the same.</S>
			<S sid ="34" ssid = "24">Popat et al.</S>
			<S sid ="35" ssid = "25">(2013) argue that full machine translation is not useful for resource-scarce languages, and propose to use cross-lingual clustering both to improve the generalization capability within a single language as well as for crosslingual projection, which works better than machine translation with the EnglishHindi language pair.</S>
			<S sid ="36" ssid = "26">It should be noted that most of the work presented in the last two paragraph works with document-level sentiment, or (in the case of Choi and Cardie) with shallower annotations, and offers additional challenges in the case of sentiment composition.</S>
	</SECTION>
	<SECTION title="Low-Budget Treebanking for Sentiment. " number = "3">
			<S sid ="37" ssid = "1">For both supervised training and for evaluation, we created a German dataset that is close in domain to the Stanford Sentiment treebank (Socher et al., 2013), covering opinionated sentences from movie reviews with phrase-level sentiment annotations.</S>
			<S sid ="38" ssid = "2">“What really gets on my nerves . . .</S>
			<S sid ="39" ssid = "3">” Figure 2: A multiword expression in HeiST The original Stanford Sentiment Treebank is based on the dataset of Pang and Lee (2005), which includes 10,662 sentences from excerpts of movie reviews published on rottentomatoes.com.</S>
			<S sid ="40" ssid = "4">It should be noted that these excerpts are much more likely to express an opinion than general text or even the main body of a movie review since they contain precisely a summary of the opinion.</S>
			<S sid ="41" ssid = "5">In order to match both domain and role of these sentences most precisely, we collected creative- commons-licensed reviews from a German movie review site, filmrezensionen.de, and used only the summary part of these documents, yielding 1184 sentences, for which we crowdsourced annotation for each individual phrase in the binary tree (see Figure 2 for an example tree fragment).</S>
			<S sid ="42" ssid = "6">For the purpose of getting binary phrase trees, sentences were processed with the Berkeley Parser (Petrov et al., 2006), NP nodes were added inside PPs (Samuelsson and Volk, 2004) and the resulting parse trees binarized using the head table in CoreNLP (Rafferty and Manning, 2008), yielding 14,321 unique phrases.</S>
			<S sid ="43" ssid = "7">Annotation was outsourced via the CrowdFlower service, which collects three judgements for each phrase and computes an end result through voting, using unambiguous test items (which we composed from strongly positive or strongly negative adjective-noun combinations) to filter out annotators lacking the requisite understanding of German.</S>
			<S sid ="44" ssid = "8">Experiment A: Features, Confidence Prec Recl SentiWS 0.882 0.959 SentiWS+Regression 0.894 0.967 SentiWS+Regression, @50% 0.935 0.985 SentiWS+Regression+POS 0.912 0.960 SentiWS+Regression+POS, @50% 0.978 0.997 Experiment B: Classifier (@50%) Prec Recl Linear SVM 0.975 0.980 Random Forest 0.984 0.992 Gradient Boosting 0.978 0.997 Table 1: Filtering out objective phrases The HeiST treebank, as well as the code used in these experiments, are available for research purposes.1 3.1 Selecting Subjective Phrases.</S>
			<S sid ="45" ssid = "9">One possible approach to reduce annotation effort would be to annotate only those phrases that a classification model deems to have sentiment content in the first place.</S>
			<S sid ="46" ssid = "10">As a more extreme example of such an approach, consider the MLSA sentiment dataset for German, where 270 sentences were selected that already contained two words from an existing sentiment lexicon (Clematide et al., 2012), with the goal of getting sentences with interesting interactions between sentiment words.</S>
			<S sid ="47" ssid = "11">Given the potential benefits (getting more data for the same annotation effort), an approach that filters out non-interesting (confidently objective) phrases would be highly appealing.</S>
			<S sid ="48" ssid = "12">For the pre-classification experiment, we used cross-validation on 20 to assess the potential impact of strategies for saving.</S>
			<S sid ="49" ssid = "13">For the corresponding classifier, we used features from a German general- domain sentiment lexicon, a regression model for document-level sentiment (see section 5.2), as well as part-of-speech tag features in a gradient boosting classifier.</S>
			<S sid ="50" ssid = "14">As seen in table 1, the sentiment lexicon, especially in conjunction with the regression model and a POS-based filter, would allow to detect uninteresting (objective) phrases with high accuracy.</S>
			<S sid ="51" ssid = "15">We limit ourselves to the 50% of most confident classifications, and as a measure of caution, the filter is bypassed for any phrase that contains a word in one of several sentiment dictionaries (see section 5).</S>
			<S sid ="52" ssid = "16">The classifier has a precision of 96.5% for objective 1 http://www.cl.uniheidelberg.de/ ˜versley/HeiST/ System Node Acc.</S>
			<S sid ="53" ssid = "17">Root Acc.</S>
			<S sid ="54" ssid = "18">HeiST, only pos+neg sentences supervised RNTN (tuned) 0.776 0.687 SVM (unigrams, coarse) 0.850∗∗ 0.774∗∗ SVM (unigrams, fine) 0.835∗∗ 0.735 cross-lingual CLSA (simple feat.)</S>
			<S sid ="55" ssid = "19">0.823∗∗ 0.737 CLSA (complex feat.)</S>
			<S sid ="56" ssid = "20">0.810∗∗ 0.738∗ Comparison: HeiST, all sentences supervised RNTN (tuned) 0.803 0.703 */**: significantly better than RNTN (p &lt; 0.05 / p &lt; 0.005) Table 2: HeiST baseline, cross-lingual projection, SVM.</S>
			<S sid ="57" ssid = "21">System Node Acc.</S>
			<S sid ="58" ssid = "22">Root Acc.</S>
			<S sid ="59" ssid = "23">Comparison: SSTb sample, pos+neg sentences lexicon-based General Inquirer 0.824 0.715 SubjectivityClues 0.820 0.695 supervised RNTN, 500 sent.</S>
			<S sid ="60" ssid = "24">0.704 0.526 RNTN, 1000 sent.</S>
			<S sid ="61" ssid = "25">0.738 0.539 RNTN, 1500 sent.</S>
			<S sid ="62" ssid = "26">0.756 0.569 SVM, 500 sent.</S>
			<S sid ="63" ssid = "27">0.803 0.652 SVM, 1000 sent.</S>
			<S sid ="64" ssid = "28">0.814 0.675 SVM, 1500 sent.</S>
			<S sid ="65" ssid = "29">0.823 0.683 RNTN, 6920 sent.a 0.876 0.854 SVM, 6920 sent.a 0.846 0.794 a : published figures from Socher et al.</S>
			<S sid ="66" ssid = "30">(2013) Table 3: Comparison figures on subsets of the Stanford Sentiment Treebank phrases while catching about 66.7% of all objective nodes.</S>
			<S sid ="67" ssid = "31">While this would correspond to substantial savings (about a quarter of all nodes would be assigned the “neutral” label and not annotated), we would also lose a fraction of non-neutral phrase and introduce an unwanted bias (towards lexicon-based resources) into our dataset.</S>
			<S sid ="68" ssid = "32">3.2 Baseline results.</S>
			<S sid ="69" ssid = "33">We use the existing RNTN implementation of Socher et al.</S>
			<S sid ="70" ssid = "34">(2013) to train and test supervised learning for sentiment composition, using cross- validation.</S>
			<S sid ="71" ssid = "35">For parameter tuning, we varied the number of vector dimensions as well as the size of the minibatches used in training, and found that the resulting classifier yields very sensible results compared to a similarly-sized sample from SSTb (see Tables 2 and 3).</S>
			<S sid ="72" ssid = "36">We evaluate our results as in Socher et al.</S>
			<S sid ="73" ssid = "37">(2013): we consider the recall of positive and negative nodes while ignoring both neutral nodes and the difference between positive (+) and strongly positive (++) or between negative (-) and strongly negative (--) nodes, respectively.</S>
			<S sid ="74" ssid = "38">Socher et al. remove sentences with neutral overall sentiment in training as well as in testing, which seems to worsen the RNTN performance on our dataset (see Table 2), although other methods seem to be less affected by it.</S>
			<S sid ="75" ssid = "39">For comparability reasons, all other reported figures are based on Socher’s non-neutral-sentences- only setting.</S>
			<S sid ="76" ssid = "40">In comparison results on SSTb (see Table 3), classification experiments from the English data also show poor results for the RNTN classifier at small data sizes, in parallel with anecdotal evidence on recurrent neural networks having trouble with small dataset sizes.2</S>
	</SECTION>
	<SECTION title="Crosslingual Projection for. " number = "4">
			<S sid ="77" ssid = "1">Compositional Sentiment Our crosslingual approach follows Banea et al.</S>
			<S sid ="78" ssid = "2">(2013) in assuming that machine translation of the target documents to the source language, then applying a source-language sentiment analysis, and finally projecting the result back to the target side will yield usable sentiment classification.</S>
			<S sid ="79" ssid = "3">In difference to previous approaches for cross-lingual sentiment analysis, however, our annotation transfer concerns not just analysis results for the complete sentence, but for individual syntactic nodes.</S>
			<S sid ="80" ssid = "4">After translating the target-language trees using the Google Translate API, we parsed the sentences using the English model of the Stanford parser, and applied the RNTN model of Socher et al.</S>
			<S sid ="81" ssid = "5">(2013) trained on the English Stanford Sentiment Treebank, yielding a labeling for each syntactic node with a sentiment value.</S>
			<S sid ="82" ssid = "6">We then performed word alignment using the PostCAT word aligner (Ganchev et al., 2008) with a model trained on the OPUS version of the EuroParl corpus (Tiedemann, 2012), and alignment of syntactic nodes using the Lingua::Align toolbox for tree alignment Tiedemann (2010) with a model trained on the Smultron parallel treebank (Volk et al., 2010).</S>
			<S sid ="83" ssid = "7">2 Alec Radford (2015): General Sequence Learning using Recurrent Neural Nets, https://indico.io/blog/ general- sequence- learning- using- recurrent- neural- nets/ Table 4: Lexicon-based phrase labeling Using the word alignment and our Lingua::Align model, we are able to map 98.6% of the target-language nodes to a corresponding node on the source (English) side, whereas the remaining nodes are assigned the same sentiment label as the root.</S>
			<S sid ="84" ssid = "8">As can be seen in table 2, a model that uses simpler features for Lingua::Align works less well than the full feature model.</S>
			<S sid ="85" ssid = "9">Considering that the RNTN on the Stanford Sentiment Treebank reaches 87.6% node accuracy and 85.4% root accuracy, we see that the crosslingual projection step induces a loss in accuracy, but still performs well in comparison to the approaches that use the HeiST training data.</S>
	</SECTION>
	<SECTION title="Lexicon-based Approaches. " number = "5">
			<S sid ="86" ssid = "1">Considering that the size of HeiST creates a sparse data problem for the RNTN learner, it is natural to ask whether we can improve the generalization capabilities of the system by either using a less- supervised approach or by generalizing over individual word forms to alleviate the sparse data problem.</S>
			<S sid ="87" ssid = "2">5.1 General-domain lexicon.</S>
			<S sid ="88" ssid = "3">Several general-domain sentiment lexicons exist for German, including those of Klenner et al.</S>
			<S sid ="89" ssid = "4">(2009), Waltinger (2010a), Remus et al.</S>
			<S sid ="90" ssid = "5">(2010), and Emerson and Declerck (2014).</S>
			<S sid ="91" ssid = "6">Klenner et al.</S>
			<S sid ="92" ssid = "7">(2009) created their polarity lexicon by semiautomatic extension of an existing one: starting from a set of 2866 adjective seeds, they looked for adjectives that often co-occur in coordinations with known sentiment-bearing adjectives, which were added to the lexicon after a manual filtering step.</S>
			<S sid ="93" ssid = "8">The current version of Klenner et al.’s PolArt lexicon also contains other parts of speech, and a list of shifters and intensifiers that interact with subjective terms.</S>
			<S sid ="94" ssid = "9">The GermanPolarityClues lexicon of Waltinger (2010a) combines translation from English lexicons with a semiautomatic approach for merging and manually correcting lexicon entries.</S>
			<S sid ="95" ssid = "10">The SentiWS lexicon (Remus et al., 2010) contains translations of the English General Inquirer (Stone et al., 1966), which have been translated via Google Translate, as well as a small number of terms that were mined from positive and negative product reviews, expanded using a collocation dictionary.</S>
			<S sid ="96" ssid = "11">Finally, the SentiMerge lexicon (Emerson and Declerck, 2014) has been constructed as a Bayesian combination (i.e., averaging with imputation for missing entries) of the three resources above together with the German SentiSpin resource of Waltinger (2010b), which contains automatic (dictionary-based) translations of the SentiSpin lexicon of Tamura et al.</S>
			<S sid ="97" ssid = "12">(2005).</S>
			<S sid ="98" ssid = "13">We tested all lexicons using two approaches: In the vote-only approach, the sentiment of a phrase is determined by the sum of the scores of the words in that phrase as they are assigned in the sentiment lexicon.</S>
			<S sid ="99" ssid = "14">In the vote-and-flip approach, we consider the average of the sentiment terms, but invert the sentiment value whenever a term from the shifter category of Klenner et al.’s lexicon is found within the yield of the node.</S>
			<S sid ="100" ssid = "15">A similar strategy was used in many papers on sentiment composition, usually with a performance rather close to the best system (see e.g. the CompoMC baseline in Choi and Cardie, 2008, or the Vote-and-Flip baseline in Choi and Cardie, 2009).</S>
			<S sid ="101" ssid = "16">5.2 Near-domain lexicon construction.</S>
			<S sid ="102" ssid = "17">While the filmrezensionen web site offers a good number of reviews, the final collection is rather ting text with document-level annotations, namely customer-written reviews from the movies section of amazon.de web site.</S>
			<S sid ="103" ssid = "18">Perhaps expectedly, customer reviews do not focus exclusively on the film and its performance.</S>
			<S sid ="104" ssid = "19">Rather, it often occurs that customer reviews include a discussion of the physical (or other) medium that the film came on:3 (1) I am with Lovefilm (now Prime) and tried to stream the series.</S>
			<S sid ="105" ssid = "20">Terrible!</S>
			<S sid ="106" ssid = "21">Always [issues with] loading time and loss of the stream.</S>
			<S sid ="107" ssid = "22">It seems that Amazon hasn’t come to terms with the technology yet.</S>
			<S sid ="108" ssid = "23">Other reviews on Amazon match our domain fairly well, as in the following: (2) If this is truly a sequel to “Speed”, it only shows in the second hour of the film.</S>
			<S sid ="109" ssid = "24">It’s only then that deBont shows why he would be an action [film] specialist.</S>
			<S sid ="110" ssid = "25">Admittedly, even then we don’t get the same tension as in the predecessor, but in any case it’s better than the first hour of the film.</S>
			<S sid ="111" ssid = "26">While we found that a small quantity of data (20+20 hand-classified sentences) together with a 300-class LDA representation was sufficient to reach 100% accuracy in separating content-related versus media- related text, we found that filtering out the irrelevant texts made no difference for the mean square error, in sharp contrast to L1/Lasso regularization, which allows to learn a sparse lexicon.</S>
	</SECTION>
	<SECTION title="Variants of the RNTN Model. " number = "6">
			<S sid ="112" ssid = "1">While the RNTN model certainly performs well on the full Stanford Sentiment Treebank, it is likely that its performance on HeiST is suffering from sparse data problems, and that both words and particular constructions can be novel and unseen.</S>
			<S sid ="113" ssid = "2">In syntactic parsing, Koo et al.</S>
			<S sid ="114" ssid = "3">(2008) and Candito and Seddah (2010) have shown that using Brown clusters can be beneficial for alleviating sparse data problems in parsing.</S>
			<S sid ="115" ssid = "4">In a similar vein, Popat et al.</S>
			<S sid ="116" ssid = "5">(2013) have successfully applied crosslingual clustering to generalizing over potentially unseen words small.</S>
			<S sid ="117" ssid = "6">To complement our small in-domain dataset we use the most common way of get 3 German original text has been omitted for space reasons.</S>
			<S sid ="118" ssid = "7">rul e ty pe # in S S Tb # in H ei S T A V G I N V I N T M W E 1 1 9 4 6 8 2 1 5 8 6 6 1 4 1 8 2 3 5 1 9 2 2 8 2 8 9 6 4 6 1 9 3 6 nd HeiST replace-gold 0.844 0.730 replGermanPolarityClues 0.789∗∗ 0.681 replSentiMerge[0.23] 0.780∗ 0.648 Table 5: Incorporating additional information in (document-level) sentiment analysis for English, Hindi and Marathi.</S>
			<S sid ="119" ssid = "8">In our experiments, we follow Candito and Seddah (2010) in simply replacing words by clusters: in their experiments, even this simple procedure can yield an improvement, with improved results when the unlabeled data stems from the target domain.</S>
			<S sid ="120" ssid = "9">Since Brown clusters are mostly syntactic/semantic in nature and do not automatically distinguish positive or negative sentiment, we additionally performed multiple experiments to use clusters while incorporating additional sentiment information: On one hand, we try to incorporate the judgements on the Amazon near-domain dataset more directly into the clusters by using the repeated bisecting K-Means algorithm as implemented in CLUTO (Zhao and Karypis, 2005), with previous/next word, part-of-speech tag, and the score of the containing review as features.</S>
			<S sid ="121" ssid = "10">On the other hand, we split the Brown clusters according to the sentiment value that they have in a particular sentiment lexicon (e.g. SentiMerge), yielding three clusters 01101+, 01101- and 01101?</S>
			<S sid ="122" ssid = "11">instead of the original cluster 01101.</S>
			<S sid ="123" ssid = "12">As a final experiment, we consider replacing only sentiment words by a concatenation of their part- of-speech and the sentiment class (turning “a great film” into “a JJ++ film”), and leaving neutral words intact.</S>
			<S sid ="124" ssid = "13">As an upper baseline for this approach, we can get words’ sentiment polarity directly from training and testing data, which yields the replace- gold entry in table 5.</S>
			<S sid ="125" ssid = "14">Looking at the results in tables 2, 4 and 5, we see that simple support vector machine classification is very effective for reproducing the positive/negative sentiment of nodes and complete sentences, followed by crosslingual projection and a simple averaging approach; we also see that handling negation in the vote-and-flip approach seems to lower the score, just as the best model with word clusters and splitting (using the GermanPolarityClues lexicon) performs better than the word-based RNTN approach, but less well than the lexicon by itself.</S>
			<S sid ="126" ssid = "15">Even the replace- gold upper baseline – replacing sentiment-carrying words by their sentiment label, which raises the performance substantially – gives results below the simpler SVM approach, which is counterintuitive.</S>
			<SUBSECTION>7.1 Is it about Compositionality?.</SUBSECTION>
			<S sid ="127" ssid = "16">One motivation for using sub-sentence structure both in approaches for rule-based composition (as, e.g. in Taboada et al.</S>
			<S sid ="128" ssid = "17">(2011) and other lexicon-based approaches) as well as in more complex learning approaches such as RNN (Socher et al., 2011) and RNTN (Socher et al., 2013) is the idea that such approaches are able to model the interaction between sentiment-bearing words and sentiment-modifying words.</S>
			<S sid ="129" ssid = "18">An example for investigations based on this assumption is the work by Zhu et al.</S>
			<S sid ="130" ssid = "19">(2014), who contrast different lexicon-based approaches for handling negation with an RNTN model of negation and a modification of said model.</S>
			<S sid ="131" ssid = "20">Given the results using a lexicon-based approach implementing the vote-and-flip heuristic in comparison to the vote-only heuristic, we found it worth investigating what specific types of interaction exist in compositional sentiment treebanks, also considering that Zhu et al.’s investigations yielded a more precise picture of the sentiment-shifting action of negators as a highly lexicalized phenomenon.</S>
			<S sid ="132" ssid = "21">For our analysis, we grouped the production rules sp → sl sr in a sentiment treebank into one of the following categories: AVG A production is said to be averaging if the parent category is within the range of either daughter category.</S>
			<S sid ="133" ssid = "22">(e.g. mind-numbingly good would be the composition of a negative term and a positive term to a positive term, which still fits the averaging heuristic).</S>
			<S sid ="134" ssid = "23">INV A production is said to be inverting if one daughter category is neutral and the other daughter category is on the other side on the spectrum (e.g. “not great” landing on the negative side) INT A production is said to be intensifying if the parent category is on the same side of the scale as the daughters but more extreme.</S>
			<S sid ="135" ssid = "24">MWE A production is said to be a multi-word production if the daughter categories are classified as neutral while the parent category is not.4 As can be seen in table 6, the number of inverting and intensifying productions is dwarfed, both for the SST and for HeiST, by the number of multi-word rules.</S>
			<S sid ="136" ssid = "25">While it is likely that these counts are slightly distorted by noise in the annotation (as both datasets are the product of crowdsourcing), this fact is remarkable and merits further investigation.</S>
			<S sid ="137" ssid = "26">Types of multiword expressions If we try to group the nodes with a “multiword” production, we can distinguish at least the following categories: • aspect descriptions: In some cases, an adjective is specifically used to describe a (positive or negative) aspect of the movie, such as an elaborate continuation, or an expanded vision, where individual words have a neutral sentiment label (and conceivable could have been used in a non-aspect-specific way to convey a neutral or negative sentiment, such as an elaborate perversion, or an expanded nightshift).</S>
			<S sid ="138" ssid = "27">Similarly, wenig Handlung (not much action) 4 The MWE category also contains a small number – about.</S>
			<S sid ="139" ssid = "28">5% of total MWE productions – of positive-to-neutral and negative-to-neutral productions, which we found to be predominantly noise from the crowdsourcing process.</S>
			<S sid ="140" ssid = "29">has a negative meaning as a construction despite “wenig” (few/not much) not having a negative meaning itself.</S>
			<S sid ="141" ssid = "30">• expression strengthening is a phenomenon that occurs when a term is judged as neutral by annotators by itself, but gains a sentiment value when paired with an intensifier or nega- tor.</S>
			<S sid ="142" ssid = "31">For example, intrusive was labeled as neutral in SSTb, but simply intrusive as negative.</S>
			<S sid ="143" ssid = "32">• comparatives are a very regular construction where too much of something is almost always bad: too long, too insistent, too much, too many are all negative in SSTb, just as zu viel (too many) and zu wenig (not enough) and other counterparts in HeiST are negative.</S>
			<S sid ="144" ssid = "33">• true constructions such as plot holes or historically significant in SSTB, or ruhigen Gewis sens (with a calm conscience) and Finger weg (don’t touch it) in HeiST are both a problem for approaches relying purely on composition and not regular enough that we would expect to model it as a regular construction.</S>
			<S sid ="145" ssid = "34">Some of the neutral-to-positive or neutral-to- negative transitions don’t seem well-motivated and may be regarded as artifacts from the crowdsourcing, as does n’t, is n’t and are n’t are negative in SSTb whereas ’s not, do n’t and did n’t get a neutral label.</S>
			<S sid ="146" ssid = "35">In HeiST, nicht immer (not always) as well as nicht ganz (not quite) are negative, whereas auch nicht (neither) and nicht so (not as) or nicht unbedingt (not necessarily) are neutral.</S>
			<S sid ="147" ssid = "36">The MWE productions seem to overlap with well- known linguistic phenomena – consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a ‘prior’ polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).</S>
			<S sid ="148" ssid = "37">Yet, others may be idiosyncracies introduced by the crowdsourcing process, and powerful learners such as RNTN or the approach of Hall et al.</S>
			<S sid ="149" ssid = "38">(2014) will gain performance from simply memorizing the idiosyncracies of the data when there is Ty pe Total S V M C o rr Prec C L S A C o rr Prec R N T N C o rr Prec +r ep lace go ld C or r Prec AV G 6341 M W E 1638 IN T 612 ID 392 IN V 259 34 08 0.546 3 6 9 0.225 3 7 0 0.605 2 8 3 0.722 7 6 0.293 31 58 0.506 5 3 8 0.328 3 6 2 0.592 2 6 9 0.686 6 5 0.251 36 04 0.577 5 3 8 0.359 4 1 3 0.675 2 8 6 0.730 9 3 0.359 43 09 0.690 5 4 6 0.333 4 7 0 0.768 3 2 3 0.824 7 9 0.305 Table 7: Precision of rules with non-neutral parent label (ID: daughters and parent have identical labels) enough of it – because of the way the Stanford Sentiment Treebank is constructed, phrases always have the same (context-independent) label, while we may get a more accurate (and possibly different) picture from introducing additional means of quality control (which in turn increases the necessary investment for such a sentiment treebank).</S>
			<SUBSECTION>7.2 Contrasting SVM and RNTN behaviour.</SUBSECTION>
			<S sid ="150" ssid = "39">In table 7, we tabulated the classification accuracy for the parent node in different types of productions in HeiST.</S>
			<S sid ="151" ssid = "40">In this evaluation, we counted a production as correct whenever the parent node has the right sentiment label (in parallel with the labeled recall in syntactic evaluation), ignoring for the moment the question whether the production produced by a system falls into the same category.</S>
			<S sid ="152" ssid = "41">It is easy to see that AVG-type productions are the least error- prone for all classifiers, whereas MWE and INV productions pose a significant challenge for the models.</S>
			<S sid ="153" ssid = "42">We also see that on these challenging production, the RNTN performs better than the other methods.</S>
			<S sid ="154" ssid = "43">8 Summary.</S>
			<S sid ="155" ssid = "44">We presented a novel dataset for subsentential sentiment classification, which uses the same conventions as the Stanford Sentiment Treebank (SSTb), which is the only German resource of this type besides the smaller (270 sentences) MLSA corpus (Clematide et al., 2012).</S>
			<S sid ="156" ssid = "45">We performed a systematic exploration into supervised, cross-lingual, and lexicon-based approaches on this dataset and found that, paradoxically, the performance of the state- of-the-art recursive neural tensor network (RNTN) models are severely impeded in this data-sparse situation, unlike latent-variable models for syntax which can deal with such conditions quite well: Lavelli and Corazza (2009), for example, reports that the best results for parsing on the very small TUT treebank (slightly more than 2000 sentences) can be achieved using a PCFG-LA model.</S>
			<S sid ="157" ssid = "46">We showed that a wide variety of models – from lexicon-based sentiment prediction over SVM with unigram features to crosslingual classification – performs better than the RNTN, and that methods to improve RNTN performance that work in other settings (syntax) do not offer any easy fix.</S>
			<S sid ="158" ssid = "47">In a second step, we took a closer look at the crowdsourced data in order to explain certain counterintuitive results (such as the fact that most sentiment lexicons do not benefit from negation handling, or that the upper baseline achievable with the RNTN by getting gold-standard information on positive and negative words is at about the same level as our SVM classifier), and found that SSTb-type resources show marked differences from e.g., the MLSA dataset as they incorporate multiword items, but seem to be challenging for the study of compositionality due to noise that is not present in expert-annotated resources.</S>
	</SECTION>
</PAPER>
