<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">More and more product reviews emerge on E-commerce sites and microblog systems nowadays.</S>
		<S sid ="2" ssid = "2">This information is useful for consumers to know the others&apos; opinion on the products before purchasing, or companies who want to learn the public sentiment of their products.</S>
		<S sid ="3" ssid = "3">In order to effectively utilize this information, this paper has done some sentiment analysis on these multi-source reviews.</S>
		<S sid ="4" ssid = "4">For one thing, a binary classification framework based on the aspects of product is proposed.</S>
		<S sid ="5" ssid = "5">Both explicit and implicit aspect is considered and multiple kinds of feature weighing and classifiers are compared in our framework.</S>
		<S sid ="6" ssid = "6">For another, we use several machine learning algorithms to classify the product reviews in microblog systems into positive, negative and neutral classes, and find OVA-SVMs perform best.</S>
		<S sid ="7" ssid = "7">Part of our work in this paper has been applied in a Chinese Product Review Mining System.</S>
		<S sid ="8" ssid = "8">Keywords: product review, sentiment analysis, microblog, SVM.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">With the development of Internet, more and more customers get used to purchasing products on E-commerce sites such as 360buy1 and Newegg2.</S>
			<S sid ="10" ssid = "10">They also write reviews on the products after using them, which produce a large number of reviews on the Internet.</S>
			<S sid ="11" ssid = "11">In addition, microblog, a system that allows users to post messages of no more than 140 words, and share information instantaneously based on the relationship between users, is under rapid development, such as Twitter3, Sina microblog4 and Tencent microblog5.</S>
			<S sid ="12" ssid = "12">A lot of microblogs contain latest product reviews.</S>
			<S sid ="13" ssid = "13">Reviews from the above two large data sources contain much useful information for users and companies.</S>
			<S sid ="14" ssid = "14">Users can make better purchasing decisions based on these reviews, while companies can also analyze customers&apos; satisfaction according to these reviews, and further improve the quality of their products.</S>
			<S sid ="15" ssid = "15">Since there is a mass of 1 http://www.360buy.com 2 http://www.newegg.com.cn 3 http://www.twitter.com 4 http://weibo.com 5 http://t.qq.cn D.-S.</S>
			<S sid ="16" ssid = "16">Huang et al.</S>
			<S sid ="17" ssid = "17">(Eds.): ICIC 2012, LNCS 7389, pp.</S>
			<S sid ="18" ssid = "18">301–308, 2012.</S>
			<S sid ="19" ssid = "19">© SpringerVerlag Berlin Heidelberg 2012 product reviews and a single user cannot read all of them, automatically mining the reviews from multiple sources is particularly important.</S>
			<S sid ="20" ssid = "20">Most reviews in Chinese E-commerce sites are labeled with advantage or disadvantage, which is naturally suitable for binary classification.</S>
			<S sid ="21" ssid = "21">The state-of-art research in Chinese sentiment analysis mainly focuses on the whole review classification, while customers often desire a more detailed understanding of products.</S>
			<S sid ="22" ssid = "22">For example, they want to know others&apos; opinion on the battery of a cell phone.</S>
			<S sid ="23" ssid = "23">Therefore, we propose a framework of sentiment classification at aspect level to solve this problem.</S>
			<S sid ="24" ssid = "24">In our framework, not only explicit but also implicit aspects are taken into account.</S>
			<S sid ="25" ssid = "25">To our knowledge, no implicit aspect discovery work of product review in Chinese language has been reported before.</S>
			<S sid ="26" ssid = "26">For the reviews of each aspect, the unigram features of words are used as text features.</S>
			<S sid ="27" ssid = "27">We also compare the performance of three feature weighing strategies, three reduction dimension, and three classification approaches.</S>
			<S sid ="28" ssid = "28">The sentiment analysis for reviews of products on microblogs is in its infancy.</S>
			<S sid ="29" ssid = "29">Besides the microblogs that express opinion on the products, some microblogs only give some statements relative to the products, which contain no sentiment polarity, or are neutral.</S>
			<S sid ="30" ssid = "30">Therefore, in this paper, we exploit linear regression, multi-class classification, two-stage classification and Mincut model optimization to classify the product related microblogs into three classes, and compare the performance of these methods.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="31" ssid = "1">Sentiment classification mainly includes two methods: supervised learning and unsupervised learning.</S>
			<S sid ="32" ssid = "2">Turney[1] proposed a simple unsupervised method to classify reviews into positive and negative categories.</S>
			<S sid ="33" ssid = "3">Pang[2] used Naive Bayes (NB), Max entropy (ME) and SVM separately as supervised learning algorithms for binary classification of reviews.</S>
			<S sid ="34" ssid = "4">Besides these, Pang[3] exploited regression and One-Vs-All SVMs to predict the score for classification, namely the multi-classification, which can also be realized by combining binary classifiers in a two-stage manner.</S>
			<S sid ="35" ssid = "5">Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.</S>
			<S sid ="36" ssid = "6">For sentiment classification in microblogs, Go[6] was the first to classify Twitter data into two classes (positive and negative).</S>
			<S sid ="37" ssid = "7">Barbosa[7] classified Twitter data into three classes in a two-stage manner.</S>
			<S sid ="38" ssid = "8">Jiang[8] analyzed topic related sentiment for Twitter text.</S>
			<S sid ="39" ssid = "9">Based on the two-stage binary classification result, and the forwarding relationship between users, the performance was promoted using the method of graph.</S>
			<S sid ="40" ssid = "10">But for Chinese microblogs systems, only text can be obtained instead of the relationship between users and microblogs.</S>
	</SECTION>
	<SECTION title="Binary Sentiment Classification Based on Product Aspect. " number = "3">
			<S sid ="41" ssid = "1">The overall flow chart about polarity classification (positive and negative) based on product aspect is shown in Fig.1.</S>
			<S sid ="42" ssid = "2">Fig.</S>
			<S sid ="43" ssid = "3">1.</S>
			<S sid ="44" ssid = "4">The polarity classification (positive and negative) based on product aspect framework Aspect, Aspect Word.</S>
			<S sid ="45" ssid = "5">Product aspect refers to some attribute, component, or function of the product.</S>
			<S sid ="46" ssid = "6">The word or phrase used to represent the aspect of product is called the aspect word.</S>
			<S sid ="47" ssid = "7">Aspect is actually the concept at semantic level, and the aspect word is the external presentation form of the aspect.</S>
			<S sid ="48" ssid = "8">Different categories of products generally have a different set of aspects.</S>
			<S sid ="49" ssid = "9">Product aspect can be divided into explicit aspect and implicit aspect.</S>
			<S sid ="50" ssid = "10">Explicit aspect refers to the aspect word which describes the product performance or function can be directly found in the product reviews.</S>
			<S sid ="51" ssid = "11">Implicit aspect may be found after the sentence semantic understanding because there are no aspect words in review sentence.</S>
			<S sid ="52" ssid = "12">3.1 Preprocessing.</S>
			<S sid ="53" ssid = "13">In order to get relatively clean text about product reviews, messy code will be removed in the preprocessing stage according to a user-defined messy code list.</S>
			<S sid ="54" ssid = "14">Next we use the word segmentation software tools to make the sentence processed.</S>
			<S sid ="55" ssid = "15">There are some words that appear in almost all of the text which are called stop words.</S>
			<S sid ="56" ssid = "16">The text content is represented more accurately after deleting stop words.</S>
			<S sid ="57" ssid = "17">Some words in the traditional stop words list is retained because they have sentiment polarity which is useful information for sentiment classification.</S>
			<S sid ="58" ssid = "18">3.2 Build Aspect Words List.</S>
			<S sid ="59" ssid = "19">We use a statistics-based algorithm to automatically build the vocabulary about the product aspects.</S>
			<S sid ="60" ssid = "20">Here is the algorithm: 1.</S>
			<S sid ="61" ssid = "21">Count occurrences number of all the nominal words..</S>
			<S sid ="62" ssid = "22">We count the occurrences number of all the unigram whose part of speech(POS) tagging is noun in the reviews of same product category.</S>
			<S sid ="63" ssid = "23">2.</S>
			<S sid ="64" ssid = "24">Filter high-frequency words.</S>
			<S sid ="65" ssid = "25">The final aspect words list is filtered by threshold of high frequency and product specifications.</S>
			<S sid ="66" ssid = "26">3.3 Review Sentences Breakup.</S>
			<S sid ="67" ssid = "27">First of all, we break a review into sentences according to various sentence end punctuations, each of which is further cut into short sentences (SS) with comma.</S>
			<S sid ="68" ssid = "28">Then each SS is assigned to different sentiment categories of different aspects according to the aspect vocabulary made in advance.</S>
			<S sid ="69" ssid = "29">Finally, if an SS has implicit aspect, its words should also be correctly distributed.</S>
			<S sid ="70" ssid = "30">3.4 Implicit Aspect Discovery.</S>
			<S sid ="71" ssid = "31">For example, if there is a word &quot;beautiful&quot; in the sentence and without any explicit aspect word appeared, we also need to know &quot;beautiful&quot; is likely in the description of the product&apos;s appearance which is just the implicit aspect of product.</S>
			<S sid ="72" ssid = "32">The discovery of implicit aspects needs to balance the following factors: 1.</S>
			<S sid ="73" ssid = "33">Calculate mutual information (MI) scores between adjectives in the current SS and each aspect.</S>
			<S sid ="74" ssid = "34">Adjectives with most relevant aspects will be calculated and the aspect of the entire SS is determined by voting.</S>
			<S sid ="75" ssid = "35">2.</S>
			<S sid ="76" ssid = "36">If the above rule doesn&apos;t work, the SS is just assigned to the previous SS&apos;s aspect..</S>
			<S sid ="77" ssid = "37">3.5 Text Feature Representation.</S>
			<S sid ="78" ssid = "38">After the processing mentioned above, we get all the review sentences in each aspect in each category, and vector space model is employed to represent them.</S>
			<S sid ="79" ssid = "39">Features of the review text using word-based unigram, but different word have the different ability and importance to represent the text which is called weight.</S>
			<S sid ="80" ssid = "40">In this paper, BOOL, TF, and TFIDF are used as feature weights.</S>
			<S sid ="81" ssid = "41">BOOL weight means that if the count of the term appears in the current document greater than zero, the weight is 1, otherwise 0.</S>
			<S sid ="82" ssid = "42">TF (term frequency) refers to the frequency of the words occurrence in the file.</S>
			<S sid ="83" ssid = "43">IDF (inverse document frequency) measure the common importance of the word.</S>
			<S sid ="84" ssid = "44">In addition, we use chi-square statistics to reduce feature dimension.</S>
			<S sid ="85" ssid = "45">3.6 Classifier.</S>
			<S sid ="86" ssid = "46">Naive Bayes.</S>
			<S sid ="87" ssid = "47">Navie Bayes is a simple model which calculates posterior probability of each class based on the priori probability and the likelihood.</S>
			<S sid ="88" ssid = "48">Class with the largest posterior probability is assigned as the class of the document.</S>
			<S sid ="89" ssid = "49">KNN.</S>
			<S sid ="90" ssid = "50">The basic idea of the KNN algorithm is considering the K nearest (the most K similar) texts in the training set of the new given text.</S>
			<S sid ="91" ssid = "51">The label of the new text is determined by these K texts.</S>
			<S sid ="92" ssid = "52">SVM.</S>
			<S sid ="93" ssid = "53">SVM is a popular classification algorithm which compresses the raw data set to support vector set and learn a decision hyperplane in the vector space.</S>
			<S sid ="94" ssid = "54">This hyperplane best splits the data points into the two classes.</S>
			<S sid ="95" ssid = "55">In this paper, NB, KNN and SVM are implemented using data mining tool Weka6.</S>
			<S sid ="96" ssid = "56">6 http://www.cs.waikato.ac.nz/ml/weka/</S>
	</SECTION>
	<SECTION title="Sentiment Analysis on Product Reviews in Microblog System. " number = "4">
			<S sid ="97" ssid = "1">4.1 Text Feature Extraction.</S>
			<S sid ="98" ssid = "2">Like mentioned in Section 3.1, stop words and messy code in the microblog will be removed, and then the Chinese word segmentation and POS tagging will be done.</S>
			<S sid ="99" ssid = "3">The vector space model is still employed to represent each microblog.</S>
			<S sid ="100" ssid = "4">The features are unigrams and the weight is BOOL value.</S>
			<S sid ="101" ssid = "5">However, we can take the advantage of characteristic of microblog to portray it better and reduce the feature dimension.</S>
			<S sid ="102" ssid = "6">Emoticons.</S>
			<S sid ="103" ssid = "7">For example, the text form of emoticon in microblog text is &quot;[ha ha]&quot;.</S>
			<S sid ="104" ssid = "8">All the positive emoticons will be converted into token &quot;POS&quot;, and all negative into token &quot;NEG&quot; according to the manual defined emotions list.</S>
			<S sid ="105" ssid = "9">Usernames.</S>
			<S sid ="106" ssid = "10">Forwarding mechanism of microblog systems makes microblog amazing transmitted.</S>
			<S sid ="107" ssid = "11">It is in the form of @ + username (such as @ryanking1219).</S>
			<S sid ="108" ssid = "12">So we use string &quot;USERNAME&quot; to instead all words beginning with @.</S>
			<S sid ="109" ssid = "13">Links.</S>
			<S sid ="110" ssid = "14">Users like to include the URL which usually begins with &quot;http&quot; when they share videos or news, such as http://t.cn/aCKddG.</S>
			<S sid ="111" ssid = "15">All website links are normalized to the token &quot;URL&quot;.</S>
			<S sid ="112" ssid = "16">Topics.</S>
			<S sid ="113" ssid = "17">In microblog systems, topic starts with &quot;#&quot; and also ends with &quot;#&quot;.</S>
			<S sid ="114" ssid = "18">We will replace all the string in this form with an equivalence class string &quot;TOPIC&quot;.</S>
			<S sid ="115" ssid = "19">4.2 Two-Stage SVM.</S>
			<S sid ="116" ssid = "20">Step 1: Subjectivity Classifier Neutral samples in the training data is considered as positive examples, while positive and negative samples as negative cases to train the classifier.</S>
			<S sid ="117" ssid = "21">Do binary classification on the test data for subjective and objective detection.</S>
			<S sid ="118" ssid = "22">Step 2: Polarity Classifier Positive samples in the training data is considered as positive examples, while negative samples as negative cases to train the classifier.</S>
			<S sid ="119" ssid = "23">Do binary classification on the test data which is correctly divided into subject class in Step 1 for positive and negative detection.</S>
			<S sid ="120" ssid = "24">4.3 Minimum Cut Model Optimization.</S>
			<S sid ="121" ssid = "25">Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.</S>
			<S sid ="122" ssid = "26">Binary classification with Mincut in graph is based on the idea that similar items should be split in the same cut.</S>
			<S sid ="123" ssid = "27">We build an undirected graph G with vertices {s, t, v1, v2, …, vn}; s is the source and t is the sink, and all items in the test data are seen as vertices vi.</S>
			<S sid ="124" ssid = "28">Each vertex v is connected to s and t via a weighted edge.</S>
			<S sid ="125" ssid = "29">The weight is the estimation of the probability converted from SVM classifier output.</S>
			<S sid ="126" ssid = "30">Each edges (vi, vk) with weight assoc(vi, vk) expresses the similarity between vi and vk and how important they should be in the same class.</S>
			<S sid ="127" ssid = "31">Then we remove a set of edges to divide graph into two disconnected sub-graphs.</S>
			<S sid ="128" ssid = "32">The vertices via s are positive instances and the vertices via t are negative instances.</S>
			<S sid ="129" ssid = "33">We penalize when putting highly similar items into different classes, so the best split is one which removes edges with lowest weights.</S>
			<S sid ="130" ssid = "34">This is exactly the Mincut of graph.</S>
			<S sid ="131" ssid = "35">Because the capacity of the Mincut equals the max flow value, we can employ EdmondsKarp algorithm to compute the Mincut.</S>
			<S sid ="132" ssid = "36">In experiment, we set α * cos (vi , v j ) , if α * cos (vi , v j ) &gt; t i j  0 , otherwise (1) where cos(vi, vj) is the text similarity between vi and vj.</S>
			<S sid ="133" ssid = "37">Constant α is the scale factor of association scores.</S>
			<S sid ="134" ssid = "38">Only scores higher than threshold t will be taken into consideration.</S>
			<S sid ="135" ssid = "39">4.4 Regression.</S>
			<S sid ="136" ssid = "40">Text classification is also a problem that utilizes text features to predict a category label, and similar entries should have similar category labels.</S>
			<S sid ="137" ssid = "41">The most classic model is the linear regression function which is actually a linear weighted sum of all the features.</S>
			<S sid ="138" ssid = "42">The final score is rounded as a category label.</S>
			<S sid ="139" ssid = "43">4.5 OVA-SVMs.</S>
			<S sid ="140" ssid = "44">Although the original SVM is used to solve binary classification problem, we can combine several SVM classifiers to achieve multi-class classification, such as one- versus-all method (for short of OVA-SVMs).</S>
			<S sid ="141" ssid = "45">When Training each classifier we mark one category samples as positive examples, and all the rest samples as negative examples.</S>
			<S sid ="142" ssid = "46">So that k categories of samples can construct k SVM classifiers in turn.</S>
			<S sid ="143" ssid = "47">Class label of the test sample assigned to the class which has the largest classification function value.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="144" ssid = "1">5.1 Sentiment Classification Based on Product Aspect.</S>
			<S sid ="145" ssid = "2">We downloaded 821 cell phone products data from the 360buy and Newegg, and finally get 663,537 advantages reviews and 314,529 disadvantage reviews.</S>
			<S sid ="146" ssid = "3">CHI dimension reduction selects 5%, 20% and 100% words most relevant to each aspect of each product category, represented as CHI-5, CHI-20, and CHI-100.</S>
			<S sid ="147" ssid = "4">Of course, from the whole view of each product category, the most relevant words of each aspect will be overlapped with each other.</S>
			<S sid ="148" ssid = "5">Performance evaluation methods commonly apply PRECISION, RECALL, and F- values, but these only represent local significance (i.e., the performance of each aspect in each product category).</S>
			<S sid ="149" ssid = "6">If you want to evaluate the overall classification performance, all the aspects of product category need to be taken into account to calculate the F-value of the MICRO AVERAGING and MACRO AVERAGING.</S>
			<S sid ="150" ssid = "7">Micro-average gives the same weight to each document in each product category while Macro-average gives the same weight to each aspect in each product category.</S>
			<S sid ="151" ssid = "8">Results are illustrated in Fig.2.</S>
			<S sid ="152" ssid = "9">Fig.</S>
			<S sid ="153" ssid = "10">2.</S>
			<S sid ="154" ssid = "11">Cell phone experiment result (17 aspects) The following conclusions could be drawn from the experimental result: First, the best combination is BOOLSVM.</S>
			<S sid ="155" ssid = "12">Second, the consequent of TF is very close to the TFIDF, but is slightly lower than BOOL.</S>
			<S sid ="156" ssid = "13">In addition, the SVM classification performance is better than NB and KNN.</S>
			<S sid ="157" ssid = "14">Last but not least, CHI-5 result is similar with CHI-20 and CHI-100, even sometimes CHI-5 perform best, so dimension reduction is proved effective.</S>
			<S sid ="158" ssid = "15">5.2 Sentiment Analysis on Product Reviews in Microblog System.</S>
			<S sid ="159" ssid = "16">We select 5 popular items including &quot;Nokia&quot;, &quot;iphone 4s&quot;, &quot;E72i&quot;, &quot;Lenovo&quot; and &quot;Canon&quot;.</S>
			<S sid ="160" ssid = "17">For all of these items, we downloaded 2,100 microblogs through the API provided by Sina microblog and Tencent microblog from October 2011 to November 2011.</S>
			<S sid ="161" ssid = "18">We manually labeled all the microblogs and finally obtain 729 positive, 345 negative, and 1,026 neutral microblogs respectively.</S>
			<S sid ="162" ssid = "19">The results are listed in Table 1.</S>
			<S sid ="163" ssid = "20">Table 1.</S>
			<S sid ="164" ssid = "21">Result for microblog classification Regression OVA-SV Ms Two-stage SVM Two-stage SVM + MinCut Accuracy 51.6% 64.5% 62.6% 64.0% As shown in Table 1, the OVA-SVMs classifier is best.</S>
			<S sid ="165" ssid = "22">Two-stage SVM classifier after optimization with the minimum cut model (when parameter α = 0.2, t = 0.5) has 1.4% increase in performance, and reach almost the same effect of OVA-SVMs.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="166" ssid = "1">This paper makes sentiment analysis on product reviews from multiple data sources.</S>
			<S sid ="167" ssid = "2">Firstly, a binary sentiment classification framework based on the aspects of the product is proposed.</S>
			<S sid ="168" ssid = "3">On the granularity of aspect, we use unigram as review feature, and adopt BOOL weight and SVM classifier to get the best results.</S>
			<S sid ="169" ssid = "4">Secondly, we classify our product review in microblog systems into three classes.</S>
			<S sid ="170" ssid = "5">OVA-SVMs method offers the optimal result.</S>
			<S sid ="171" ssid = "6">Finally, Work in Section 3 has been partially applied in a Chinese Product Review Mining System7, and Section 4 will be applied soon.</S>
	</SECTION>
</PAPER>
