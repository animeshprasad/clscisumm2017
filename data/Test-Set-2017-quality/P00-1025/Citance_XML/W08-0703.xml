<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">A stochastic approach to learning phonology.</S>
		<S sid ="2" ssid = "2">The model presented captures 715% more phonologically plausible underlying forms than a simple majority solution, because it prefers “pure” alternations.</S>
		<S sid ="3" ssid = "3">It could be useful in cases where an approximate solution is needed, or as a seed for more complex models.</S>
		<S sid ="4" ssid = "4">A similar process could be involved in some stages of child language acquisition; in particular, early learning of phonotactics.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Sound changes in natural language, such as stem variation in inflected forms, can be described as phonological processes.</S>
			<S sid ="6" ssid = "6">These are governed by a constraint hierarchy as in Optimality Theory (OT), or by a set of ordered rules.</S>
			<S sid ="7" ssid = "7">Both rely on a single lexical representation of each morpheme (i.e., its underlying form), and context-sensitive transformations to surface forms.</S>
			<S sid ="8" ssid = "8">Phonological changes often affect segments near morpheme boundaries, but can also apply over an entire prosodic word, as in vowel harmony.</S>
			<S sid ="9" ssid = "9">It does not seem straightforward to incorporate context into a Bayesian model of phonology, although a clever solution may yet be found.</S>
			<S sid ="10" ssid = "10">A standard way of incorporating conditioning environments is to treat them as factors in a Gibbs model (Liang and Klein, 2007), but such models require an explicit calculation of the partition function.</S>
			<S sid ="11" ssid = "11">Unless the rule contexts possess some kind of locality, we don’t know how to compute this partition function efficiently.</S>
			<S sid ="12" ssid = "12">Some context could be captured by generating underlying phonemes from an n-gram model, or by annotating surface forms with neighborhood features.</S>
			<S sid ="13" ssid = "13">However, the effects of autosegmental phonology and other long-range dependencies (like vowel harmony) cannot be easily Bayesianized.</S>
			<S sid ="14" ssid = "14">1.1 Related Work.</S>
			<S sid ="15" ssid = "15">In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.</S>
			<S sid ="16" ssid = "16">A finite-state approximation of optimality theory (Karttunen, 1998) was later refined into a compact treatment of gradient constraints (Gerdemann and van Noord, 2000).</S>
			<S sid ="17" ssid = "17">Recent work on Bayesian models of morphological segmentation (Johnson et al., 2007) could be combined with phonological rule induction (Goldwater and Johnson, 2004) in a variety of ways, some of which will be explored in our discussion of future work.</S>
			<S sid ="18" ssid = "18">Also, the Hierarchical Bayes Compiler (Daume III, 2007) could be used to generate a model similar to the one presented here, but less con- strained1 which makes correspondingly more random, less accurate predictions.</S>
			<S sid ="19" ssid = "19">1.2 Dataset.</S>
			<S sid ="20" ssid = "20">As we describe the model and its implementation in this and subsequent sections, we will refer to a sam 1 Recent updates to HBC, inspired by discussions with the author, have addressed some of these limitations.</S>
			<S sid ="21" ssid = "21">12 Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 12–19, Columbus, Ohio, USA June 2008.</S>
			<S sid ="22" ssid = "22">Qc 2008 Association for Computational Linguistics ple dataset (in Figure 1), consisting of a paradigm2 of verb stems and person/number suffixes.</S>
			<S sid ="23" ssid = "23">The head of each row or column is an /underlying/ form, which in 3rd person singular is a phonologically null segment (represented as /ø/).</S>
			<S sid ="24" ssid = "24">In [surface] forms, the realization of each morpheme is affected by phonological processes.</S>
			<S sid ="25" ssid = "25">For example, in the combination of /tieta¨/ + /vat/, the result is [tieta¨+va¨t], where the 3rd person plural /a/ becomes [a¨] due to vowel harmony.</S>
			<S sid ="26" ssid = "26">1.3 Bayesian Approach.</S>
			<S sid ="27" ssid = "27">As a baseline model, we select the most frequently occurring allophone as the underlying form.</S>
			<S sid ="28" ssid = "28">Our goal is to outperform this baseline using a Bayesian model.</S>
			<S sid ="29" ssid = "29">In other words, what patterns in phonological processes can be inferred with such a statistical model?</S>
			<S sid ="30" ssid = "30">This simple framework begins learning with the assumption that the underlying forms are faithful to the surface (i.e., without considering markedness or phonotactics).</S>
			<S sid ="31" ssid = "31">We model the generation of surface forms from underlying ones on the segmental (character) level.</S>
			<S sid ="32" ssid = "32">Input is an inflectional paradigm, with tokens of the form stem+suffix.</S>
			<S sid ="33" ssid = "33">Morphology is limited to a single suffix (no agglutination), and is already identified.</S>
			<S sid ="34" ssid = "34">Each character of an underlying stem or suffix (ui ) generates surface characters (sij ) in an entire row or column of the input.</S>
			<S sid ="35" ssid = "35">To capture the phonology of a variety of languages with a single model, we need constraints from linguistically plausible priors (universal grammar).</S>
			<S sid ="36" ssid = "36">We prefer that underlying characters be preserved in surface forms, especially when there is no alternation.</S>
			<S sid ="37" ssid = "37">It is also reasonable that there be fewer underlying forms (phonemes) than surface forms (phones, phonetic inventory), to account for allo- phones.</S>
			<S sid ="38" ssid = "38">We expect to be able to capture a significant subset of phonological processes using a simple model (only faithfulness constraints).</S>
			<S sid ="39" ssid = "39">1.4 Pure Generators.</S>
			<S sid ="40" ssid = "40">Our model has an advantage over the baseline in its preference for “purity” in underlying forms.</S>
			<S sid ="41" ssid = "41">Each underlying segment should generate as few distinct 2 The paradigm format lends itself to analysis of word types, but if supplemented with surface counts, can also handle tokens.</S>
			<S sid ="42" ssid = "42">surface segments as possible: if it generates non- alternating (identical) segments, it will be less likely to generate an alternation in addition.</S>
			<S sid ="43" ssid = "43">This means that when two segments alternate, the underlying form should be the one that appears less frequently in other contexts, irrespective of the majority within the alternation.</S>
			<S sid ="44" ssid = "44">In the first stem of our Finnish verb conjugation (Figure 1), we see a [t,d] alternation (a case of consonant gradation), as well as unalternating [t].</S>
			<S sid ="45" ssid = "45">If we isolate three of the surface forms where /tieta¨/ is inflected (1st person singular, and 3rd person singular and plural), and consider only the dental segments in the stem of each, we have two underlying segments.</S>
			<S sid ="46" ssid = "46">Here, we use question marks to indicate unknown underlying segments.</S>
			<S sid ="47" ssid = "47">/??/ [dt] [tt] [tt] In this subset of the data, the reasonable candidate underlying forms are /t/ and /d/.</S>
			<S sid ="48" ssid = "48">These two compete to explain the observed data (surface forms).</S>
			<S sid ="49" ssid = "49">The nature of the prior probability distribution determines whether the majority is hypothesized for each underlying form, so /t/ produces both alternating and unalternating surface segments, or /d/ is hypothesized as the source of the alternation (and /t/ remains “pure”).</S>
			<S sid ="50" ssid = "50">In a Bayesian setting, we impose a sparse prior over underlying forms conditioned on the surface forms they generate.If u2 is hypothesized to be /t/, the posterior prob ability of u1 being /t/ goes down: P (u1 = /t/|u2 = /t/) &lt; P (u1 = /t/) The probability of u1 being the competitor, /d/, correspondingly increases: P (u1 = /d/|u2 = /t/) &gt; P (u1 = /d/) Even though the majority in this case would be /t/, the favored candidate for the alternating form was /d/.</S>
			<S sid ="51" ssid = "51">This happened because of how we defined the model’s prior, in combination with the evidence that /t/ (assigned to u2) generated the sequence of [t].</S>
			<S sid ="52" ssid = "52">So selection bias prefers /d/ as the source of an ambiguous segment, leaving /t/ to always generate itself.</S>
			<S sid ="53" ssid = "53">A similar effect can occur if there are both unalternating [t]’s and [d]’s on the surface, in addition to the [t,d] alternation.</S>
			<S sid ="54" ssid = "54">The candidate (/t/ or /d/) that is ❛ S Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.</S>
			<S sid ="55" ssid = "55">generating fewer unalternating segments is preferred to explain the alternation.</S>
			<S sid ="56" ssid = "56">For example, if there were 1000 cases of [t], 500 [d] and 500 [t,d], we would expect the following hypotheses: /t/ → [t], /d/ → [d] and /d/ → [t, d].</S>
			<S sid ="57" ssid = "57">This is because one of the two candidates must be responsible for both unalternating and alternating segments, but we prefer to have as much “‘purity” as possible, to minimize ambiguity.</S>
			<S sid ="58" ssid = "58">With this solution, we still have 1000 pure /t/ → [t], and only the 500 /d/ → [d] are now indistinct from /d/ → [t, d].</S>
			<S sid ="59" ssid = "59">If we had selected /t/ as the source of the alternation, there would be only 500 remaining “pure” (/d/) segments, and 1500 ambiguous /t/.</S>
			<S sid ="60" ssid = "60">Our Bayesian model should prefer the less prior over underlying form encourages sparse solutions, so βu &lt; 1 for all u. The prior over surface form given underlying encourages identity mapping, /x/ → [x], so αxx &gt; 1, and discourages different segments, /x/ → [y], so αxy &lt; 1 for all x /= y. β α φ θ ambiguous (“purer”) solution, given an appropriate nc prior.</S>
	</SECTION>
	<SECTION title="Model. " number = "2">
			<S sid ="61" ssid = "1">We will use boldface to indicate vectors, and subscripts to identify an element from a vector or matrix.</S>
			<S sid ="62" ssid = "2">The variable N(u) is a vector of observed counts with the current underlying form hypothe ses.</S>
			<S sid ="63" ssid = "3">The notation we use for a vector u with one u s mnu nuFigure 2: Bayesian network: α and β are vectors of hy element i removed is u−i, so we can exclude the counts associated with a particular underlying form perparameters, and θi (for i ∈ {1, . . .</S>
			<S sid ="64" ssid = "4">, nc}) and φ are by indicating that in the parenthesized variable (i.e., N(u−4) is all the counts except those associated with the fourth underlying form).</S>
			<S sid ="65" ssid = "5">Ni (u) is the number of times character i is used as an underlying form, and Nij (u) is the number of times character i generated surface character j. The priors over surface s and underlying u segments in Figure 2 are captured by Dirichlet priors α and β, which generate the multinomial distributions θ and φ, respectively (see Figure 3).</S>
			<S sid ="66" ssid = "6">The distributions.</S>
			<S sid ="67" ssid = "7">u is a vector of underlying forms, generated from φ, and si (for i ∈ nu) is a set of observed surface forms generated from the hidden variable ui according to θi Phones and phonemes are drawn from a set of characters (e.g., IPA, unicode) C used to represent them.</S>
			<S sid ="68" ssid = "8">φi is the probability of a character (Ci for i ∈ nc) being an underlying form, irrespective of current alignments or its position in the paradigm.θij is the conditional probability of a surface char θc | α ∼ DIR(α), c = 1, . . .</S>
			<S sid ="69" ssid = "9">, nc φ | β ∼ DIR(β) ui | φi ∼ MU LTI(φi), i = 1, . . .</S>
			<S sid ="70" ssid = "10">, nu sij | ui , θui ∼ MU LTI(θ ui ), i = 1, . . .</S>
			<S sid ="71" ssid = "11">, nu, j = 1, . . .</S>
			<S sid ="72" ssid = "12">, mi Figure 3: Model parameters: nc is # different segments, nu is # underlying segments acter (skn = Cj for j ∈ nc, n ∈ mk ) given the underlying character it is generated from (uk = Cifor i ∈ nc, k ∈ nu), which is determined by its po sition in the paradigm.</S>
			<S sid ="73" ssid = "13">In our Finnish example (Figure 1), if k = 1, we are looking at the first underlying character, which is /t/ (from /tieta¨/), so assuming our character set is the Finnish alphabet, of which ‘t’ is the 20th char put in the paradigm.</S>
			<S sid ="74" ssid = "14">Rather than reestimate θ and φ at each iteration before sampling from u, we can marginalize these intermediate probability distributions in order to ease implementation and speed convergence.</S>
			<S sid ="75" ssid = "15">Our search procedure tries to sample from the posterior probability, according to Bayes’ rule.</S>
			<S sid ="76" ssid = "16">posterior ∝ likelihood ∗ prior P (u, s|β, α) ∝ P (u|β)P (s, u|α) Each of these probabilities is drawn from a Dirichlet distribution, which is defined in terms of the multivariate Beta function, C . The prior β added to underlying counts N(u) forms the posterior Dirichlet corresponding to P (u|β).</S>
			<S sid ="77" ssid = "17">In P (s|u, α), each αi vector is supplemented by the observed counts of (si).</S>
			<S sid ="78" ssid = "18">acter, u1 = C20 = t. It generates the first character of each inflected form (1st, 2nd, 3rd person, singu (underlying, surface) pairs N (β + N (u)) lar and plural) of that stem, so m1 = 6, and since there is no alternation s1n = t (for n ∈ {1, . . .</S>
			<S sid ="79" ssid = "19">, 6}).</S>
			<S sid ="80" ssid = "20">C P (u, s|β, α) = C (β)Given the phonologically plausible (gold) underly nc C (αc + i:ui =c N (si)) ing forms, the probability of /t/ is φ20 = 7/41.On the other hand, k = 33 identifies the 3rd per c=1 C (α) son singular /ø/, which inflects each of the seven stems, so m33 = 7.</S>
			<S sid ="81" ssid = "21">Since we need our alphabet to identify a null character, we’ll give it index zero (i.e., u33 = C0 = ø).</S>
			<S sid ="82" ssid = "22">For each of the (underlying, surface) alignments in this alternation (caused by vowel gemination), we can identify the probability in θ.</S>
			<S sid ="83" ssid = "23">For 3rd person singular [tieta¨+a¨], where s33,1 = C28 = a¨, the conditional probability θ0,28 = 1/7.</S>
			<S sid ="84" ssid = "24">The prior hyperparameters can be understood as follows.</S>
			<S sid ="85" ssid = "25">As βi gets smaller, an underlying form ukThe collapsed update procedure consists of re sampling each underlying form, u, incorporating the prior hyperparameters α, β and counts N over the rest of the dataset.</S>
			<S sid ="86" ssid = "26">The relevant counts for a candidate k being the underlying form ui are Nk (u−i) and Nksij (u−i) for j ∈ mi.</S>
			<S sid ="87" ssid = "27">P (ui = k|u−i, α, β) is proportional to the probability of generating ui = k, given the other u−i and all sij (for j ∈ mi), given s−i and u−i. Nc(u−i ) + βcis less likely to be Ci.</S>
			<S sid ="88" ssid = "28">As αij gets smaller, an un P (ui = c|u−i, α, β) ∝ n − 1 + β• derlying uk = Ci is less likely to generate a surface segment skn = Cj ∀n ∈ mk . In our experiments, C (α + i = i:u =c N (s! ) + N (si))we will vary αi=j (prior over identity map from un C (α + i =i:u =c N (s! )) derlying to surface) and αi =j . Our implementation of this model uses Gibbs sampling (c.f., (Bishop, 2006), pp 5428), an algorithm that produces samples from the posterior distribution.</S>
			<S sid ="89" ssid = "29">Each sample is an assignment of the hidden variables, u (i.e., a set of hypothesized underlying forms).</S>
			<S sid ="90" ssid = "30">Our sampler initializes u from a uniform distribution over segments in the training data, and resamples underlying forms in a fixed order, as in Suppose we were updating this sampler running on the Finnish verb inflections.</S>
			<S sid ="91" ssid = "31">If we had all segments as in Figure 1, but wanted to resample u31 (1st person singular /n/), we would consider the counts N excluding that form (i.e., under u−31).</S>
			<S sid ="92" ssid = "32">The prior for /n/, β14, is fixed, and there are no other occurrences, so N14(u−31) = 0.</S>
			<S sid ="93" ssid = "33">Another potential underlying form, like /t/, would have higher unconditioned posterior probability, because of the counts (7, in this case) added to its prior from β.</S>
			<S sid ="94" ssid = "34">Then, we have to multiply by the probability of each generated surface segment (all are [n], so 7 ∗ P ([n]|c, α) for a given hypothesis u31 = c).</S>
			<S sid ="95" ssid = "35">We select a given character c ∈ C for u31 from a distribution at random.</S>
			<S sid ="96" ssid = "36">Depending on the prior, /n/ will be the most likely choice, but other values are 3.2 Convergence.</S>
			<S sid ="97" ssid = "37">Testing convergence, we run again on the sample data (Figure 1), using αij = 0.1 when i /= j and 10 when i = j and β = 0.1, starting from different initializations, we get the same solution.</S>
			<S sid ="98" ssid = "38">6 x 10 still possible with smaller probability.</S>
			<S sid ="99" ssid = "39">The counts used for the next resampling, N (u−31), are affected by this choice, because the new identity of u31 has contributed to the posterior distribution.</S>
			<S sid ="100" ssid = "40">After unbounded iterations, Gibbs sampling is guaranteed to converge and produce samples from the true posterior (Geman and Geman, 1984).</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "3">
			<S sid ="101" ssid = "1">This model provides a language agnostic solution to a subset of phonological problems.</S>
			<S sid ="102" ssid = "2">We will first examine performance on the sample Finnish data (from Figure 1), and then look more closely at the issue of convergence.</S>
			<S sid ="103" ssid = "3">Finally, we present results from larger corpora 3 . 3.1 Finnish.</S>
			<S sid ="104" ssid = "4">Output from a trial run on Finnish verbs (from Figure 1) follows, with hyperparameters αij {100 ⇐⇒ i = j, 0.05 ⇐⇒ i /= j} and βi = {0.1}.</S>
			<S sid ="105" ssid = "5">In the paradigm (a sample after 1000 iterations), each [sur+face] form is followed by its hypothesized /under/ + /lying/ morphemes.</S>
			<S sid ="106" ssid = "6">[tieda¨+n] : /tieda¨/ + /n/ [tieda¨+t] : /tieda¨/ + /t/ [tieta¨+a¨] : /tieda¨/ + /a¨/ [tieda¨+mme] : /tieda¨/ + /mme/ [tieda¨+tte] : /tieda¨/ + /tte/ [tieta¨+va¨t] : /tieda¨/ + /va¨t/ [aiøo+n] : /aiøo/ + /n/ ...</S>
			<S sid ="107" ssid = "7">[pelka¨a¨+va¨t] : /pelka¨a¨/ + /vat/ With strong enough priors (faithfulness constraints), our sampler often selects the most common surface form aligned with an underlying segment.</S>
			<S sid ="108" ssid = "8">Although [vat] is more common than [va¨t], we choose the latter as the purer underlying form.</S>
			<S sid ="109" ssid = "9">So /a/ is always [a], but /a¨/ can be either [a¨] or [a].</S>
			<S sid ="110" ssid = "10">3 2.8 million word types from Morphochallenge2007 (Kurimo et al., 2007) 3.05 3.04 3.03 3.02 3.01 3 2.99 2.98 2.97 0 10 20 30 40 50 60 70 80 90 100 Iteration Figure 4: Posterior likelihood at each of the first 100 iterations, from 4 runs (with different random seeds) on 10% of the Morphochallenge dataset (αi/=j = 0.001, αi=j = 100, β = 0.1), indicating convergence within the first 15 iterations.</S>
			<S sid ="111" ssid = "11">To confirm that the sampler has converged, we output and plot trace statistics at each iteration, including marginal probability, log likelihood, and changes in underlying forms (i.e., variables resam- pled).</S>
			<S sid ="112" ssid = "12">If the sampler has converged, there should no longer be a trend (consistent slope) in any of these statistics (as in Figure 4).</S>
			<S sid ="113" ssid = "13">Examining the posterior probability of each selected underlying form reveals interesting patterns that also help explain the variation.</S>
			<S sid ="114" ssid = "14">In the above run, the ambiguous segments (with surface alternations) were drawn from the distributions (with improbable segments elided) in Figure 5.</S>
			<S sid ="115" ssid = "15">We expect this model to maximize the probability of either the “majority” solution or a solution demonstrating selection bias.</S>
			<S sid ="116" ssid = "16">We compare likelihood of the posterior sample with that of a “phono- logically plausible” solution (in which underlying forms are determined by referring to formal linguistic accounts of phonological derivation) and a “majority solution” (see Figure 6 for a log-log plot, where lower is more likely).</S>
			<S sid ="117" ssid = "17">The posterior sample has optimal likelihood with each parameter setting, as expected.</S>
			<S sid ="118" ssid = "18">The majority parse is selected with αi=j = 0.5 With lower values of αi=j , the “phonologically plausible” parse is u4=/d/ s4=[d,d,t,d,d,t] P (ui = c) ≈ d 0.99968 t 0.00014 u8=/k/ s8 =[ø,ø,k,ø,ø,k] (same behavior as u12) P (ui = c) ≈ ø 0.642 k 0.124 u33=/e/ s33=[a¨,o,e,u,ø,e,ø] P (ui = c) ≈ a ¨ , o , u 0.0 02 9 ø 0.2 15 a 0.0 00 3 e 0.2 97 Figure 5: Resampling probabilities for alternations, after 1000 iterations.</S>
			<S sid ="119" ssid = "19">posterior sample majority solution phonologically plausible 4.26 10 4.25 10 M ajo rit y Ba ye sia n ty p e s 5 0 . 8 4 6 9 . 5 3 tok en s 6 5 . 2 3 7 2 . 1 1 Figure 7: Accuracy of underlying segment hypotheses.</S>
			<S sid ="120" ssid = "20">notated morphemes.</S>
			<S sid ="121" ssid = "21">For example, the word ajavalle is listed in the corpus as follows: ajavalle aja:ajaa|V va:PCP1 lle:ALL The word is segmented into a verb stem, ‘aja’ (drive), a present participle marker ‘va’, and the allative suffix (“for”).</S>
			<S sid ="122" ssid = "22">Each surface realization of a given morpheme is identified by the same tag (e.g., PCP1).</S>
			<S sid ="123" ssid = "23">However, in this corpus, insertion and deletion are not explicitly marked (as they were in the paradigm, by ø).</S>
			<S sid ="124" ssid = "24">Rather than introduce another component to determine which segments in the form were dropped, we ignore these cases.</S>
			<S sid ="125" ssid = "25">The sampling algorithm proceeds as described in section 2.</S>
			<S sid ="126" ssid = "26">To run on tokens (as opposed to types), we incorporate another input file that contains counts from the original text (ajavalle appeared 8 times).</S>
			<S sid ="127" ssid = "27">The counts of each morpheme’s surface forms then reflect the number of times that form appeared in any word in the corpus.</S>
			<S sid ="128" ssid = "28">4.24 10 −2 −1 10 10 alpha Figure 6: Parse likelihood 3.</S>
			<S sid ="129" ssid = "29">3. 1 T y p e o r T o k e n 0 In Finnish verb conjugation, 3rd person (esp. sin gular) forms have high frequency and tend to be unmarked (i.e., closer to underlying).</S>
			<S sid ="130" ssid = "30">In types, unmarked is a minority (one third), but incorporating token frequency shifts that balance, benefitingmore likely than the majority.</S>
			<S sid ="131" ssid = "31">However, the sam pler does not converge to this solution, because in this [t,d] alternation, the “phonologically plausible” solution identifies /t/, but neither selection bias nor majority rules would lead to that with the given data.</S>
			<S sid ="132" ssid = "32">3.3 Morphologically segmented corpora.</S>
			<S sid ="133" ssid = "33">In our search for appropriate data for additional, larger-scale experiments, we found several viable alternatives.</S>
			<S sid ="134" ssid = "34">The correct morphological segmentations for Finnish data used in Morphochallenge2007 (Kurimo et al., 2007) provide a rich and varied set of words, and are readily analyzable by our sampler.</S>
			<S sid ="135" ssid = "35">Rather than associating each surface form with a position in the paradigm, we use the an the “majority learner.” Among noun inflections, unmarked has higher frequency in speech, but marked tokens may still dominate in text.</S>
			<S sid ="136" ssid = "36">We might expect that it is easier to learn from tokens than types, in part because more data is often helpful.</S>
			<S sid ="137" ssid = "37">Testing on half of the Morphochallenge 2007 Finnish data (1M word types, 5M morph types, 17.5M word tokens, 48M morph tokens), we ran both our Bayesian model and a majority solver on the morphological analyses, and compared against phonologically plausible (gold) underlying forms.</S>
			<S sid ="138" ssid = "38">Results are reported in Figure 7.</S>
			<S sid ="139" ssid = "39">The Bayesian estimate consistently outperformed the majority solution, and cases where the two differ could often be ascribed to the preference for “pure” analyses.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "4">
			<S sid ="140" ssid = "1">We have described a model where surface forms are generated from underlying representations segment by segment.</S>
			<S sid ="141" ssid = "2">Taking this approach allowed us to investigate the properties of a Bayesian statistical learner, and how these can be useful in the context of sound systems, a basic component of language.</S>
			<S sid ="142" ssid = "3">Experiments with our implementation of a collapsed sampler have produced results largely confirming our hypotheses.</S>
			<S sid ="143" ssid = "4">Without context, we can often learn about 60 to 80 percent of the mapping from underlying phonemes to surface phones.</S>
			<S sid ="144" ssid = "5">Especially with lower values of αi=j , closer to 0, our model does prefer pure alternations.</S>
			<S sid ="145" ssid = "6">Gibbs sampling tends to select the majority underlying form, particularly with αi=j relatively high, closer to 1.</S>
			<S sid ="146" ssid = "7">So, a sparser prior leads us further from the baseline, and often closer to a phonologically plausible solution.</S>
			<S sid ="147" ssid = "8">4.1 Directions.</S>
			<S sid ="148" ssid = "9">In future research, we hope to integrate morphological analysis into this sort of a treatment of phonology.</S>
			<S sid ="149" ssid = "10">This is a natural approach for children learning their first language.</S>
			<S sid ="150" ssid = "11">They intuitively discover phonotactics, and how it affects the prosodic shape of each word, as they learn meaningful units and compose them together.</S>
			<S sid ="151" ssid = "12">It is clear that many layers of linguistic information interact in the early stages of child language acquisition (Demuth and Ellis, 2005 in press), so they should also interact in our models.</S>
			<S sid ="152" ssid = "13">As discussed above, the present model should be applicable to analysis of language- learners’ speech errors, and this connection should be explored in greater depth.</S>
			<S sid ="153" ssid = "14">It might be interesting to predispose the sampler to select underlying forms from open syllables.</S>
			<S sid ="154" ssid = "15">That is, set α to increase the probability of matching one of the surface segments if its context (feature annotations) includes a vocalic segment or a word boundary immediately following.</S>
			<S sid ="155" ssid = "16">The probability of phonological processes like assimilation could be similarly modeled, with the prior higher for choosing a segment that appears on the surface in a con- trastive context (where it shares few features with neighboring segments).</S>
			<S sid ="156" ssid = "17">If we define a MaxEnt distribution over Optimal- ity Theoretic constraints, we might use that to inform our selection of underlying forms.</S>
			<S sid ="157" ssid = "18">In (Goldwater and Johnson, 2003), the learning algorithm was given a set of candidate surface forms associated with an underlying form, and tried to optimize the constraint weights.</S>
			<S sid ="158" ssid = "19">In addition to the constraint weights, we must also optimize the underlying form, since our goal is to take as input only observable data.</S>
			<S sid ="159" ssid = "20">Sampling from this type of complex distribution is quite difficult, but some approaches (e.g., (Murray et al., 2006)) may help reduce the intractability.</S>
	</SECTION>
</PAPER>
