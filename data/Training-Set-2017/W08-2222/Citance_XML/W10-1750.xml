<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes the joint submission of Universitat Polite`cnica de Catalunya and Universitat de Barcelona to the Metrics MaTr 2010 evaluation challenge, in collaboration with ELDA/ELRA.</S>
		<S sid ="2" ssid = "2">Our work is aimed at widening the scope of current automatic evaluation measures from sentence to document level.</S>
		<S sid ="3" ssid = "3">Preliminary experiments, based on an extension of the metrics by Gime´nez and Ma`rquez (2009) operating over discourse representations, are presented.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Current automatic similarity measures for Machine Translation (MT) evaluation operate all, without exception, at the segment level.</S>
			<S sid ="5" ssid = "5">Translations are analyzed on a segment-by-segment1 fashion, ignoring the text structure.</S>
			<S sid ="6" ssid = "6">Document and system scores are obtained using aggregate statistics over individual segments.</S>
			<S sid ="7" ssid = "7">This strategy presents the main disadvantage of ignoring cross- sentential/discursive phenomena.</S>
			<S sid ="8" ssid = "8">In this work we suggest widening the scope of evaluation methods.</S>
			<S sid ="9" ssid = "9">We have defined genuine document-level measures which are able to exploit the structure of text to provide more informed evaluation scores.</S>
			<S sid ="10" ssid = "10">For that purpose we take advantage of two coincidental facts.</S>
			<S sid ="11" ssid = "11">First, test beds employed in recent MT evaluation campaigns include a document structure grouping sentences related to the same event, story or topic (Przybocki et al., 2008; Przybocki et al., 2009; CallisonBurch et al., 2009).</S>
			<S sid ="12" ssid = "12">Second, we count on automatic linguistic processors which provide very detailed discourse- level representations of text (Curran et al., 2007).</S>
			<S sid ="13" ssid = "13">Discourse representations allow us to focus on relevant pieces of information, such as the agent 1 A segment typically consists of one or two sentences.</S>
			<S sid ="14" ssid = "14">(who), location (where), time (when), and theme (what), which may be spread all over the text.</S>
			<S sid ="15" ssid = "15">Counting on a means of discerning the events, the individuals taking part in each of them, and their role, is crucial to determine the semantic equivalence between a reference document and a candidate translation.</S>
			<S sid ="16" ssid = "16">Moreover, the discourse analysis of a document is not a mere concatenation of the analyses of its individual sentences.</S>
			<S sid ="17" ssid = "17">There are some phenomena which may go beyond the scope of a sentence and can only be explained within the context of the whole document.</S>
			<S sid ="18" ssid = "18">For instance, in a newspaper article, facts and entities are progressively added to the discourse and then referred to anaphorically later on.</S>
			<S sid ="19" ssid = "19">The following extract from the development set illustrates the importance of such a phenomenon in the discourse analysis: ‘Among the current or underlying crises in the Middle East, Rod Larsen mentioned the ArabIsraeli conflict and the Iranian nuclear portfolio, as well as the crisis between Lebanon and Syria.</S>
			<S sid ="20" ssid = "20">He stated: “All this leads us back to crucial values and opinions, which render the situation prone at any moment to getting out of control, more so than it was in past days.”’.</S>
			<S sid ="21" ssid = "21">The subject pronoun “he” works as an anaphoric pronoun whose antecedent is the proper noun “Rod Larson”.</S>
			<S sid ="22" ssid = "22">The anaphoric relation established between these two elements can only be identified by analyzing the text as a whole, thus considering the gender agreement between the third person singular masculine subject pronoun “he” and the masculine proper noun “Rod Larson”.</S>
			<S sid ="23" ssid = "23">However, if the two sentences were analyzed separately, the identification of this anaphoric relation would not be feasible due to the lack of connection between the two elements.</S>
			<S sid ="24" ssid = "24">Discourse representations allow us to trace links across sentences between the different facts and entities appearing in them.</S>
			<S sid ="25" ssid = "25">Therefore, providing an approach to the text more similar to that of a human, which implies taking into account the whole text structure instead of considering each sentence separately.</S>
			<S sid ="26" ssid = "26">The rest of the paper is organized as follows.</S>
			<S sid ="27" ssid = "27">Section 2 describes our evaluation methods and the linguistic theory upon which they are based.</S>
			<S sid ="28" ssid = "28">Experimental results are reported and discussed in Section 3.</S>
			<S sid ="29" ssid = "29">Section 4 presents the metric submitted to the evaluation challenge.</S>
			<S sid ="30" ssid = "30">Future work is outlined in Section 5.</S>
			<S sid ="31" ssid = "31">As an additional result, document-level metrics generated in this study have been incorporated to the IQMT package for automatic MT evaluation2 .</S>
	</SECTION>
	<SECTION title="Metric Description. " number = "2">
			<S sid ="32" ssid = "1">This section provides a brief description of our approach.</S>
			<S sid ="33" ssid = "2">First, in Section 2.1, we describe the underlying theory and give examples on its capabilities.</S>
			<S sid ="34" ssid = "3">Then, in Section 2.2, we describe the associated similarity measures.</S>
			<S sid ="35" ssid = "4">2.1 Discourse Representations.</S>
			<S sid ="36" ssid = "5">As previously mentioned in Section 1, a document has some features which need to be analyzed considering it as a whole instead of dividing it up into sentences.</S>
			<S sid ="37" ssid = "6">The anaphoric relation between a subject pronoun and a proper noun has already been exemplified.</S>
			<S sid ="38" ssid = "7">However, this is not the only anaphoric relation which can be found inside a text, there are some others which are worth mentioning: • the connection between a possessive adjective and a proper noun or a subject pronoun, as exemplified in the sentences “Maria bought a new sweater.</S>
			<S sid ="39" ssid = "8">Her new sweater is blue.”, where the possessive feminine adjective “her” refers to the proper noun “Maria”.</S>
			<S sid ="40" ssid = "9">• the link between a demonstrative pronoun and its referent, which is exemplified in the sentences “He developed a new theory on grammar.</S>
			<S sid ="41" ssid = "10">However, this is not the only theory he developed”.</S>
			<S sid ="42" ssid = "11">In the second sentence, the demonstrative pronoun ”this” refers back to the noun phrase “new theory on grammar” which occurs in the previous sentence.</S>
			<S sid ="43" ssid = "12">• the relation between a main verb and an auxiliary verb in certain contexts, as illustrated in the following pair of sentences “Would you 2 http://www.lsi.upc.edu/˜nlp/IQMT like more sugar?</S>
			<S sid ="44" ssid = "13">Yes, I would”.</S>
			<S sid ="45" ssid = "14">In this example, the auxiliary verb “would” used in the short answer substitutes the verb phrase “would like”.</S>
			<S sid ="46" ssid = "15">In addition to anaphoric relations, other features need to be highlighted, such as the use of discourse markers which help to give cohesion to the text, link parts of a discourse and show the relations established between them.</S>
			<S sid ="47" ssid = "16">Below, some examples are given: • “Moreover”, “Furthermore”, “In addition” indicate that the upcoming sentence adds more information.</S>
			<S sid ="48" ssid = "17">• “However”, “Nonetheless”, “Nevertheless” show contrast with previous ideas.</S>
			<S sid ="49" ssid = "18">• “Therefore”, “As a result”, “Consequently” show a cause and effect relation.</S>
			<S sid ="50" ssid = "19">• “For instance”, “For example” clarify or illustrate the previous idea.</S>
			<S sid ="51" ssid = "20">It is worth noticing that anaphora, as well as discourse markers, are key features in the interface between syntax, semantics and pragmatics.</S>
			<S sid ="52" ssid = "21">Thus, when dealing with these phenomena at a text level we are not just looking separately at the different language levels, but we are trying to give a complete representation of both the surface and the deep structures of a text.</S>
			<S sid ="53" ssid = "22">2.2 Definition of Similarity Measures.</S>
			<S sid ="54" ssid = "23">In this work, as a first proposal, instead of elaborating on novel similarity measures, we have borrowed and extended the Discourse Representation (DR) metrics defined by Gime´nez and Ma`rquez (2009).</S>
			<S sid ="55" ssid = "24">These metrics analyze similarities between automatic and reference translations by comparing their respective discourse representations over individual sentences.</S>
			<S sid ="56" ssid = "25">For the discursive analysis of texts, DR metrics rely on the C&amp;C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).</S>
			<S sid ="57" ssid = "26">This software is based on the Discourse Representation Theory (DRT) by Kamp and Reyle (1993).</S>
			<S sid ="58" ssid = "27">DRT is a theoretical framework offering a representation language for the examination of con- textually dependent meaning in discourse.</S>
			<S sid ="59" ssid = "28">A discourse is represented in a discourse representation structure (DRS), which is essentially a variation of first-order predicate calculus —its forms are pairs of first-order formulae and the free variables that occur in them.</S>
			<S sid ="60" ssid = "29">DRSs are viewed as semantic trees, built through the application of two types of DRS conditions: basic conditions: one-place properties (predicates), two-place properties (relations), named entities, time-expressions, cardinal expressions and equalities.</S>
			<S sid ="61" ssid = "30">complex conditions: disjunction, implication, negation, question, and propositional attitude operations.</S>
			<S sid ="62" ssid = "31">For instance, the DRS representation for the sentence “Every man loves Mary.” is as follows: ∃y named(y, mary, per) ∧ (∀x man(x) → ∃z love(z) ∧ event(z) ∧ agent(z, x) ∧ patient(z, y)).</S>
			<S sid ="63" ssid = "32">DR integrates three different kinds of metrics: DRSTM These metrics are similar to the Syntactic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSs instead of constituent trees.</S>
			<S sid ="64" ssid = "33">All semantic sub- paths in the candidate and reference trees are retrieved.</S>
			<S sid ="65" ssid = "34">The fraction of matching subpaths of a given length (l=4 in our experiments) is computed.</S>
			<S sid ="66" ssid = "35">DR-Or (⋆) Average lexical overlap between discourse representation structures of the same type.</S>
			<S sid ="67" ssid = "36">Overlap is measured according to the formulae and definitions by Gime´nez and Ma`rquez (2007).</S>
			<S sid ="68" ssid = "37">DROrp(⋆) Average morphosyntactic overlap, i.e., between grammatical categories –parts- of-speech– associated to lexical items, between discourse representation structures of the same type.</S>
			<S sid ="69" ssid = "38">We have extended these metrics to operate at document level.</S>
			<S sid ="70" ssid = "39">For that purpose, instead of running the C&amp;C Tools in a sentence-by-sentence fashion, we run them document by document.</S>
			<S sid ="71" ssid = "40">This is as simple as introducing a “&lt;META&gt;” tag at the beginning of each document to denote document boundaries3 . 3 Details on the advanced use of Boxer are available at http://svn.ask.it.usyd.edu.au/trac/ candc/wiki/BoxerComplex.</S>
	</SECTION>
	<SECTION title="Experimental Work. " number = "3">
			<S sid ="72" ssid = "1">In this section, we analyze the behavior of the new DR metrics operating at document level with respect to their sentence-level counterparts.</S>
			<S sid ="73" ssid = "2">3.1 Settings.</S>
			<S sid ="74" ssid = "3">We have used the ‘mt06’ part of the development set provided by the Metrics MaTr 2010 organization, which corresponds to a subset of 25 documents from the NIST 2006 Open MT Evaluation Campaign Arabic-to-English translation.</S>
			<S sid ="75" ssid = "4">The total number of segments is 249.</S>
			<S sid ="76" ssid = "5">The average number of segments per document is, thus, 9.96.</S>
			<S sid ="77" ssid = "6">The number of segments per document varies between 2 and 30.</S>
			<S sid ="78" ssid = "7">For the purpose of automatic evaluation, 4 human reference translations and automatic outputs by 8 different MT systems are available.</S>
			<S sid ="79" ssid = "8">In addition, we count on the results of a process of manual evaluation.</S>
			<S sid ="80" ssid = "9">Each translation segment was assessed by two judges.</S>
			<S sid ="81" ssid = "10">After independently and completely assessing the entire set, the judges reviewed their individual assessments together and settled on a single final score.</S>
			<S sid ="82" ssid = "11">Average system adequacy is 5.38.</S>
			<S sid ="83" ssid = "12">In our experiments, metrics are evaluated in terms of their correlation with human assessments.</S>
			<S sid ="84" ssid = "13">We have computed Pearson, Spearman and Kendall correlation coefficients between metric scores and adequacy assessments.</S>
			<S sid ="85" ssid = "14">Document- level and system-level assessments have been obtained by averaging over segment-level assessments.</S>
			<S sid ="86" ssid = "15">We have computed correlation coefficients and confidence intervals applying bootstrap re- sampling at a 99% statistical significance (Efron and Tibshirani, 1986; Koehn, 2004).</S>
			<S sid ="87" ssid = "16">Since the cost of exhaustive resampling was prohibitive, we have limited to 1,000 resamplings.</S>
			<S sid ="88" ssid = "17">Confidence intervals, not shown in the tables, are in all cases lower than 10−3 . 3.2 Metric Performance.</S>
			<S sid ="89" ssid = "18">Table 1 shows correlation coefficients at the document level for several DR metric representatives, and their document-level counterparts (DRdoc).</S>
			<S sid ="90" ssid = "19">For the sake of comparison, the performance of the METEOR metric is also reported4 . Contrary to our expectations, DRdoc variants obtain lower levels of correlation than their DR 4 We have used METEOR version 1.0 with default parameters optimized by its developers over adequacy and fluency assessments.</S>
			<S sid ="91" ssid = "20">The METEOR metric is publicly available at http://www.cs.cmu.edu/˜alavie/METEOR/ Metric Table 1: Meta-evaluation results at document level Metric Table 2: Meta-evaluation results at system level counterparts.</S>
			<S sid ="92" ssid = "21">There are three different factors which could provide a possible explanation for this negative result.</S>
			<S sid ="93" ssid = "22">First, the C&amp;C Tools, like any other automatic linguistic processor are not perfect.</S>
			<S sid ="94" ssid = "23">Parsing errors could be causing the metric to confer less informed scores.</S>
			<S sid ="95" ssid = "24">This is especially relevant taking into account that candidate translations are not always well-formed.</S>
			<S sid ="96" ssid = "25">Secondly, we argue that the way in which we have obtained document-level quality assessments, as an average of segment-level assessments, may be biasing the correlation.</S>
			<S sid ="97" ssid = "26">Thirdly, perhaps the similarity measures employed are not able to take advantage of the document-level features provided by the discourse analysis.</S>
			<S sid ="98" ssid = "27">In the following subsection we show some error analysis we have conducted by inspecting particular cases.</S>
			<S sid ="99" ssid = "28">Table 2 shows correlation coefficients at system level.</S>
			<S sid ="100" ssid = "29">In the case of DR and DRdoc metrics, system scores are computed by simple average over individual documents.</S>
			<S sid ="101" ssid = "30">Interestingly, in this case DRdoc variants seem to obtain higher correlation than their DR counterparts.</S>
			<S sid ="102" ssid = "31">The improvement is especially substantial in terms of Spearman and Kendall coefficients, which do not consider absolute values but ranking positions.</S>
			<S sid ="103" ssid = "32">However, it could be the case that it was just an average ef fect.</S>
			<S sid ="104" ssid = "33">While DR metrics compute system scores as an average of segment scores, DRdoc metrics average directly document scores.</S>
			<S sid ="105" ssid = "34">In order to clarify this result, we have modified DR metrics so as to compute system scores as an average of document scores (DR′ variants, the last three rows in the table).</S>
			<S sid ="106" ssid = "35">It can be observed that DR’ variants outperform their DRdoc counterparts, thus confirming our suspicion about the averaging effect.</S>
			<S sid ="107" ssid = "36">3.3 Analysis.</S>
			<S sid ="108" ssid = "37">It is worth noting that DRdoc metrics are able to detect and deal with several linguistic phenomena related to both syntax and semantics at sentence and document level.</S>
			<S sid ="109" ssid = "38">Below, several examples illustrating the potential of this metric are presented.</S>
			<S sid ="110" ssid = "39">Control structures.</S>
			<S sid ="111" ssid = "40">Control structures (either subject or object control) are always a difficult issue as they mix both syntactic and semantic knowledge.</S>
			<S sid ="112" ssid = "41">In Example 1 a couple of control structures must be identified and DRdoc metrics deal correctly with the argument structure of all the verbs involved.</S>
			<S sid ="113" ssid = "42">Thus, in the first part of the sentence, a subject control verb can be identified being “the minister” the agent of both verb forms “go” and “say”.</S>
			<S sid ="114" ssid = "43">On the other hand, in the quoted question, the verb “invite” works as an object control verb because its patient “Chechen representatives” is also the agent of the verb visit.</S>
			<S sid ="115" ssid = "44">Example 1: The minister went on to say, “What would Moscow say if we were to invite Chechen representatives to visit Jerusalem?” Anaphora and pronoun resolution.</S>
			<S sid ="116" ssid = "45">Whenever there is a pronoun whose antecedent is a named entity (NE), the metric identifies correctly its antecedent.</S>
			<S sid ="117" ssid = "46">This feature is highly valuable because a relationship between syntax and semantics is established.</S>
			<S sid ="118" ssid = "47">Moreover, when dealing with Semantic Roles the roles of Agent or Patient are given to the antecedents instead of the pronouns.</S>
			<S sid ="119" ssid = "48">Thus, in Example 2 the antecedent of the relative pronoun “who” is the NE “Putin” and the patient of the verb “classified” is also the NE “Putin” instead of the relative pronoun “who”.</S>
			<S sid ="120" ssid = "49">Example 2: Putin, who was not classified as his country Hamas as “terrorist organizations”, recently said that the European Union is “a big mistake” if it decided to suspend financial aid to the Palestinians.</S>
			<S sid ="121" ssid = "50">Nevertheless, although Boxer was expected to deal with long-distance anaphoric relations beyond the sentence, after analyzing several cases, results show that it did not succeed in capturing this type of relations as shown in Example 3.</S>
			<S sid ="122" ssid = "51">In this example, the antecedent of the pronoun “he” in the second sentence is the NE “Roberto Calderoli” which appears in the first sentence.</S>
			<S sid ="123" ssid = "52">DRdoc metrics should be capable of showing this connection.</S>
			<S sid ="124" ssid = "53">However, although the proper noun “Roberto Calderoli” is identified as a NE, it does not share the same reference as the third person singular pronoun “he”.</S>
			<S sid ="125" ssid = "54">Example 3: Roberto Calderoli does not intend to apologize.</S>
			<S sid ="126" ssid = "55">The newspaper Corriere Della Sera reported today, Saturday, that he said “I don’t feel responsible for those deaths.”</S>
	</SECTION>
	<SECTION title="Our Submission. " number = "4">
			<S sid ="127" ssid = "1">Instead of participating with individual metrics, we have combined them by averaging their scores as described in (Gime´nez and Ma`rquez, 2008).</S>
			<S sid ="128" ssid = "2">This strategy has proven as an effective means of combining the scores conferred by different metrics (CallisonBurch et al., 2008; CallisonBurch et al., 2009).</S>
			<S sid ="129" ssid = "3">Metrics submitted are: DRdoc an arithmetic mean over a heuristically- defined set of DRdoc metric variants, respectively computing lexical overlap, morphosyn- tactic overlap, and semantic tree matching (M = {‘DRdoc-Or (⋆)’, ‘DRdocOrp (⋆)’, ‘DRdoc STM4 ’}).</S>
			<S sid ="130" ssid = "4">Since DRdoc metrics do not operate over individual segments, we have assigned each segment the score of the document in which it is contained.</S>
			<S sid ="131" ssid = "5">DR a measure analog to DRdoc but using the default version of DR metrics operating at the segment level (M = {‘DR-Or (⋆)’, ‘DROrp (⋆)’, ‘DRSTM4 ’}).</S>
			<S sid ="132" ssid = "6">ULCh an arithmetic mean over a heuristically- defined set of metrics operating at different linguistic levels, including lexical metrics, and measures of overlap between constituent parses, dependency parses, seman tic roles, and discourse representations (M = {‘ROUGEW ’, ‘METEOR’, ‘DPHWCr ’, ‘DPOc (⋆)’, ‘DPOl (⋆)’, ‘DP-Or (⋆)’, ‘CPSTM4 ’, ‘SR-Or (⋆)’,‘SROrv ’, ‘DROrp(⋆)’}).</S>
			<S sid ="133" ssid = "7">This metric corre sponds exactly to the metric submitted in our previous participation.</S>
			<S sid ="134" ssid = "8">The performance of these metrics at the document and system levels is shown in Table 3.</S>
	</SECTION>
	<SECTION title="Conclusions and Future Work. " number = "5">
			<S sid ="135" ssid = "1">We have presented a modified version of the DR metrics by Gime´nez and Ma`rquez (2009) which, instead of limiting their scope to the segment level, are able to capture and exploit document-level features.</S>
			<S sid ="136" ssid = "2">However, results in terms of correlation with human assessments have not reported any improvement of these metrics over their sentence- level counterparts as document and system quality predictors.</S>
			<S sid ="137" ssid = "3">It must be clarified whether the problem is on the side of the linguistic tools, in the similarity measure, or in the way in which we have built document-level human assessments.</S>
			<S sid ="138" ssid = "4">For future work, we plan to continue the error analysis to clarify why DRdoc metrics do not outperform their DR counterparts at the document level, and how to improve their behavior.</S>
			<S sid ="139" ssid = "5">This Metric Table 3: Meta-evaluation results at document and system level for submitted metrics may imply defining new metrics possibly using alternative linguistic processors.</S>
			<S sid ="140" ssid = "6">In addition, we plan to work on the identification and analysis of discourse markers.</S>
			<S sid ="141" ssid = "7">Finally, we plan to repeat this experiment over other test beds with document structure, such as those from the 2009 Workshop on Statistical Machine Translation shared task (CallisonBurch et al., 2009) and the 2009 NIST MT Evaluation Campaign (Przybocki et al., 2009).</S>
			<S sid ="142" ssid = "8">In the case that document-level assessments are not provided, we will also explore the possibility of producing them ourselves.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="143" ssid = "9">This work has been partially funded by the Spanish Government (projects OpenMT2, TIN200914675-C03, and KNOW, TIN-2009 14715C0403) and the European Community’s Seventh Framework Programme (FP7/20072013) under grant agreement numbers 247762 (FAUST project, FP7ICT-20094-247762) and 247914 (MOLTO project, FP7ICT-20094-247914).</S>
			<S sid ="144" ssid = "10">We are also thankful to anonymous reviewers for their comments and suggestions.</S>
	</SECTION>
</PAPER>
