<S sid ="12" ssid = "12">The second tests for collocations - patterns of words and part-of-speech tags around the target word.</S
<S sid ="17" ssid = "17">This paper takes Yarowsky&apos;s method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.</S>
<S sid ="18" ssid = "18">A method is presented for doing this, based on Bayesian classifiers.</S>  
<S sid ="19" ssid = "19">The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction.</S>
<S sid ="24" ssid = "24">\Ve then apply each of the two component methods mentioned aboveÂ­ context words and collocations.</S>
<S sid ="25" ssid = "25">\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.</S> | Discourse Facet:  Method_Citatio
<S sid ="32" ssid = "4">\Ve treat context-sensitive spelling correction as a task of word disambiguation.</S>
<S sid ="33" ssid = "5">The ambiguity among words is modelled by confusion sets.</S>
<S sid ="34" ssid = "6">A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.</S>
<S sid ="35" ssid = "7">Thus if C = {deserÂ·t, desserÂ·t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.</S>
<S sid ="58" ssid = "7">The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.</S>
<S sid ="59" ssid = "8">and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].</S>
<S sid ="63" ssid = "12">Table 1 shows the performance of the baseline method for 18 confusion sets.</S>
<S sid ="85" ssid = "34">The probability for each Wi is calculated using Bayes&apos; rule: As it stands, the likelihood term, p( c_k.</S>
<S sid ="86" ssid = "35">, c_ 1, c1, ...</S><S sid ="87" ssid = "36">, cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.</S>
<S sid ="88" ssid = "37">Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word.</S>
<S sid ="162" ssid = "111">The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;.</S>
<S sid ="163" ssid = "112">An ambiguous target word is then classified by finding all collocations that match its context.</S>
<S sid ="216" ssid = "165">Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)</S> 
<S sid ="322" ssid = "38">Trigrams are at their worst when the words in the confusion set have the same part of speech.</S>
<S sid ="325" ssid = "41">In such cases, the Bayesian hybrid method is clearly better.</S> 
<S sid ="333" ssid = "4">A method for doing this, based on Bayesian classifiers, was presented.</S>
<S sid ="334" ssid = "5">It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.</S>