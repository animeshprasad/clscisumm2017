<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a classification-based word prediction model based on IGTR EE, a decision-tree induction algorithm with favorable scaling abilities and a functional equivalence to n-gram models with back- off smoothing.</S>
		<S sid ="2" ssid = "2">Through a first series of experiments, in which we train on Reuters newswire text and test either on the same type of data or on general or fictional text, we demonstrate that the system exhibits log-linear increases in prediction accuracy with increasing numbers of training examples.</S>
		<S sid ="3" ssid = "3">Trained on 30 million words of newswire text, prediction accuracies range between 12.6% on fictional text and 42.2% on newswire text.</S>
		<S sid ="4" ssid = "4">In a second series of experiments we compare all-words prediction with confusable prediction, i.e., the same task, but specialized to predicting among limited sets of words.</S>
		<S sid ="5" ssid = "5">Con- fusable prediction yields high accuracies on nine example confusable sets in all genres of text.</S>
		<S sid ="6" ssid = "6">The confusable approach outperforms the all-words-prediction approach, but with more data the difference decreases.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Word prediction is an intriguing language engineering semi-product.</S>
			<S sid ="8" ssid = "8">Arguably it is the “archetypical prediction problem in natural language processing” (Even-Zohar and Roth, 2000).</S>
			<S sid ="9" ssid = "9">It is usually not an engineering end in itself to predict the next word in a sequence, or fill in a blanked-out word in a sequence.</S>
			<S sid ="10" ssid = "10">Yet, it could be an asset in higher-level proofing or authoring tools, e.g. to be able to automatically discern among confusables and thereby to detect con- fusable errors (Golding and Roth, 1999; Even-Zohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001).</S>
			<S sid ="11" ssid = "11">It could alleviate problems with low- frequency and unknown words in natural language processing and information retrieval, by replacing them with likely and higher-frequency alternatives that carry similar information.</S>
			<S sid ="12" ssid = "12">And also, since the task of word prediction is a direct interpretation of language modeling, a word prediction system could provide useful information for to be used in speech recognition systems.</S>
			<S sid ="13" ssid = "13">A unique aspect of the word prediction task, as compared to most other tasks in natural language processing, is that real-world examples abound in large amounts.</S>
			<S sid ="14" ssid = "14">Any digitized text can be used as training material for a word prediction system capable of learning from examples, and nowadays gigascale and terascale document collections are available for research purposes.</S>
			<S sid ="15" ssid = "15">A specific type of word prediction is confus- able prediction, i.e., learn to predict among limited sets of confusable words such as to/two/too and there/their/they’re (Golding and Roth, 1999; Banko and Brill, 2001).</S>
			<S sid ="16" ssid = "16">Having trained a confusable predictor on occurrences of words within a confusable set, it can be applied to any new occurrence of a word from the set; if its prediction based on the context deviates from the word actually present, then 25 Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 25–32, New York City, New York, June 2006.</S>
			<S sid ="17" ssid = "17">Qc 2006 Association for Computational Linguistics this word might be a confusable error, and the classifier’s prediction might be its correction.</S>
			<S sid ="18" ssid = "18">Confusable prediction and correction is a strong asset in proofing tools.</S>
			<S sid ="19" ssid = "19">In this paper we generalize the word prediction task to predicting any word in context.</S>
			<S sid ="20" ssid = "20">This is basically the task of a generic language model.</S>
			<S sid ="21" ssid = "21">An explicit choice for the particular study on “all-words” prediction is to encode context only by words, and not by any higher-level linguistic non-terminals which have been investigated in related work on word prediction (Wu et al., 1999; Even-Zohar and Roth, 2000).</S>
			<S sid ="22" ssid = "22">This choice leaves open the question how the same tasks can be learned from examples when non-terminal symbols are taken into account as well.</S>
			<S sid ="23" ssid = "23">The choice for our algorithm, a decision-tree approximation of k-nearest-neigbor (k-NN) based or memory-based learning, is motivated by the fact that, as we describe later in this paper, this particular algorithm can scale up to predicting tens of thousands of words, while simultaneously being able to scale up to tens of millions of examples as training material, predicting words at useful rates of hundreds to thousands of words per second.</S>
			<S sid ="24" ssid = "24">Another motivation for our choice is that our decision-tree approximation of k-nearest neighbor classification is functionally equivalent to back-off smoothing (Zavrel and Daelemans, 1997); not only does it share its performance capacities with n-gram models with back-off smoothing, it also shares its scaling abilities with these models, while being able to handle large values of n. The article is structured as follows.</S>
			<S sid ="25" ssid = "25">In Section 2 we describe what data we selected for our experiments, and we provide an overview of the experimental methodology used throughout the experiments, including a description of the IGTR EE algorithm central to our study.</S>
			<S sid ="26" ssid = "26">In Section 3 the results of the word prediction experiments are presented, and the subsequent Section 4 contains the experimental results of the experiments on confusables.</S>
			<S sid ="27" ssid = "27">We briefly relate our work to earlier work that inspired the current study in Section 5.</S>
			<S sid ="28" ssid = "28">The results are discussed, and conclusions are drawn in Section 6.</S>
	</SECTION>
	<SECTION title="Data preparation and. " number = "2">
			<S sid ="29" ssid = "1">experimental setup First, we identify the textual corpora used.</S>
			<S sid ="30" ssid = "2">We then describe the general experimental setup of learning curve experiments, and the IGTR EE decision- tree induction algorithm used throughout all experiments.</S>
			<S sid ="31" ssid = "3">2 . 1 D a t a To generate our word prediction examples, we used the “Reuters Corpus Volume 1 (English Language, 199608-20 to 199708-19)”1 . We tokenized this corpus with a rule-based tokenizer, and used all 130,396,703 word and punctuation tokens for experimentation.</S>
			<S sid ="32" ssid = "4">In the remainder of the article we make no difference between words and punctuation markers; both are regarded as tokens.</S>
			<S sid ="33" ssid = "5">We separated the final 100,000 tokens as a held-out test set, henceforth referred to as R EU TER S, and kept the rest as training set, henceforth TR A IN -R EU TER S. Additionally, we selected two test sets taken from different corpora.</S>
			<S sid ="34" ssid = "6">First, we used the Project Gutenberg2 version of the novel Alice’s Adventures in Wonderland by Lewis Carroll (Carroll, 1865), henceforth A LIC E. As the third test set we selected all tokens of the Brown corpus part of the Penn Tree- bank (Marcus et al., 1993), a selected portion of the original one million word Brown corpus (Kucˇera and Francis, 1967), a collection of samples of American English in many different genres, from sources printed in 1961; we refer to this test set as B ROWN.</S>
			<S sid ="35" ssid = "7">In sum, we have three test sets, covering texts from the same genre and source as the training data, a fictional novel, and a mix of genres wider than the training set.</S>
			<S sid ="36" ssid = "8">Table 1 summarizes the key training and test set statistics.</S>
			<S sid ="37" ssid = "9">As the table shows, the cross-domain coverages for unigrams and bigrams are rather low; not only are these numbers the best-case performance ceilings, they also imply that a lot of contextual information used by the machine learning method used in this paper will be partly unknown to the learner, especially in texts from other domains than the training set.</S>
			<S sid ="38" ssid = "10">1 For availability of the Reuters corpus, see.</S>
			<S sid ="39" ssid = "11">http://about.reuters.com/researchandstanda rds/corpus/.</S>
			<S sid ="40" ssid = "12">2 Project Gutenberg:.</S>
			<S sid ="41" ssid = "13">http://www.gutenberg.net.</S>
			<S sid ="42" ssid = "14">Dat a set Ge nre # T o ke ns C o v er a g e ( % ) T R A I N R E U T E R S ne ws 30 mil lio n uni gra m big ra m R E U T E R S A L I C E B RO WN ne ws fict ion mi xe d 1 0 0 , 0 0 0 3 3 , 3 6 1 4 5 3 , 4 4 6 9 1 . 0 8 3 . 6 8 5 . 2 7 0 . 1 7 5 . 9 7 2 . 3 Table 1: Training and test set sources, genres, sizes in terms of numbers of tokens, and unigram and bi- gram coverage (%) of the training set on the test sets.</S>
			<S sid ="43" ssid = "15">2.2 Experimental setup.</S>
			<S sid ="44" ssid = "16">All experiments described in this article take the form of learning curve experiments (Banko and Brill, 2001), in which a sequence of training sets is generated with increasing size, where each size training set is used to train a model for word prediction, which is subsequently tested on a held-out test set – which is fixed throughout the whole learning curve experiment.</S>
			<S sid ="45" ssid = "17">Training set sizes are exponentially grown, as earlier studies have shown that at a linear scale, performance effects tend to decrease in size, but that when measured with exponentially growing training sets, near-constant (i.e. log-linear) improvements are observed (Banko and Brill, 2001).</S>
			<S sid ="46" ssid = "18">We create incrementally-sized training sets for the word prediction task on the basis of the TR A IN - R EU TER S set.</S>
			<S sid ="47" ssid = "19">Each training subset is created backward from the point at which the final 100,000-word R EU TER S set starts.</S>
			<S sid ="48" ssid = "20">The increments are exponential with base number 10, and for every power of 10 we cut off training sets at n times that power, where n = 1, 2, 3, . . .</S>
			<S sid ="49" ssid = "21">, 8, 9 (for example, 10, 20, . . .</S>
			<S sid ="50" ssid = "22">, 80, 90).</S>
			<S sid ="51" ssid = "23">The actual examples to learn from are created by windowing over all sequences of tokens.</S>
			<S sid ="52" ssid = "24">We encode examples by taking a left context window spanning seven tokens, and a right context also spanning seven tokens.</S>
			<S sid ="53" ssid = "25">Thus, the task is represented by a growing number of examples, each characterized by 14 positional features carrying tokens as values, and one class label representing the word to be predicted.</S>
			<S sid ="54" ssid = "26">The choice for 14 is intended to cover at least the superficially most important positional features.</S>
			<S sid ="55" ssid = "27">We assume that a word more distant than seven positions left or right of a focus word will almost never be more informative for the task than any of the words within this scope.</S>
			<S sid ="56" ssid = "28">2.3 IGTree.</S>
			<S sid ="57" ssid = "29">IGTree (Daelemans et al., 1997) is an algorithm for the top-down induction of decision trees.</S>
			<S sid ="58" ssid = "30">It compresses a database of labeled examples into a lossless-compression decision-tree structure that preserves the labeling information of all examples, and technically should be named a trie according to (Knuth, 1973).</S>
			<S sid ="59" ssid = "31">A labeled example is a feature- value vector, where features in our study represent a sequence of tokens representing context, associated with a symbolic class label representing the word to be predicted.</S>
			<S sid ="60" ssid = "32">An IGTR EE is composed of nodes that each represent a partition of the original example database, and are labeled by the most frequent class of that partition.</S>
			<S sid ="61" ssid = "33">The root node of the trie thus represents the entire example database and carries the most frequent value as class label, while end nodes (leafs) represent a homogeneous partition of the database in which all examples have the same class label.</S>
			<S sid ="62" ssid = "34">A node is either a leaf, or is a non-ending node that branches out to nodes at a deeper level of the trie.</S>
			<S sid ="63" ssid = "35">Each branch represents a test on a feature value; branches fanning out of one node test on values of the same feature.</S>
			<S sid ="64" ssid = "36">To attain high compression levels, IGTR EE adopts the same heuristic that most other decision- tree induction algorithms adopt, such as C 4.5 (Quinlan, 1993), which is to always branch out testing on the most informative, or most class-discriminative features first.</S>
			<S sid ="65" ssid = "37">Like C 4.5, IGTR EE uses information gain (IG) to estimate the most informative features.</S>
			<S sid ="66" ssid = "38">The IG of feature i is measured by computing the difference in uncertainty (i.e. entropy) between the situations without and with knowledge of the value of that feature with respect to predicting the class la bel: I Gi = H (C ) − I: ∈Vi P (v) × H (C |v), where C is the set of class labels, Vi is the set of values for feature i, and H (C ) = − I:c∈C P (c) log2 P (c) is the entropy of the class labels.</S>
			<S sid ="67" ssid = "39">In contrast with C 4.5, IGTR EE computes the IG of all features once on the full database of training examples, makes a feature ordering once on these computed IG values, and uses this ordering throughout the whole trie.</S>
			<S sid ="68" ssid = "40">Another difference with C 4.5 is that IGTR EE does not prune its produced trie, so that it performs a lossless compression of the labeling information of the original example database.</S>
			<S sid ="69" ssid = "41">As long as the database does not contain fully ambiguous examples (with the same features, but different class labels), the trie produced by IGTR EE is able to reproduce the classifications of all examples in the original example database perfectly.</S>
			<S sid ="70" ssid = "42">Due to the fact that IGTR EE computes the IG of all features once, it is functionally equivalent to IB 1IG (Daelemans et al., 1999), a k-nearest neighbor classifier for symbolic features, with k = 1 and using a particular feature weighting in the sim 50 TEST-BROWN TEST-ALICE TEST REUTERS 40 30 20 10 0ilarity function in which the weight of each fea ture is larger than the sum of all weights of features 100 1,000 10,000 100,000 examples 1,000,000 10,000,000 30,000,000 with a lower weight (e.g. as in the exponential sequence 1, 2, 4, 8, . . .</S>
			<S sid ="71" ssid = "43">where 2 &gt; 1, 4 &gt; (1 + 2), 8 &gt; (1 + 2 + 4), etc.).</S>
			<S sid ="72" ssid = "44">Both algorithms will base their classification on the example that matches on most features, ordered by their IG, and guess a majority class of the set of examples represented at the level of mismatching.</S>
			<S sid ="73" ssid = "45">IGTR EE, therefore, can be seen as an approximation of IB 1IG with k = 1 that has favorable asymptotic complexities as compared to IB 1IG.</S>
			<S sid ="74" ssid = "46">IGTR EE’s computational bottleneck is the trie construction process, which has an asymptotic complexity of O(n lg(v) f ) of CPU, where n is the number of training examples, v is the average branching factor of IGTR EE (how many branches fan out of a node, on average), and f is the number of features.</S>
			<S sid ="75" ssid = "47">Storing the trie, on the other hand, costs O(n) in memory, which is less than the O(n f ) of IB 1IG.</S>
			<S sid ="76" ssid = "48">Classification in IGTR EE takes an efficient O(f lg(v)) of CPU, versus the cumbersome worst- case O(n f ) of IB 1IG, that is, in the typical case that n is much higher than f or v. Interestingly, IGTR EE is functionally equivalent to back-off smoothing (Zavrel and Daelemans, 1997), with the IG of the features determining the order in which to back off, which in the case of word prediction tends to be from the outer context to the inner context of the immediately neighboring words.</S>
			<S sid ="77" ssid = "49">Like with probabilistic n-gram based models with a back-off smoothing scheme, IGTR EE will prefer matches that are as exact as possible (e.g. matching on all 14 features), but will back-off by disregarding lesser important features first, down to a simple bigram model drawing on the most important feature, the immediately preceding left word.</S>
			<S sid ="78" ssid = "50">In sum, IGTR EE shares its scaling abilities with nFigure 1: Learning curves of word prediction accu racies of IGTR EE trained on TR A IN -R EU TER S, and tested on R EU TER S, A LIC E, and B ROWN.</S>
			<S sid ="79" ssid = "51">gram models, and its implementation allows it to handle large values of n.</S>
	</SECTION>
	<SECTION title="All-words prediction. " number = "3">
			<S sid ="80" ssid = "1">3.1 Learning curve experiments.</S>
			<S sid ="81" ssid = "2">The word prediction accuracy learning curves computed on the three test sets, and trained on increasing portions of TR A IN -R EU TER S, are displayed in Figure 1.</S>
			<S sid ="82" ssid = "3">The best accuracy observed is 42.2% with 30 million training examples, on R EU TER S. Apparently, training and testing on the same type of data yields markedly higher prediction accuracies than testing on a different-type corpus.</S>
			<S sid ="83" ssid = "4">Accuracies on B ROWN are slightly higher than on A LIC E, but the difference is small; at 30 million training examples, the accuracy on A LIC E is 12.6%, and on B ROWN 15.8%.</S>
			<S sid ="84" ssid = "5">A second observation is that all three learning curves are progressing upward with more training examples, and roughly at a constant log-linear rate.</S>
			<S sid ="85" ssid = "6">When estimating the rates after about 50,000 examples (before which the curves appear to be more volatile), with every tenfold increase of the number of training examples the prediction accuracy on R EU TER S increases by a constant rate of about 8%, while the increases on A LIC E and B ROWN are both about 2% at every tenfold.</S>
			<S sid ="86" ssid = "7">3.2 Memory requirements and classification.</S>
			<S sid ="87" ssid = "8">speed TEST-BROWN TEST-ALICE TEST-REUTERS The numbers of nodes exhibit an interesting sublin- ear relation with respect to the number of training examples, which is in line with the asymptotic complexity order O(n), where n is the number of training instances.</S>
			<S sid ="88" ssid = "9">An increasingly sublinear amount of nodes is necessary; while at 10,000 training instances the number of nodes is 7,759 (0.77 nodes per instance), at 1 million instances the number of nodes is 652,252 (0.65 nodes per instance), and at 30 10000 1000 100 100 1,000 10,000 100,000 training examples 1,000,000 10,000,000 30,000,000 million instances the number of nodes is 15,956,878 (0.53 nodes per instance).</S>
			<S sid ="89" ssid = "10">A factor in classification speed is the average amount of branching.</S>
			<S sid ="90" ssid = "11">Conceivably, the word prediction task can lead to a large branching factor, especially in the higher levels of the tree.</S>
			<S sid ="91" ssid = "12">However, not every word can be the neighbor of every other word in finite amounts of text.</S>
			<S sid ="92" ssid = "13">To estimate the average branching factor of a tree we compute the f th root of the total number of nodes (f being the number of features, i.e. 14).</S>
			<S sid ="93" ssid = "14">The largest decision tree currently constructed is the one on the basis of a training set of 30 million examples, having 15,956,878 nodes.</S>
			<S sid ="94" ssid = "15">This tree has an average branching factor of 1√4 15, 956, 878 ≈ 3.27; all other trees have smaller branching factors.</S>
			<S sid ="95" ssid = "16">Together with the fact that we have but 14 features, and the asymptotic complexity order of classification is O(f lg(v)), where v is the average branching factor, classification can be expected to be fast.</S>
			<S sid ="96" ssid = "17">Indeed, depending on the machine’s CPU on which the experiment is run, we observe quite favorable classification speeds.</S>
			<S sid ="97" ssid = "18">Figure 2 displays the various speeds (in terms of the number of test tokens predicted per second) attained on the three test sets3 . The best prediction accuracies are still attained at classification speeds of over a hundred predicted tokens per second.</S>
			<S sid ="98" ssid = "19">Two other relevant observations are that first, the classification speed hardly differs between the three test sets (B ROWN is classified only slightly slower than the other two test sets), indicating that the classifier is spending a roughly comparable amount of searching through the decision trees regardless of genre differences.</S>
			<S sid ="99" ssid = "20">Second, the decrease in speed settles 3 Measurements were made on a GNU/Linux x86-based machine with 2.0 Ghz AMD Opteron processors.</S>
			<S sid ="100" ssid = "21">Figure 2: Word prediction speed, in terms of the number of classified test examples per second, measured on the three test sets, with increasing training examples.</S>
			<S sid ="101" ssid = "22">Both axes have a logarithmic scale.</S>
			<S sid ="102" ssid = "23">on a low log-linear rate after about one million examples.</S>
			<S sid ="103" ssid = "24">Thus, while trees grow linearly, and accuracy increases log-linearly, the speed of classification slowly diminishes at decreasing rates.</S>
	</SECTION>
	<SECTION title="Confusables. " number = "4">
			<S sid ="104" ssid = "1">Word prediction from context can be considered a very hard task, due to the many choices open to the predictor at many points in the sequence.</S>
			<S sid ="105" ssid = "2">Predicting content words, for example, is often only possible through subtle contextual clues or by having the appropriate domain or world knowledge, or intimate knowledge of the writer’s social context and intentions.</S>
			<S sid ="106" ssid = "3">In contrast, certain function words tend to be predictable due to the positions they take in syntactic phrase structure; their high frequency tends to ensure that plenty of examples of them in context are available.</S>
			<S sid ="107" ssid = "4">Due to the important role of function words in syntactic structure, it can be quite disruptive for a parser and for human readers alike to encounter a mistyped function word that in its intended form is another function word.</S>
			<S sid ="108" ssid = "5">In fact, confusable errors between frequent forms occur relatively frequently.</S>
			<S sid ="109" ssid = "6">Examples of these so-called confusables in English are there versus their and the contraction they’re; or the duo than and then.</S>
			<S sid ="110" ssid = "7">Confusables can arise from having the same pronunciation (homophones), or having very similar pronunciation (country or county) or spelling (dessert, desert), hav ing very close lexical semantics (as between among and between), or being inflections or case variants of the same stem (I versus me, or walk versus walks), and may stem from a lack of concentration or experience by the writer.</S>
			<S sid ="111" ssid = "8">Distinguishing between confusables is essentially the same task as word prediction, except that the number of alternative outcomes is small, e.g. two or three, rather than thousands or more.</S>
			<S sid ="112" ssid = "9">The typical application setting is also more specific: given that a 100 80 60 40 20 0 Brown Alice Reuters there / their / they’re confusible expert generic word predictor Reuters NYT writer has produced a text (e.g. a sentence in a word processor), it is possible to check the correctness of each occurrence of a word known to be part of a pair or triple of confusables.</S>
			<S sid ="113" ssid = "10">We performed a series of experiments on dis- ambiguating nine frequent confusables in English adopted from (Golding and Roth, 1999).</S>
			<S sid ="114" ssid = "11">We employed an experimental setting in which we use the same experimental data as before, in which only examples of the confusable words are drawn – note that we ignore possible confusable errors in both training and test set.</S>
			<S sid ="115" ssid = "12">This data set generation procedure reduces the amount of examples considerably.</S>
			<S sid ="116" ssid = "13">Despite having over 130 million words in TR A IN - R EU TER S, frequent words such as there and than occur just over 100,000 times.</S>
			<S sid ="117" ssid = "14">To be able to run learning curves with more than this relatively small amount of examples, we expanded our training material with the New York Times of 1994 to 2002 (henceforth TR A INN Y T), part of the English Gigaword collection published by the Linguistic Data Consortium, offering 1,096,950,281 tokens.</S>
			<S sid ="118" ssid = "15">As a first illustration of the experimental outcomes, we focus on the three-way confusable there – their – they’re for which we trained one classifier, which we henceforth refer to as a confusable expert.</S>
			<S sid ="119" ssid = "16">The learning curve results of this confus- able expert are displayed in Figure 3 as the top three graphs.</S>
			<S sid ="120" ssid = "17">The logarithmic x-axis displays the full number of instances from TR A IN -R EU TER S up to 130.3 million examples, and from TR A INN Y T after this point.</S>
			<S sid ="121" ssid = "18">Counter to the learning curves in the all- words prediction experiments, and to the observation by (Banko and Brill, 2001), the learning curves of this confusable triple in the three different data sets flatten, and converge, remarkably, to a roughly similar score of about 98%.</S>
			<S sid ="122" ssid = "19">The convergence only occurs after examples from TR A INN Y T are added.</S>
			<S sid ="123" ssid = "20">100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 training examples Figure 3: Learning curves in terms of word prediction accuracy on deciding between the confusable pair there, their, and they’re, by IGTR EE trained on TR A IN -R EU TER S, and tested on R EU TER S, A L- IC E, and B ROWN.</S>
			<S sid ="124" ssid = "21">The top graphs are accuracies attained by the confusable expert; the bottom graphs are attained by the all-words predictor trained on TR A IN -R EU TER S until 130 million examples, and on TR A INN Y T beyond (marked by the vertical bar).</S>
			<S sid ="125" ssid = "22">In the bottom of the same Figure 3 we have also plotted the word prediction accuracies on the three words there, their, and they’re attained by the all- words predictor described in the previous section on the three test sets.</S>
			<S sid ="126" ssid = "23">The accuracies, or rather recall figures (i.e. the percentage of occurrences of the three words in the test sets which are correctly predicted as such), are considerably lower than those on the confusable disambiguation task.</S>
			<S sid ="127" ssid = "24">Table 2 presents the experimental results obtained on nine confusable sets when training and testing on Reuters material.</S>
			<S sid ="128" ssid = "25">The third column lists the accuracy (or recall) scores of the all-words word prediction system at the maximal training set size of 30 million labeled examples.</S>
			<S sid ="129" ssid = "26">The fourth columns lists the accuracies attained by the confusable expert for the particular confusable pair or triple, measured at 30 million training examples, from which each particular confusable expert’s examples are extracted.</S>
			<S sid ="130" ssid = "27">The amount of examples varies for the selected con- fusable sets, as can be seen in the second column.</S>
			<S sid ="131" ssid = "28">Scores attained by the all-words predictor on these words vary from below 10% for relatively low- frequent words to around 60% for the more frequent confusables; the latter numbers are higher than the Co nf us ab le set Accurac y on test set (%) R EU TE R S A LI C E B RO W N c i t e s i t e s i g h t a c c e p t e x c e p t a f f e c t e f f e c t f e w e r l e s s a m o n g b e t w e e n I m e tha n - the n th er e - th ei r - th ey ’r e to - to o - tw o 1 0 0 . 0 8 4 . 6 9 2 . 3 9 0 . 5 9 4 . 4 9 9 . 0 9 7 . 2 9 8 . 1 9 4 . 3 1 0 0 . 0 1 0 0 . 0 1 0 0 . 0 1 0 0 . 0 7 7 . 8 9 8 . 3 9 2 . 9 9 7 . 8 9 3 . 4 6 9 . 0 9 7 . 0 8 9 . 5 9 7 . 2 7 4 . 4 9 8 . 3 9 5 . 8 9 7 . 3 9 2 . 9 Table 2: Disambiguation scores on nine confusable set, attained by the all-words prediction classifier trained on 30 million examples of TR A IN -R EU TER S, and by confusable experts on the same training set.</S>
			<S sid ="132" ssid = "29">The second column displays the number of examples of each confusable set in the 30-million word training set; the list is ordered on this column.</S>
			<S sid ="133" ssid = "30">overall accuracy of this system on R EU TER S. Nevertheless they are considerably lower than the scores attained by the confusable disambiguation classifiers, while being trained on many more examples (i.e., all 30 million available).</S>
			<S sid ="134" ssid = "31">Most of the confus- able disambiguation classifiers attain accuracies of well above 90%.</S>
			<S sid ="135" ssid = "32">When the learning curves are continued beyond TR A IN -R EU TER S into TR A INN Y T, about a thousand times as many training examples can be gathered as training data for the confusable experts.</S>
			<S sid ="136" ssid = "33">Table 3 displays the nine confusable expert’s scores after being trained on examples extracted from a total of one billion words of text, measured on all three test sets.</S>
			<S sid ="137" ssid = "34">Apart from a few outliers, most scores are above 90%, and more importantly, the scores on A L- IC E and B ROWN do not seriously lag behind those on R EU TER S; some are even better.</S>
	</SECTION>
	<SECTION title="Related work. " number = "5">
			<S sid ="138" ssid = "1">As remarked in the cases reported in the literature directly related to the current article, word prediction is a core task to natural language processing, and one of the few that takes no annotation layer to provide data for supervised machine learning and probabilistic modeling (Golding and Roth, 1999; Even-Zohar Table 3: Disambiguation scores on nine confusable set, attained by confusable experts trained on examples extracted from 1 billion words of text from TR A IN -R EU TER S plus TR A INN Y T, on the three test sets.</S>
			<S sid ="139" ssid = "2">and Roth, 2000; Banko and Brill, 2001).</S>
			<S sid ="140" ssid = "3">Our discrete, classificationased approach has the same goal as probabilistic methods for language modeling for automatic speech recognition (Jelinek, 1998), and is also functionally equivalent to n-gram models with back-off smoothing (Zavrel and Daelemans, 1997).</S>
			<S sid ="141" ssid = "4">The papers by Golding and Roth, and Banko and Brill on confusable correction focus on the more common type of than/then confusion that occurs a lot in the process of text production.</S>
			<S sid ="142" ssid = "5">Both pairs of authors use the confusable correction task to illustrate scaling issues, as we have.</S>
			<S sid ="143" ssid = "6">Golding and Roth illustrate that multiplicative weight-updating algorithms such as Winnow can deal with immense input feature spaces, where for each single classification only a small number of features is actually relevant (Golding and Roth, 1999).</S>
			<S sid ="144" ssid = "7">With IGTR EE we have an arguably competitive efficient, but one-shot learning algorithm; IGTR EE does not need an iterative procedure to set weights, and can also handle a large feature space.</S>
			<S sid ="145" ssid = "8">Instead of viewing all positional features as containers of thousands of atomic word features, it treats the positional features as the basic tests, branching on the word values in the tree.</S>
			<S sid ="146" ssid = "9">More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Huang and Powers, 2001).</S>
	</SECTION>
	<SECTION title="Discussion. " number = "6">
			<S sid ="147" ssid = "1">In this article we explored the scaling abilities of IGTR EE, a simple decision-tree algorithm with favorable asymptotic complexities with respect to multi-label classification tasks.</S>
			<S sid ="148" ssid = "2">IGTR EE is applied to word prediction, a task for which virtually unlimited amounts of training examples are available, with very large amounts of predictable class labels; and confusable disambiguation, a specialization of word prediction focusing on small sets of confusable words.</S>
			<S sid ="149" ssid = "3">Best results are 42.2% correctly predicted tokens (words and punctuation markers) when training and testing on data from the Reuters newswire corpus; and confusable disambiguation accuracies of well above 90%.</S>
			<S sid ="150" ssid = "4">Memory requirements and speeds were shown to be realistic.</S>
			<S sid ="151" ssid = "5">Analysing the results of the learning curve experiments with increasing amounts of training examples, we observe that better word prediction accuracy can be attained simply by adding more training examples, and that the progress in accuracy proceeds at a log-linear rate.</S>
			<S sid ="152" ssid = "6">The best rate we observed was an 8% increase in performance every tenfold multiplication of the number of training examples, when training and testing on the same data.</S>
			<S sid ="153" ssid = "7">Despite the fact that all-words prediction lags behind in disambiguating confusibles, in comparison with classifiers that are focused on disambiguating single sets of confusibles, we see that this lag is only relative to the amount of training material available.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="154" ssid = "8">This research was funded by the Netherlands Organ- isation for Scientific Research (NWO).</S>
			<S sid ="155" ssid = "9">The author wishes to thank Ko van der Sloot for programming assistance.</S>
	</SECTION>
</PAPER>
