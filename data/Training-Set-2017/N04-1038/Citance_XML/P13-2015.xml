<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words.</S>
		<S sid ="2" ssid = "2">In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution.</S>
		<S sid ="3" ssid = "3">We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Coreference resolvers are typically evaluated on collections of news articles that cover a wide range of topics, such as the ACE (ACE03, 2003; ACE04, 2004; ACE05, 2005) and OntoNotes (Pradhan et al., 2007) data sets.</S>
			<S sid ="5" ssid = "5">Many NLP applications, however, involve text analysis for specialized domains, such as clinical medicine (Gooch and Roudsari, 2012; Glinos, 2011), legal text analysis (BouayadAgha et al., 2009), and biological literature (BatistaNavarro and Ananiadou, 2011; Castan˜ o et al., 2002).</S>
			<S sid ="6" ssid = "6">Learning-based coreference resolvers can be easily retrained for a specialized domain given annotated training texts for that domain.</S>
			<S sid ="7" ssid = "7">However, we found that retraining an off-the-shelf coreference resolver with domain- specific texts showed little benefit.</S>
			<S sid ="8" ssid = "8">This surprising result led us to question the nature of the feature sets used by noun phrase (NP) coreference resolvers.</S>
			<S sid ="9" ssid = "9">Nearly all of the features employed by recent systems fall into three categories: string match and word overlap, syntactic properties (e.g., appositives, predicate nominals, parse features, etc.), and semantic matching (e.g., gender agreement, WordNet similarity, named entity classes, etc.).</S>
			<S sid ="10" ssid = "10">Conspicuously absent from most systems are lexical features that allow the classifier to consider the specific words when making a coreference decision.</S>
			<S sid ="11" ssid = "11">A few researchers have experimented with lexical features, but they achieved mixed results in evaluations on broad-coverage corpora (Bengston and Roth, 2008; Bjo¨ rkelund and Nugues, 2011; Rahman and Ng, 2011a).</S>
			<S sid ="12" ssid = "12">We hypothesized that lexicalized features can have a more substantial impact in domain-specific settings.</S>
			<S sid ="13" ssid = "13">Lexical features can capture domain- specific knowledge and subtle semantic distinctions that may be important within a domain.</S>
			<S sid ="14" ssid = "14">For example, based on the resolutions found in domain-specific training sets, our lexicalized features captured the knowledge that “tomcat” can be coreferent with “plane”, “UAW” can be coreferent with “union”, and “anthrax” can be coreferent with “diagnosis”.</S>
			<S sid ="15" ssid = "15">Capturing these types of domain-specific information is often impossible using only general-purpose resources.</S>
			<S sid ="16" ssid = "16">For example, WordNet defines “tomcat” only as an animal, does not contain an entry for “UAW”, and categorizes “anthrax” and “diagnosis” very differently.1 In this paper, we evaluate the impact of lexicalized features on 4 domains: management succession (MUC6 data), vehicle launches (MUC7 data), disease outbreaks (ProMed texts), and terrorism (MUC4 data).</S>
			<S sid ="17" ssid = "17">We incorporate lexical- ized feature sets into two different coreference architectures: Reconcile (Stoyanov et al., 2010), a pairwise coreference classifier, and Sieve (Raghunathan et al., 2010), a rule-based system.</S>
			<S sid ="18" ssid = "18">Our results show that lexicalized features significantly improve performance in all four domains and in both types of coreference architectures.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="19" ssid = "1">We are not the first researchers to use lexicalized features for coreference resolution.</S>
			<S sid ="20" ssid = "2">However, pre 1 WordNet defines “anthrax” as a disease (condition/state).</S>
			<S sid ="21" ssid = "3">and “diagnosis” as an identification (discovery event).</S>
			<S sid ="22" ssid = "4">Test Train --.--.--.</S>
			<S sid ="23" ssid = "5">MUC6 MUC7 Promed MUC4 P R F P R F P R F P R F MUC6 80.79 62.71 70.61 84.33 61.74 71.29 83.54 70.34 76.37 80.22 60.81 69.18 MUC7 74.78 65.59 69.88 82.73 64.09 72.23 85.29 71.82 77.98 77.35 64.19 70.16 Promed 73.60 64.20 68.60 82.88 63.37 71.82 80.31 72.66 76.29 74.52 65.65 69.80 MUC4 69.27 65.66 67.42 71.49 67.22 69.29 76.92 74.25 75.56 71.76 67.37 69.50 Table 1: Cross-domain B3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set.</S>
			<S sid ="24" ssid = "6">The Paired Permutation test (Pesarin, 2001) was used for statistical significance testing and gray cells represent results that are not significantly different from the best result.</S>
			<S sid ="25" ssid = "7">vious work has evaluated the benefit of lexical features only for broad-coverage data sets.</S>
			<S sid ="26" ssid = "8">Bengston and Roth (2008) incorporated a memorization feature to learn which entities can refer to one another.</S>
			<S sid ="27" ssid = "9">They created a binary feature for every pair of head nouns, including pronouns.</S>
			<S sid ="28" ssid = "10">They reported no significant improvement from these features on the ACE 2004 data.</S>
			<S sid ="29" ssid = "11">Rahman and Ng (2011a) also utilized lexical features, going beyond strict memorization with methods to combat data sparseness and incorporating semantic information.</S>
			<S sid ="30" ssid = "12">They created a feature for every ordered pair of head nouns (for pronouns and nominals) or full NPs (for proper nouns).</S>
			<S sid ="31" ssid = "13">Semi-lexical features were also used when one NP was a Named Entity, and unseen features were used when the NPs were not in the training set.</S>
			<S sid ="32" ssid = "14">Their features did yield improvements on both the ACE 2005 and OntoNotes2 data, but the semi- lexical features included Named Entity classes as well as word-based features.</S>
			<S sid ="33" ssid = "15">Rahman and Ng (2011b) explored the use of lexical features in greater detail and showed their benefit on the ACE05 corpus independent of, and combined with, a conventional set of coreference features.</S>
			<S sid ="34" ssid = "16">The ACE05 corpus is drawn from six sources (Newswire, Broadcast News, Broadcast Conversations, Conversational Telephone Speech, Webblogs, and Usenet).</S>
			<S sid ="35" ssid = "17">The authors experimented with utilizing lexical information drawn from different sources.</S>
			<S sid ="36" ssid = "18">The results showed that the best performance came from training and testing with lexical knowledge drawn from the same source.</S>
			<S sid ="37" ssid = "19">Although our approach is similar, this paper focuses on learning lexical information from different domains as opposed to the different genres found in the six sources of the ACE05 corpus.</S>
			<S sid ="38" ssid = "20">Bjo¨ rkelund and Nugues (2011) used lexical word pairs for the 2011 CoNLL Shared Task, showing significant positive impact on performance.</S>
			<S sid ="39" ssid = "21">They used over 2000 annotated documents from the broad-coverage OntoNotes corpus for training.</S>
			<S sid ="40" ssid = "22">Our work aims to show the benefit of lexical features using much smaller training sets (&lt; 50 documents) focused on specific domains.</S>
			<S sid ="41" ssid = "23">Lexical features have also been used for slightly different purposes.</S>
			<S sid ="42" ssid = "24">Florian et al.</S>
			<S sid ="43" ssid = "25">(2004) utilized lexical information such as mention spelling and context for entity tracking in ACE.</S>
			<S sid ="44" ssid = "26">Ng (2007) used lexical information to assess the likelihood of a noun phrase being anaphoric, but this did not show clear improvements on ACE data.</S>
			<S sid ="45" ssid = "27">There has been previous work on domain- specific coreference resolution for several domains, including biological literature (Castan˜ o et al., 2002; Liang and Lin, 2005; Gasperin and Briscoe, 2008; Kim et al., 2011; BatistaNavarro and Ananiadou, 2011), clinical medicine (He, 2007; Zheng et al., 2011; Glinos, 2011; Gooch and Roudsari, 2012) and legal documents (BouayadAgha et al., 2009).</S>
			<S sid ="46" ssid = "28">In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.</S>
			<S sid ="47" ssid = "29">But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.</S>
			<S sid ="48" ssid = "30">To the best of our knowledge, our work is the first to examine the impact of lexicalized features for domain-specific coreference resolution.</S>
	</SECTION>
	<SECTION title="Exploiting Lexicalized Features. " number = "3">
			<S sid ="49" ssid = "1">Table 1 shows the performance of a learning-based coreference resolver, Reconcile (Stoyanov et al., 2010), with its default feature set using different combinations of training and testing data.</S>
			<S sid ="50" ssid = "2">Reconcile does not include any lexical features, but does contain over 60 general features covering semantic agreement, syntactic constraints, string match and recency.</S>
			<S sid ="51" ssid = "3">Each row represents a training set, each column represents a test set, and each cell shows precision (P), recall (R), and F score results under the B3 metric when using the corresponding training and test data.</S>
			<S sid ="52" ssid = "4">The best results for each test set appear MUC6 MUC7 ProM ED MUC4 P R F P R F P R F P R F Re co nci le 80.79 62.71 70.61 82.73 64.09 72.23 80.31 72.66 76.29 71.76 67.37 69.50 +L ex Lo ok up 87.</S>
			<S sid ="53" ssid = "5">01 63.40 73.</S>
			<S sid ="54" ssid = "6">35 87.</S>
			<S sid ="55" ssid = "7">39 62.86 73.</S>
			<S sid ="56" ssid = "8">12 86.</S>
			<S sid ="57" ssid = "9">66 70.95 78.</S>
			<S sid ="58" ssid = "10">02 82.</S>
			<S sid ="59" ssid = "11">89 67.53 74.</S>
			<S sid ="60" ssid = "12">42 +L ex Set s 86.</S>
			<S sid ="61" ssid = "13">50 63.76 73.</S>
			<S sid ="62" ssid = "14">41 85.</S>
			<S sid ="63" ssid = "15">86 64.35 73.</S>
			<S sid ="64" ssid = "16">56 86.</S>
			<S sid ="65" ssid = "17">19 72.14 78.</S>
			<S sid ="66" ssid = "18">54 81.</S>
			<S sid ="67" ssid = "19">98 67.73 74.</S>
			<S sid ="68" ssid = "20">18 Sie ve 92.20 61.70 73.90 91.46 59.59 72.16 94.43 67.25 78.55 91.30 59.84 72.30 +L ex Be gin 91.</S>
			<S sid ="69" ssid = "21">22 62.97 74.</S>
			<S sid ="70" ssid = "22">51 91.</S>
			<S sid ="71" ssid = "23">24 60.28 72.</S>
			<S sid ="72" ssid = "24">59 93.</S>
			<S sid ="73" ssid = "25">51 69.15 79.</S>
			<S sid ="74" ssid = "26">51 89.</S>
			<S sid ="75" ssid = "27">01 62.84 73.</S>
			<S sid ="76" ssid = "28">67 +L ex En d 90.</S>
			<S sid ="77" ssid = "29">59 63.47 74.</S>
			<S sid ="78" ssid = "30">64 91.</S>
			<S sid ="79" ssid = "31">17 60.56 72.</S>
			<S sid ="80" ssid = "32">78 93.</S>
			<S sid ="81" ssid = "33">99 68.87 79.</S>
			<S sid ="82" ssid = "34">49 89.</S>
			<S sid ="83" ssid = "35">04 64.03 74.</S>
			<S sid ="84" ssid = "36">47 Table 2: B3 results for baselines and lexicalized feature sets across four domains.</S>
			<S sid ="85" ssid = "37">in boldface.</S>
			<S sid ="86" ssid = "38">We performed statistical significance testing using the Paired Permutation test (Pesarin, 2001) and the gray cells represent results where there was not significant difference from the best results in the same column.</S>
			<S sid ="87" ssid = "39">If just one cell is gray in a column, that indicates the result was significantly better than the other results in the same column with p ≤ 0.05.Table 1 does not show much benefit from train ing on the same domain as the test set.</S>
			<S sid ="88" ssid = "40">Three different training sets produce F scores that are not significantly different for both the MUC6 and MUC4 test data.</S>
			<S sid ="89" ssid = "41">For ProMed, training on the MUC7 data yields significantly better results than training on all the other data sets, including ProMed texts!</S>
			<S sid ="90" ssid = "42">Based on these results, it would seem that training on the MUC7 texts is likely to yield the best results no matter what domain you plan to use the coreference resolver for.</S>
			<S sid ="91" ssid = "43">The goal of our work is to investigate whether lexical features can extract additional knowledge from domain-specific training texts to help tailor a coreference resolver to perform better for a specific domain.</S>
			<S sid ="92" ssid = "44">3.1 Extracting Coreferent Training Pairs.</S>
			<S sid ="93" ssid = "45">We adopt the terminology introduced by Stoyanov et al.</S>
			<S sid ="94" ssid = "46">(2009) to define a coreference element (CE) as a noun phrase that can participate in a coreference relation based on the task definition.</S>
			<S sid ="95" ssid = "47">Each training document has manually annotated gold coreference chains corresponding to the sets of CEs that are coreferent.</S>
			<S sid ="96" ssid = "48">For each CE in a gold chain, we pair that CE with all of the other CEs in the same chain.</S>
			<S sid ="97" ssid = "49">We consider the coreference relation to be bidirectional, so we don’t retain information about which CE was the antecedent.</S>
			<S sid ="98" ssid = "50">We do not extract CE pairs that share the same head noun because they are better handled with string match.</S>
			<S sid ="99" ssid = "51">For nominal NPs, we retain only the head noun, but we use the entire NP for proper names.</S>
			<S sid ="100" ssid = "52">We discard pairs that include a pronoun, and nor malize strings to lower case for consistency.</S>
			<S sid ="101" ssid = "53">3.2 Lexicalized Feature Sets.</S>
			<S sid ="102" ssid = "54">We explore two ways to capture lexicalized information as features.</S>
			<S sid ="103" ssid = "55">The first approach indicates whether two CEs have ever been coreferent in the training data.</S>
			<S sid ="104" ssid = "56">We create a single feature called LEXLOOKUP(X,Y) that receives a value of 1 when x and y have been coreferent at least twice, or a value of 0 otherwise.2 LEXLOOKUP(X,Y) is a single feature that captures all CE pairs that were coreferent in the training data.</S>
			<S sid ="105" ssid = "57">We also created set-based features that capture the set of terms that have been coreferent with a particular CE.</S>
			<S sid ="106" ssid = "58">The C oref Set(x) is the set of CEs that have appeared in the same coreference chain as mention x at least twice.</S>
			<S sid ="107" ssid = "59">We create a set of binary-valued features LEXSET(X,Y), one for each CE x in the training data.</S>
			<S sid ="108" ssid = "60">Given a pair of CEs, x and y, LEXSET(X,Y)= 1 if y ∈ C oref Set(x), or 0 otherwise.</S>
			<S sid ="109" ssid = "61">The benefit of the set-based features over a single mono lithic feature is that the classifier has one set-based feature for each mention found in the training data, so it can learn to handle individual terms differently.</S>
			<S sid ="110" ssid = "62">We also tried encoding a separate feature for each distinct pair of words, analogous to the memorization feature in Bengston and Roth (2008).</S>
			<S sid ="111" ssid = "63">This did not improve performance as much as the other feature representations presented here.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "4">
			<S sid ="112" ssid = "1">4.1 Data Sets.</S>
			<S sid ="113" ssid = "2">We evaluated the performance of lexicalized features on 4 domain-specific corpora including two standard coreference benchmarks, the MUC6 and MUC7 data sets.</S>
			<S sid ="114" ssid = "3">The MUC6 domain is management succession and consists of 30 training texts and 30 test texts.</S>
			<S sid ="115" ssid = "4">The MUC7 domain is vehicle 2 We require a frequency ≥ 2 to minimize overfitting because many cases occur only once in the training data.</S>
			<S sid ="116" ssid = "5">launches and consists of 30 training texts and 20 test texts.</S>
			<S sid ="117" ssid = "6">We used these standard train/test splits to be consistent with previous work.</S>
			<S sid ="118" ssid = "7">We also created 2 new coreference data sets which we will make freely available.</S>
			<S sid ="119" ssid = "8">We manually annotated 45 ProMed-mail articles (www.promedmail.org) about disease outbreaks and 45 MUC4 texts about terrorism, following the MUC guidelines (Hirschman, 1997).</S>
			<S sid ="120" ssid = "9">Inter- annotator agreement between two annotators was .77 (κ) on ProMed and .84 (MUC F Score)(Villain et al., 1995) on both ProMed and MUC4.3 We performed 5-fold cross-validation on both data sets and report the micro-averaged results.</S>
			<S sid ="121" ssid = "10">Gold CE spans were used in all experiments to factor out issues with markable identification and anaphoricity across the different domains.</S>
			<S sid ="122" ssid = "11">4.2 Coreference Resolution Models.</S>
			<S sid ="123" ssid = "12">We conducted experiments using two coreference resolution architectures.</S>
			<S sid ="124" ssid = "13">Reconcile4 (Stoyanov et al., 2010) is a freely available pairwise mention classifier.</S>
			<S sid ="125" ssid = "14">For classification, we chose Weka’s (Witten and Frank, 2005) Decision Tree learner inside Reconcile.</S>
			<S sid ="126" ssid = "15">Reconcile contains roughly 60 features (none lexical), largely modeled after Ng and Cardie (2002).</S>
			<S sid ="127" ssid = "16">We modified Reconcile’s Single Link clustering scheme to enforce an additional rule that non-overlapping proper names cannot be merged into the same chain.</S>
			<S sid ="128" ssid = "17">We also conducted experiments with the Sieve coreference resolver, which applies high precision heuristic rules to incrementally build coreference chains.</S>
			<S sid ="129" ssid = "18">We implemented the LEXLOOKUP(X,Y) feature as an additional heuristic rule.</S>
			<S sid ="130" ssid = "19">We tried inserting this heuristic before Sieve’s other rules (LexBegin), and also after Sieve’s other rules (LexEnd).</S>
			<S sid ="131" ssid = "20">4.3 Experimental Results.</S>
			<S sid ="132" ssid = "21">Table 2 presents results for Reconcile trained with and without lexical features and when adding a lexical heuristic with data drawn from same- domain texts to Sieve.</S>
			<S sid ="133" ssid = "22">The first row shows the results without the lexicalized features (from Table 1).</S>
			<S sid ="134" ssid = "23">All F scores for Reconcile with lexicalized features are significantly better than without these features based on the Paired Permutation test (Pesarin, 2001) with 3 We also computed κ on MUC4, but unfortunately the score and original data were lost.</S>
			<S sid ="135" ssid = "24">4 http://www.cs.utah.edu/nlp/reconcile/ p ≤ 0.05.</S>
			<S sid ="136" ssid = "25">MUC4 showed the largest gain for Reconcile, with the F score increasing from 69.5 to over 74.</S>
			<S sid ="137" ssid = "26">For most domains, adding the lexical features to Reconcile substantially increased precision with comparable levels of recall.</S>
			<S sid ="138" ssid = "27">The bottom half of Table 2 contains the results of adding a lexical heuristic to Sieve.</S>
			<S sid ="139" ssid = "28">The first row shows the default system with no lexical information.</S>
			<S sid ="140" ssid = "29">All F scores with the lexical heuristic are significantly better than without it.</S>
			<S sid ="141" ssid = "30">In Sieve’s high-precision coreference architecture, the lexical heuristic yields additional recall gains without sacrificing much precision.</S>
			<S sid ="142" ssid = "31">A C E 2 0 0 4 P R F Re co nci le 70.</S>
			<S sid ="143" ssid = "32">59 83.09 76.</S>
			<S sid ="144" ssid = "33">33 +L ex Lo ok up 71.</S>
			<S sid ="145" ssid = "34">32 82.93 76.</S>
			<S sid ="146" ssid = "35">69 +L ex Set s 71.</S>
			<S sid ="147" ssid = "36">44 83.45 76.</S>
			<S sid ="148" ssid = "37">98 Sie ve 90.</S>
			<S sid ="149" ssid = "38">09 74.23 81.</S>
			<S sid ="150" ssid = "39">39 +L ex Be gin 86.54 75.43 80.61 +L ex En d 87.00 75.45 80.82 Table 3: B3 results for baselines and lexicalized feature sets on the broad-coverage ACE 2004 data set.</S>
			<S sid ="151" ssid = "40">Table 3 shows the results for Reconcile and Sieve when training and testing on the ACE 2004 data.</S>
			<S sid ="152" ssid = "41">Here, we see little improvement from adding lexical information.</S>
			<S sid ="153" ssid = "42">For Reconcile, the small differences in F scores are not statistically significant.</S>
			<S sid ="154" ssid = "43">For Sieve, the unlexicalized system yields a significantly higher F score than when adding the lexical heuristic.</S>
			<S sid ="155" ssid = "44">These results support our hypothesis that lexicalized information can be beneficial for capturing domain-specific word associations, but may not be as helpful in a broad-coverage setting where the language covers a diverse set of topics.</S>
			<S sid ="156" ssid = "45">Table 4 shows a re-evaluation of the cross- domain experiments from Table 1 for Reconcile with the LexSet features added.</S>
			<S sid ="157" ssid = "46">The bottom half of the table shows cross-domain experiments for Sieve using the lexical heuristic at the end of its rule set (LexEnd).</S>
			<S sid ="158" ssid = "47">Results are presented using both the B3 metric and the MUC Score (Villain et al., 1995).</S>
			<S sid ="159" ssid = "48">Training and testing on the same domain always produced the highest recall scores for MUC 7, ProMed, and MUC4 when utilizing lexical features.</S>
			<S sid ="160" ssid = "49">In all cases, lexical features acquired from same-domain texts yield results that are either clearly the best or not significantly different from the best.</S>
			<S sid ="161" ssid = "50">Train Test --.--.</S>
			<S sid ="162" ssid = "51">Table 4: Cross-domain B3 and MUC results for Reconcile and Sieve with lexical features.</S>
			<S sid ="163" ssid = "52">Gray cells represent results that are not significantly different from the best results in the column at the 0.05 p-level.</S>
			<S sid ="164" ssid = "53">For MUC6 and MUC7, the highest F score results almost always come from training on same- domain texts, although in some cases these results are not significantly different from training on other domains.</S>
			<S sid ="165" ssid = "54">Lexical features can yield improvements when training on a different domain if there is overlap in the vocabulary across the domains.</S>
			<S sid ="166" ssid = "55">For the ProMed domain, the Sieve system performs significantly better, under both metrics, with same-domain lexical features than with lexical features acquired from a different domain.</S>
			<S sid ="167" ssid = "56">For Reconcile, there is not a significant difference in the F score for ProMed when training on ProMed, MUC4, or MUC7.</S>
			<S sid ="168" ssid = "57">In the MUC4 domain, using same-domain lexical information always produces the best F score, under both metrics and in both coreference systems.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "5">
			<S sid ="169" ssid = "1">We explored the use of lexical information for domain-specific coreference resolution using 4 domain-specific data sets and 2 coreference re- solvers.</S>
			<S sid ="170" ssid = "2">Lexicalized features consistently improved performance for all of the domains and in both coreference architectures.</S>
			<S sid ="171" ssid = "3">We see benefits from lexicalized features in cross-domain training, but the gains are often more substantial when utilizing same-domain lexical knowledge.</S>
			<S sid ="172" ssid = "4">In the future, we plan to explore additional types of lexical information to benefit domain-specific coreference resolution.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="173" ssid = "5">This material is based upon work supported by the National Science Foundation under Grant No.</S>
			<S sid ="174" ssid = "6">IIS1018314 and the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.</S>
			<S sid ="175" ssid = "7">FA875009-C-0172.</S>
			<S sid ="176" ssid = "8">Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the U.S. government.</S>
	</SECTION>
</PAPER>
