<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Coreference resolution systems rely heavily on string overlap (e.g., Google Inc. and Google), performing badly on mentions with very different words (opaque mentions) like Google and the search giant.</S>
		<S sid ="2" ssid = "2">Yet prior attempts to resolve opaque pairs using ontologies or distributional semantics hurt precision more than improved recall.</S>
		<S sid ="3" ssid = "3">We present a new unsupervised method for mining opaque pairs.</S>
		<S sid ="4" ssid = "4">Our intuition is to restrict distributional semantics to articles about the same event, thus promoting referential match.</S>
		<S sid ="5" ssid = "5">Using an English comparable corpus of tech news, we built a dictionary of opaque coreferent mentions (only 3% are in WordNet).</S>
		<S sid ="6" ssid = "6">Our dictionary can be integrated into any coreference system (it increases the performance of a state-of-the-art system by 1% F1 on all measures) and is easily extendable by using news aggregators.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Repetition is one of the most common coreferential devices in written text, making string-match features important to all coreference resolution systems.</S>
			<S sid ="8" ssid = "8">In fact, the scores achieved by just head match and a rudimentary form of pronominal resolution1 are not far from that of state-of-the-art systems (Recasens and Hovy, 2010).</S>
			<S sid ="9" ssid = "9">This suggests that opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate are a serious problem for modern systems: they comprise 65% of the non-pronominal 1 Closest NP with the same gender and number..</S>
			<S sid ="10" ssid = "10">errors made by the Stanford system on the CoNLL 2011 data.</S>
			<S sid ="11" ssid = "11">Solving this problem is critical for overcoming the recall gap of state-of-the-art systems (Haghighi and Klein, 2010; Stoyanov et al., 2009).</S>
			<S sid ="12" ssid = "12">Previous systems have turned either to ontologies (Ponzetto and Strube, 2006; Uryupina et al., 2011; Rahman and Ng, 2011) or distributional semantics (Yang and Su, 2007; Kobdani et al., 2011; Bansal and Klein, 2012) to help solve these errors.</S>
			<S sid ="13" ssid = "13">But neither semantic similarity nor hypernymy are the same as coreference: Microsoft and Google are distributionally similar but not coreferent; people is a hypernym of both voters and scientists, but the people can corefer with the voters, but is less likely to corefer with the scientists.</S>
			<S sid ="14" ssid = "14">Thus ontologies lead to precision problems, and to recall problems like missing NE descriptions (e.g., Apple and the iPhone maker) and metonymies (e.g., agreement and wording), while distributional systems lead to precision problems like coreferring Microsoft and the Mountain View giant because of their similar vector representation (release, software, update).</S>
			<S sid ="15" ssid = "15">We increase precision by drawing on the intuition that referents that are both similar and participate in the same event are likely to corefer.</S>
			<S sid ="16" ssid = "16">We restrict dis- tributional similarity to collections of articles that discuss the same event.</S>
			<S sid ="17" ssid = "17">In the following two documents on the Nexus One from different sources, we take the subjects of the identical verb release— Google and the Mountain View giant—as coreferent.</S>
			<S sid ="18" ssid = "18">Document 1: Google has released a software update.</S>
			<S sid ="19" ssid = "19">Document 2: The Mountain View giant released an update.</S>
			<S sid ="20" ssid = "20">Based on this idea, we introduce a new unsupervised method that uses verbs in comparable corpora 897 Proceedings of NAACLHLT 2013, pages 897–906, Atlanta, Georgia, 9–14 June 2013.</S>
			<S sid ="21" ssid = "21">Qc 2013 Association for Computational Linguistics as pivots for extracting the hard cases of coreference resolution, and build a dictionary of opaque coreferent mentions (i.e., the dictionary entries are pairs of mentions).</S>
			<S sid ="22" ssid = "22">This dictionary is then integrated into the Stanford coreference system (Lee et al., 2011), resulting in an average 1% improvement in the F1 score of all the evaluation measures.</S>
			<S sid ="23" ssid = "23">Our work points out the importance of context to decide whether a specific mention pair is coreferent.</S>
			<S sid ="24" ssid = "24">On the one hand, we need to know what semantic relations are potentially coreferent (e.g., content and video).</S>
			<S sid ="25" ssid = "25">On the other, we need to distinguish contexts that are compatible for coreference—(1) and (2-a)— from those that are not—(1) and (2-b).</S>
			<S sid ="26" ssid = "26">(1) Elemental helps those big media entities process content across a full slate of mobile devices.</S>
			<S sid ="27" ssid = "27">(2) a. Elemental provides the picks and shovels to b. make video work across multiple devices.</S>
			<S sid ="28" ssid = "28">Elemental is powering the video for HBO Go.</S>
			<S sid ="29" ssid = "29">Our dictionary of opaque coreferent pairs is our solution to the first problem, and we report on some preliminary work on context compatibility to address the second problem.</S>
	</SECTION>
	<SECTION title="Building a Dictionary for Coreference. " number = "2">
			<S sid ="30" ssid = "1">To build a dictionary of semantic relations that are appropriate for coreference we will use a cluster of documents about the same news event, which we call a story.</S>
			<S sid ="31" ssid = "2">Consider as an example the story Sprint blocks out vacation days for employees.</S>
			<S sid ="32" ssid = "3">We determine using tfidf the representative verbs for this story, the main actions and events of the story (e.g., block out).</S>
			<S sid ="33" ssid = "4">Since these verbs are representative of the story, different instances across documents in the cluster are likely to refer to the same events (Sprint blocks out.</S>
			<S sid ="34" ssid = "5">and the carrier blocks out.</S>
			<S sid ="35" ssid = "6">By the same logic, the subjects and objects of the verbs are also likely to be coreferent (Sprint and the carrier).</S>
			<S sid ="36" ssid = "7">2.1 Comparable corpus.</S>
			<S sid ="37" ssid = "8">To build our dictionary, we require a monolingual comparable corpus, containing clusters of documents from different sources that discuss the same story.</S>
			<S sid ="38" ssid = "9">To ensure likely coreference, the story must be the very same; documents that are merely clustered by (general) topic do not suffice.</S>
			<S sid ="39" ssid = "10">The corpus does not need to be parallel in the sense that documents in the same cluster do not need to be sentence aligned.</S>
			<S sid ="40" ssid = "11">We used Techmeme,2 a news aggregator for technology news, to construct a comparable corpus.</S>
			<S sid ="41" ssid = "12">Its website lists the major tech stories, each with links to several articles from different sources.</S>
			<S sid ="42" ssid = "13">We used the Readability API3 to download and extract the article text for each document.</S>
			<S sid ="43" ssid = "14">We scraped two years worth of data from Techmeme and only took stories containing at least 5 documents.</S>
			<S sid ="44" ssid = "15">Our corpus contains approximately 160 million words, 25k stories, and 375k documents.</S>
			<S sid ="45" ssid = "16">Using a corpus from Techmeme means that our current coreference dictionary is focused on the technological domain.</S>
			<S sid ="46" ssid = "17">Our method can be easily extended to other domains, however, since getting comparable corpora is relatively simple from the many similar news aggregator sites.</S>
			<S sid ="47" ssid = "18">2.2 Extraction.</S>
			<S sid ="48" ssid = "19">After building our corpus, we used Stanford’s CoreNLP tools4 to tokenize the text and annotate it with POS tags and named entity types.</S>
			<S sid ="49" ssid = "20">We parsed the text using the MaltParser 1.7, a linear time dependency parser (Nivre et al., 2004).5 We then extracted the representative verbs of each story by ranking the verbs in each story according to their tfidf scores.</S>
			<S sid ="50" ssid = "21">We took the top ten to be the representative set.</S>
			<S sid ="51" ssid = "22">For each of these verbs, we clustered together its subjects and objects (separately) across instances of the verb in the document cluster, excluding pronouns and NPs headed by the same noun.</S>
			<S sid ="52" ssid = "23">For example, suppose that crawl is a representative verb and that in one document we have Google crawls web pages and The search giant crawls sites in another document.</S>
			<S sid ="53" ssid = "24">We will create the clusters {Google, the search giant} and {web pages, sites}.</S>
			<S sid ="54" ssid = "25">When detecting representative verbs, we kept phrasal verbs as a unit (e.g., give up) and excluded auxiliary and copular verbs,6 light verbs,7 and report 2 http://www.techmeme.com 3 http://www.readability.com/developers/api 4 http://nlp.stanford.edu/software/corenlp.shtml 5 http://www.maltparser.org 6 Auxiliary and copular verbs include appear, be, become, do, have, seem.</S>
			<S sid ="55" ssid = "26">7 Light verbs include do, get, give, go, have, keep, make, put, set, take.</S>
			<S sid ="56" ssid = "27">verbs,8 as they are rarely representative of a story and tend to add noise to our dictionary.</S>
			<S sid ="57" ssid = "28">To increase recall, we also considered the synonyms from Word- Net and nominalizations from NomBank of the representative verbs, thus clustering together the subjects and objects of any synonym as well as the arguments of nominalizations.9 We used syntactic relations instead of semantic roles because the Malt- Parser is faster than any SRL system, but we checked for frequent syntactic structures in which the agent and patient are inverted, such as passive and ergative constructions.10 From each cluster of subject or object mentions, we generated all pairs of mentions.</S>
			<S sid ="58" ssid = "29">This forms the initial version of our dictionary.</S>
			<S sid ="59" ssid = "30">The next sections describe how we filter and generalize these pairs.</S>
			<S sid ="60" ssid = "31">2.3 Filtering.</S>
			<S sid ="61" ssid = "32">We manually analyzed 200 random pairs and classified them into coreference and spurious relations.</S>
			<S sid ="62" ssid = "33">The spurious relations were caused by errors due to the parser, the text extraction, and violations of our algorithm assumption (i.e., the representative verb does not refer to a unique event).</S>
			<S sid ="63" ssid = "34">We employed a filtering strategy to improve the precision of the dictionary.</S>
			<S sid ="64" ssid = "35">We used a total of thirteen simple rules, which are shown in Table 1.</S>
			<S sid ="65" ssid = "36">For instance, we sometimes get the same verb with non-coreferent arguments, especially in tech news that compare companies or products.</S>
			<S sid ="66" ssid = "37">In these cases, NEs are often used, and so we can get rid of a large number of errors by automatically removing pairs in which both mentions are NEs (e.g., Google and Samsung).</S>
			<S sid ="67" ssid = "38">Before filtering, 53% of all relations were good coreference relations versus 47% spurious ones.</S>
			<S sid ="68" ssid = "39">Of the relations that remained after filtering, 74% were 8 Report verbs include argue, claim, say, suggest, tell, etc..</S>
			<S sid ="69" ssid = "40">9 As a general rule, we extract possessive phrases as subjects (e.g. Samsung’s plan) and of -phrases as objects (e.g. development of the new logo).</S>
			<S sid ="70" ssid = "41">10 We can easily detect passive subjects (i-b) as they have their own dependency label, and ergative subjects (ii-b) using a list of ergative verbs extracted from Levin (1993).</S>
			<S sid ="71" ssid = "42">Both mentions are NEs Both mentions appear in the same document Object of a negated verb Enumeration or list environment Sentence is ill-formed Number NE Temporal NE Quantifying noun Coordinated Verb is preceded by a determiner or an adjective Head is not nominal Sentence length ≥ 100 Mention length ≥ 70% of sentence length Table 1: Filters to improve the dictionary precision.</S>
			<S sid ="72" ssid = "43">Unless otherwise noted, the filter was applied if either mention in the relation satisfied the condition.</S>
			<S sid ="73" ssid = "44">coreferent and only 26% were spurious.</S>
			<S sid ="74" ssid = "45">In total, about half of the dictionary relations were removed in the filtering process, resulting in a total of 128,492 coreferent pairs.</S>
			<S sid ="75" ssid = "46">2.4 Generalization.</S>
			<S sid ="76" ssid = "47">The final step of generating our dictionary is to process the opaque mention pairs so that they generalize better.</S>
			<S sid ="77" ssid = "48">We strip mentions of any determiners, relative clauses, and -ing and -ed clauses.</S>
			<S sid ="78" ssid = "49">However, we retain adjectives and prepositional modifiers because they are sometimes necessary for coreference to hold (e.g., online piracy and distribution of pirated material).</S>
			<S sid ="79" ssid = "50">We also generalize NEs to their types so that our dictionary entries can function as templates (e.g., Cook’s departure becomes &lt;person&gt;’s departure), but we keep NE tokens that are in the head position as these are pairs containing world knowledge (e.g., iPad and slate).</S>
			<S sid ="80" ssid = "51">Finally, we replace all tokens with their lemmas.</S>
			<S sid ="81" ssid = "52">Table 2 shows a snapshot of the dictionary.</S>
			<S sid ="82" ssid = "53">2.5 Semantics of coreference.</S>
			<S sid ="83" ssid = "54">From manually classifying a sample of 200 dictionary pairs (e.g., Table 2), we find that our dictionary includes many synonymy (e.g., IPO and offering) and hypernymy relations (e.g., phone and device), which are the relations that are typically extracted from ontologies for coreference resolution.</S>
			<S sid ="84" ssid = "55">However, not all synonyms and hypernyms are valid for coreference (recall the voters-people vs. scientists-people example in the introduction), so our dic Mention 1 Mention 2 offering IPO user consumer phone device Apple company hardware key digital lock iPad slate content photo bug issue password login information Google search giant site company filing complaint company government TouchPad tablet medical record medical file version handset information credit card government chairman app software Android platform the leadership change &lt;person&gt;’s departure change update Table 2: Coreference relations in our dictionary.</S>
			<S sid ="85" ssid = "56">tionary only includes the ones that are relevant for coreference (e.g., update and change).</S>
			<S sid ="86" ssid = "57">Furthermore, only 3% of our 128,492 opaque pairs are related in WordNet, confirming that our method is introducing a large number of new semantic relations.</S>
			<S sid ="87" ssid = "58">We also discover other semantic relations that are relevant for coreference, such as various metonymy relations like mentioning the part for the whole.</S>
			<S sid ="88" ssid = "59">Again though, we can use some part-whole relations coreferentially (e.g., car and engine) but not others (e.g., car and window).</S>
			<S sid ="89" ssid = "60">Our dictionary includes part-whole relations that have been observed as coreferent at least once (e.g., company and site).</S>
			<S sid ="90" ssid = "61">We also extract world-knowledge descriptions for NEs (e.g., Google and the Internet giant).</S>
	</SECTION>
	<SECTION title="Integration into a Coreference System. " number = "3">
			<S sid ="91" ssid = "1">We next integrated our dictionary into an existing coreference resolution system to see if it improves resolution.</S>
			<S sid ="92" ssid = "2">Sieve number Sieve name 1 Discourse processing.</S>
			<S sid ="93" ssid = "3">2 Exact string match.</S>
			<S sid ="94" ssid = "4">3 Relaxed string match.</S>
	</SECTION>
	<SECTION title="Precise constructs. " number = "4">
			<S sid ="95" ssid = "1">5–7 Strict head match 8 Proper head noun match.</S>
			<S sid ="96" ssid = "2">9 Relaxed head match.</S>
			<S sid ="97" ssid = "3">10 Pronoun match.</S>
			<S sid ="98" ssid = "4">Table 3: Rules of the baseline system.</S>
			<S sid ="99" ssid = "5">and was also part of the highest-scoring system in the CoNLL2012 Shared Task (Fernandes et al., 2012).</S>
			<S sid ="100" ssid = "6">It is a rule-based system that includes a total of ten rules (or “sieves”) for entity coreference, shown in Table 3.</S>
			<S sid ="101" ssid = "7">The sieves are applied from highest to lowest precision, each rule extending entities (i.e., mention clusters) built by the previous tiers, but never modifying links previously made.</S>
			<S sid ="102" ssid = "8">The majority of the sieves rely on string overlap.11 The highly modular architecture made it easy for us to integrate additional sieves using our dictionary to increase recall.</S>
			<S sid ="103" ssid = "9">3.2 Dictionary sieves.</S>
			<S sid ="104" ssid = "10">We propose four new sieves, each one using a different granularity level from our dictionary, with each consecutive sieve using higher precision relations than the previous one.</S>
			<S sid ="105" ssid = "11">The Dict 1 sieve uses only the heads of mentions in each relation (e.g., devices).</S>
			<S sid ="106" ssid = "12">Dict 2 uses the heads and one premodifier, if it exists (e.g., iOS devices).</S>
			<S sid ="107" ssid = "13">Dict 3 uses the heads and up to two premodifiers (e.g., new iOS devices).</S>
			<S sid ="108" ssid = "14">Dict 4 uses the full mentions, including any postmodifiers (e.g., new iOS devices for businesses).</S>
			<S sid ="109" ssid = "15">We take advantage of frequency counts to get rid of low-precision coreference pairs and only keep (i) pairs that have been seen more than 75 times (Dict 1) or 15 times (Dict 2, Dict 3, Dict 4); and (ii) pairs with a frequency count larger than 8 (Dict 1) or 2 (Dict 2, Dict 3, Dict 4) and a normalized PMI score larger than 0.18.</S>
			<S sid ="110" ssid = "16">We use the normalized PMI score (Bouma, 2009) as a measure of association between the mentions mi and mj of a 3.1 Stanford coreference resolution system.</S>
			<S sid ="111" ssid = "17">11 Exceptions: sieve 1 links first-person pronouns inside a. Our baseline is the Stanford coreference resolution system (Lee et al., 2011) which was the highest- scoring system in the CoNLL2011 Shared Task, quotation with the speaker; sieve 4 links mention pairs that appear in an appositive, copular, acronym, etc., construction; sieve 10 implements generic pronominal coreference resolution.</S>
			<S sid ="112" ssid = "18">p(mi )p(mj ) ) / − ln p(mi, mj ) These thresholds were set on the development set.</S>
			<S sid ="113" ssid = "19">Since the different coreference rules in the Stanford system are arranged in decreasing order of precision, we start by applying the sieve that uses the highest-precision relations in the dictionary (Dict 4), followed by Dict 3, Dict 2, and Dict 1.</S>
			<S sid ="114" ssid = "20">We add these new sieves right before the last sieve, as the pronominal sieve can perform better if opaque mentions have been successfully linked.</S>
			<S sid ="115" ssid = "21">The current sieves only use the dictionary for linking singular mentions, as the experiments on the dev showed that plural mentions brought too much noise.</S>
			<S sid ="116" ssid = "22">For any mention pair under analysis, each sieve checks whether it is supported by the dictionary as well as whether basic constraints are satisfied, such as number, animacy and NE-type agreement, and NE–common noun order (not the opposite).</S>
			<S sid ="117" ssid = "23">4 Experiments.</S>
			<S sid ="118" ssid = "24">4.1 Data.</S>
			<S sid ="119" ssid = "25">Although our dictionary creation technology can apply across domains, our current coreference dictionary is focused on the technical domain, so we created a coreference labeled corpus in this domain for evaluation.</S>
			<S sid ="120" ssid = "26">We extracted new data from Techmeme (different from that used to extract the dictionary) to create a development and a test set.</S>
			<S sid ="121" ssid = "27">It is important to note that we do not need comparable data at this stage.</S>
			<S sid ="122" ssid = "28">A massive comparable corpus is only needed for mining the coreference dictionary (Section 2); once it is built, it can be used for solving coreference within and across documents.</S>
			<S sid ="123" ssid = "29">The annotation was performed by two experts, using the Callisto annotation tool.</S>
			<S sid ="124" ssid = "30">The development and test sets were annotated with coreference relations following the OntoNotes guidelines (Pradhan et al., 2007).</S>
			<S sid ="125" ssid = "31">We annotated full NPs (with all modifiers), excluding appositive phrases and predicate nominals.</S>
			<S sid ="126" ssid = "32">Only premodifiers that were proper nouns or possessive phrases were annotated.</S>
			<S sid ="127" ssid = "33">We extended the OntoNotes guidelines by also annotating singletons.</S>
			<S sid ="128" ssid = "34">Table 4 shows the dataset statistics.</S>
			<S sid ="129" ssid = "35">Dev 4 27 7837 1360 2279 Test 24 24 8547 1341 2452 Table 4: Dataset statistics: development (dev) and test.</S>
			<S sid ="130" ssid = "36">4.2 Evaluation measures.</S>
			<S sid ="131" ssid = "37">We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard.</S>
			<S sid ="132" ssid = "38">We used the scorer of the CoNLL2011 Shared Task (Pradhan et al., 2011).</S>
			<S sid ="133" ssid = "39">• MUC (Vilain et al., 1995).</S>
			<S sid ="134" ssid = "40">Link-based metric that measures how many links the true and system partitions have in common.</S>
			<S sid ="135" ssid = "41">3 • B (Bagga and Baldwin, 1998).</S>
			<S sid ="136" ssid = "42">Mention-based metric that measures the proportion of mention overlap between gold and predicted entities.</S>
			<S sid ="137" ssid = "43">• CEAF-φ3 (Luo, 2005).</S>
			<S sid ="138" ssid = "44">Mention-based metricthat, unlike B3, enforces a one-to-one align ment between gold and predicted entities.• CEAF-φ4 (Luo, 2005).</S>
			<S sid ="139" ssid = "45">The entity-based ver sion of the above metric.• BLANC (Recasens and Hovy, 2011).</S>
			<S sid ="140" ssid = "46">Link based metric that considers both coreference and non-coreference links.</S>
			<S sid ="141" ssid = "47">• CoNLL (Denis and Baldridge, 2009).</S>
			<S sid ="142" ssid = "48">Average of MUC, B3 and CEAF-φ4.</S>
			<S sid ="143" ssid = "49">It was the official metric of the CoNLL2011 Shared Task.</S>
			<S sid ="144" ssid = "50">4.3 Results.</S>
			<S sid ="145" ssid = "51">We always start from the baseline, which corresponds to the Stanford system with the sieves listed in Table 3.</S>
			<S sid ="146" ssid = "52">This is the set of sieves that won the CoNLL2011 Shared Task (Pradhan et al., 2011), and they exclude WordNet.</S>
			<S sid ="147" ssid = "53">Table 5 shows the incremental scores, on the development set, for the four sieves that use the dictionary, corresponding to the different granularity levels, from the highest precision one (Dict 4) to the lowest one (Dict 1).</S>
			<S sid ="148" ssid = "54">The largest improvement is achieved by Dict 4 and Dict 3, as they improve recall (R) without hurting precision (P).</S>
			<S sid ="149" ssid = "55">R is equivalent to P for CEAF-φ4, and vice versa.</S>
			<S sid ="150" ssid = "56">The other two sieves increase R further, especially Dict 1, but also decrease P, although the trade-off for the F-score (F1) is still positive.</S>
			<S sid ="151" ssid = "57">It is the best score, with the exception of B3.</S>
			<S sid ="152" ssid = "58">System R P F1 R P F1 R / P / F1 R P F1 R P B F1 Baseline 55.9 72.8 63.3 74.1 89.8 81.2 74.6 85.2 73.6 79.0 66.6 87.1 72.6 74.5 +Dict 4 57.0 72.8 63.9 75.1 89.4 81.6 75.3 85.2 74.3 79.4 68.2 87.3 74.2 75.0 +Dict 3 57.6 72.8 64.3 75.4 89.3 81.7 75.5 85.1 74.6 79.5 68.4 87.2 74.4 75.2 +Dict 2 57.6 72.5 64.2 75.4 89.1 81.7 75.4 85.0 74.6 79.5 68.4 87.0 74.3 75.1 +Dict 1 58.4 71.9 64.5 75.7 88.5 81.6 75.5 84.6 75.1 79.6 68.6 86.6 74.4 75.2 Table 5: Incremental results for the four sieves using our dictionary on the development set.</S>
			<S sid ="153" ssid = "59">Baseline is the Stanford system without the WordNet sieves.</S>
			<S sid ="154" ssid = "60">Scores are on gold mentions.</S>
			<S sid ="155" ssid = "61">MUC B3 CEAF-φ3 CEAF-φ4 BLANC CoNLL System R P F1 R P F1 R / P / F1 R P F1 R P B F1 Baseline 62.4 78.2 69.4 73.7 89.5 80.8 75.1 86.2 73.8 79.5 71.4 88.6 77.3 76.6 w/ WN 63.5 75.3 68.9 74.2 87.5 80.3 74.1 83.7 74.1 78.6 71.8 87.3 77.3 75.9 w/ Dict 64.7* 77.6* 70.6* 75.7* 88.5* 81.6* 76.5* 85.3* 75.0* 79.9* 74.6* 88.6 79.9* 77.3* w/ Dict + 64.8* 77.8* 70.7* 75.7* 88.6* 81.7* 76.5* 85.5* 75.1* 80.0* 74.6* 88.7 79.9* 77.5* Context Table 6: Performance on the test set.</S>
			<S sid ="156" ssid = "62">Scores are on gold mentions.</S>
			<S sid ="157" ssid = "63">Stars indicate a statistically significant difference with respect to the baseline.</S>
			<S sid ="158" ssid = "64">Table 6 reports the scores on the test set and compares the scores obtained by adding the WordNet sieves to the baseline (w/ WN) with those obtained by adding the dictionary sieves (w/ Dict).</S>
			<S sid ="159" ssid = "65">Whereas adding WordNet only brings a small improvement in R that is much lower than the loss in P, the dictionary sieves succeed in increasing R by a larger amount and at a smaller cost to P, resulting in a significant improvement in F1: 1.2 points according to MUC, 0.8 points according to B3, 1.4 points according to CEAF-φ3, 0.4 points according to CEAF-φ4,2.6 points according to BLANC, and 0.7 points ac cording to CoNLL.</S>
			<S sid ="160" ssid = "66">Section 5.2 presents the last line (w/ Dict + Context).</S>
	</SECTION>
	<SECTION title="Discussion. " number = "5">
			<S sid ="161" ssid = "1">5.1 Error analysis.</S>
			<S sid ="162" ssid = "2">Thanks to the dictionary, the coreference system improves the baseline by establishing coreference links between the bolded mentions in (3) and (4).</S>
			<S sid ="163" ssid = "3">(3) With Groupon Inc.’s stock down by half from its IPO price and the company heading into its first earnings report since an accounting blowup [...]</S>
			<S sid ="164" ssid = "4">outlining opportunity ahead and the promise of new products for the daily-deals company.</S>
			<S sid ="165" ssid = "5">(4) Thompson revealed the diagnosis as evidence arose that seemed to contradict his story about why he was not responsible for a degree listed on his resume that he does not have, the newspaper reports, citing anonymous sources familiar with the situation [...]</S>
			<S sid ="166" ssid = "6">a Yahoo board committee appointed to investigate the matter.</S>
			<S sid ="167" ssid = "7">The first case requires world knowledge and the second case, semantic knowledge.</S>
			<S sid ="168" ssid = "8">We manually analyzed 40 false positive errors caused by the dictionary sieves.</S>
			<S sid ="169" ssid = "9">Only a small number of them were due to noise in the dictionary.</S>
			<S sid ="170" ssid = "10">The majority of errors were due to the discourse context: the two mentions could be coreferent, but not in the given context.</S>
			<S sid ="171" ssid = "11">For example, Apple and company are potentially coreferent—which is successfully captured by our dictionary—and while they are coreferent in (5), they are not in (6).12 (5) It will only get better as Apple will be updating it with iOS6, an operating system that the company will likely be showing off this summer.</S>
			<S sid ="172" ssid = "12">(6) Since Apple reinvented the segment, Microsoft is the latest entrant into the tablet market, banking on its Windows 8 products to bridge the gap between PCs and tablets.</S>
			<S sid ="173" ssid = "13">The company showed off Windows 8 last September.</S>
			<S sid ="174" ssid = "14">12 Examples in this section show gold coreference relations in bold and incorrectly predicted coreferent mentions in italics.</S>
			<S sid ="175" ssid = "15">the opaque mention pair is included in the coreference dictionary, but we need a method for taking the surrounding context into account.</S>
			<S sid ="176" ssid = "16">In the next section we present our preliminary work in this direction.</S>
			<S sid ="177" ssid = "17">5.2 Context fit.</S>
			<S sid ="178" ssid = "18">To help the coreference system choose the right antecedent in examples like (6), we exploit the fact that the company is closely followed by Windows 8, which is a clue for selecting Microsoft instead of Apple as the antecedent.</S>
			<S sid ="179" ssid = "19">We devise a contextual constraint that rules out a mention pair if the contexts are incompatible.</S>
			<S sid ="180" ssid = "20">To check for context compatibility, we borrow the idea of topic signatures from Lin and Hovy (2000) and that Agirre et al.</S>
			<S sid ="181" ssid = "21">(2001) used for Word Sense Disambiguation.</S>
			<S sid ="182" ssid = "22">Instead of identifying the keywords of a topic, we find the NEs that tend to co-occur with another NE.</S>
			<S sid ="183" ssid = "23">For example, the signature for Apple should include terms like iPhone, MacBook, iOS, Steve Jobs, etc. This is what we call the NE signature for Apple.</S>
			<S sid ="184" ssid = "24">To construct NE signatures, we first compute the log-likelihood ratio (LLR) statistic between NEs in our corpus (the same one used to build the dictionary).</S>
			<S sid ="185" ssid = "25">Then, the signature for a NE, w, is the list of k other NEs that have the highest LLR with w. The LLR between two NEs, w1 and w2, is −2 ln L(H1 ) , where H1 is the hypothesis that P (w1 ∈ sent|w2 ∈ sent) = P (w1 ∈ sent|w2 ∈/ sent), H2 is the hypothesis that P (w1 ∈ sent|w2 ∈ sent) = P (w1 ∈ sent|w2 ∈/ sent), and L(·) is the likelihood.</S>
			<S sid ="186" ssid = "26">We assume a binomial distribution for the likelihood.</S>
			<S sid ="187" ssid = "27">Once we have NE signatures, we determine the context fit as follows.</S>
			<S sid ="188" ssid = "28">When the system compares a NE antecedent with a (non-NE) anaphor, we check whether any NEs in the anaphor’s sentence are in the antecedent’s signature.</S>
			<S sid ="189" ssid = "29">We also check whether the antecedent is in the signature list of any NE’s in the anaphor’s sentence.</S>
			<S sid ="190" ssid = "30">If neither of these is true, we do not allow the system to link the antecedent and the anaphor.</S>
			<S sid ="191" ssid = "31">In (6), Apple is not linked with the company because it is not in Windows’ signature, and Windows is not in Apple’s signature either (but Microsoft is in Windows’ signature).</S>
			<S sid ="192" ssid = "32">The last two lines in Table 6 compare the scores without using this contextual feature (w/ Dict) with for context compatibility leads to a small but positive improvement, taking the final improvement of the dictionary sieves to be about 1 percentage point above the baseline according to all six evaluation measures.</S>
			<S sid ="193" ssid = "33">We leave as future work to test this idea on a larger test set and refine it further so as to address more challenging cases where comparing NEs is not enough, like in (7).</S>
			<S sid ="194" ssid = "34">(7) Snapchat will notify users [...]</S>
			<S sid ="195" ssid = "35">The program is available for free in Apple’s App Store [...]</S>
			<S sid ="196" ssid = "36">While the company “attempts to delete image data as soon as possible after the message is transmitted,” it cannot guarantee messages will always be deleted.</S>
			<S sid ="197" ssid = "37">To resolve (7), it would be helpful to know that Snapchat is a picture messaging platform, as the context mentions image data and messages.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "6">
			<S sid ="198" ssid = "1">Existing ontologies are not optimal for solving opaque coreferent mentions because of both a precision and a recall problem (Lee et al., 2011; Uryupina et al., 2011).</S>
			<S sid ="199" ssid = "2">On the other hand, using data-driven methods such as distributional semantics for coreference resolution suffers especially from a precision problem (Ng, 2007).</S>
			<S sid ="200" ssid = "3">Our work combines ideas from distributional semantics and paraphrase acquisition methods in order to efficiently use contextual information to extract coreference relations.</S>
			<S sid ="201" ssid = "4">The main idea that we borrow from paraphrase acquisition is the use of monolingual (non-parallel) comparable corpora, which have been exploited to extract both sentence-level (Barzilay and McKeown, 2001) and sub-sentential-level paraphrases (Shinyama and Sekine, 2003; Wang and CallisonBurch, 2011).</S>
			<S sid ="202" ssid = "5">To ensure that the NPs are coreferent, we limit the meaning of comparable corpora to collections of documents that report on the very same story, as opposed to collections of documents that are about the same (general) topic.</S>
			<S sid ="203" ssid = "6">However, the distinguishing factor is that while most paraphrasing studies, including Lin and Pantel (2001), use NEs— or nouns in general—as pivots to learn paraphrases of their surrounding context, we use verbs as pivots to learn coreference relations at the NP level.</S>
			<S sid ="204" ssid = "7">There are many similarities between paraphrase and coreference, and our work is most similar to that by Wang and CallisonBurch (2011).</S>
			<S sid ="205" ssid = "8">However, some paraphrases that might not be considered to be valid (e.g., under $200 and around $200) can be acceptable coreference relations.</S>
			<S sid ="206" ssid = "9">Unlike Wang and CallisonBurch (2011), we do not work on document pairs but on sets of at least five (comparable) documents, and we do not require sentence alignment, but just verb alignment.</S>
			<S sid ="207" ssid = "10">Another source of inspiration is the work by Bean and Riloff (2004).</S>
			<S sid ="208" ssid = "11">They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.</S>
			<S sid ="209" ssid = "12">However, they use a very small corpus (two domains) and do not aim to build a dictionary.</S>
			<S sid ="210" ssid = "13">The idea of creating a repository of extracted concept-instance relations appears in Fleischman et al.</S>
			<S sid ="211" ssid = "14">(2003), but restricted to person-role pairs, e.g. Yasser Arafat and leader.</S>
			<S sid ="212" ssid = "15">Although it was originally designed for answering who-is questions, Daume´ III and Marcu (2005) successfully used it for coreference resolution.</S>
			<S sid ="213" ssid = "16">The coreference relations that we extract might overlap but go beyond those detected by Bansal and Klein (2012)’s Web-based features.</S>
			<S sid ="214" ssid = "17">First, they focus on NP headwords, while we extract full NPs, including multi-word mentions.</S>
			<S sid ="215" ssid = "18">Second, the fact that they use the Google n-gram corpus means that the two headwords must appear at most four words apart, thus ruling out coreferent mentions that can only appear far from each other.</S>
			<S sid ="216" ssid = "19">Finally, while their extraction patterns focus on synonymy and hypernymy relations, we discover other types of semantic relations that are relevant for coreference (Section 2.5).</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "7">
			<S sid ="217" ssid = "1">We have pointed out an important problem with current coreference resolution systems: their heavy reliance on string overlap.</S>
			<S sid ="218" ssid = "2">Pronouns aside, opaque mentions account for 65% of the errors made by state-of-the-art systems.</S>
			<S sid ="219" ssid = "3">To improve coreference scores beyond 6070%, we therefore need to make better use of semantic and world knowledge to deal with non-identical-string coreference.</S>
			<S sid ="220" ssid = "4">But, as we have also shown, coreference is not the same as semantic similarity or hypernymy.</S>
			<S sid ="221" ssid = "5">Only certain semantic relations in certain contexts are good cues for coreference.</S>
			<S sid ="222" ssid = "6">We therefore need semantic resources specifically targeted at coreference.</S>
			<S sid ="223" ssid = "7">We proposed a new solution for detecting opaque mention pairs: restricting distributional similarity to a comparable corpus of articles about the very same story, thus ensuring that similar mentions will also likely be coreferent.</S>
			<S sid ="224" ssid = "8">We used this corpus to build a dictionary focused on coreference, and successfully extracted the specific semantic and world knowledge relevant for coreference.</S>
			<S sid ="225" ssid = "9">The resulting dictionary can be added on top of any coreference system to increase recall at a minimum cost to precision.</S>
			<S sid ="226" ssid = "10">Integrated into the Stanford coreference resolution system, which won the CoNLL2011 shared task, the F-score increases about 1 percentage point according to all of the six evaluation measures.</S>
			<S sid ="227" ssid = "11">The dictionary and NE signatures are available on the Web.13 We showed that apart from the need for extracting coreference-specific semantic and world knowledge, we need to take into account the context surrounding the mentions.</S>
			<S sid ="228" ssid = "12">The results from our preliminary work for identifying incompatible contexts is promising.</S>
			<S sid ="229" ssid = "13">Our unsupervised method for extracting opaque coreference relations can be easily extended to other domains by using online news aggregators, and trained on more data to build a more comprehensive dictionary that can increase recall even further.</S>
			<S sid ="230" ssid = "14">We integrated the dictionary into a rule-based coreference system, but it remains for future work to integrate it into a learning-based architecture, where the system can combine the dictionary features with other features.</S>
			<S sid ="231" ssid = "15">This can also make it easier to include contextual features that take into account how well a dictionary pair fits in a specific context.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="232" ssid = "16">We would like to thank the members of the Stanford NLP Group, Valentin Spitkovsky, and Ed Hovy for valuable comments at various stages of the project.</S>
			<S sid ="233" ssid = "17">The first author was supported by a Beatriu de Pino´ s postdoctoral scholarship (2010 BP-A 00149) from Generalitat de Catalunya.</S>
			<S sid ="234" ssid = "18">We also gratefully acknowledge the support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.</S>
			<S sid ="235" ssid = "19">FA875009-C-0181.</S>
			<S sid ="236" ssid = "20">13 http://nlp.stanford.edu/pubs/coref-dictionary.zip</S>
	</SECTION>
</PAPER>
