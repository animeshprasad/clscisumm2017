<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Semantic information is a very important factor in coreference resolution.</S>
		<S sid ="2" ssid = "2">The combination of large corpora and ‘deep’ analysis procedures has made it possible to acquire a range of semantic information and apply it to this task.</S>
		<S sid ="3" ssid = "3">In this paper, we generate two statistically-based semantic features from a large corpus and measure their influence on pronoun coreference.</S>
		<S sid ="4" ssid = "4">One is contextual compatibility, which decides if the antecedent can be used in the anaphor’s context; the other is role pair, which decides if the actions asserted of the antecedent and the anaphor are likely to apply to the same entity.</S>
		<S sid ="5" ssid = "5">We apply a semantic labeling system and a baseline coreference system to a large corpus to generate semantic patterns and convert them into features in a MaxEnt model.</S>
		<S sid ="6" ssid = "6">These features produce an absolute gain of 1.5% to 1.7% in resolution accuracy (a 6% reduction in errors).</S>
		<S sid ="7" ssid = "7">To understand the limitations of these features, we also extract patterns from the test corpus, use these patterns to train a coreference model, and examine some of the cases where coreference still fails.</S>
		<S sid ="8" ssid = "8">We also compare the performance of patterns extracted from semantic role labeling and syntax.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">Coreference resolution is the task of determining whether two phrases refer to the same entity.</S>
			<S sid ="10" ssid = "10">Coreference is critical to most NLP tasks, yet even the sub-problem of pronoun coreference remains very challenging.</S>
			<S sid ="11" ssid = "11">In principle, we need several types of information to identify the right antecedent.</S>
			<S sid ="12" ssid = "12">First, number and gender agreement constraints can narrow the candidate set.</S>
			<S sid ="13" ssid = "13">If multiple candidates remain, we would next use some sequence or syntactic features, like position, word, word salience and discourse focus.</S>
			<S sid ="14" ssid = "14">For example, whether an antecedent is in subject position might be helpful because the subject is more likely to be referred to; or an entity that has been referred to repeatedly is more likely to be referred to again.</S>
			<S sid ="15" ssid = "15">However, these features do not suffice to pick the correct antecedent, and sometimes similar syntactic structures might have quite different coreference solutions.</S>
			<S sid ="16" ssid = "16">For example, for the following two sentences: (1) The terrorist shot a 13-year-old boy; he was arrested after the attack.</S>
			<S sid ="17" ssid = "17">(2) The terrorist shot a 13-year-old boy; he was fatally wounded in the attack.</S>
			<S sid ="18" ssid = "18">it is likely that “he” refers to “terrorist” in (1) and “boy” in (2).</S>
			<S sid ="19" ssid = "19">However, we cannot get the right antecedent using the features we mentioned above because the examples share the same antecedent words and syntactic structure.</S>
			<S sid ="20" ssid = "20">People can still resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot.</S>
			<S sid ="21" ssid = "21">In such cases, semantic constraints and preferences are required for correct coreference resolution.</S>
			<S sid ="22" ssid = "22">Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution.</S>
			<S sid ="23" ssid = "23">Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.</S>
			<S sid ="24" ssid = "24">However, this task is difficult because it requires the acquisition of a large amount of semantic information.</S>
			<S sid ="25" ssid = "25">Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference.</S>
			<S sid ="26" ssid = "26">Kehler et al.</S>
			<S sid ="27" ssid = "27">(2004) reported that such information did not produce apparent improvement in overall pronoun resolution.</S>
			<S sid ="28" ssid = "28">In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference resolution can benefit from such knowledge, which is automatically extracted from a large corpus.</S>
			<S sid ="29" ssid = "29">We studied two features: the contextual compatibility feature which has been demonstrated to work at the syntactic level by previous work; and the role pair feature, which has not previously been applied to general domain pronoun co-reference.</S>
			<S sid ="30" ssid = "30">In addition, to obtain a rough upper bound on the benefits of our approach and understand its limitations, we conducted a second experiment in which the semantic knowledge is extracted from the evaluation corpus.</S>
			<S sid ="31" ssid = "31">We will use the term mention to describe an individual referring phrase.</S>
			<S sid ="32" ssid = "32">For most studies of coreference, mentions are noun phrases and may be headed by a name, a common noun, or a pronoun.</S>
			<S sid ="33" ssid = "33">We will use the term entity to refer to a set of coreferential mentions.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="34" ssid = "1">Contextual compatibility features have long been studied for pronoun coreference: Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution.</S>
			<S sid ="35" ssid = "2">It determined the preference of candidates based on predicate-argument frequencies.</S>
			<S sid ="36" ssid = "3">Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.</S>
			<S sid ="37" ssid = "4">They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.</S>
			<S sid ="38" ssid = "5">They got substantial gains on articles in two specific domains, terrorism and natural disasters.</S>
			<S sid ="39" ssid = "6">Yang et al.</S>
			<S sid ="40" ssid = "7">(2005) use statistically-based semantic compatibility information to improve pronoun resolution.</S>
			<S sid ="41" ssid = "8">They use corpus-based and web-based extraction strategies, and their work shows that statistically-based semantic compatibility information can improve coreference resolution.</S>
			<S sid ="42" ssid = "9">In contrast, Kehler et al.</S>
			<S sid ="43" ssid = "10">(2004) claimed that the contextual compatibility feature does not help much for pronoun coreference: existing learning- based approaches already performed well; such statistics are simply not good predictors for pronoun interpretation; data is sparse in the collected predicate-argument statistics.</S>
			<S sid ="44" ssid = "11">The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of &apos;templates&apos; which share an &apos;anchor&apos; argument; these correspond closely to our role pairs.</S>
			<S sid ="45" ssid = "12">Association statistics of the template pairs were used to acquire verb entailments.</S>
			<S sid ="46" ssid = "13">Abe et al.</S>
			<S sid ="47" ssid = "14">(2008) looked for pairs appearing in specific syntactic patterns in order to acquire finer-grained event relations.</S>
			<S sid ="48" ssid = "15">Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common protagonist.</S>
			<S sid ="49" ssid = "16">They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model.</S>
			<S sid ="50" ssid = "17">Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.</S>
			<S sid ="51" ssid = "18">They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.</S>
			<S sid ="52" ssid = "19">Their result raises the natural question as to whether the approach (which may capture domain- specific pairs such as “kidnap—release” in the terrorism domain) can be successfully extended to a general news corpus.</S>
			<S sid ="53" ssid = "20">We address this question in the experiments reported here.</S>
	</SECTION>
	<SECTION title="Corpus Analysis. " number = "3">
			<S sid ="54" ssid = "1">In order to extract semantic features from our large training corpus, we apply a sequence of analyzers.</S>
			<S sid ="55" ssid = "2">These include name tagging, parsing, a baseline coreference analyzer, and, most important, a semantic labeling system that can generate the logical grammatical and predicate- argument representation automatically from a parse tree (Meyers et al. 2009).</S>
			<S sid ="56" ssid = "3">We use semantic labeling because it provides more general and meaningful patterns, with a “deeper” analysis than parsed text.</S>
			<S sid ="57" ssid = "4">The output of the semantic labeling is the dependency representation of the text, where each sentence is a graph consisting of nodes (corresponding to words) and arcs.</S>
			<S sid ="58" ssid = "5">Each arc captures up to three relations between two words: (1) a SURFACE relation, the relation between a predicate and an argument in the parse of a sentence; (2) a LOGIC1 (grammatical logical) relation which regularizes for lexical and syntactic phenomena like passive, relative clauses, and deleted subjects; and (3) a LOGIC2 (predicate- argument) relation corresponding to relations in PropBank and NomBank.</S>
			<S sid ="59" ssid = "6">It is designed to be compatible with the Penn TreeBank (Marcus et al., 1994) framework and therefore, Penn Tree- Bank-based parsers, while incorporating Named Entities, PropBank, and NomBank.</S>
			<S sid ="60" ssid = "7">Because nouns and verbs provide the most relevant contexts and capture the events in which the entities participate, we generate semantic patterns (triples) only for those arcs with verb or noun heads.</S>
			<S sid ="61" ssid = "8">We use the following relations: • Logic2 relations: We use in particular the Arg0 relation (which corresponds roughly to agent) and Arg1 relation (which corresponds roughly to patient).</S>
			<S sid ="62" ssid = "9">• Logic1 relations: We use in particular the Sbj and Obj relations, representing the logical subject and object of a verb (regularizing passive, relative clauses, deleted subjects) • Surface relations: T-pos relation is particularly used, which captures the head noun – determiner relation for possessive constructs such as “bomber’s attack” and “his responsibility”.</S>
			<S sid ="63" ssid = "10">For example, for the sentence: John is hit by Tom’s brother.</S>
			<S sid ="64" ssid = "11">we generate the semantic patterns &lt;Arg1 hit John&gt; &lt;Arg0 hit brother&gt; &lt;T-pos brother Tom&gt; We apply this labeling system to all the data we use, and to generate the semantic pattern, we take first its predicate-argument role; if that is null, we take its logical grammatical role; if both are null, we take its surface role.</S>
			<S sid ="65" ssid = "12">To reduce data sparseness, all inflected words are changed to their base form (e.g. “attackers”→“attacker”).</S>
			<S sid ="66" ssid = "13">All names are replaced by their ACE types (person, organization, location, etc.).</S>
			<S sid ="67" ssid = "14">Only patterns with noun arguments are extracted because we only consider noun phrases as possible antecedents.</S>
	</SECTION>
	<SECTION title="Semantic Features. " number = "4">
			<S sid ="68" ssid = "1">4.1 Contextual Compatibility Patterns.</S>
			<S sid ="69" ssid = "2">Pronouns, especially neutral pronouns (“it”, “they”), carry little semantics of their own, so examining the compatibility of the context of a pronoun and its candidate antecedents is a good way to improve antecedent selection.</S>
			<S sid ="70" ssid = "3">Specifically, we want to determine whether the predicate, which is applied to the anaphor, can be applied to the antecedents.</S>
			<S sid ="71" ssid = "4">We take the semantic pattern with the anaphor in third position.</S>
			<S sid ="72" ssid = "5">Then, each candidate antecedent is substituted for the anaphor to see if it is suitable for the context.</S>
			<S sid ="73" ssid = "6">For example, consider the sentence The company issued a statement that it bought G.M. which would generate the semantic patterns &lt;Arg0 issue company&gt; &lt;Arg1 issue statement&gt; &lt;Arg0 buy it&gt; &lt;Arg1 buy Organization&gt; (here “G.M” is a name of type organization and so is replaced by the token Organization).</S>
			<S sid ="74" ssid = "7">The relevant context of the anaphor is the semantic pattern &lt;Arg0 buy it&gt;.</S>
			<S sid ="75" ssid = "8">Suppose there are two candidate antecedents for “it”: “company” and “statement”.</S>
			<S sid ="76" ssid = "9">We would generate the two semantic patterns &lt;Arg0 buy company&gt; and &lt;Arg0 buy statement&gt;.</S>
			<S sid ="77" ssid = "10">Assuming &lt;Arg0 buy company&gt; is more highly ranked than &lt;Arg0 buy statement&gt;, we can infer that the anaphor is more likely to refer to “company”.</S>
			<S sid ="78" ssid = "11">(We describe the specific metric we use for ranking below, in section 4.3.) As further examples consider: (3) The suspect&apos;s lawyer, Chifumu Banda, told the court he had advised Chiluba not to appear in court Friday.</S>
			<S sid ="79" ssid = "12">(4) Foreign military analysts said it would be highly unusual for an accident to kill a whole submarine crew and they suggested possible causes to a disaster… For (3), if we know that a lawyer is more likely to give advice than a suspect, we could link “he” to “lawyer” instead of “suspect” in the first sentence.</S>
			<S sid ="80" ssid = "13">For (4), if we know that analysts are more likely to “suggest” than crew, we can link “they” to “analysts” in the second sentence.</S>
			<S sid ="81" ssid = "14">4.2 Role Pair Patterns.</S>
			<S sid ="82" ssid = "15">The role pair pattern is a new feature in general pronoun co-reference.</S>
			<S sid ="83" ssid = "16">The original intuition for introducing it into coreference is that there are pairs of actions involving the same entity that are much more likely to occur together than would be true if one assumed statistical independence.</S>
			<S sid ="84" ssid = "17">The second action may be a rephrasing or elaboration of the first, or the two might be actions that are part of a common ‘script’.</S>
			<S sid ="85" ssid = "18">For example: (5) Prime Minister Mahathir Mohamad sacked the former deputy premier in 1998, who was sentenced to a total of 15 years in jail after being convicted of corruption and sodomy.</S>
			<S sid ="86" ssid = "19">He was released after four years because….</S>
			<S sid ="87" ssid = "20">(6) The robber attacked the boy with a knife; he was bleeding heavily and died in the hospital the next day.</S>
			<S sid ="88" ssid = "21">For (5), if we know that the person who was lecting statistics from a large corpus and filtering out the low frequency patterns; this process can eliminate much of the noise due to coreference errors.</S>
			<S sid ="89" ssid = "22">Here is an example of the extracted role pairs involving “attack”: Arg0 attack x ↔ Obj volley x Arg0 bombard x Obj barrage x Arg0 snatch x Sbj attack x Arg0 pound x Obj reoccupy x Arg1 halt x Arg0 assault x Arg1 bombard x Table1.</S>
			<S sid ="90" ssid = "23">Top 10 role pairs associated with “Arg0 attack x” 4.3 Contextual Compatibility Scores.</S>
			<S sid ="91" ssid = "24">To properly compare the patterns involving alternative candidate antecedents, we need to normalize the raw frequencies first.</S>
			<S sid ="92" ssid = "25">We followed Yang et al.</S>
			<S sid ="93" ssid = "26">(2005)’s idea, which normalizes the pattern frequency by the frequency of the candidates, and use a relative score that is normalized by the maximum score of all its candidates: CompScore(Pcontext ,Cand ) CompFreq(Pcontext , Cand ) = MaxCi∈Set ( cands)CompFreq(Pcontext , Ci) sentenced is more likely to be released than the person who sacked others, we would know “he”refers to “deputy premier” instead of “prime min and context,Cand freq(Pcontext ,Cand ) ) = freq(Cand) ister”.</S>
			<S sid ="94" ssid = "27">And in (6), because someone being attacked is more likely to die than the attacker, we can infer that “he” refers to “boy”.</S>
			<S sid ="95" ssid = "28">To acquire such information, we need to identify those pairs of predicates which are likely to apply to the same entity.</S>
			<S sid ="96" ssid = "29">We collect this data from a large corpus.</S>
			<S sid ="97" ssid = "30">The basic process is: apply a baseline coreference system to produce mentions and entities for a large corpus.</S>
			<S sid ="98" ssid = "31">For every entity, record the predicates for every mention, and then the pairs of predicates for successive mentions within each entity.</S>
			<S sid ="99" ssid = "32">Although the performance of the baseline coreference is not very high, and individual documents may yield many idiosyncratic pairs, we can gather many significant role pairs by col where Pcontext ,Cand is the contextual compatibility pattern built from the context of the pronoun and the base form of the candidate.In contrast to Yang’s work, which used con textual compatibility on the mention level, we consider the contextual compatibility of an entity to an anaphor: we calculate the contextual information of all the mentions and choose the one with highest score as the contextual compatibility score for this entity1: 1 Note that all the mentions in the entity are generated by.</S>
			<S sid ="100" ssid = "33">the overall coreference system.</S>
			<S sid ="101" ssid = "34">Also, the ACE entity type of names is determined by the system.</S>
			<S sid ="102" ssid = "35">No key annotations are considered in the entire coreference phase.</S>
			<S sid ="103" ssid = "36">freq(context,entity) = Maxmention ∈entity freq(Pcontext,mention ) names or proper nouns, and generates entities when it is finished.</S>
			<S sid ="104" ssid = "37">The whole is a one-pass process, resolving coreference in the order in i 4.4 Role Pair Scores.</S>
			<S sid ="105" ssid = "38">i which mentions appear in the document.</S>
			<S sid ="106" ssid = "39">In the pronoun coreference process, every pronoun Unlike the contextual compatibility feature, we only take the role pair of the successive mentions in the candidate entity and the anaphor, because they are more reliably coreferential than arbitrary pairs of mentions within an entity: where and are the contextual patterns of the anaphor and the last mention in the candidate entity.</S>
			<S sid ="107" ssid = "40">For a set of possible candidates, we compute a relative score: PairScore( pana , pcand ) mention is assigned to one of the candidate entities.</S>
			<S sid ="108" ssid = "41">= PairFreq( pana , pcand ) Max pi∈Set ( cands)PairFreq( pana , pi) Both scores are quantized (binned) in intervals of 0.1 for use as MaxEnt features.</S>
	</SECTION>
	<SECTION title="Experiment. " number = "5">
			<S sid ="109" ssid = "1">Our coreference solution system uses ACE annotated data and follows the ACE 2005 English entity guidelines.2 The baseline coreference system to compare with is the same one used for extracting semantic features from the large corpus.</S>
			<S sid ="110" ssid = "2">It employs an entity-mention (rather than a mention-pair) model.</S>
			<S sid ="111" ssid = "3">Besides entity and mention information, which (as mentioned above) is system output, the semantic information is also automatically extracted by a semantic labeling system.</S>
			<S sid ="112" ssid = "4">As a result, we report results in section 5.4 which involve no information from the reference (key) annotation.</S>
			<S sid ="113" ssid = "5">5.1 Baseline System Description.</S>
			<S sid ="114" ssid = "6">The baseline system first applies processes like parsing, semantic labeling, name tagging, and entity mention tagging, producing a set of mentions to which coreference analysis is then applied.</S>
			<S sid ="115" ssid = "7">The coreference phase deals with coreference among mentions that might be pronouns, 2 Automatic Content Extraction evaluation,.</S>
			<S sid ="116" ssid = "8">http://projects.ldc.upenn.edu/ace/ Table 2.</S>
			<S sid ="117" ssid = "9">Features used in baseline system The baseline co-reference system has separate, quite elaborate, primarily rule-based systems to handle names, nominals, headless NP&apos;s, and adverbs (&quot;here&quot;, &quot;there&quot;) which may be anaphoric, as well as first- and second-person pronouns.</S>
			<S sid ="118" ssid = "10">The MaxEnt model under study in this paper is only responsible for third-person pronouns.</S>
			<S sid ="119" ssid = "11">Also, gender, number, and human/nonhuman are handled separately outside of the MaxEnt model, and the model only resolves mentions that satisfy these constraints.3 In the MaxEnt model, 5 basic features (described in table 2) are used.</S>
			<S sid ="120" ssid = "12">Thus, while the set of features used in the model is relatively small in comparison to many current statistically based reference resolvers, these are the primary features relevant to the limited task 3 Gender information is obtained from a dictionary of gen-.</S>
			<S sid ="121" ssid = "13">der-specific nouns and from first-name lists from the US Census.</S>
			<S sid ="122" ssid = "14">Number information comes from large syntactic dictionaries, corpus annotation of collective nouns (syntactically singular nouns which may take plural anaphors), and name tagger information (some organizations and political entities may take plural anaphors).</S>
			<S sid ="123" ssid = "15">of the MaxEnt model, and its performance is still competitive4.</S>
			<S sid ="124" ssid = "16">5.2 Corpus Description.</S>
			<S sid ="125" ssid = "17">There are two kinds of corpora used in our experiment, a small coreference-annotated corpus used for training and evaluating (in cross- validation) the pronoun coreference model, and a large raw-text corpus for extracting semantic information.</S>
			<S sid ="126" ssid = "18">For model training and evaluation, we assembled two small corpora from the available ACE data.</S>
			<S sid ="127" ssid = "19">One consists of news articles (460 documents) from ACE 2005 (330 documents) and ACE 2003 (130 documents), which together contain 3934 pronouns.</S>
			<S sid ="128" ssid = "20">The other is the full ACE 2005 training set (592 documents), which includes newswire, broadcast news, broadcast conversations (interviews and discussions), web logs, web forums, and Fisher telephone transcripts, and contains 5659 pronouns.</S>
			<S sid ="129" ssid = "21">In evaluation, we consider a pronoun to be correctly resolved if its antecedent in the system output (the most recent prior mention in the entity to which the pronoun is assigned) matches the antecedent in the key.</S>
			<S sid ="130" ssid = "22">We report accuracy (percentage of pronouns which are correctly resolved).We used a large corpus to extract semantic in formation, consisting of five years of AFP newswire from the LDC English Gigaword corpus (1996, 2002, 2004, 2005 and 2006), a total of 907,368 documents.</S>
			<S sid ="131" ssid = "23">We omit news articles written in 1998, 2000 and 2003 to insure there is no overlap between the ACE data and Gigaword data.</S>
			<S sid ="132" ssid = "24">We pre-processed each document (parsing, name identification, and semantic labeling) and ran the baseline coreference system, which automatically identified all the mentions (including name mentions and nominal mentions) and built a set of entities for each document.</S>
			<S sid ="133" ssid = "25">4 For example, among papers reporting a pronoun accuracy metric, Kehler et al.</S>
			<S sid ="134" ssid = "26">(2004), testing on a 2002 ACE news corpus, get a pronoun accuracy (without semantic features) of 75.7%; (Yang et al. 2005), testing on the MUC coreference corpora (also news) get for their single-candidate baseline (without semantic features) 75.1% pronoun accuracy.</S>
			<S sid ="135" ssid = "27">Although the testing conditions in each case are different, these are comparable to our baseline performance.</S>
			<S sid ="136" ssid = "28">5.3 Semantic Information Extraction from.</S>
			<S sid ="137" ssid = "29">Large Corpus In order to remove noise, we only keep contextual compatibility patterns that appear more than 5 times; and only keep role pair patterns which appear more than 15 times, and appear in more than three different years to avoid random pairs extracted from repeated stories.</S>
			<S sid ="138" ssid = "30">We automatically extracted 626,008 contextual compatibility patterns and 4,736,359 role pairs.</S>
			<S sid ="139" ssid = "31">Note that we extract fewer patterns than Yang (2005), who extracted in total 2,203,203 contextual compatibility patterns, from a much smaller corpus (173,252 Wall Street Journal articles).</S>
			<S sid ="140" ssid = "32">This might be for two reasons: first, we pruned low frequency patterns; second, we used a semantic labeling system instead of shallow parsing.</S>
			<S sid ="141" ssid = "33">Section 5.6 gives a comparison of pattern extraction based on different levels of analysis.</S>
			<S sid ="142" ssid = "34">5.4 Results Table 3.</S>
			<S sid ="143" ssid = "35">Accuracy of 5-fold cross-validation with statistics-based semantic features We did a 5-fold cross validation to test the contribution from statistically-based semantic features, and report an average accuracy.</S>
			<S sid ="144" ssid = "36">All the mentions and their features are obtained from system output; as a result, if the correct antecedent is not correctly discovered and analyzed from the previous phases, we will not be able to co-refer the pronoun correctly.</S>
			<S sid ="145" ssid = "37">Experiments on the news articles show that each feature provides approximately 1% gain by itself, and contributes to a substantial overall gain of 1.45%.</S>
			<S sid ="146" ssid = "38">For the 2005 corpus, the baseline is lower because the documents come from different genres, and we get more gain from each semantic feature.</S>
			<S sid ="147" ssid = "39">We also computed the significance over the baseline using the sign test5.</S>
			<S sid ="148" ssid = "40">5 In applying the sign test, we treated each pronoun as an.</S>
			<S sid ="149" ssid = "41">independent sample, which is either correctly resolved or incorrectly resolved.</S>
			<S sid ="150" ssid = "42">Where the individual observations are 5.5 Self-Extracted Bound.</S>
			<S sid ="151" ssid = "43">To better understand the potential maximum contribution of our semantic features, we constructed an approximation to the most favorable possible semantic features for each test set.</S>
			<S sid ="152" ssid = "44">We did this by using perfect coreference knowledge and by collecting patterns for each test set from the test set itself.</S>
			<S sid ="153" ssid = "45">For each corpus used for cross-validation, we first collect all the contextual compatibility and role pair patterns corresponding to the correct antecedents (we ignore the patterns corresponding to the wrong antecedents, because we can not get this negative information when we extract them from a large corpus), and score these patterns to produce semantic features for the MaxEnt Model, both training and testing.</S>
			<S sid ="154" ssid = "46">We then use these features in the model and do a cross-validation as before.</S>
			<S sid ="155" ssid = "47">Also, as before, we rely on system output to identify and analyze potential antecedents; if the prior phases do not do so correctly, coreference analysis may well fail.</S>
			<S sid ="156" ssid = "48">This experiment shows that we can get about 3 to 4% gain from each feature type separately; 4.5 to 5.5% gain is achieved from the two features together.</S>
			<S sid ="157" ssid = "49">Table 4.</S>
			<S sid ="158" ssid = "50">Accuracy of 5-fold cross-validation with self- extracted semantic features 5.6 Comparison between Semantic and.</S>
			<S sid ="159" ssid = "51">Syntax Patterns To better understand the difference between semantic role labeling and syntactic relations, we did a comparison between patterns extracted from the syntax level and those extracted from semantic role labeling: Experiments show that using semantic roles (such as Arg0 and Arg1) works better.</S>
			<S sid ="160" ssid = "52">This may (changes in) binary outcomes, the sign test provides a suitably sensitive significance test.</S>
			<S sid ="161" ssid = "53">(In particular, it is comparable to performing a paired t-test over counts of correct resolutions, aggregated over documents.)</S>
			<S sid ="162" ssid = "54">be because the &quot;deeper&quot; representation provides more generalization of relations.</S>
			<S sid ="163" ssid = "55">For example, the phrases “weapon’s use” and “use weapon” share the same semantic relation &lt;Arg1 use weapon&gt;, while they yield different grammatical relations: &lt;T-pos use weapon&gt; and &lt;Obj use weapon&gt;.</S>
			<S sid ="164" ssid = "56">News Corpus 2005 Corpus.</S>
			<S sid ="165" ssid = "57">semantic syntax semantic syntax baseline 75.54 72.04 context 79.23 77.73 76.04 75.83 role pair 78.85 76.87 75.95 74.17 combine 79.97 78.42 77.50 76.76 Table 5.</S>
			<S sid ="166" ssid = "58">Accuracy of 5-fold cross-validation with self- extracted semantic features based on different levels of syntactic/semantic relations 5.7 Error Analysis.</S>
			<S sid ="167" ssid = "59">We analyzed the errors in the self-extracted results, to see why such corpus-specific semantic features do not produce an even greater reduction in errors.</S>
			<S sid ="168" ssid = "60">For the contextual compatibility feature, we find cases where an incorrect candidate is equally compatible with the context of the anaphor; for example, if all the candidates are person names, they will share the same context feature because they generate the same ACE type.</S>
			<S sid ="169" ssid = "61">In other cases, the context does not provide enough information.</S>
			<S sid ="170" ssid = "62">For example, in a context tuple &lt;Arg0 get x&gt;, x can be almost any noun, because “get” is too vague to predicate the compatible subjects.</S>
			<S sid ="171" ssid = "63">There are similar limitations with the role pair feature; for example, &lt;Arg0 get they&gt; can be associated with a lot of other actions.</S>
			<S sid ="172" ssid = "64">To quantify this problem, we counted the patterns that appear in both positive examples (correct antecedents) and negative examples (incorrect antecedents).</S>
			<S sid ="173" ssid = "65">For contextual compatibility patterns, 39.5% of the patterns which appear with positive examples also appear in the negative sample, while for role pair patterns, 19% of the patterns which appear with positive examples also appear in the negative sample.</S>
			<S sid ="174" ssid = "66">So we see that, even with a pattern set highly tuned to the test set, many patterns do not by themselves serve to distinguish correct from incorrect coreference.</S>
			<S sid ="175" ssid = "67">We analyzed some of the cases where the semantic information does not help, or even harms the analysis.</S>
			<S sid ="176" ssid = "68">In some cases all the antecedent scores are very low, either because the patterns are very rare or the antecedent is a common word that appears in a lot of patterns.</S>
			<S sid ="177" ssid = "69">In other cases, several antecedents have a high compatibility score but the correct one does not have the top score.</S>
			<S sid ="178" ssid = "70">In these cases, the contextual compatibility is not reliable, as was pointed out by Kehler et al.</S>
			<S sid ="179" ssid = "71">(2004): (7) The model for a republic, adopted over bitter objections from those advocating direct election of a president, is for presidential nominations to be made with public input and the winning candidate decided by a two-thirds majority of Parliament.</S>
			<S sid ="180" ssid = "72">Former prime minister Paul Keating, who put the republic issue in the spotlight in his unsuccessful 1996 campaign for reelection, welcomed the result.</S>
			<S sid ="181" ssid = "73">Here adding semantic features leads “his” to be incorrectly resolved to “president” rather than the entity with mentions “prime minister” and “Paul Keating”; all the relevant patterns are common, but the score for &lt;Arg0 campaign president&gt; is higher (around 0.0012) than for &lt;Arg0 campaign minister&gt; (0.0004) or &lt;Arg0 campaign Person&gt; (0.0006).</S>
			<S sid ="182" ssid = "74">Another problem is that the patterns do not capture enough context information, for example: (8) The U.S. administration has been pressing the Security Council to adopt a statement condemning Pyongyang for failing to meet its obligations.</S>
			<S sid ="183" ssid = "75">If we can get the semantic context of “fail to meet its obligations” instead of “its obligations”, we might get better solutions for (8).</S>
			<S sid ="184" ssid = "76">The role pair information raises similar problems.</S>
			<S sid ="185" ssid = "77">Some verbs are very vague, like “get”, “take”, “have”, and role pairs with these verbs might not be very useful.</S>
			<S sid ="186" ssid = "78">Here is an example: (9) The retired Greek officer tried to get Ocalan to the Netherlands, home to a large Kurdish community.</S>
			<S sid ="187" ssid = "79">He claimed he had been manipulated by the government.</S>
			<S sid ="188" ssid = "80">In this sentence, the role pair information is very vague and it is hard to select a proper antecedent by connecting the subject of “try” or “get” or the object of “get” to the subject of “claim”.</S>
			<S sid ="189" ssid = "81">5.8 Limitations of Semantic Features.</S>
			<S sid ="190" ssid = "82">The availability of very large corpora coupled with improved pre-processing (e.g., faster pars- ers, accurate semantic labelers) is making it easier to extract large sets of semantic patterns.</S>
			<S sid ="191" ssid = "83">However, results on “perfect” semantic information show that even if we can get very good semantic features, there are at least two concerns to address: • How to best capture the context information: larger context patterns may suffer from data sparseness; simple patterns may be insufficiently selective, appearing in both positive and negative samples.</S>
			<S sid ="192" ssid = "84">• In some cases, the baseline features are sufficient to select the antecedent and the semantic features only do harm.</S>
			<S sid ="193" ssid = "85">If we are able to better gauge our confidence in the decisions based on the baseline features and on the semantic features, we may be able to combine these two sources more effectively.</S>
	</SECTION>
	<SECTION title="Conclusions and Future Work. " number = "6">
			<S sid ="194" ssid = "1">We have presented two ways to incorporate semantic features into a MaxEnt model-based pronoun coreference system, where these features have been extracted from a large corpus using a baseline IE (Information Extraction) system and a semantic labeling system, with no specific domain information.</S>
			<S sid ="195" ssid = "2">We also estimated the maximal benefit of these features and did some error analysis to identify cases where this semantic knowledge did not suffice.</S>
			<S sid ="196" ssid = "3">Our experiments show the value of these semantic features for pronoun coreference, but also the limitations of our current context representation and reference resolution models.</S>
			<S sid ="197" ssid = "4">Last, we compared the features extracted from different levels of analysis, and showed that &apos;deeper&apos; representations worked better.</S>
	</SECTION>
</PAPER>
