<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a novel method of applying the results of coreference resolution to improve Name Recognition for Chinese.</S>
		<S sid ="2" ssid = "2">We consider first some methods for gauging the confidence of individual tags assigned by a statistical name tagger.</S>
		<S sid ="3" ssid = "3">For names with low confidence, we show how these names can be filtered using coreference features to improve accuracy.</S>
		<S sid ="4" ssid = "4">In addition, we present rules which use coreference information to correct some name tagging errors.</S>
		<S sid ="5" ssid = "5">Finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters.</S>
		<S sid ="6" ssid = "6">These combined methods yield an absolute improvement of about 3.1% in tagger F score.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">The problem of name recognition and classification has been intensively studied since1995, when it was introduced as part of the MUC 6 Evaluation (Grishman and Sundheim, 1996).</S>
			<S sid ="8" ssid = "8">A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.</S>
			<S sid ="9" ssid = "9">However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.</S>
			<S sid ="10" ssid = "10">In particular, most of these methods classify an instance of a name based on the information about that instance alone, and very local context of that instance – typically, one or 1 The best results reported for Chinese named entity recognition, on the MET-2 test corpus, are 0.92 to 0.95 F-measure for the different name types (Ye et al. 2002).</S>
			<S sid ="11" ssid = "11">two words preceding and following the name.</S>
			<S sid ="12" ssid = "12">If a name has not been seen before, and appears in a relatively uninformative context, it becomes very hard to classify.</S>
			<S sid ="13" ssid = "13">We propose to use more global information to improve the performance of name recognition.</S>
			<S sid ="14" ssid = "14">Some name taggers have incorporated a name cache or similar mechanism which makes use of names previously recognized in the document.</S>
			<S sid ="15" ssid = "15">In our approach, we perform coreference analysis and then use detailed evidence from other phrases in the document which are co-referential with this name in order to disambiguate the name.</S>
			<S sid ="16" ssid = "16">This allows us to perform a richer set of corrections than with a name cache.</S>
			<S sid ="17" ssid = "17">We then go one step further and process similar documents containing instances of the same name, and combine the evidence from these additional instances.</S>
			<S sid ="18" ssid = "18">At each step we are able to demonstrate a small but consistent improvement in named entity recognition.</S>
			<S sid ="19" ssid = "19">The rest of the paper is organized as follows.</S>
			<S sid ="20" ssid = "20">Section 2 briefly describes the baseline name tagger and coreference resolver used in this paper.</S>
			<S sid ="21" ssid = "21">Section 3 considers methods for assessing the confidence of name tagging decisions.</S>
			<S sid ="22" ssid = "22">Section 4 examines the distribution of name errors, as a motivation for using coreference information.</S>
			<S sid ="23" ssid = "23">Section 5 shows the coreference features we use and how they are incorporated into a statistical name filter.</S>
			<S sid ="24" ssid = "24">Section 6 describes additional rules using coreference to improve name recognition.</S>
			<S sid ="25" ssid = "25">Section 7 provides the flow graph of the improved system.</S>
			<S sid ="26" ssid = "26">Section 8 reports and discusses the experimental results while Section 9 summarizes the conclusions.</S>
	</SECTION>
	<SECTION title="Baseline Systems. " number = "2">
			<S sid ="27" ssid = "1">The task we consider in this paper is to identify three classes of names in Chinese text: persons (PER), organizations (ORG), and geopolitical entities (GPE).</S>
			<S sid ="28" ssid = "2">Geopolitical entities are locations which have an associated government, such as cities, states, and countries.2 Name recognition in Chinese poses extra challenges because neither capitalization nor word segmentation clues are explicitly provided, although most of the techniques we describe are more generally applicable.</S>
			<S sid ="29" ssid = "3">Our study builds on an extraction system developed for the ACE evaluation, a multi-site evaluation of information extraction organized by the U.S. Government.</S>
			<S sid ="30" ssid = "4">Following ACE terminology, we will use the term mention to refer to a name or noun phrase of one of the types of interest, and the term entity for a set of coreferring mentions.</S>
			<S sid ="31" ssid = "5">We briefly describe in this section the baseline Chinese named entity tagger, as well as the coreference system, used in our experiments.</S>
			<S sid ="32" ssid = "6">2.1 Chinese Name Tagger.</S>
			<S sid ="33" ssid = "7">Our baseline name tagger consists of an HMM tagger augmented with a set of post-processing rules.</S>
			<S sid ="34" ssid = "8">The HMM tagger generally follows the NYMBLE model (Bikel et al, 1997), but with a larger number of states (12) to handle name prefixes and suffixes, and transliterated foreign names separately.</S>
			<S sid ="35" ssid = "9">It operates on the output of a word segmenter from Tsinghua University.</S>
			<S sid ="36" ssid = "10">It uses a trigram model with dynamic backoff.</S>
			<S sid ="37" ssid = "11">The post- processing rules correct some omissions and systematic errors using name lists (for example, a list of all Chinese last names; lists of organization and location suffixes) and particular contextual patterns (for example, verbs occurring with people’s names).</S>
			<S sid ="38" ssid = "12">They also deal with abbreviations and nested organization names.</S>
			<S sid ="39" ssid = "13">2.2 Chinese Coreference Resolver.</S>
			<S sid ="40" ssid = "14">For this study we have used a rule-based coreference resolver.</S>
			<S sid ="41" ssid = "15">Table 1 lists the main rules and patterns used.</S>
			<S sid ="42" ssid = "16">We have extensive rules for name-name coreference, including rules specific to the particular name types.</S>
			<S sid ="43" ssid = "17">For these experiments, we do not attempt to resolve pronouns, and we only resolve names with nominals when the name and nominal appear in close proximity in a specific structure, as listed in Table 1.</S>
			<S sid ="44" ssid = "18">We have used the MUC coreference scoring metric (Vilain et al, 1995) to evaluate this resolver, excluding all pronouns and limiting ourselves to noun phrases of semantic type PER, ORG, and Using the mentions generated by our extraction system, we obtain a recall of 74.3%, a precision of 84.5%, and an F score of 79.07%.3</S>
	</SECTION>
	<SECTION title="Confidence Measures. " number = "3">
			<S sid ="45" ssid = "1">In order to decide when we need to rely on global (coreference) information for name tagging, we want to have some assessment of the confidence that the name tagger has in individual tagging decisions.</S>
			<S sid ="46" ssid = "2">In this paper, we use two tools to reach this goal.</S>
			<S sid ="47" ssid = "3">The first method is to use three manually built proper name lists which include common names of each type (selected from the high frequency names in the user query blog of COMPASS, a Chinese search engine, and name lists provided by Linguistic Data Consortium; the PER list includes 147 names, the GPE list 226 names, and the ORG list 130 names).</S>
			<S sid ="48" ssid = "4">Names on these lists are accepted without further review.</S>
			<S sid ="49" ssid = "5">The second method is to have the HMM tagger compute a probability margin for the identification of a particular name as being of a particular type.</S>
			<S sid ="50" ssid = "6">Scheffer et al.</S>
			<S sid ="51" ssid = "7">(2001) used a similar method to identify good candidates for tagging in an active learner.</S>
			<S sid ="52" ssid = "8">During decoding, the HMM tagger seeks the path of maximal probability through the Viterbi lattice.</S>
			<S sid ="53" ssid = "9">Suppose we wish to evaluate the confidence with which words wi, …, wj are identified as a name of type T. We compute Margin (wi,…, wj; T) = log P1 – log P2 Here P1 is the maximum path probability and P2 is the maximum probability among all paths for which some word in wi, …, wj is assigned a tag other than T. A large margin indicates greater confidence in the tag assignment.</S>
			<S sid ="54" ssid = "10">If we exclude names tagged with a margin below a threshold, we can increase the precision of name tagging at some cost in recall.</S>
			<S sid ="55" ssid = "11">Figure 1 shows the trade-off between margin threshold and name recognition performance.</S>
			<S sid ="56" ssid = "12">Names with a margin over 3.0 are accepted on this basis.</S>
			<S sid ="57" ssid = "13">GPE.</S>
			<S sid ="58" ssid = "14">Using a perfect (hand-generated) set of mentions, we obtain a recall of 82.7% and precision of 95.1%, for an F score of 88.47%.</S>
			<S sid ="59" ssid = "15">2 This class is used in the U.S. Government’s ACE evaluations; it excludes locations without governments, such as bodies of water and mountains.</S>
			<S sid ="60" ssid = "16">3 In our scoring, we use the ACE keys and only score.</S>
			<S sid ="61" ssid = "17">mentions which appear in both the key and system response.</S>
			<S sid ="62" ssid = "18">This therefore includes only mentions identified as being in the ACE semantic categories by both the key and the system response.</S>
			<S sid ="63" ssid = "19">Thus these scores cannot be directly compared against coreference scores involving all noun phrases.</S>
			<S sid ="64" ssid = "20">99 97 95 93 91 89 87 85 0 1 2 3 4 5 6 7 8 9 10 11 12 Threshold Figure 1: Tradeoff between Margin Threshold and name recognition performance</S>
	</SECTION>
	<SECTION title="Distribution of Name Errors. " number = "4">
			<S sid ="65" ssid = "1">We consider now names which did not pass the confidence measure tests: names not on the common name list, which were tagged with a margin below the threshold.</S>
			<S sid ="66" ssid = "2">We counted the accuracy of these “obscure” names as a function of the number of mentions in an entity; the results are shown in Table 2.</S>
			<S sid ="67" ssid = "3">The table shows that the accuracy of name recognition increases as the entity includes more mentions.</S>
			<S sid ="68" ssid = "4">In other words, if a name has more corefed mentions, it is more likely to be correct.</S>
			<S sid ="69" ssid = "5">This also provides us a linguistic intuition: if people mention an obscure name in a text, they tend to emphasize it later by repeating the same name or describe it with nominal mentions.</S>
			<S sid ="70" ssid = "6">The table also indicates that the accuracy of single name entities (singletons) is much lower than the overall accuracy.</S>
			<S sid ="71" ssid = "7">So, although they constitute only about 10% of all names, increasing their accuracy can significantly improve overall performance.</S>
			<S sid ="72" ssid = "8">Coreference information can play a great role here.</S>
			<S sid ="73" ssid = "9">Take the 157 PER singletons as an example; 56% are incorrect names.</S>
			<S sid ="74" ssid = "10">Among these incorrect names, 73% actually belong to the other two name types.</S>
			<S sid ="75" ssid = "11">Many of these can be easily fixed by searching for coreference to other mentions without type restriction.</S>
			<S sid ="76" ssid = "12">Among the correct names, 71% can be confirmed by the presence of a title word or a Chinese last name.</S>
			<S sid ="77" ssid = "13">From these observations we can conclude that without strong confirmation features, singletons are much less likely to be correct names.</S>
	</SECTION>
	<SECTION title="Incorporating	Coreference	Information. " number = "5">
			<S sid ="78" ssid = "1">into Name Recognition We make use of several features of the coreference relations a name is involved in; the we built an independent classifier to predict if a name identified by the baseline name tagger is correct or not.</S>
			<S sid ="79" ssid = "2">(Note that this classifier is trained on all name mentions, but during test only ‘obscure’ names which failed the tests in section 3 are processed by this classifier.)</S>
			<S sid ="80" ssid = "3">Each name corresponds to a feature vector which consists of the factors described in Table 3.</S>
			<S sid ="81" ssid = "4">The PER context words are generated from the context patterns described in (Ji and Luo, 2001).</S>
			<S sid ="82" ssid = "5">We used a Support Vector Machine to implement the classifier, because of its state-of-the-art performance and good generalization ability.</S>
			<S sid ="83" ssid = "6">We used a polynomial kernel of degree 3.</S>
	</SECTION>
	<SECTION title="Name Rules based on Coreference. " number = "6">
			<S sid ="84" ssid = "1">Besides the factors in the above statistical model, additional coreference information can be used to filter and in some cases correct the tagging produced by the HMM.</S>
			<S sid ="85" ssid = "2">We developed the following rules to correct names generated by the baseline tagger.</S>
			<S sid ="86" ssid = "3">6.1 Name Structure Errors.</S>
			<S sid ="87" ssid = "4">Sometimes the Name tagger outputs names which are too short (incomplete) or too long.</S>
			<S sid ="88" ssid = "5">We can make use of the relation among mentions in the same entity to fix them.</S>
			<S sid ="89" ssid = "6">For example, nested ORGs are traditionally difficult to recognize correctly.</S>
			<S sid ="90" ssid = "7">Errors in ORG names can take the following forms: (1) Head Missed.</S>
			<S sid ="91" ssid = "8">Examples: “中国艺术（团）/ Chinese Art (Group)”, “中国学生（会）/ Chinese Student (Union)”, “俄罗斯核动力（所）/ Russian Nuclear Power (Instituition)” Rule 1: If an ORG name x is corefed with other mentions with head y (an ORG suffix), and in the original text x is immediately followed by y, then tag xy instead of x; otherwise discard x.</S>
			<S sid ="92" ssid = "9">(2) Modifier Missed.</S>
			<S sid ="93" ssid = "10">Rule 1 can also be used to restore missed modifiers.</S>
			<S sid ="94" ssid = "11">For example, “（爱丁 堡）大学 / (Edinburgh) University”; “（鹏程）有 限公司 / (Peng Cheng) Limited Corporation”, and some incomplete translated PER names such as “（巴）勒斯坦 / (Pa)lestine”.</S>
			<S sid ="95" ssid = "12">(3) Name Too Long Rule 2: If a name x has no corefed mentions but part of it, x&apos;, is identical to a name in another entity y, and y includes at least two mentions; then tag x&apos; instead of x. . Rule Type Rule Description Name &amp; Name All Ident(i, j) Mentioni and Mentionj are identical Abbrev(i, j) Mentioni is an abbreviation of Mentionj Modifier(i, j) Mentionj = Modifier + “de” + Mentioni Formal(i, j) Formal and informal ways of referring to the same entity (Ex.</S>
			<S sid ="96" ssid = "13">“美国国防部 / American Defense Dept. &amp; 五角大楼/ Pentagon”) PER Substring(i, j) Mentioni is a substring of Mentionj Title(i, j) Mentionj = Mentioni + title word; or Mentionj = LastName + title word ORG Head(i, j) Mentioni and Mentionj have the same head GPE Head(i, j) Mentioni and Mentionj have the same head Capital(i, j) Mentioni: country name; Mentionj: name of the capital of this country Applied in restricted context.</S>
			<S sid ="97" ssid = "14">Country(i, j) Mentioni and Mentionj are different names referring to the same country.</S>
			<S sid ="98" ssid = "15">(Ex.</S>
			<S sid ="99" ssid = "16">“中国 / China &amp; 华夏 / Huaxia &amp; 共和国 / Republic”) Name &amp; Nominal All RSub(i, j) Namei is a right substring of Nominalj Apposition(i, j) Nominalj is the apposite of Namei Modifier2(i, j) Nominalj = Determiner/Modifier + Namei/ head GPE Ref(i, j) Nominalj = Namei + GPE Ref Word (examples of GPE Ref Word: “方面 / Side”, “政府/Government”, “共和国 / Republic”, “自治政府/ Municipality”) Nominal&amp; Nominal All IdentN(i, j) Nominali and Nominalj are identical Modifier3(i, j) Nominalj = Determiner/Modifier + Nominali Table1: Main rules used in the Coreference Resolver Number of mentions per entity Name Type 1 2 3 4 5 6 7 8 &gt;8 PER 43.94 87.07 91.23 87.95 91.57 91.92 94.74 92.31 97.36 GPE 55.81 88.8 96.07 100 100 100 100 95.83 97.46 ORG 64.71 80.59 89.47 94.29 100 100-- 100 Table 2 Accuracy(%) of ‘obscure’ name recognition Factor Description Coreference Type Weight Average of weights of coreference relations for which this mention is antecedent: 0.8 for name-name coreference; 0.5 for apposition; 0.3 for other name-nominal coreference Mention Weight First Mention Is first name mention in the entity Head Includes head word of name Idiom Name is part of an idiom PER context For PER Name, has context word in text PER title For PER Name, includes title word ORG suffix For ORG Name, includes suffix word Entity Weight Number of mentions in entity / total number of mentions in all entities in document which include a name mention Table 3 Coreference factors for name recognition 6.2 Name Type Errors.</S>
			<S sid ="100" ssid = "17">Some names are mistakenly recognized as other name types.</S>
			<S sid ="101" ssid = "18">For example, the name tagger has difficulty in distinguishing transliterated PER name and transliterated GPE names.</S>
			<S sid ="102" ssid = "19">To solve this problem we designed the following rules based on the relation among entities.</S>
	</SECTION>
	<SECTION title="System Flow. " number = "7">
			<S sid ="103" ssid = "1">Combining all the methods presented above, the flow of our final system is shown in Figure 2: Input Rule 3: If namei is recognized as type1, the entity it belongs to has only one mention; and namej is recognized as type2, the entity it belongs to has at least two mentions; and namei is identical with namej or namei is a substring of namej, then correct type1 to type2.</S>
			<S sid ="104" ssid = "2">For example, if “ 克里姆林 / Kremlin” is mistakenly identified as PER, while “克里姆林宫 Name tagger Coreference Resolver Coreference Nominal mention tagger / Kremlin Palace” is correctly identified as ORG, and in coreference results, “克里姆林 / Kremlin” belongs to a singleton entity, while “克里姆林宫 / Kremlin Palace” has corefed mentions, then we correct the type of “克里姆林 / Kremlin” to ORG.</S>
			<S sid ="105" ssid = "3">Another common mistake gives rise to the sequence “PER+title+PER”, because our name tagger uses the title word as an important context feature for a person name (either preceding or following the title).</S>
			<S sid ="106" ssid = "4">But this is an impossible structure in Chinese.</S>
			<S sid ="107" ssid = "5">We can also use coreference information to fix it.</S>
			<S sid ="108" ssid = "6">Rule 4: If “PER+title+PER” appears in the name tagger’s output, then we discard the PER name with lower coref certainty; and check whether it is corefed to other mentions in a GPE entity or ORG entity; if it is, correct the type.</S>
			<S sid ="109" ssid = "7">Using this rule we can correctly identify “[斯里 兰卡 / Sri Lanka GPE] 总理 / Premier [班达拉耐 克 / Bandaranaike PER]”, instead of “[斯里兰卡 / Sri Lanka PER] 总理 / Premier [班达拉耐克 / Bandaranaike PER]”.</S>
			<S sid ="110" ssid = "8">6.3 Name Abbreviation Errors.</S>
			<S sid ="111" ssid = "9">Name abbreviations are difficult to recognize correctly due to a lack of training data.</S>
			<S sid ="112" ssid = "10">Usually people adopt a separate list of abbreviations or design separate rules (Sun et al. 2002) to identify them.</S>
			<S sid ="113" ssid = "11">But many wrong abbreviation names might be produced.</S>
			<S sid ="114" ssid = "12">We find that coreference information helps to select abbreviations.</S>
			<S sid ="115" ssid = "13">Rule 5: If an abbreviation name has no corefed mentions and it is not adjacent to another abbreviation (ex.</S>
			<S sid ="116" ssid = "14">“ 中/China 美/America”), then we discard it.</S>
			<S sid ="117" ssid = "15">Rules to fix name errors SVM classifier to select correct names using coreference features Output Figure 2 System Flow</S>
	</SECTION>
	<SECTION title="Experiments. " number = "8">
			<S sid ="118" ssid = "1">8.1 Training and Test Data.</S>
			<S sid ="119" ssid = "2">For our experiments, we used the Beijing University Insititute of Computational Linguistics corpus – 2978 documents from the People’s Daily in 1998, one million words with name tags – and the training corpus for the 2003 ACE evaluation, 223 documents.</S>
			<S sid ="120" ssid = "3">153 of our ACE documents were used as our test set.</S>
			<S sid ="121" ssid = "4">4 The 153 documents contained 1614 names.</S>
			<S sid ="122" ssid = "5">Of the system-tagged names, 959 were considered ‘obscure’: were not on a name list and had a margin below the threshold.</S>
			<S sid ="123" ssid = "6">These were the names to which the rules and classifier were applied.</S>
			<S sid ="124" ssid = "7">We ran all the following experiments using the MUC scorer.</S>
			<S sid ="125" ssid = "8">4 The test set was divided into two parts, of 95 documents and 58 documents.</S>
			<S sid ="126" ssid = "9">We trained two name tagger and classifier models, each time using one part of the test set along with all the other documents, and evaluated on the other part of the test set.</S>
			<S sid ="127" ssid = "10">The results reported here are the combined results for the entire test set.</S>
			<S sid ="128" ssid = "11">Table 4 shows the performance of the baseline system; Table 5 the system with rule-based corrections; and Table 6 the system with both rules and the SVM classifier.</S>
			<S sid ="129" ssid = "12">retrieving related documents from sina.com.cn.</S>
			<S sid ="130" ssid = "13">In total, we added 52 texts to these 11 clusters.</S>
			<S sid ="131" ssid = "14">The net result was a further improvement of 0.3% in F score (Table 8).6 Name Precision Recall F PER 90.9 88.2 89.5 GPE 82.3 90.8 86.3 ORG 92.1 91.8 91.9 ALL 87.8 90.5 89.1 Table 4 Baseline Name Tagger Table 7 Results for Mutiple Document System Name Precision Recall F PER 93.3 87.5 90.3 GPE 83.5 90.4 86.8 ORG 90.9 92.1 91.5 ALL 88.5 90.3 89.4 Table 5 Results with Coref Rules Alone Name Precision Recall F PER 95.7 84.4 89.7 GPE 88.0 91.7 89.8 ORG 94.5 91.2 92.8 ALL 92.2 89.6 90.9 Table 6 Results for Single Document System The gains we observed from coreference within single documents suggested that further improvement might be possible by gathering evidence from several related documents.</S>
			<S sid ="132" ssid = "15">5 We did this in two stages.</S>
			<S sid ="133" ssid = "16">First, we clustered the 153 documents in the test set into 38 topical clusters.</S>
			<S sid ="134" ssid = "17">Most (29) of the clusters had only two documents; the largest had 28 documents.</S>
			<S sid ="135" ssid = "18">We then applied the same procedures, treating the entire cluster as a single document.</S>
			<S sid ="136" ssid = "19">This yielded another 1.0% improvement in overall F score (Table 7).</S>
			<S sid ="137" ssid = "20">The improvement in F score was consistent for the larger clusters (3 or more documents): the F score improved for 8 of those clusters and remained the same for the 9th.</S>
			<S sid ="138" ssid = "21">To heighten the multi-document benefit, we took 11 of the small 5 Borthwick (1999) did use some cross-document information across the entire test corpus, maintaining in effect a name cache for the corpus, in addition to one for the document.</S>
			<S sid ="139" ssid = "22">No attempt was made to select or cluster documents.</S>
			<S sid ="140" ssid = "23">Table 8 Results for Mutiple Document System with additional retrieved texts 8.3 Contribution of Coreference Features.</S>
			<S sid ="141" ssid = "24">Since feature selection is crucial to SVMs, we did experiments to determine how precision increased as each feature was added.</S>
			<S sid ="142" ssid = "25">The results are shown in Figure 3.</S>
			<S sid ="143" ssid = "26">We can see that each feature in the SVM helps to select correct names from the output of the baseline name tagger, although some (like FirstMention) are more crucial than others.</S>
			<S sid ="144" ssid = "27">93 92 91 90 89 88 87 Feature Figure 3 Contributions of features 6 Scores are still computed on the 153 test documents ; the retrieved documents are excluded from the scoring.</S>
			<S sid ="145" ssid = "28">8.4 Comparison to Cache Model.</S>
			<S sid ="146" ssid = "29">Some named entity systems use a name cache, in which tokens or complete names which have been previously assigned a tag are available as features in tagging the remainder of a document.</S>
			<S sid ="147" ssid = "30">Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).</S>
			<S sid ="148" ssid = "31">Our system, while more complex, makes use of a richer set of global features, involving the detailed structure of individual mentions, and in particular makes use of both name – name and name – nominal relations.</S>
			<S sid ="149" ssid = "32">We have compared the performance of our method (applied to single documents) with a voted cache model, which takes into account the number of times a particular name has been previously assigned each type of tag: System Precision Recall F baseline 88.8 90.5 89.1 voted cache 87.6 92.8 90.1 current 92.2 89.6 90.9 Table 9.</S>
			<S sid ="150" ssid = "33">Comparison with voted cache Compared to a simple voted cache model, our model provides a greater improvement in name recognition F score; in particular, it can substantially increase the precision of name recognition.</S>
			<S sid ="151" ssid = "34">The voted cache model can recover some missed names, but at some loss in precision.</S>
	</SECTION>
	<SECTION title="Conclusions and Future Work. " number = "9">
			<S sid ="152" ssid = "1">In this paper, we presented a novel idea of applying coreference information to improve name recognition.</S>
			<S sid ="153" ssid = "2">We used both a statistical filter based on a set of coreference features and rules for correcting specific errors in name recognition.</S>
			<S sid ="154" ssid = "3">Overall, we obtained an absolute improvement of 3.1% in F score.</S>
			<S sid ="155" ssid = "4">Put another way, we were able to eliminate about 60% of erroneous name tags with only a small loss in recall.</S>
			<S sid ="156" ssid = "5">The methods were tested on a Chinese name tagger, but most of the techniques should be applicable to other languages.</S>
			<S sid ="157" ssid = "6">More generally, it offers an example of using global and cross- document information to improve local decisions for information extraction.</S>
			<S sid ="158" ssid = "7">Such methods will be important for breaking the ‘performance ceiling’ in many areas of information extraction.</S>
			<S sid ="159" ssid = "8">In the future, we plan to experiment with improvements in coreference resolution (in particular, adding pronoun resolution) to see if we can obtain further gains in name recognition.</S>
			<S sid ="160" ssid = "9">We also intend to explore the production of multiple tagging hypotheses by our statistical name tagger, with the alternative hypotheses then reranked using global information.</S>
			<S sid ="161" ssid = "10">This may allow us to replace some of our hand-coded error-correction rules with corpus-trained methods.</S>
	</SECTION>
	<SECTION title="Acknowledgements. " number = "10">
			<S sid ="162" ssid = "1">This research was supported by the Defense Advanced Research Projects Agency as part of the Translingual Information Detection, Extraction and Summarization (TIDES) program, under Grant N66001001-18917 from the Space and Naval Warfare Systems Center San Diego, and by the National Science Foundation under Grants IIS0081962 and 0325657.</S>
			<S sid ="163" ssid = "2">This paper does not necessarily reflect the position or the policy of the U.S. Government.</S>
	</SECTION>
</PAPER>
