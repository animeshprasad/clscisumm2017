<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes how a machine- learning named entity recognizer (NER) on upper case text can be improved by using a mixed case NER and some unlabeled text.</S>
		<S sid ="2" ssid = "2">The mixed case NER can be used to tag some unlabeled mixed case text, which are then used as additional training material for the upper case NER.</S>
		<S sid ="3" ssid = "3">We show that this approach reduces the performance Mixed Case: Consuela Washington, a longtime House staffer and an expert in securities laws, is a leading candidate to be chairwoman of the Securities and Exchange Commission in the Clinton administration.</S>
		<S sid ="4" ssid = "4">Upper Case: CONSUELA WASHINGTON, A LONGTIME HOUSE STAFFER AND AN EXPERT IN SECURITIES LAWS, IS A LEADING CANDIDATE TO BE CHAIRWOMAN OF THE SECURITIES AND EXCHANGE COMMISSION IN THE CLINTON ADMINISTRATION.</S>
		<S sid ="5" ssid = "5">gap between the mixed case NER and the upper case NER substantially, by 39% for MUC6 and 22% for MUC7 named entity test data.</S>
		<S sid ="6" ssid = "6">Our method is thus useful in improving the accuracy of NERs on upper case text, such as transcribed text from automatic speech recognizers where case information is missing.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">In this paper, we propose using a mixed case named entity recognizer (NER) that is trained on labeled text, to further train an upper case NER.</S>
			<S sid ="8" ssid = "8">In the Sixth and Seventh Message Understanding Conferences (MUC6, 1995; MUC7, 1998), the named entity task consists of labeling named entities with the classes PERSON, ORGANIZATION, LOCATION, DATE, TIME, MONEY, and PERCENT.</S>
			<S sid ="9" ssid = "9">We conducted experiments on upper case named entity recognition, and showed how unlabeled mixed case text can be used to improve the results of an upper case NER on the official MUC6 and MUC7 Figure 1: Examples of mixed and upper case text test data.</S>
			<S sid ="10" ssid = "10">Besides upper case text, this approach can also be applied on transcribed text from automatic speech recognizers in Speech Normalized Orthographic Representation (SNOR) format, or from optical character recognition (OCR) output.</S>
			<S sid ="11" ssid = "11">For the English language, a word starting with a capital letter often designates a named entity.</S>
			<S sid ="12" ssid = "12">Upper case NERs do not have case information to help them to distinguish named entities from non-named entities.</S>
			<S sid ="13" ssid = "13">When data is sparse, many named entities in the test data would be unknown words.</S>
			<S sid ="14" ssid = "14">This makes upper case named entity recognition more difficult than mixed case.</S>
			<S sid ="15" ssid = "15">Even a human would experience greater difficulty in annotating upper case text than mixed case text (Figure 1).</S>
			<S sid ="16" ssid = "16">We propose using a mixed case NER to “teach” an upper case NER, by making use of unlabeled mixed case text.</S>
			<S sid ="17" ssid = "17">With the abundance of mixed case un the Internet, it will be easy to apply our approach to improve the performance of NER on upper case text.</S>
			<S sid ="18" ssid = "18">Our approach does not satisfy the usual assumptions of co-training (Blum and Mitchell, 1998).</S>
			<S sid ="19" ssid = "19">Intuitively, however, one would expect some information to be gained from mixed case unlabeled text, where case information is helpful in pointing out new words that could be named entities.</S>
			<S sid ="20" ssid = "20">We show empirically that such an approach can indeed improve the performance of an upper case NER.</S>
			<S sid ="21" ssid = "21">In Section 5, we show that for MUC6, this way of using unlabeled text can bring a relative reduction in errors of 38.68% between the upper case and mixed case NERs.</S>
			<S sid ="22" ssid = "22">For MUC7 the relative reduction in errors is 22.49%.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="23" ssid = "1">Considerable amount of work has been done in recent years on NERs, partly due to the Message Understanding Conferences (MUC6, 1995; MUC7, 1998).</S>
			<S sid ="24" ssid = "2">Machine learning methods such as BBN’s IdentiFinder (Bikel, Schwartz, and Weischedel, 1999) and Borthwick’s MENE (Borth- wick, 1999) have shown that machine learning NERs can achieve comparable performance with systems using hand-coded rules.</S>
			<S sid ="25" ssid = "3">Bikel, Schwartz, and Weischedel (1999) have also shown how mixed case text can be automatically converted to upper case SNOR or OCR format to train NERs to work on such formats.</S>
			<S sid ="26" ssid = "4">There is also some work on unsupervised learning for mixed case named entity recognition (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999).</S>
			<S sid ="27" ssid = "5">Collins and Singer (1999) investigated named entity classification using Ad- aboost, CoBoost, and the EM algorithm.</S>
			<S sid ="28" ssid = "6">However, features were extracted using a parser, and performance was evaluated differently (the classes were person, organization, location, and noise).</S>
			<S sid ="29" ssid = "7">Cucerzan and Yarowsky (1999) built a cross language NER, and the performance on English was low compared to supervised single-language NER such as Identi- Finder.</S>
			<S sid ="30" ssid = "8">We suspect that it will be hard for purely unsupervised methods to perform as well as supervised ones.</S>
			<S sid ="31" ssid = "9">Seeger (2001) gave a comprehensive summary of recent work in learning with labeled and unlabeled such as (Blum and Mitchell, 1998; Collins and Singer, 1999; Pierce and Cardie, 2001).</S>
			<S sid ="32" ssid = "10">Most co- training methods involve using two classifiers built on different sets of features.</S>
			<S sid ="33" ssid = "11">Instead of using distinct sets of features, Goldman and Zhou (2000) used different classification algorithms to do co-training.</S>
			<S sid ="34" ssid = "12">Blum and Mitchell (1998) showed that in order for PAC-like guarantees to hold for co-training, features should be divided into two disjoint sets satisfying: (1) each set is sufficient for a classifier to learn a concept correctly; and (2) the two sets are conditionally independent of each other.</S>
			<S sid ="35" ssid = "13">Each set of features can be used to build a classifier, resulting in two independent classifiers, A and B. Classifications by A on unlabeled data can then be used to further train classifier B, and vice versa.</S>
			<S sid ="36" ssid = "14">Intuitively, the independence assumption is there so that the classifications of A would be informative to B. When the independence assumption is violated, the decisions of A may not be informative to B. In this case, the positive effect of having more data may be offset by the negative effect of introducing noise into the data (classifier A might not be always correct).</S>
			<S sid ="37" ssid = "15">Nigam and Ghani (2000) investigated the difference in performance with and without a feature split, and showed that co-training with a feature split gives better performance.</S>
			<S sid ="38" ssid = "16">However, the comparison they made is between co-training and self-training.</S>
			<S sid ="39" ssid = "17">In self-training, only one classifier is used to tag unlabeled data, after which the more confidently tagged data is reused to train the same classifier.</S>
			<S sid ="40" ssid = "18">Many natural language processing problems do not show the natural feature split displayed by the web page classification task studied in previous co- training work.</S>
			<S sid ="41" ssid = "19">Our work does not really fall under the paradigm of co-training.</S>
			<S sid ="42" ssid = "20">Instead of cooperation between two classifiers, we used a stronger classifier to teach a weaker one.</S>
			<S sid ="43" ssid = "21">In addition, it exhibits the following differences: (1) the features are not at all independent (upper case features can be seen as a subset of the mixed case features); and (2) The additional features available to the mixed case system will never be available to the upper case system.</S>
			<S sid ="44" ssid = "22">Co-training often involves combining the two different sets of features to obtain a final system that outperforms either system alone.</S>
			<S sid ="45" ssid = "23">In our context, however, the upper case system will never have access to some of the case-based features available to the mixed case system.</S>
			<S sid ="46" ssid = "24">Due to the above reason, it is unreasonable to expect the performance of the upper case NER to match that of the mixed case NER.</S>
			<S sid ="47" ssid = "25">However, we still manage to achieve a considerable reduction of errors between the two NERs when they are tested on the official MUC6 and MUC7 test data.</S>
	</SECTION>
	<SECTION title="System Description. " number = "3">
			<S sid ="48" ssid = "1">We use the maximum entropy framework to build two classifiers: an upper case NER and a mixed case NER.</S>
			<S sid ="49" ssid = "2">The upper case NER does not have access to case information of the training and test data, and hence cannot make use of all the features used by the mixed case NER.</S>
			<S sid ="50" ssid = "3">We will first describe how the mixed case NER is built.</S>
			<S sid ="51" ssid = "4">More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).</S>
			<S sid ="52" ssid = "5">Our approach is similar to the MENE system of (Borthwick, 1999).</S>
			<S sid ="53" ssid = "6">Each word is assigned a name class based on its features.</S>
			<S sid ="54" ssid = "7">Each name class is subdivided into 4 classes, i.e., N begin, N continue, N end, and N unique.</S>
			<S sid ="55" ssid = "8">Hence, there is a total of 29 classes (7 name classes 4 sub-classes 1 not-a-name class).</S>
			<S sid ="56" ssid = "9">3.1 Maximum Entropy.</S>
			<S sid ="57" ssid = "10">The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible, other than the constraints imposed.</S>
			<S sid ="58" ssid = "11">Such constraints are derived from training data, expressing some relationship between features and outcome.</S>
			<S sid ="59" ssid = "12">The probability distribution that satisfies the above property is the one with the highest entropy.</S>
			<S sid ="60" ssid = "13">It is unique, agrees with the maximum- likelihood distribution, and has the exponential form (Della Pietra, Della Pietra, and Lafferty, 1997): where refers to the outcome, the history (or context), and is a normalization function.</S>
			<S sid ="61" ssid = "14">In addition, each feature function is a binary function.</S>
			<S sid ="62" ssid = "15">For example, in predicting if a word belongs to a word class, is either true or false, and refers to the surrounding context: if = true, previous word = the otherwise The parameters are estimated by a procedure called Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972).</S>
			<S sid ="63" ssid = "16">This is an iterative method that improves the estimation of the parameters at each iteration.</S>
			<S sid ="64" ssid = "17">3.2 Features for Mixed Case NER.</S>
			<S sid ="65" ssid = "18">The features we used can be divided into 2 classes: local and global.</S>
			<S sid ="66" ssid = "19">Local features are features that are based on neighboring tokens, as well as the token itself.</S>
			<S sid ="67" ssid = "20">Global features are extracted from other occurrences of the same token in the whole document.</S>
			<S sid ="68" ssid = "21">Features in the maximum entropy framework are binary.</S>
			<S sid ="69" ssid = "22">Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used.</S>
			<S sid ="70" ssid = "23">We group the features used into feature groups.</S>
			<S sid ="71" ssid = "24">Each group can be made up of many binary features.</S>
			<S sid ="72" ssid = "25">For each token , zero, one, or more of the features in each group are set to 1.</S>
			<S sid ="73" ssid = "26">The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens.</S>
			<S sid ="74" ssid = "27">This feature imposes constraints that are based on the probability of each name class during training.</S>
			<S sid ="75" ssid = "28">Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones).</S>
			<S sid ="76" ssid = "29">The zone to which a token belongs is used as a feature.</S>
			<S sid ="77" ssid = "30">For example, in MUC6, there are four zones (TXT, HL, DATELINE, DD).</S>
			<S sid ="78" ssid = "31">Hence, for each token, one of the four features zone-TXT, zone-HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0.</S>
			<S sid ="79" ssid = "32">Case and Zone: If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1.</S>
			<S sid ="80" ssid = "33">If it is made up of all capital letters, then (allCaps, zone) is set to 1.</S>
			<S sid ="81" ssid = "34">If it contains both upper and lower case letters, then (mixedCaps, zone) is set to 1.</S>
			<S sid ="82" ssid = "35">A token that is allCaps will also be initCaps.</S>
			<S sid ="83" ssid = "36">This group consists of (3 total number of possible zones) features.</S>
			<S sid ="84" ssid = "37">Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, Table 1: Features based on the token string zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.</S>
			<S sid ="85" ssid = "38">For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword.</S>
			<S sid ="86" ssid = "39">If the token is the first word of a sentence, then this feature is set to 1.</S>
			<S sid ="87" ssid = "40">Otherwise, it is set to 0.</S>
			<S sid ="88" ssid = "41">Lexicon Feature: The string of the token is used as a feature.</S>
			<S sid ="89" ssid = "42">This group contains a large number of features (one for each token string present in the training data).</S>
			<S sid ="90" ssid = "43">At most one feature in this group will be set to 1.</S>
			<S sid ="91" ssid = "44">If is seen infrequently during training (less than a small count), then will not selected as a feature and all features in this group are set to 0.</S>
			<S sid ="92" ssid = "45">Lexicon Feature of Previous and Next Token: The string of the previous token and the next token is used with the initCaps information of . If has initCaps, then a feature (initCaps, ) is set to 1.</S>
			<S sid ="93" ssid = "46">If is not initCaps, then (not- initCaps, ) is set to 1.</S>
			<S sid ="94" ssid = "47">Same for . In the case where the next token is a hyphen, then is also used as a feature: (initCaps, ) of hyphens can be considered to be optional (e.g., “third-quarter” or “third quarter”).</S>
			<S sid ="95" ssid = "48">Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1.</S>
			<S sid ="96" ssid = "49">Dictionaries: Due to the limited amount of training material, name dictionaries have been found to be useful in the named entity task.</S>
			<S sid ="97" ssid = "50">The sources of our dictionaries are listed in Table 2.</S>
			<S sid ="98" ssid = "51">A token is tested against the words in each of the four lists of location names, corporate names, person first names, and person last names.</S>
			<S sid ="99" ssid = "52">If is found in a list, the corresponding feature for that list will be set to 1.</S>
			<S sid ="100" ssid = "53">For example, if Barry is found in the list of person first names, then the feature PersonFirstName will be set to 1.</S>
			<S sid ="101" ssid = "54">Similarly, the tokens and are tested against each list, and if found, a corresponding feature will be set to 1.</S>
			<S sid ="102" ssid = "55">For example, if is found in the list of person first names, the feature PersonFirstName is set to 1.</S>
			<S sid ="103" ssid = "56">Month Names, Days of the Week, and Numbers: If is one of January, February, . . .</S>
			<S sid ="104" ssid = "57">, December, then the feature MonthName is set to 1.</S>
			<S sid ="105" ssid = "58">If is one of Monday, Tuesday, . . .</S>
			<S sid ="106" ssid = "59">, Sunday, then the feature DayOfTheWeek is set to 1.</S>
			<S sid ="107" ssid = "60">If is a number string (such as one, two, etc), then the feature NumberString is set to 1.</S>
			<S sid ="108" ssid = "61">Suffixes and Prefixes: This group contains only two features: Corporate-Suffix and Person-Prefix.</S>
			<S sid ="109" ssid = "62">Two lists, Corporate-Suffix-List (for corporate suffixes) and Person-Prefix-List (for person prefixes), are collected from the training data.</S>
			<S sid ="110" ssid = "63">For a token that is in a consecutive sequence of initCaps tokens , if any of the tokens from to is in Corporate-Suffix-List, then a fea ture Corporate-Suffix is set to 1.</S>
			<S sid ="111" ssid = "64">If any of the tokens from to is in Person-Prefix-List, then another feature Person-Prefix is set to 1.</S>
			<S sid ="112" ssid = "65">Note that we check for , the word preceding the consecutive sequence of initCaps tokens, since person prefixes like Mr., Dr. etc are not part of person names, whereas corporate suffixes like Corp., Inc. etc are part of corporate names.</S>
			<S sid ="113" ssid = "66">The global feature groups are: InitCaps of Other Occurrences: There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous posi De scr ipt io n So ur ce Lo cat io n Na m es h t t p : / / w w w . t i m e a n d d a t e . c o m h t t p : / / w w w . c i t y g u i d e . t r a v e l g u i d e s . c o m h t t p : / / w w w . w o r l d t r a v e l g u i d e . n e t Co rp or ate Na m es htt p:/ /w w w. fm lx.</S>
			<S sid ="114" ssid = "67">co m Pe rs on Fir st Na m es htt p:/ /w w w. ce ns us.</S>
			<S sid ="115" ssid = "68">go v/ ge ne al og y/ na m es Pe rs on La st Na m es Table 2: Sources of Dictionaries tion (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.</S>
			<S sid ="116" ssid = "69">For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.</S>
			<S sid ="117" ssid = "70">Corporate Suffixes and Person Prefixes of Other Occurrences: With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1.</S>
			<S sid ="118" ssid = "71">Acronyms: Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM).</S>
			<S sid ="119" ssid = "72">The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.</S>
			<S sid ="120" ssid = "73">Such sequences are given additional features of A begin, A continue, or A end, and the acronym is given a feature A unique.</S>
			<S sid ="121" ssid = "74">For example, if “FCC” and “Federal Communications Commission” are both found in a document, then “Federal” has A begin set to 1, “Communications” has A continue set to 1, “Commission” has A end set to 1, and “FCC” has A unique set to 1.</S>
			<S sid ="122" ssid = "75">Sequence of Initial Caps: In the sentence “Even News Broadcasting Corp., noted for its accurate reporting, made the erroneous announcement.”, a NER may mistake “Even News Broadcasting Corp.” as an organization name.</S>
			<S sid ="123" ssid = "76">However, it is unlikely that other occurrences of “News Broadcasting Corp.” in the same document also co-occur with “Even”.</S>
			<S sid ="124" ssid = "77">This group of features attempts to capture such information.</S>
			<S sid ="125" ssid = "78">For every sequence of initial capitalized words, its longest substring that occurs in the same document is identified.</S>
			<S sid ="126" ssid = "79">For this example, since the sequence “Even News Broadcasting Corp.” only appears once in the document, its longest substring that occurs in the same document is “News Broadcasting Corp.”.</S>
			<S sid ="127" ssid = "80">In this case, “News” has an additional feature of I begin set to 1,“Broadcasting” has an additional feature of I continue set to 1, and “Corp.” has an additional feature of I end set to 1.</S>
			<S sid ="128" ssid = "81">Unique Occurrences and Zone: This group of features indicates whether the word is unique in the whole document.</S>
			<S sid ="129" ssid = "82">needs to be in initCaps to be considered for this feature.</S>
			<S sid ="130" ssid = "83">If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears.</S>
			<S sid ="131" ssid = "84">3.3 Features for Upper Case NER.</S>
			<S sid ="132" ssid = "85">All features used for the mixed case NER are used by the upper case NER, except those that require case information.</S>
			<S sid ="133" ssid = "86">Among local features, Case and Zone, InitCap- Period, and OneCap are not used by the upper case NER.</S>
			<S sid ="134" ssid = "87">Among global features, only Other-CS and Other-PP are used for the upper case NER, since the other global features require case information.</S>
			<S sid ="135" ssid = "88">For Corporate-Suffix and Person-Prefix, as the sequence of initCaps is not available in upper case text, only the next word (previous word) is tested for Corporate-Suffix (Person-Prefix).</S>
			<S sid ="136" ssid = "89">3.4 Testing.</S>
			<S sid ="137" ssid = "90">During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., person begin followed by location unique).</S>
			<S sid ="138" ssid = "91">To eliminate such sequences, we define a transition probability between word classes to be equal to 1 if the sequence is admissible, and 0 otherwise.</S>
			<S sid ="139" ssid = "92">The probability of the classes assigned to the words in a sentence in a document is defined as follows: Figure 2: The whole process of retraining the upper case NER.</S>
			<S sid ="140" ssid = "93">signifies that the text is converted to upper case before processing.</S>
			<S sid ="141" ssid = "94">where is determined by the maximum entropy classifier.</S>
			<S sid ="142" ssid = "95">A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability.</S>
	</SECTION>
	<SECTION title="Teaching Process. " number = "4">
			<S sid ="143" ssid = "1">The teaching process is illustrated in Figure 2.</S>
			<S sid ="144" ssid = "2">This process can be divided into the following steps: Training NERs.</S>
			<S sid ="145" ssid = "3">First, a mixed case NER (MNER) is trained from some initial corpus , manually tagged with named entities.</S>
			<S sid ="146" ssid = "4">This corpus is also converted to upper case in order to train another upper case NER (UNER).</S>
			<S sid ="147" ssid = "5">UNER is required by our method of example selection.</S>
			<S sid ="148" ssid = "6">Baseline Test on Unlabeled Data.</S>
			<S sid ="149" ssid = "7">Apply the trained MNER on some unlabeled mixed case texts to produce mixed case texts that are machine-tagged with named entities (text-mner-tagged).</S>
			<S sid ="150" ssid = "8">Convert the original unlabeled mixed case texts to upper case, and similarly apply the trained UNER on these texts to obtain upper case texts machine-tagged with named entities (text-uner-tagged).</S>
			<S sid ="151" ssid = "9">Example Selection.</S>
			<S sid ="152" ssid = "10">Compare text-mner-tagged and text-uner-tagged and select tokens in which the classification by MNER differs from that of UNER.</S>
			<S sid ="153" ssid = "11">The class assigned by MNER is considered to be correct, and will be used as new training data.</S>
			<S sid ="154" ssid = "12">These tokens are collected into a set . Retraining for Final Upper Case NER.</S>
			<S sid ="155" ssid = "13">Both and are used to retrain an upper case NER.</S>
			<S sid ="156" ssid = "14">However, tokens from are given a weight of 2 (i.e., each token is used twice in the training data), and tokens from a weight of 1, since is more reliable than (human-tagged versus machine-tagged).</S>
	</SECTION>
	<SECTION title="Experimental Results. " number = "5">
			<S sid ="157" ssid = "1">For manually labeled data (corpus C), we used only the official training data provided by the MUC6 and MUC7 conferences, i.e., using MUC6 training data and testing on MUC6 test data, and using MUC7 training data and testing on MUC7 test data.1 The task definitions for MUC6 and MUC 7 are not exactly identical, so we could not combine the training data.</S>
			<S sid ="158" ssid = "2">The original MUC6 training data has a total of approximately 160,000 tokens and 1 MUC data can be obtained from the Linguistic Data Consortium: http://www.ldc.upenn.edu Figure 3: Improvements in F-measure on MUC6 plotted against amount of selected unlabeled data used MUC7 a total of approximately 180,000 tokens.</S>
			<S sid ="159" ssid = "3">The unlabeled text is drawn from the TREC (Text REtrieval Conference) corpus, 1992 Wall Street Journal section.</S>
			<S sid ="160" ssid = "4">We have used a total of 4,893 articles with a total of approximately 2,161,000 tokens.</S>
			<S sid ="161" ssid = "5">After example selection, this reduces the number of tokens to approximately 46,000 for MUC6 and 67,000 for MUC7.</S>
			<S sid ="162" ssid = "6">Figure 3 and Figure 4 show the results for MUC6 and MUC7 obtained, plotted against the number of unlabeled instances used.</S>
			<S sid ="163" ssid = "7">As expected, it increases the recall in each domain, as more names or their contexts are learned from unlabeled data.</S>
			<S sid ="164" ssid = "8">However, as more unlabeled data is used, precision drops due to the noise introduced in the machine tagged data.</S>
			<S sid ="165" ssid = "9">For MUC6, F-measure performance peaked at the point where 30,000 tokens of machine labeled data are added to the original manually tagged 160,000 tokens.</S>
			<S sid ="166" ssid = "10">For MUC7, performance peaked at 20,000 tokens of machine labeled data, added to the original manually tagged 180,000 tokens.</S>
			<S sid ="167" ssid = "11">The improvements achieved are summarized in Table 3.</S>
			<S sid ="168" ssid = "12">It is clear from the table that this method of using unlabeled data brings considerable improvement for both MUC6 and MUC7 named entity task.</S>
			<S sid ="169" ssid = "13">The result of the teaching process for MUC6 is a lot better than that of MUC7.</S>
			<S sid ="170" ssid = "14">We think that this isFigure 4: Improvements in F-measure on MUC 7 plotted against amount of selected unlabeled data used Table 3: F-measure on MUC6 and MUC7 test data due to the following reasons: Better Mixed Case NER for MUC6 than MUC7.</S>
			<S sid ="171" ssid = "15">The mixed case NER trained on the MUC 6 officially released training data achieved an F- measure of 93.27% on the official MUC6 test data, while that of MUC7 (also trained on only the official MUC7 training data) achieved an F measure of only 87.24%.</S>
			<S sid ="172" ssid = "16">As the mixed case NER is used as the teacher, a bad teacher does not help as much.</S>
			<S sid ="173" ssid = "17">Domain Shift in MUC7.</S>
			<S sid ="174" ssid = "18">Another possible cause is that there is a domain shift in MUC7 for the formal test (training articles are aviation disasters articles and test articles are missile/rocket launch articles).</S>
			<S sid ="175" ssid = "19">The domain of the MUC7 test data is also very specific, and hence it might exhibit different properties from the training and the unlabeled data.</S>
			<S sid ="176" ssid = "20">The Source of Unlabeled Data.</S>
			<S sid ="177" ssid = "21">The unlabeled data used is from the same source as MUC6, but different for MUC7 (MUC6 articles and the unlabeled articles are all Wall Street Journal articles, whereas MUC7 articles are New York Times articles).</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="178" ssid = "1">In this paper, we have shown that the performance of NERs on upper case text can be improved by using a mixed case NER with unlabeled text.</S>
			<S sid ="179" ssid = "2">Named entity recognition on mixed case text is easier than on upper case text, where case information is unavailable.</S>
			<S sid ="180" ssid = "3">By using the teaching process, we can reduce the performance gap between mixed and upper case NER by as much as 39% for MUC6 and 22% for MUC7.</S>
			<S sid ="181" ssid = "4">This approach can be used to improve the performance of NERs on speech recognition output, or even for other tasks such as part-of-speech tagging, where case information is helpful.</S>
			<S sid ="182" ssid = "5">With the abundance of unlabeled text available, such an approach requires no additional annotation effort, and hence is easily applicable.</S>
			<S sid ="183" ssid = "6">This way of teaching a weaker classifier can also be used in other domains, where the task is to infer , and an abundance of unlabeled data is available.</S>
			<S sid ="184" ssid = "7">If one possesses a second classifier such that provides additional “useful” information that can be utilized by this second classifier, then one can use this second classifier to automatically tag the unlabeled data , and select from examples that can be used to supplement the training data for training .</S>
	</SECTION>
</PAPER>
