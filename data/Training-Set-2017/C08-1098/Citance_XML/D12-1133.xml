<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.</S>
		<S sid ="2" ssid = "2">We present a transition- based system for joint part-of-speech tagging and labeled dependency parsing with non- projective trees.</S>
		<S sid ="3" ssid = "3">Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-the- art results for all languages.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning.</S>
			<S sid ="5" ssid = "5">Graph- based models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010).</S>
			<S sid ="6" ssid = "6">Transition- based models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011).</S>
			<S sid ="7" ssid = "7">Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-the- art accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011).</S>
			<S sid ="8" ssid = "8">It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger.</S>
			<S sid ="9" ssid = "9">This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so.</S>
			<S sid ="10" ssid = "10">This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing.</S>
			<S sid ="11" ssid = "11">It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other.</S>
			<S sid ="12" ssid = "12">Thus, Lee et al.</S>
			<S sid ="13" ssid = "13">(2011) show that a discriminative model for joint morphological disambiguation and dependency parsing outperforms a pipeline model in experiments on Latin, Ancient Greek, Czech and Hungarian.</S>
			<S sid ="14" ssid = "14">However, Li et al.</S>
			<S sid ="15" ssid = "15">(2011) and Hatori et al.</S>
			<S sid ="16" ssid = "16">(2011) report improvements with a joint model also for Chinese, which is not a richly inflected language but is nevertheless rich in part-of-speech ambiguities.</S>
			<S sid ="17" ssid = "17">In this paper, we present a transition-based model for joint part-of-speech tagging and labeled dependency parsing with non-projective trees.</S>
			<S sid ="18" ssid = "18">Exper 1455 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1455–1465, Jeju Island, Korea, 12–14 July 2012.</S>
			<S sid ="19" ssid = "19">Qc 2012 Association for Computational Linguistics iments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English.</S>
			<S sid ="20" ssid = "20">To our knowledge, this is the first joint system that performs labeled dependency parsing.</S>
			<S sid ="21" ssid = "21">It is also the first joint system that achieves state-of-the-art accuracy for non-projective dependency parsing.</S>
	</SECTION>
	<SECTION title="Transition-Based Tagging and Parsing. " number = "2">
			<S sid ="22" ssid = "1">Transition-based dependency parsing was pioneered by Yamada and Matsumoto (2003) and Nivre et al.</S>
			<S sid ="23" ssid = "2">(2004), who used classifiers trained to predict individual actions of a deterministic shift-reduce parser.</S>
			<S sid ="24" ssid = "3">Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Clark, 2011; Zhang and Nivre, 2011; Bohnet, 2011).</S>
			<S sid ="25" ssid = "4">In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early systems (Attardi, 2006; Nivre, 2007; Nivre, 2009; Titov et al., 2009).</S>
			<S sid ="26" ssid = "5">In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009).</S>
			<S sid ="27" ssid = "6">We then show how to perform beam search and structured online learning with this model, and conclude by discussing feature representations.</S>
			<S sid ="28" ssid = "7">2.1 Transition System.</S>
			<S sid ="29" ssid = "8">Given a set P of part-of-speech tags and a set D of dependency labels, a tagged dependency tree for artificial root node 0.</S>
			<S sid ="30" ssid = "9">The set A of arcs is a set of pairs (i, j), where i is the head node and j is the dependent node.</S>
			<S sid ="31" ssid = "10">The functions π and δ assign a unique part-of-speech label to each node/word and a unique dependency label to each arc, respectively.</S>
			<S sid ="32" ssid = "11">This notion of dependency tree differs from the standard definition only by including part-of-speech labels as well as dependency labels (Ku¨ bler et al., 2009).</S>
			<S sid ="33" ssid = "12">Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S = (C, T , cs, Ct), where 1.</S>
			<S sid ="34" ssid = "13">C is a set of configurations, 2.</S>
			<S sid ="35" ssid = "14">T is a set of transitions, each of which is a (partial) function t : C → C , 3.</S>
			<S sid ="36" ssid = "15">cs is an initialization function, mapping a sentence x to a configuration c ∈ C , 4.</S>
			<S sid ="37" ssid = "16">Ct ⊆ C is a set of terminal configurations..</S>
			<S sid ="38" ssid = "17">A transition sequence for a sentence x in S is a sequence of configuration-transition pairs C0,m = [(c0, t0), (c1, t1), . . .</S>
			<S sid ="39" ssid = "18">, (cm, tm)] where c0 = cs(x), tm(cm) ∈ Ct and ti(ci) = ci+1 (0 ≤ i &lt; m).1 In this paper, we take the set C of configurations to be the set of all 5tuples c = (Σ, B, A, π, δ) such that Σ (the stack) and B (the buffer) are disjoint sublists of the nodes Vx of some sentence x, A is a set of dependency arcs over Vx, and π and δ are labeling functions as defined above.</S>
			<S sid ="40" ssid = "19">We take the initial configuration for a sentence x = w1, . . .</S>
			<S sid ="41" ssid = "20">, wn to be cs(x) = ([0], [1, . . .</S>
			<S sid ="42" ssid = "21">, n], { }, ⊥, ⊥), where ⊥ is the function that is undefined for all arguments, and we take the set Ct of terminal configurations to be the set of all configurations of the form c = ([0], [ ], A, π, δ) (for any A, π and δ).</S>
			<S sid ="43" ssid = "22">The tagged dependency tree defined for x by c = (Σ, B, A, π, δ) a sentence x = w1, . . .</S>
			<S sid ="44" ssid = "23">, wn is a directed tree T = (Vx, A) with labeling functions π and δ such that: is the tree (Vx , A) with labeling functions π and δ, 1.</S>
			<S sid ="45" ssid = "24">Vx = {0, 1, . . .</S>
			<S sid ="46" ssid = "25">, n} is a set of nodes,.</S>
			<S sid ="47" ssid = "26">2.</S>
			<S sid ="48" ssid = "27">A ⊆ Vx × Vx is a set of arcs, 3.</S>
			<S sid ="49" ssid = "28">π : Vx → P is a labeling function for nodes, 4.</S>
			<S sid ="50" ssid = "29">δ : A → D is a labeling function for arcs, 5.</S>
			<S sid ="51" ssid = "30">0 is the root of the tree.</S>
			<S sid ="52" ssid = "31">The set Vx of nodes is the set of positive integers up to and including n, each corresponding to the linear position of a word in the sentence, plus an extra which we write TREE(x, c).</S>
			<S sid ="53" ssid = "32">The set T of transitions is shown in Figure 1.</S>
			<S sid ="54" ssid = "33">The LEFT-ARCd and RIGHT-ARCd transitions both add an arc (with dependency label d) between the two nodes on top of the stack and replaces these nodes by the head node of the new arc (which is the rightmost node for LEFT-ARCd and the leftmost node for RIGHT-ARCd).</S>
			<S sid ="55" ssid = "34">The SHIFTp transition extracts the 1 This definition of transition sequence differs from that of.</S>
			<S sid ="56" ssid = "35">Nivre (2008) but is equivalent and suits our presentation better.</S>
			<S sid ="57" ssid = "36">T r a n s it i o n C o n d i t i o n L E F T A R C d R I G H T A R C d S H I F T p S W A P ( [ σ | i , j ] , B , A , π , δ ) ⇒ ( [ σ | j ] , B , A ∪ { ( j , i ) } , π , δ [ ( j , i ) → d ] ) ( [ σ | i , j ] , B , A , π , δ ) ⇒ ( [ σ | i ] , B , A ∪ { ( i , j ) } , π , δ [ ( i , j ) → d ] ) ( σ , [ i | β ] , A , π , δ ) ⇒ ( [ σ | i ] , β , A , π [ i → p ] , δ ) ( [ σ | i , j ] , β , A , π , δ ) ⇒ ( [ σ | j ] , [ i | β ] , A , π , δ ) i / = 0 0 &lt; i &lt; j Figure 1: Transitions for joint tagging and dependency parsing extending the system of Nivre (2009).</S>
			<S sid ="58" ssid = "37">The stack Σ is represented as a list with its head to the right (and tail σ) and the buffer B as a list with its head to the left (and tail β).</S>
			<S sid ="59" ssid = "38">The notation f [a → b] is used to denote the function that is exactly like f except that it maps a to b. first node in the buffer, pushes it onto the stack and PARSE(x, w, b1, b2) lab els it wit h the part-of spe ech tag p. Th e SW AP 1 h0.</S>
			<S sid ="60" ssid = "39">c ← cs( x) tran sitio n extr act s the sec ond top mo st nod e fro m the 2 h0.</S>
			<S sid ="61" ssid = "40">s ← 0.0 sta ck and mo ves it bac k to the buff er, sub ject to the 3 h0.</S>
			<S sid ="62" ssid = "41">f ← {0.</S>
			<S sid ="63" ssid = "42">0}d im( w) con diti on that the two top nod es on the sta ck are still 4 BE AM ← [h0 ] in the ord er giv en by the se nte nc e. 5 wh ile ∃h ∈ BE AM : h.c /∈ Ct E x c e p t f o r t h e a d d it i o n o f a t a g p a r a m e t e r p t o 6 T M P ← [ ] the SHI FT tra nsit ion, thi s is equ ival ent to the sys 7 f o r e a c h h ∈ B E A M tem des crib ed in Niv re (20 09) , whi ch tha nks to the 8 f o r e a c h t ∈ T : P E R M I S S I B L E ( h . c , t ) SW AP tra nsit ion can han dle arbi trar y non pro ject ive 9 h . f ← h . f + f ( h . c , t ) tree s. The sou ndn ess and co mpl ete nes s res ults giv 1 0 h . s ← h . s + f ( h . c , t ) · w en in that pap er trivi ally car ry ove r to the ne w sys 1 1 h . c ← t ( h . c ) tem . Th e onl y thin g to not e is that , bef ore a ter mi nal 1 2 T M P ← I N S E R T ( h , T M P ) configuratio n can be reached, every word has to be 13 BEAM ← PRUNE(TMP, b1, b2) pus hed ont o the sta ck in a SHI FTp tra nsit ion, whi ch 1 4 h ← TO P( BE AM ) ens ure s that eve ry nod e/w ord in the out put tree will 1 5 y ← TR EE( x, h.c ) be tag ge d. 1 6 ret ur n y 2.2 Inference and Learning.</S>
			<S sid ="64" ssid = "43">While early transition-based parsers generally used greedy best-first inference and locally trained classifiers, recent work has shown that higher accuracy can be obtained using beam search and global structure learning to mitigate error propagation.</S>
			<S sid ="65" ssid = "44">In particular, it seems that the globally learned models can exploit a much richer feature space than locally trained classifiers, as shown by Zhang and Nivre (2011).</S>
			<S sid ="66" ssid = "45">Since joint tagging and parsing increases the size of the search space and is likely to require novel features, we use beam search in combination with structured perceptron learning.</S>
			<S sid ="67" ssid = "46">The beam search algorithm used to derive the best parse y for a sentence x is outlined in Figure 2.</S>
			<S sid ="68" ssid = "47">In addition to the sentence x, it takes as input a weight vector w corresponding to a linear model for scoring transitions out of configurations and two prun Figure 2: Beam search algorithm for joint tagging and dependency parsing of input sentence x with weight vector w and beam parameters b1 and b2 . The symbols h.c, h.s and h.f denote, respectively, the configuration, score and feature representation of a hypothesis h; h.c.A denotes the arc set of h.c. ing parameters b1 and b2.</S>
			<S sid ="69" ssid = "48">A parse hypothesis h is represented by a configuration h.c, a score h.s and a feature vector h.f for the transition sequence up to h.c. Hypotheses are stored in the list BEAM, which is sorted by descending scores and initialized to hold the hypothesis h0 corresponding to the initial configuration cs(x) with score 0.0 and all features set to 0.0 (lines 1–4).</S>
			<S sid ="70" ssid = "49">In the main loop (lines 5–13), a set of new hypotheses is derived and stored in the list TMP, which is finally pruned and assigned as the new value of BEAM.</S>
			<S sid ="71" ssid = "50">The main loop terminates when all hypotheses in BEAM contain terminal configurations, and the dependency tree extracted from the top scoring hypothesis is returned (lines 14–16).</S>
			<S sid ="72" ssid = "51">The set of new hypotheses is created in two nested loops (lines 7–12), where every hypothesis h in BEAM is updated using every permissible transition t for the configuration h.c. The feature representation of the new hypothesis is obtained by adding the feature vector f(t, h.c) for the current configuration- transition pair to the feature vector of the old hypothesis (line 9).</S>
			<S sid ="73" ssid = "52">Similarly, the score of the new ly dependency labels or tags.</S>
			<S sid ="74" ssid = "53">Thus, in the experiments later on, we will typically constrain the parser so that SHIFTp is permissible only if p is one of the k best part-of-speech tags with a score no more than α below the score of the 1-best tag, as determined by a preprocessing tagger.</S>
			<S sid ="75" ssid = "54">We also filter out instances of LEFT-ARCd and RIGHT-ARCd, where d does not occur in the training data for the predicted part-of- speech tag combination of the head and dependent.</S>
			<S sid ="76" ssid = "55">This procedure leads to a significant speed up.</S>
			<S sid ="77" ssid = "56">In order to learn a weight vector w from a training hypothesis is the sum of the score f(t, h.c) · w of set {(xj , yj )}Tof sentences with their tagged dethe current configuration-transition pair and the s core of the old hypothesis (line 10).</S>
			<S sid ="78" ssid = "57">The feature representation/score of a complete parse y for x with transition sequence C0,m is thus the sum of the feature representations/scores of the configuration- transition pairs in C0,m: pendency trees, we use a variant of the structured perceptron, introduced by Collins (2002), which makes N iterations over the training data and updates the weight vector for every sentence xj where the highest scoring parse y∗ is different from yj .More precisely, we use the passive-aggressive up date of Crammer et al.</S>
			<S sid ="79" ssid = "58">(2006): f(x, y) = ) (c,t)∈C0,m s(x, y) = ) (c,t)∈C0,m f(c, t) f(c, t) · w where wi+1 = wi + τ (f(xj , yj ) − f(xj , y∗)) f(xj , yj ) − f(xj , y∗) Finally, the configuration of the new hypothesis is obtained by evaluating t(h.c) (line 11).</S>
			<S sid ="80" ssid = "59">The new hypothesis is then inserted into TMP in score-sorted order (line 12).</S>
			<S sid ="81" ssid = "60">The pruning parameters b1 and b2 determine the number of hypotheses allowed in the beam and at the same time control the tradeoff between syntactic and morphological ambiguity.</S>
			<S sid ="82" ssid = "61">First, we extract the b1 highest scoring hypotheses with distinct dependency trees.</S>
			<S sid ="83" ssid = "62">Then we extract the b2 highest scoring remaining hypotheses, which will typically be tagging variants of dependency trees that are already in the beam.</S>
			<S sid ="84" ssid = "63">In this way, we prevent the beam from getting filled up with too many tagging variants of the same dependency tree, which was found to be harmful in preliminary experiments.</S>
			<S sid ="85" ssid = "64">One final thing to note about the inference algorithm is that the notion of permissibility for a transition t out of a configuration c can be used to capture not only formal constraints on transitions – such as the fact that it is impossible to perform a SHIFTp transition with an empty buffer or illegal to perform a LEFT-ARCd transition with the special root node on top of the stack – but also to filter out unlike τ = ||f(xj , yj ) − f(xj , y∗)||2 We also use the early update strategy found beneficial for parsing in several previous studies (Collins and Roark, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010), which means that, during learning, we terminate the beam search as soon as the hypothesis corresponding to the gold parse yj falls out of the beam and update with respect to the partial transition sequence constructed up to that point.</S>
			<S sid ="86" ssid = "65">Finally, we use the standard technique of averaging over all weight vectors, as originally proposed by Collins (2002).</S>
			<S sid ="87" ssid = "66">2.3 Feature Representations.</S>
			<S sid ="88" ssid = "67">As already noted, the feature representation f(x, y) of an input sentence x with parse y decomposes into feature representations f(c, t) for the transitions t(c) needed to derive y from cs(x).</S>
			<S sid ="89" ssid = "68">Features may refer to any aspect of a configuration, as encoded in the stack Σ, the buffer B, the arc set A and the labelings π and δ.</S>
			<S sid ="90" ssid = "69">In addition, we assume that each word w in the input is assigned up to k candidate part-of-speech tags πi(w) with corresponding scores s(πi(w)).</S>
			<S sid ="91" ssid = "70">Fea ture s inv olvi ng wor d pref ixe s and suff ixe s πi (B0 )p2 (B0 ), πi (B0 )s2 (B0 ), πi (B0 )p1 (B0 )p1 (Σ0 ) πi (Σ0 )p1 (Σ0 )p1 (Σ1 ), πi (Σ0 )s1 (Σ0 )s1 (Σ0 ) πi (Σ0 )p2 (Σ0 )s3 (Σ1 ),πi (Σ0 )s3 (Σ0 )p2 (Σ1 ) πi (Σ0 )w( B0 )s1 (Σ0 ), πi (Σ0 )w( B0 )s2 (Σ0 ) Fea ture s inv olvi ng tag sco re diff ere nce s and ran ks πi (B0 )[s( π1 (B0 )) − s(π i (B0 ))] πi (B0 )πi (Σ0 )[s( π1 (B0 )) − s(π i (B0 ))] i πi (B0 )[s( π1 (B0 )) − s(π i (B0 ))]π (Σ0 ) w( B0 )[s( π1 (B0 )) − s(π i (B0 ))]π (Σ0 ) Figure 3: Specialized feature templates for tagging.</S>
			<S sid ="92" ssid = "71">We use Σi and Bi to denote the ith token in the stack Σ and buffer B, respectively, with indexing starting at 0, and we use the following functors to extract properties of a token: πi () = ith best tag; s(πi ()) = score of ith best tag; π() = finally predicted tag; w() = word form; pi () = word prefix of i characters; si () = word suffix of i characters.</S>
			<S sid ="93" ssid = "72">Score differences are binned in discrete steps of 0.05.</S>
			<S sid ="94" ssid = "73">The bulk of features used in our system are taken from Zhang and Nivre (2011), although with two important differences.</S>
			<S sid ="95" ssid = "74">First of all, like Hatori et al.</S>
			<S sid ="96" ssid = "75">(2011), we have omitted all features that presuppose an arc-eager parsing order, since our transition system defines an arc-standard order.</S>
			<S sid ="97" ssid = "76">Secondly, any feature that refers to the part-of-speech tag of a word w in the buffer B will in our system refer to the top- scoring tag π1(w), rather than the finally predicted tag.</S>
			<S sid ="98" ssid = "77">By contrast, for a word in the stack Σ, part-of- speech features refer to the tag π(w) chosen when shifting w onto the stack (which may or may not be the same as π1(w)).</S>
			<S sid ="99" ssid = "78">In addition to the standard features for transition- based dependency parsing, we have added features specifically to improve the tagging step in the joint model.</S>
			<S sid ="100" ssid = "79">The templates for these features, which are specified in Figure 3, all involve the ith best tag assigned to the first word of the buffer B (the next word to be shifted in a SHIFTp transition) in combination with neighboring words, word prefixes, word suffixes, score differences and tag rank.</S>
			<S sid ="101" ssid = "80">limited to certain first- and second-order factors, we use features over second- and third-order factors as found in the parsers of Bohnet and Kuhn (2012).</S>
			<S sid ="102" ssid = "81">These features are scored as soon as the factors are completed, using a technique that is similar to what Hatori et al.</S>
			<S sid ="103" ssid = "82">(2011) call delayed features, although they use it for part-of-speech tags in the lookahead while we use it for subgraphs of the dependency tree.</S>
			<S sid ="104" ssid = "83">Cluster features, finally, are features over word clusters, as first used by Koo et al.</S>
			<S sid ="105" ssid = "84">(2008), which replace part-of-speech tag features.2 We use a hash kernel to map features to weights.</S>
			<S sid ="106" ssid = "85">It has been observed that most of the computing time in feature-rich parsers is spent retrieving the index of each feature in the weight vector (Bohnet, 2010).</S>
			<S sid ="107" ssid = "86">This is usually done via a hash table, but significant speedups can be achieved by using a hash kernel, which simply replaces table lookup by a hash function (Bloom, 1970; Shi et al., 2009; Bohnet, 2010).</S>
			<S sid ="108" ssid = "87">The price to pay for these speedups is that there may be collisions, so that different features are mapped to the same index, but this is often compensated by the fact that the lower time and memory requirements of the hash kernel enables the use of negative features, that is, features that are never seen in the training set but occur in erroneous hypotheses at training time and can therefore be helpful also at inference time.</S>
			<S sid ="109" ssid = "88">As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010).</S>
	</SECTION>
	<SECTION title="Experiments. " number = "3">
			<S sid ="110" ssid = "1">We have evaluated the model for joint tagging and dependency parsing on four typologically diverse languages: Chinese, Czech, English, and German.</S>
			<S sid ="111" ssid = "2">3.1 Setup.</S>
			<S sid ="112" ssid = "3">Most of the experiments use the CoNLL 2009 data sets with the training, development and test s Finally, in some experiments, we make use of two plit used in the Shared Task (Hajicˇ et al., 2009), additional feature sets, which we call graph features (G) and cluster features (C), respectively.</S>
			<S sid ="113" ssid = "4">Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008).</S>
			<S sid ="114" ssid = "5">However, while their features were but for better comparison with previous work we also report results for the standard benchmark data sets for Chinese and English.</S>
			<S sid ="115" ssid = "6">For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted 2 For replicability, a complete description of all features can be found at http://stp.lingfil.uu.se/∼nivre/exp/emnlp12.html.</S>
			<S sid ="116" ssid = "7">Par ser k α C h i n e s e TLA S LAS UAS POS C z e c h TLA S LAS UAS POS E n g l i s h TLA S LAS UAS POS G e r m a n TLA S LAS UAS POS 1 0.0 73.</S>
			<S sid ="117" ssid = "8">85 76.</S>
			<S sid ="118" ssid = "9">12 80.</S>
			<S sid ="119" ssid = "10">01 92.</S>
			<S sid ="120" ssid = "11">78 82.</S>
			<S sid ="121" ssid = "12">36 82.</S>
			<S sid ="122" ssid = "13">65 88.</S>
			<S sid ="123" ssid = "14">03 93.</S>
			<S sid ="124" ssid = "15">26 85.</S>
			<S sid ="125" ssid = "16">82 87.</S>
			<S sid ="126" ssid = "17">17 90.</S>
			<S sid ="127" ssid = "18">41 97.</S>
			<S sid ="128" ssid = "19">32 85.</S>
			<S sid ="129" ssid = "20">08 86.</S>
			<S sid ="130" ssid = "21">60 89.</S>
			<S sid ="131" ssid = "22">17 97.</S>
			<S sid ="132" ssid = "23">24 2 0.1 74.</S>
			<S sid ="133" ssid = "24">39 76.</S>
			<S sid ="134" ssid = "25">52 80.</S>
			<S sid ="135" ssid = "26">41 93.</S>
			<S sid ="136" ssid = "27">37 82.</S>
			<S sid ="137" ssid = "28">74 83.</S>
			<S sid ="138" ssid = "29">01 88.</S>
			<S sid ="139" ssid = "30">34 99.</S>
			<S sid ="140" ssid = "31">39 86.</S>
			<S sid ="141" ssid = "32">43 87.</S>
			<S sid ="142" ssid = "33">79 91.</S>
			<S sid ="143" ssid = "34">02 97.</S>
			<S sid ="144" ssid = "35">49 86.</S>
			<S sid ="145" ssid = "36">12 87.</S>
			<S sid ="146" ssid = "37">22 89.</S>
			<S sid ="147" ssid = "38">69 97.</S>
			<S sid ="148" ssid = "39">85 3 0.1 3 0.2 3 0.3 3 0.4 74.</S>
			<S sid ="149" ssid = "40">47 76.</S>
			<S sid ="150" ssid = "41">63 80.</S>
			<S sid ="151" ssid = "42">50 93.</S>
			<S sid ="152" ssid = "43">38 74.</S>
			<S sid ="153" ssid = "44">35 76.</S>
			<S sid ="154" ssid = "45">48 80.</S>
			<S sid ="155" ssid = "46">38 93.</S>
			<S sid ="156" ssid = "47">43 74.</S>
			<S sid ="157" ssid = "48">18 76.</S>
			<S sid ="158" ssid = "49">33 80.</S>
			<S sid ="159" ssid = "50">28 93.</S>
			<S sid ="160" ssid = "51">48 82.</S>
			<S sid ="161" ssid = "52">76 82.</S>
			<S sid ="162" ssid = "53">97 88.</S>
			<S sid ="163" ssid = "54">33 99.</S>
			<S sid ="164" ssid = "55">40 82.</S>
			<S sid ="165" ssid = "56">85 83.</S>
			<S sid ="166" ssid = "57">11 88.</S>
			<S sid ="167" ssid = "58">44 99.</S>
			<S sid ="168" ssid = "59">32 82.</S>
			<S sid ="169" ssid = "60">78 83.</S>
			<S sid ="170" ssid = "61">05 88.</S>
			<S sid ="171" ssid = "62">38 99.</S>
			<S sid ="172" ssid = "63">33 86.</S>
			<S sid ="173" ssid = "64">40 87.</S>
			<S sid ="174" ssid = "65">78 90.</S>
			<S sid ="175" ssid = "66">99 97.</S>
			<S sid ="176" ssid = "67">43 86.</S>
			<S sid ="177" ssid = "68">35 87.</S>
			<S sid ="178" ssid = "69">79 91.</S>
			<S sid ="179" ssid = "70">01 97.</S>
			<S sid ="180" ssid = "71">52 85.</S>
			<S sid ="181" ssid = "72">94 87.</S>
			<S sid ="182" ssid = "73">57 90.</S>
			<S sid ="183" ssid = "74">87 96.</S>
			<S sid ="184" ssid = "75">97 86.</S>
			<S sid ="185" ssid = "76">03 87.</S>
			<S sid ="186" ssid = "77">27 89.</S>
			<S sid ="187" ssid = "78">60 97.</S>
			<S sid ="188" ssid = "79">74 86.</S>
			<S sid ="189" ssid = "80">24 87.</S>
			<S sid ="190" ssid = "81">37 89.</S>
			<S sid ="191" ssid = "82">72 97.</S>
			<S sid ="192" ssid = "83">90 86.</S>
			<S sid ="193" ssid = "84">35 87.</S>
			<S sid ="194" ssid = "85">46 89.</S>
			<S sid ="195" ssid = "86">86 97.</S>
			<S sid ="196" ssid = "87">90 86.</S>
			<S sid ="197" ssid = "88">14 87.</S>
			<S sid ="198" ssid = "89">23 89.</S>
			<S sid ="199" ssid = "90">66 97.</S>
			<S sid ="200" ssid = "91">79 Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and the score threshold α.</S>
			<S sid ="201" ssid = "92">Beam parameters fixed at b1 = 40, b2 = 4.</S>
			<S sid ="202" ssid = "93">with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al.</S>
			<S sid ="203" ssid = "94">(2011).3 For English, this is the WSJ section of the Penn Tree- bank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 In order to assign k-best part-of-speech tags and scores to words in the training set, we used a per- ceptron tagger with 10-fold jackknifing.</S>
			<S sid ="204" ssid = "95">The same type of tagger was trained on the entire training set in order to supply tags for the development and test sets.</S>
			<S sid ="205" ssid = "96">The feature set of the tagger was optimized for English and German and provides state-of-the- art accuracy for these two languages.</S>
			<S sid ="206" ssid = "97">The 1-best tagging accuracy for section 23 of the Penn Tree- bank is 97.28, which is on a par with Toutanova et al.</S>
			<S sid ="207" ssid = "98">(2003).</S>
			<S sid ="208" ssid = "99">For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RFTagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Tree- bank.</S>
			<S sid ="209" ssid = "100">We could not use the larger training set as it contains the test set of the CoNLL 2009 data that we use to evaluate the joint model.</S>
			<S sid ="210" ssid = "101">For Czech, the 1- best tagging accuracy is 99.11 and for Chinese 92.65 on the CoNLL 2009 test set.</S>
			<S sid ="211" ssid = "102">We trained parsers with 25 iterations and report 3 Training: 001–815, 1001–1136.</S>
			<S sid ="212" ssid = "103">Development: 886–931,.</S>
			<S sid ="213" ssid = "104">1148–1151.</S>
			<S sid ="214" ssid = "105">Test: 816–885, 1137–1147.</S>
			<S sid ="215" ssid = "106">5 The RFTagger can take advantage of an additional lexicon and then reaches 97.97.</S>
			<S sid ="216" ssid = "107">The lexicon supplies entries for addi results for the model obtained after the last iteration.</S>
			<S sid ="217" ssid = "108">For cluster features, available only for English and German, we used standard Brown clusters based on the English and German Gigaword Corpus.</S>
			<S sid ="218" ssid = "109">We restricted the vocabulary to words that occur at least 10 times, used 800 clusters, and took cluster prefixes of length 6 to define features.</S>
			<S sid ="219" ssid = "110">We report the following evaluation metrics: part- of-speech accuracy (POS), unlabeled attachment score (UAS), labeled attachment score (LAS), and tagged labeled attachment score (TLAS).</S>
			<S sid ="220" ssid = "111">TLAS is a new metric defined as the percentage of words that are assigned the correct part-of-speech tag, the correct head and the correct dependency label.</S>
			<S sid ="221" ssid = "112">In line with previous work, punctuation is included in the evaluation for the CoNLL data sets but excluded for the two benchmark data sets.</S>
			<S sid ="222" ssid = "113">3.2 Results.</S>
			<S sid ="223" ssid = "114">Table 1 presents results on the development sets of the CoNLL 2009 shared task with varying values of the two tag parameters k (number of candidates) and α (maximum score difference to 1-best tag) and beam parameters fixed at b1 = 40 and b2 = 4.</S>
			<S sid ="224" ssid = "115">We use the combined TLAS score on the development set to select the optimal settings for each language.</S>
			<S sid ="225" ssid = "116">For Chinese, we obtain the best result with 3 tags and a threshold of 0.1.6 Compared to the baseline, we observe a POS improvement of 0.60 and a LAS improvement of 0.51.</S>
			<S sid ="226" ssid = "117">For Czech, we get the best T- LAS with k = 3 and α = 0.2, where POS improves by 0.06 and LAS by 0.46.</S>
			<S sid ="227" ssid = "118">For English, the best setting is k = 2 and α = 0.1 with a POS improvement of 0.17 and a LAS improvement of 0.62.</S>
			<S sid ="228" ssid = "119">For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).</S>
			<S sid ="229" ssid = "120">6 While tagging accuracy (POS) increases with larger values.</S>
			<S sid ="230" ssid = "121">of α, TLAS decreases because of a drop in LAS.</S>
			<S sid ="231" ssid = "122">Par ser C h i n e s e TLA S LAS UAS POS C z e c h TLA S LAS UAS POS E n g l i s h TLA S LAS UAS POS G e r m a n TLA S LAS UAS POS Ges mun do et al.</S>
			<S sid ="232" ssid = "123">(20 09) Boh net (20 10) 7 6 . 1 1 9 2 . 3 7 7 6 . 9 9 9 2 . 3 7 8 0 . 3 8 9 9 . 3 3 8 0 . 9 6 9 9 . 3 3 8 8 . 7 9 9 7 . 4 8 9 0 . 3 3 9 7 . 4 8 8 7 . 2 8 9 5 . 4 6 8 8 . 0 6 9 5 . 4 6 Base line (k = 1), b1 = 40 Best dev setti ng, b1 = 40 73.6 6 76.5 5 80.7 7 92.</S>
			<S sid ="233" ssid = "124">65 74.7 2 77.0 0 81.1 8 93.</S>
			<S sid ="234" ssid = "125">06 82.0 7 82.4 4 87.8 3 99.</S>
			<S sid ="235" ssid = "126">11 82.5 6 82.7 0 88.0 7 99.</S>
			<S sid ="236" ssid = "127">32 87.8 9 89.1 9 91.7 4 97.</S>
			<S sid ="237" ssid = "128">57 88.2 6 89.5 4 92.0 6 97.</S>
			<S sid ="238" ssid = "129">77 86.1 1 87.7 8 90.1 3 97.</S>
			<S sid ="239" ssid = "130">24 86.9 1 88.2 3 90.4 3 97.</S>
			<S sid ="240" ssid = "131">63 Addi ng G, b1 = 80 Addi ng G+C , b1 = 80 75.8 4 78.5 1 82.5 2 93.</S>
			<S sid ="241" ssid = "132">19 83.3 8 83.7 3 88.8 2 99.</S>
			<S sid ="242" ssid = "133">33 88.9 2 90.2 0 92.6 0 97.</S>
			<S sid ="243" ssid = "134">77 89.2 2 90.6 0 92.8 7 97.</S>
			<S sid ="244" ssid = "135">84 87.8 6 89.0 5 91.1 6 97.</S>
			<S sid ="245" ssid = "136">78 88.3 1 89.3 8 91.3 7 98.</S>
			<S sid ="246" ssid = "137">05 Table 2: Accuracy scores for the CoNLL 2009 shared task test sets.</S>
			<S sid ="247" ssid = "138">Rows 1–2: Top performing systems in the shared CoNLL Shared Task 2009; Gesmundo et al.</S>
			<S sid ="248" ssid = "139">(2009) was placed first in the shared task; for Bohnet (2010), we include the updated scores later reported due to some improvements of the parser.</S>
			<S sid ="249" ssid = "140">Rows 3–4: Baseline (k = 1) and best settings for k and α on development set.</S>
			<S sid ="250" ssid = "141">Rows 5–6: Wider beam (b1 = 80) and added graph features (G) and cluster features (C).</S>
			<S sid ="251" ssid = "142">Second beam parameter b2 fixed at 4 in all cases.</S>
			<S sid ="252" ssid = "143">and α = 0.3, where POS improves by 0.66 and LAS by 0.86.</S>
			<S sid ="253" ssid = "144">Table 2 shows the results on the CoNLL 2009 test sets.</S>
			<S sid ="254" ssid = "145">For all languages except English, we obtain state-of-the-art results already with b1 = 40 (row 4), and for all languages both tagging and parsing accuracy improve compared to the baseline (row 3).</S>
			<S sid ="255" ssid = "146">The improvement in TLAS is statistically significant with p &lt; 0.01 for all languages (paired t-test).</S>
			<S sid ="256" ssid = "147">Row 5 shows the scores with a beam of 80 and the additional graph features.</S>
			<S sid ="257" ssid = "148">Here the LAS scores for Chinese, Czech and German are higher than the best results on the CoNLL 2009 data sets, and the score for English is highly competitive.</S>
			<S sid ="258" ssid = "149">For Chinese, we achieve 78.51 LAS, which is 1.5 percentage points higher than the reference score, while the POS score is 0.54 higher than our baseline.</S>
			<S sid ="259" ssid = "150">For Czech, we get 83.73 LAS, which is by far the highest score reported for this data set, together with state-of-the-art POS accuracy.</S>
			<S sid ="260" ssid = "151">For German, we obtain 89.05 LAS and 97.78 POS, which in both cases is substantially better than in the CoNLL shared task.</S>
			<S sid ="261" ssid = "152">We believe it is also the highest POS accuracy ever reported for a tagger/parser trained only on the Tiger Treebank.</S>
			<S sid ="262" ssid = "153">Row 6, finally, presents results with added cluster features for English and German, which results in additional improvements in all metrics.</S>
			<S sid ="263" ssid = "154">Table 3 gives the results for the Penn Treebank converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).</S>
			<S sid ="264" ssid = "155">We use k = 3 and α = 0.4, which gave the best results on the development set.</S>
			<S sid ="265" ssid = "156">The UAS improves by 0.24 when we do joint tagging and parsing.</S>
			<S sid ="266" ssid = "157">The POS accuracy improves slightly by 0.12 Par ser TLA S UA S LA S PO S Mc Don ald et al.</S>
			<S sid ="267" ssid = "158">(20 05) McD onal d and Pere ira (200 6) Zha ng and Clar k (20 08) Hua ng and Sag ae (20 10) K o o a n d C o l l i n s ( 2 0 1 0 ) Z h a n g a n d N i v r e ( 2 0 1 1 ) M a r t i n s e t a l .</S>
			<S sid ="268" ssid = "159">( 2 0 1 0 ) 90.</S>
			<S sid ="269" ssid = "160">9 91.</S>
			<S sid ="270" ssid = "161">5 92.</S>
			<S sid ="271" ssid = "162">1 92.</S>
			<S sid ="272" ssid = "163">1 93.0 4 92.</S>
			<S sid ="273" ssid = "164">9 93.2 6 Koo et al.</S>
			<S sid ="274" ssid = "165">(200 8) † Carr eras et al.</S>
			<S sid ="275" ssid = "166">(200 8) † Suz uki et al.</S>
			<S sid ="276" ssid = "167">(20 09) † 93.1 6 93.</S>
			<S sid ="277" ssid = "168">5 93.7 9 Bas elin e (k = 1), b1 = 40 Best dev setti ng, b1 = 40 89.</S>
			<S sid ="278" ssid = "169">42 89.</S>
			<S sid ="279" ssid = "170">75 92.7 9 93.0 3 91.7 1 91.9 2 97.2 8 97.4 0 Add ing G, b1 = 40 Add ing G+ C, b1 = 80 † 90.</S>
			<S sid ="280" ssid = "171">12 90.</S>
			<S sid ="281" ssid = "172">41 93.3 8 93.6 7 92.4 4 92.6 8 97.3 3 97.4 2 Table 3: Accuracy scores for WSJPTB converted with head rules of Yamada and Matsumoto (2003) and labeling rules of Nivre (2006).</S>
			<S sid ="282" ssid = "173">Best dev setting: k = 3, α = 0.4.</S>
			<S sid ="283" ssid = "174">Results marked with † use additional information sources and are not directly comparable to the others.</S>
			<S sid ="284" ssid = "175">but to a lower degree than for the English CoNL- L data where we observed an improvement of 0.20.</S>
			<S sid ="285" ssid = "176">Nonetheless, the improvement in the joint TLAS score is statistically significant at p &lt; 0.01 (paired t-test).</S>
			<S sid ="286" ssid = "177">Our joint tagger and dependency parser with graph features gives very competitive unlabeled dependency scores for English with 93.38 UAS.</S>
			<S sid ="287" ssid = "178">To the best of our knowledge, this is the highest score reported for a (transition-based) dependency parser that does not use additional information sources.</S>
			<S sid ="288" ssid = "179">By adding cluster features and widening the beam to b1 = 80, we achieve 93.67 UAS.</S>
			<S sid ="289" ssid = "180">We also obtain a POS accuracy of 97.42, which is on a par with the best results obtained using semi-supervised taggers Par ser TL AS UA S LA S PO S MS TP ars er1 MS TP ars er2 Li et al.</S>
			<S sid ="290" ssid = "181">(20 11) 3rd ord er Li et al.</S>
			<S sid ="291" ssid = "182">(20 11) 2nd ord er Hat ori et al.</S>
			<S sid ="292" ssid = "183">(20 11) HS Hat ori et al.</S>
			<S sid ="293" ssid = "184">(20 11) ZN 75.</S>
			<S sid ="294" ssid = "185">56 77.</S>
			<S sid ="295" ssid = "186">73 80.</S>
			<S sid ="296" ssid = "187">60 80.</S>
			<S sid ="297" ssid = "188">55 79.</S>
			<S sid ="298" ssid = "189">60 81.</S>
			<S sid ="299" ssid = "190">20 93.</S>
			<S sid ="300" ssid = "191">51 93.</S>
			<S sid ="301" ssid = "192">51 92.</S>
			<S sid ="302" ssid = "193">80 93.</S>
			<S sid ="303" ssid = "194">08 94.</S>
			<S sid ="304" ssid = "195">01 93.</S>
			<S sid ="305" ssid = "196">94 Bas elin e (k = 1), b1 = 40 Bes t dev setti ng, b1 = 40 61.</S>
			<S sid ="306" ssid = "197">95 62.</S>
			<S sid ="307" ssid = "198">54 80.</S>
			<S sid ="308" ssid = "199">33 80.</S>
			<S sid ="309" ssid = "200">59 76.</S>
			<S sid ="310" ssid = "201">79 77.</S>
			<S sid ="311" ssid = "202">06 92.</S>
			<S sid ="312" ssid = "203">81 93.</S>
			<S sid ="313" ssid = "204">11 Add ing G, b1 = 80 63.</S>
			<S sid ="314" ssid = "205">20 81.</S>
			<S sid ="315" ssid = "206">42 77.</S>
			<S sid ="316" ssid = "207">91 93.</S>
			<S sid ="317" ssid = "208">24 Table 4: Accuracy scores for Penn Chinese Treebank converted with the head rules of Zhang and Clark (2008).</S>
			<S sid ="318" ssid = "209">Best dev setting: k = 3, α = 0.1.</S>
			<S sid ="319" ssid = "210">MSTParser results from Li et al.</S>
			<S sid ="320" ssid = "211">(2011).</S>
			<S sid ="321" ssid = "212">UAS scores from Li et al.</S>
			<S sid ="322" ssid = "213">(2011) and Ha- tori et al.</S>
			<S sid ="323" ssid = "214">(2011) recalculated from the separate accuracy scores for root words and non-root words reported in the original papers.</S>
			<S sid ="324" ssid = "215">(Søgaard, 2011).</S>
			<S sid ="325" ssid = "216">Table 4 shows the results for the Chinese Penn Treebank CTB 5.1 together with related work.</S>
			<S sid ="326" ssid = "217">In experiments with the development set, we could confirm the results from the Chinese CoNLL data set and obtained the best results with the same settings (k = 3, α = 0.1).</S>
			<S sid ="327" ssid = "218">With b1 = 40, UAS improves by 0.25 and POS by 0.30, and the TLAS improvement is again highly significant (p &lt; 0.01, paired t-test).</S>
			<S sid ="328" ssid = "219">We get the highest UAS, 81.42, with a beam of 80 and added graph features, in which case POS accuracy increases from 92.81 to 93.24.</S>
			<S sid ="329" ssid = "220">Since our tagger was not optimized for Chinese, we have lower baseline results for the tagger than both Li et al.</S>
			<S sid ="330" ssid = "221">(2011) and Hatori et al.</S>
			<S sid ="331" ssid = "222">(2011) but still manage to achieve the highest reported UAS.</S>
			<S sid ="332" ssid = "223">The speed of the joint tagger and dependency parser is quite reasonable with about 0.4 seconds per sentence on the WSJPTB test set, given that we perform tagging and labeled parsing with a beam of 80 while incorporating the features of a third-order graph-based model.</S>
			<S sid ="333" ssid = "224">Experiments were performed on a computer with an Intel i73960X CPU (3.3 GHz and 6 cores).</S>
			<S sid ="334" ssid = "225">These performance values are preliminary since we are still working on the speedup of the parser.</S>
			<S sid ="335" ssid = "226">3.3 Analysis.</S>
			<S sid ="336" ssid = "227">In order to better understand the benefits of the joint model, we performed an error analysis for German Table 5: Selected entries from the confusion matrix for parts of speech in German with F-scores for the left-hand- side category.</S>
			<S sid ="337" ssid = "228">ADJ* (ADJD or ADJA) = adjective; ADV = adverb; ART = determiner; APPR = preposition; NE = proper noun; NN = common noun; PRELS = relative pronoun; VVFIN = finite verb; VVINF = non-finite verb; VAFIN = finite auxiliary verb; VAINF = non-finite auxiliary verb; VVPP = participle; XY = not a word.</S>
			<S sid ="338" ssid = "229">We use α* to denote the set of categories with α as a prefix.</S>
			<S sid ="339" ssid = "230">and English, where we compared the baseline and the joint model with respect to F-scores for individual part-of-speech categories and dependency labels.</S>
			<S sid ="340" ssid = "231">For the part-of-speech categories, we found an improvement across the board for both languages, with no category having a significant decrease in F-score, but we also found some interesting patterns for categories that improved more than the average.</S>
			<S sid ="341" ssid = "232">Table 5 shows selected entries from the confusion matrix for German, where we see substantial improvements for finite and non-finite verbs, which are often morphologically ambiguous but which can be disambiguated using syntactic context.</S>
			<S sid ="342" ssid = "233">We also see improved accuracies for common and proper nouns, which are both capitalized in standard German orthography and therefore often mistagged, and for relative pronouns, which are less often confused for determiners in the joint model.</S>
			<S sid ="343" ssid = "234">Table 6 gives a similar snapshot for English, and we again see improvements for verb categories that are often morphologically ambiguous, such as past participles, which can be confused for past tense verbs, and present tense verbs in third person singular, which can be confused for nouns.</S>
			<S sid ="344" ssid = "235">We also see some improvement for the singular noun catego C o n f u s i o n B a s e l i n e Freq F scor e J o i n t Freq F scor e VB N → VB D VB N → JJ|V B|V BP| NN 4 0 90.5 1 3 1 9 91.5 1 8 VB Z → NN| NN S VB Z → POS |JJ| RB 1 9 97.8 6 1 3 98.3 6 NN → VB G|V B|V BN| VB D NN → JJ|J JR NN → NN *|R B|I N|D T 7 2 7 9 96.8 5 8 5 8 6 9 97.2 5 7 RB → IN RB → JJ*| RP| NN *|R BR| UH 12 6 92.4 8 6 9 3 92.9 8 9 Table 6: Selected entries from the confusion matrix for parts of speech in English with F-scores for the left-hand- side category.</S>
			<S sid ="345" ssid = "236">DT = determiner; IN = preposition or subordinating conjunction; JJ = adjective; JJR = comparative adjective; NN = singular or mass noun; NNS = plural noun; POS = possessive clitic; RB = adverb; RBR = comparative adverb; RP = particle; UH = interjection; VB = base form verb; VBD = past tense verb; VBG = gerund or present participle; VBN = past participle; VBP = present tense verb, not 3rd person singular; VBZ = present tense verb, 3rd person singular.</S>
			<S sid ="346" ssid = "237">We use α* to denote the set of categories with α as a prefix.</S>
			<S sid ="347" ssid = "238">ry and for adverbs, which are less often confused for prepositions or subordinating conjunctions thanks to the syntactic information in the joint model.</S>
			<S sid ="348" ssid = "239">For dependency labels, it is hard to extract any striking patterns and it seems that we mainly see an improvement in overall parsing accuracy thanks to less severe tagging errors.</S>
			<S sid ="349" ssid = "240">However, it is worth observing that, for both English and German, we see significant F-score improvements for the core gram matical functions subject (91.3 → 92.1 for German, 95.6 → 96.1 for English) and object (86.9 → 87.9 for German, 90.2 → 91.9 for English).</S>
	</SECTION>
	<SECTION title="Related Work. " number = "4">
			<S sid ="350" ssid = "1">Our work is most closely related to Lee et al.</S>
			<S sid ="351" ssid = "2">(2011), Li et al.</S>
			<S sid ="352" ssid = "3">(2011) and Hatori et al.</S>
			<S sid ="353" ssid = "4">(2011), who all present discriminative models for joint tagging and dependency parsing.</S>
			<S sid ="354" ssid = "5">However, all three models only perform unlabeled parsing, while our model incorporates dependency labels into the parsing process.</S>
			<S sid ="355" ssid = "6">Whereas Lee et al.</S>
			<S sid ="356" ssid = "7">(2011) and Li et al.</S>
			<S sid ="357" ssid = "8">(2011) take a graph-based approach to dependency parsing, Ha- tori et al.</S>
			<S sid ="358" ssid = "9">(2011) use a transition-based model similar to ours but limited to projective dependency trees.</S>
			<S sid ="359" ssid = "10">Both Li et al.</S>
			<S sid ="360" ssid = "11">(2011) and Hatori et al.</S>
			<S sid ="361" ssid = "12">(2011) only evaluate their model on Chinese, and of these only Hatori et al.</S>
			<S sid ="362" ssid = "13">(2011) report consistent improvements in both tagging and parsing accuracy.</S>
			<S sid ="363" ssid = "14">Like our system, the parser of Lee et al.</S>
			<S sid ="364" ssid = "15">(2011) can handle non- projective trees and experimental results are presented for four languages, but their graph-based model is relatively simple and the baselines therefore well below the state of the art.</S>
			<S sid ="365" ssid = "16">We are thus the first to show consistent improvements in both tagging and (labeled) parsing accuracy across typologically diverse languages at the state-of-the-art level.</S>
			<S sid ="366" ssid = "17">Moreover, the capacity to handle non-projective dependencies, which is crucial to attain good performance on Czech and German, does not seem to hurt performance on English and Chinese, where the benchmark sets contain only projective trees.</S>
			<S sid ="367" ssid = "18">The use of beam search in transition-based dependency parsing in order to mitigate the problem of error propagation was first proposed by Johansson and Nugues (2006), although they still used a locally trained model.</S>
			<S sid ="368" ssid = "19">Globally normalized models were first explored by Titov and Henderson (2007), who were also the first to use a parameterized SHIFT transition like the one found in both Hatori et al.</S>
			<S sid ="369" ssid = "20">(2011) and our own work, although Titov and Henderson (2007) used it to define a generative model by pa- rameterizing the SHIFT transition by an input word.</S>
			<S sid ="370" ssid = "21">Zhang and Clark (2008) was the first to combine beam search with a globally normalized discrimi- native model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph- based features to a transition-based parser.</S>
			<S sid ="371" ssid = "22">This approach was further pursued in Zhang and Clark (2011) and was used by Zhang and Nivre (2011) to achieve state-of-the-art results in dependency parsing for both Chinese and English through the addition of rich non-local features.</S>
			<S sid ="372" ssid = "23">Huang and Sagae (2010) combined structured perceptron learning and beam search with the use of a graph-structured stack to allow ambiguity packing in the beam, a technique that was reused by Hatori et al.</S>
			<S sid ="373" ssid = "24">(2011).</S>
			<S sid ="374" ssid = "25">Finally, as noted in the introduction, although joint tagging and parsing is rare in dependency parsing, most state-of-the-art parsers based on PCFG models naturally incorporate part-of-speech tagging and usually achieve better parsing accuracy (albeit not always tagging accuracy) with a joint model than with a pipeline approach (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Petrov et al., 2006).</S>
			<S sid ="375" ssid = "26">Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="376" ssid = "1">We have presented the first system for joint part- of-speech tagging and labeled dependency parsing with non-projective dependency trees.</S>
			<S sid ="377" ssid = "2">Evaluation on four languages shows consistent improvements in both tagging and parsing accuracy over a pipeline system with state-of-the-art results across the board.</S>
			<S sid ="378" ssid = "3">The error analysis reveals improvements in tagging accuracy for syntactically central categories, mainly verbs, with improvement in syntactic accuracy for core grammatical functions as a result.</S>
			<S sid ="379" ssid = "4">In future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features.</S>
	</SECTION>
</PAPER>
