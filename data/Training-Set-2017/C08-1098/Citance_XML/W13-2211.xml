<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes TU¨ B˙ITAK-B˙ILGEM statistical machine translation (SMT) systems submitted to the Eighth Workshop on Statistical Machine Translation (WMT) shared translation task for GermanEnglish language pair in both directions.</S>
		<S sid ="2" ssid = "2">We implement phrase-based SMT systems with standard parameters.</S>
		<S sid ="3" ssid = "3">We present the results of using a big tuning data and the effect of averaging tuning weights of different seeds.</S>
		<S sid ="4" ssid = "4">Additionally, we performed a linguistically motivated compound splitting in the German- to-English SMT system.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">TU¨ B˙ITAK-B˙ILGEM participated for the first time in the WMT’13 shared translation task for the GermanEnglish language pairs in both directions.</S>
			<S sid ="6" ssid = "6">We implemented a phrase-based SMT system by using the entire available training data.</S>
			<S sid ="7" ssid = "7">In the German-to-English SMT system, we performed a linguistically motivated compound splitting.</S>
			<S sid ="8" ssid = "8">We tested different language model (LM) combinations by using the parallel data, monolingual data, and Gigaword v4.</S>
			<S sid ="9" ssid = "9">In each step, we tuned systems with five different tune seeds and used the average of tuning weights in the final system.</S>
			<S sid ="10" ssid = "10">We tuned our systems on a big tuning set which is generated from the last years’ (2008, 2009, 2010, and 2012) development sets.</S>
			<S sid ="11" ssid = "11">The rest of the paper describes the details of our systems.</S>
	</SECTION>
	<SECTION title="German-English. " number = "2">
			<S sid ="12" ssid = "1">2.1 Baseline.</S>
			<S sid ="13" ssid = "2">All available data was tokenized, truecased, and the maximum number of tokens were fixed to 70 for the translation model.</S>
			<S sid ="14" ssid = "3">The Moses open SMT toolkit (Koehn et al., 2007) was used with MGIZA++ (Gao and Vogel, 2008) with the standard alignment heuristic grow-diag-final (Och and Ney, 2003) for word alignments.</S>
			<S sid ="15" ssid = "4">Good-Turing smoothing was used for phrase extraction.</S>
			<S sid ="16" ssid = "5">Systems were tuned on newstest2012 with MERT (Och, 2003) and tested on newstest2011.</S>
			<S sid ="17" ssid = "6">4- gram language models (LMs) were trained on the target side of the parallel text and the monolingual data by using SRILM (Stolcke, 2002) toolkit with KneserNey smoothing (Kneser and Ney, 1995) and then binarized by using KenLM toolkit (Heafield, 2011).</S>
			<S sid ="18" ssid = "7">At each step, systems were tuned with five different seeds with lattice- samples.</S>
			<S sid ="19" ssid = "8">Minimum Bayes risk decoding (Kumar and Byrne, 2004) and -drop-unknown parameters were used during the decoding.</S>
			<S sid ="20" ssid = "9">This configuration is common for all of the experiments decribed in this paper unless stated otherwise.</S>
			<S sid ="21" ssid = "10">Table 1 shows the number of sentences used in system training after the clean-corpus process.</S>
			<S sid ="22" ssid = "11">Data Number of sentences Europarl 1908574 News-Commentary 177712 Commoncrawl 726458 Table 1: Parallel Corpus.</S>
			<S sid ="23" ssid = "12">We trained two baseline systems in order to assess the effects of this year’s new parallel data, commoncrawl.</S>
			<S sid ="24" ssid = "13">We first trained an SMT system by using only the training data from the previous WMT shared translation tasks that is europarl and news-commentary (Baseline1).</S>
			<S sid ="25" ssid = "14">As the second baseline, we also included the new parallel data commoncrawl only in the translation model (Base- line2).</S>
			<S sid ="26" ssid = "15">Then, we included commoncrawl corpus both to the translation model and the language model (Baseline3).</S>
			<S sid ="27" ssid = "16">Table 2 compares the baseline results.</S>
			<S sid ="28" ssid = "17">For all 109 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 109–113, Sofia, Bulgaria, August 89, 2013 Qc 2013 Association for Computational Linguistics experiments throughout the paper, we present the minimum and the maximum BLEU scores obtained after five different tunes.</S>
			<S sid ="29" ssid = "18">As seen in the table, the addition of the commoncrawl corpus re- sultedin a 1.1 BLEU (Papineni et al., 2002) points improvement (on average) on the test set.</S>
			<S sid ="30" ssid = "19">Although Baseline2 is slightly better than Baseline3, we used Baseline3 and kept commoncrawl corpus in LMs for further experiments.</S>
			<S sid ="31" ssid = "20">System newstest12 newstest11 Baseline1 20.58|20.74 19.14|19.29 Baseline2 21.37|21.58 20.16|20.46 Baseline3 21.28|21.58 20.22|20.49 Table 2: Baseline Results.</S>
			<S sid ="32" ssid = "21">2.2 Bayesian Alignment.</S>
			<S sid ="33" ssid = "22">In the original IBM models (Brown et al., 1993), word translation probabilities are treated as model parameters and the expectation-maximization (EM) algorithm is used to obtain the maximum- likelihood estimates of the parameters and the resulting distributions on alignments.</S>
			<S sid ="34" ssid = "23">However, EM provides a point-estimate, not a distribution, for the parameters.</S>
			<S sid ="35" ssid = "24">The Bayesian alignment on the other hand takes into account all values of the model parameters by treating them as multinomial-distributed random variables with Dirichlet priors and integrating over all possible values.</S>
			<S sid ="36" ssid = "25">A Bayesian approach to word alignment inference in IBM Models is shown to result in significantly less “garbage collection” and a much more compact alignment dictionary.</S>
			<S sid ="37" ssid = "26">As a result, the Bayesian word alignment has better translation performances and obtains significant BLEU improvements over EM on various language pairs, data sizes, and experimental settings (Mermer et al., 2013).</S>
			<S sid ="38" ssid = "27">We compared the translation performance of word alignments obtained via Bayesian inference to those obtained via EM algorithm.</S>
			<S sid ="39" ssid = "28">We used a a Gibbs sampler for fully Bayesian inference in HMM alignment model, integrating over all possible parameter values in finding the alignment distribution by using Baseline3 word alignments for initialization.</S>
			<S sid ="40" ssid = "29">Table 3 compares the Bayesian alignment to the EM alignment.</S>
			<S sid ="41" ssid = "30">The results show a slight increase in the development set newstest12 but a decrease of 0.1 BLEU points on average in the test set newstest11.</S>
			<S sid ="42" ssid = "31">System newstest12 newstest11 Baseline3 21.28|21.58 20.22|20.49 Gibbs Sampling 21.36|21.59 19.98|20.40 Table 3: Bayesian Alignment Results.</S>
			<S sid ="43" ssid = "32">2.3 Development Data in Training.</S>
			<S sid ="44" ssid = "33">Development data from the previous years (i.e. newstest08, newstest09, newstest10), though being a small set of corpus (7K sentences), is in-domain data and can positively affect the translation system.</S>
			<S sid ="45" ssid = "34">In order to make use of this data, we experimented two methods: i) adding the development data in the translation model as described in this section and ii) using it as a big tuning set for tuning the parameters more efficiently as explained in the next section.</S>
			<S sid ="46" ssid = "35">Similar to including the commoncrawl corpus, we first add the development data both to the training and language models by concatenating it to the biggest corpus europarl (DD(tm+lm)) and then we removed this corpus from the language models (DD(tm)).</S>
			<S sid ="47" ssid = "36">Results in Table 4 show that including the development data both the tranining and language model increases the performance in development set but decreases the performance in the test set.</S>
			<S sid ="48" ssid = "37">Including the data only in the translation model shows a very slight improvement in the test set.</S>
			<S sid ="49" ssid = "38">System newstest12 newstest11 Baseline3 21.28|21.58 20.22|20.49 DD(tm+lm) 21.28|21.65 20.00|20.49 DD(tm) 21.23|21.52 20.26|20.49 Table 4: Development Sets Results.</S>
			<S sid ="50" ssid = "39">2.4 Tuning with a Big Development Data.</S>
			<S sid ="51" ssid = "40">The second method of making use of the development data is to concatenate it to the tuning set.</S>
			<S sid ="52" ssid = "41">As a baseline, we tuned the system with newstest12 as mentioned in Section 2.1.</S>
			<S sid ="53" ssid = "42">Then, we concatenated the development data of the previous years with the newstest12 and built a big tuning set.</S>
			<S sid ="54" ssid = "43">Finally, we obtained a tuning set of 10K sentences.</S>
			<S sid ="55" ssid = "44">We excluded the newstest11 as an internal test set to see the relative improvements of different systems.</S>
			<S sid ="56" ssid = "45">Table 5 shows the results of using a big tuning set.</S>
			<S sid ="57" ssid = "46">Tuning the system with a big tuning set resulted in a 0.13 BLEU points improvement.</S>
			<S sid ="58" ssid = "47">System newstest12 newstest11 newstest12 21.28|21.58 20.22|20.49 Big Tune 20.93|21.19 20.32|20.58 Table 5: Tuning Results.</S>
			<S sid ="59" ssid = "48">2.5 Effects of Different Language Models.</S>
			<S sid ="60" ssid = "49">In this set of experiments, we tested the effects of different combinations of parallel and monolingual data as language models.</S>
			<S sid ="61" ssid = "50">As the baseline, we trained three LMs, one from each parallel corpus as europarl, news-commentary, and commoncrawl and one LM from the monolingual data news- shuffled (Baseline3).</S>
			<S sid ="62" ssid = "51">We then trained two LMs, one from the whole parallel data and one from the monolingual data (2LMs).</S>
			<S sid ="63" ssid = "52">Table 6 shows that using whole parallel corpora as one LM performs better than individual corpus LMs and results in 0.1 BLEU points improvement on the baseline.</S>
			<S sid ="64" ssid = "53">Fi-.</S>
			<S sid ="65" ssid = "54">nally, we trained Gigaword v4 (LDC2009T13) as a third LM (3LMs) which gives a 0.16 BLEU points improvement over the 2LMs.</S>
			<S sid ="66" ssid = "55">Sy ste m newstest12 newstest11 Ba sel in e3 21.28|21.58 20.22|20.49 2L M s 21.46|21.70 20.28|20.57 3L M s 21.78|21.93 20.54|20.68 Table 6: Language Model Results.</S>
			<S sid ="67" ssid = "56">2.6 German Preprocessing.</S>
			<S sid ="68" ssid = "57">In German, compounding is very common.</S>
			<S sid ="69" ssid = "58">From the machine translation point of view, compounds increase the vocabulary size with high number of the singletons in the training data and hence decrease the word alignment quality.</S>
			<S sid ="70" ssid = "59">Moreover, high number of out-of-vocabulary (OOV) words in tuning and test sets results in several German words left as untranslated.</S>
			<S sid ="71" ssid = "60">A well-known solution to this problem is compound splitting.</S>
			<S sid ="72" ssid = "61">Similarly, having different word forms for a source side lemma for the same target lemma causes the lexical redundancy in translation.</S>
			<S sid ="73" ssid = "62">This redundancy results in unnecessary large phrase translation tables that overload the decoder, as a separate phrase translation entry has to be kept for each word form.</S>
			<S sid ="74" ssid = "63">For example, German definite determiner could be marked in sixteen different ways according to the possible combinations of genders, case and number, which are fused in six different tokens (e.g., der, das, die, den, dem, des).</S>
			<S sid ="75" ssid = "64">Except for the plural and genitive cases, all these forms are translated to the same English word “the”.</S>
			<S sid ="76" ssid = "65">In the German preprocessing, we aimed both normalizing lexical redundancy and splitting German compounds with corpus driven splitting algorithm based on Koehn and Knight (2003).</S>
			<S sid ="77" ssid = "66">We used the same compound splitting and lexical redundancy normalization methods described in Allauzen et al.</S>
			<S sid ="78" ssid = "67">(2010) and Durgar ElKahlout and Yvon (2010) with minor in-house changes.</S>
			<S sid ="79" ssid = "68">We used only “addition” (e.g., -s, -n, -en, -e, -es) and “truncation” (e.g., -e, -en, -n) affixes for compound splitting.</S>
			<S sid ="80" ssid = "69">We selected minimum candidate length to 8 and minimum split length to 4.</S>
			<S sid ="81" ssid = "70">By using the Treetagger (Schmid, 1994) output, we included linguistic information in compound splitting such as not splitting named entities and foreign words (CS1).</S>
			<S sid ="82" ssid = "71">We also experimented adding # as a delimiter for the splitted words except the last word (e.g., Finanzkrisen is splitted as finanz# krisen) (CS2).</S>
			<S sid ="83" ssid = "72">On top of the compound splitting, we applied the lexical redundancy normalization (CS+Norm1).</S>
			<S sid ="84" ssid = "73">We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).</S>
			<S sid ="85" ssid = "74">Similar to CS2, We tested the delimited version of normalized words (CS+Norm2).</S>
			<S sid ="86" ssid = "75">Table 7 shows the results of compound splitting and normalization methods.</S>
			<S sid ="87" ssid = "76">As a result, normalization on top of compounding did not perform well.</S>
			<S sid ="88" ssid = "77">Besides, experiments showed that compound word decomposition is crucial and helps vastly to improve translation results 0.43 BLEU points on average over the best system described in Section 2.5.</S>
			<S sid ="89" ssid = "78">Sy ste m newstest12 newstest11 3L M s 21.78|21.93 20.54|20.68 C S1 22.01|22.21 20.63|20.89 C S2 22.06|22.22 20.74|20.99 C S+ N or m 2 21.96|22.16 20.70|20.88 C S+ N or m 1 20.63|20.76 22.01|22.16 Table 7: Compound Splitting Results.</S>
			<S sid ="90" ssid = "79">2.7 Average of Weights.</S>
			<S sid ="91" ssid = "80">As mentioned in Section 2.1, we performed tuning with five different seeds.</S>
			<S sid ="92" ssid = "81">We averaged the five tuning weights and directly applied these weights during the decoding.</S>
			<S sid ="93" ssid = "82">Table 8 shows that using the average of several tuning weights performs better than each individual tuning (0.2 BLEU points).</S>
			<S sid ="94" ssid = "83">Additionally, averaging the tuning weights of different seeds results in 0.2 BLEU points improvement.</S>
			<S sid ="95" ssid = "84">System newstest12 newstest11 CS2 22.06|22.22 20.74|20.99 Avg.</S>
			<S sid ="96" ssid = "85">of Weights 22.27 21.07 Table 8: Average of Weights Results.</S>
			<S sid ="97" ssid = "86">2.8 Other parameters.</S>
			<S sid ="98" ssid = "87">In addition to the experiments described in the earlier sections, we removed the -drop-unknown parameter which gave us a 0.5 BLEU points improvement.</S>
			<S sid ="99" ssid = "88">We also included the monotone-at- punctuation, -mp in decoding.</S>
			<S sid ="100" ssid = "89">We handled out- of-vocabulary (OOV) words by lemmatizing the OOV words.</S>
			<S sid ="101" ssid = "90">Moreover, we added all development data in training after fixing the parameter weights as described in Section 2.7.</S>
			<S sid ="102" ssid = "91">Although each of these changes increases the translation scores each gave less than 0.1 BLEU point improvement.</S>
			<S sid ="103" ssid = "92">Table 9 shows the results of the final system after including all of the approaches except the ones described in Section 2.2 and 2.3.</S>
			<S sid ="104" ssid = "93">Sy ste m newstest12 newstest11 Fi na l Sy ste m 22.59|22.77 21.86|21.93 Av g. of W ei gh ts 22.66 22.00 + tu ne da ta in tra in −− 22.09 Table 9: German-to-English Final System Results.</S>
	</SECTION>
	<SECTION title="English-German. " number = "3">
			<S sid ="105" ssid = "1">For English-to-German translation system, the baseline setting is the same as described in Section 2.1.</S>
			<S sid ="106" ssid = "2">We also added the items that showed positive improvement in the German to English SMT system such as using 2 LMs, tuning with five seeds and averaging tuning parameters, using -mp, and not using -drop-unknown.</S>
			<S sid ="107" ssid = "3">Table 10 shows the experimental results for English-to-German SMT systems.</S>
			<S sid ="108" ssid = "4">Similar to the German-to-English direction, tuning with a big development data outperforms the baseline 0.26 BLEU points (on average).</S>
			<S sid ="109" ssid = "5">Table 10: English to German Final System Results.</S>
	</SECTION>
	<SECTION title="Final System and Results. " number = "4">
			<S sid ="110" ssid = "1">Table 11 shows our official submission scores for GermanEnglish SMT systems submitted to the WMT’13.</S>
			<S sid ="111" ssid = "2">System newstest13 DeEn 25.60 EnDe 19.28 Table 11: GermanEnglish Official Test Submission.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="112" ssid = "1">In this paper, we described our submissions to WMT’13 Shared Translation Task for GermanEnglish language pairs.</S>
			<S sid ="113" ssid = "2">We used phrase-based systems with a big tuning set which is a combination of the development sets from last four years.</S>
			<S sid ="114" ssid = "3">We tuned the systems on this big tuning set with five different tunes.</S>
			<S sid ="115" ssid = "4">We averaged these five tuning weights in the final system.</S>
			<S sid ="116" ssid = "5">We trained 4-gram language models one from parallel data and one from monolingual data.</S>
			<S sid ="117" ssid = "6">Moreover, we trained a 4-gram language model with Gigaword v4 for German-to-English direction.</S>
			<S sid ="118" ssid = "7">For German- to-English, we performed a different compound splitting method instead of the Moses splitter.</S>
			<S sid ="119" ssid = "8">We obtained a 1.7 BLEU point increase for German- to-English SMT system and a 0.5 BLEU point increase for English-to-German SMT system for the internal test set newstest2011.</S>
			<S sid ="120" ssid = "9">Finally, we submitted our German-to-English SMT system with a BLEU score 25.6 and English-to-German SMT system with a BLEU score 19.3 for the official test set newstest2013.</S>
	</SECTION>
</PAPER>
