<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present 5 systems of the MunichEdinburgh-Stuttgart1 joint submissions to the 2013 SMT Shared Task: FREN, ENFR, RUEN, DEEN and ENDE.</S>
		<S sid ="2" ssid = "2">The first three systems employ inflectional generalization, while the latter two employ parser-based reordering, and DEEN performs compound splitting.</S>
		<S sid ="3" ssid = "3">For our experiments, we use standard phrase-based Moses systems and operation sequence models (OSM).</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Morphologically complex languages often lead to data sparsity problems in statistical machine translation.</S>
			<S sid ="5" ssid = "5">For translation pairs with morphologically rich source languages and English as target language, we focus on simplifying the input language in order to reduce the complexity of the translation model.</S>
			<S sid ="6" ssid = "6">The pre-processing of the source-language is language-specific, requiring morphological analysis (FR, RU) as well as sentence reordering (DE) and dealing with compounds (DE).</S>
			<S sid ="7" ssid = "7">Due to time constraints we did not deal with inflection for DEEN and ENDE.</S>
			<S sid ="8" ssid = "8">The morphological simplification process consists in lemmatizing inflected word forms and dealing with word formation (splitting portmanteau prepositions or compounds).</S>
			<S sid ="9" ssid = "9">This needs to take into account translation-relevant features (e.g. number) which vary across the different language pairs: while French only has the features number and gender, a wider array of features needs to be considered when modelling Russian (cf.</S>
			<S sid ="10" ssid = "10">table 6).</S>
			<S sid ="11" ssid = "11">In addition to morphological reduction, we also apply transliteration models learned from automatically 1 The language pairs DEEN and RUEN were developed in collaboration with the Qatar Computing Research Institute and the University of Szeged.</S>
			<S sid ="12" ssid = "12">mined transliterations to handle out-of-vocabulary words (OOVs) when translating from Russian.</S>
			<S sid ="13" ssid = "13">Replacing inflected word forms with simpler variants (lemmas or the components of split compounds) aims not only at reducing the general complexity of the translation model, but also at decreasing the amount of out-of-vocabulary words in the input data.</S>
			<S sid ="14" ssid = "14">This is particularly the case with German compounds, which are very productive and thus often lack coverage in the parallel training data, whereas the individual components can be translated.</S>
			<S sid ="15" ssid = "15">Similarly, inflected word forms (e.g. adjectives) benefit from the reduction to lemmas if the full inflection paradigm does not occur in the parallel training data.</S>
			<S sid ="16" ssid = "16">For ENFR, a translation pair with a morpho- logically complex target language, we describe a two-step translation system built on non-inflected word stems with a post-processing component for predicting morphological features and the generation of inflected forms.</S>
			<S sid ="17" ssid = "17">In addition to the advantage of a more general translation model, this method also allows the generation of inflected word forms which do not occur in the training data.</S>
	</SECTION>
	<SECTION title="Experimental  setup. " number = "2">
			<S sid ="18" ssid = "1">The translation experiments in this paper are carried out with either a standard phrase-based Moses system (DEEN, ENDE, ENFR and FREN) or with an operation sequence model (RUEN, DEEN), cf.</S>
			<S sid ="19" ssid = "2">Durrani et al.</S>
			<S sid ="20" ssid = "3">(2013b) for more details.</S>
			<S sid ="21" ssid = "4">An operation sequence model (OSM) is a state- of-the-art SMT-system that learns translation and reordering patterns by representing a sentence pair and its word alignment as a unique sequence of operations (see e.g. Durrani et al.</S>
			<S sid ="22" ssid = "5">(2011), Durrani et al.</S>
			<S sid ="23" ssid = "6">(2013a) for more details).</S>
			<S sid ="24" ssid = "7">For the Moses systems we used the old train-model perl scripts rather than the EMS, so we did not perform Good-Turing smoothing; parameter tuning was carried out with batch-mira (Cherry and Foster, 2012).</S>
			<S sid ="25" ssid = "8">232 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232–239, Sofia, Bulgaria, August 89, 2013 Qc 2013 Association for Computational Linguistics System BLEU (cs) BLEU (ci) Baseline 29.90 31.02 Simplified French* 29.70 30.83 Table 1: Text normalization for FREN.</S>
			<S sid ="26" ssid = "9">Table 2: Rules for morphological simplification.</S>
			<S sid ="27" ssid = "10">The development data consists of the concate nated news-data sets from the years 20082011.</S>
			<S sid ="28" ssid = "11">Unless otherwise stated, we use all constrained data (parallel and monolingual).</S>
			<S sid ="29" ssid = "12">For the target-side language models, we follow the approach of Schwenk and Koehn (2008) and train a separate language model for each corpus and then interpolate them using weights optimized on development data.</S>
	</SECTION>
	<SECTION title="French to English. " number = "3">
			<S sid ="30" ssid = "1">French has a much richer morphology than English; for example, adjectives in French are inflected with respect to gender and number whereas adjectives in English are not inflected at all.</S>
			<S sid ="31" ssid = "2">This causes data sparsity in coverage of French inflected forms.</S>
			<S sid ="32" ssid = "3">We try to overcome this problem by simplifying French inflected forms in a pre-processing step in order to adapt the French input better to the English output.</S>
			<S sid ="33" ssid = "4">Processing of the training and test data The pre-processing of the French input consists of two steps: (1) normalizing not well-formed data (cf.</S>
			<S sid ="34" ssid = "5">table 1) and (2) morphological simplification.</S>
			<S sid ="35" ssid = "6">In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille´ et al., 2003).</S>
			<S sid ="36" ssid = "7">French forms are then simplified according to the rules given in table 2.</S>
			<S sid ="37" ssid = "8">Data and experiments We trained a French to English Moses system on the preprocessed and Table 3: Results of the French to English system (WMT2012).</S>
			<S sid ="38" ssid = "9">The marked system (*) corresponds to the system submitted for manual evaluation.</S>
			<S sid ="39" ssid = "10">(cs: case-sensitive, ci: case-insensitive) simplified constrained parallel data.</S>
			<S sid ="40" ssid = "11">Due to tractability problems with word alignment, the 109 French-English corpus and the UN corpus were filtered to a more manageable size.</S>
			<S sid ="41" ssid = "12">The filtering criteria are sentence length (between 15 and 25 words), as well as strings indicating that a sentence is neither French nor English, or otherwise not well-formed, aiming to obtain a subset of good-quality sentences.</S>
			<S sid ="42" ssid = "13">In total, we use 9M parallel sentences.</S>
			<S sid ="43" ssid = "14">For the English language model we use large training data with 287.3M true-cased sentences (including the LDC Giga-word data).</S>
			<S sid ="44" ssid = "15">We compare two systems: a baseline with regular French text, and a system with the described morphological simplifications.</S>
			<S sid ="45" ssid = "16">Results for the WMT2012 test set are shown in table 3.</S>
			<S sid ="46" ssid = "17">Even though the baseline is better than the simplified system in terms of BLEU, we assume that the translation model of the simplified system benefits from the overall generalization – thus, human annotators might prefer the output of the simplified system.</S>
			<S sid ="47" ssid = "18">For the WMT2013 set, we obtain BLEU scores of 29,97 (cs) and 31,05 (ci) with the system built on simplified French (mes-simplifiedfrench).</S>
	</SECTION>
	<SECTION title="English to French. " number = "4">
			<S sid ="48" ssid = "1">Translating into a morphologically rich language faces two problems: that of asymmetry of morphological information contained in the source and target language and that of data sparsity.</S>
			<S sid ="49" ssid = "2">In this section we describe a two-step system designed to overcome these types of problems: first, the French data is reduced to non-inflected forms (stems) with translation-relevant morphological features, which is used to built the translation model.</S>
			<S sid ="50" ssid = "3">The second step consists of predicting all necessary morphological features for the translation output, which are then used to generate fully inflected forms.</S>
			<S sid ="51" ssid = "4">This two-step setup decreases the complexity of the translation task by removing language- specific features from the translation model.</S>
			<S sid ="52" ssid = "5">Furthermore, generating inflected forms based on word stems and morphological features allows to gener ate forms which do not occur in the parallel training data – this is not possible in a standard SMT setup.</S>
			<S sid ="53" ssid = "6">The idea of separating the translation into two steps to deal with complex morphology was introduced by Toutanova et al.</S>
			<S sid ="54" ssid = "7">(2008).</S>
			<S sid ="55" ssid = "8">Fraser et al.</S>
			<S sid ="56" ssid = "9">(2012) applied this method to the language pair EnglishGerman with an additional special focus on word formation issues such as the splitting and merging of portmanteau prepositions and compounds.</S>
			<S sid ="57" ssid = "10">The presented inflection prediction systems focuses on nominal inflection; verbal inflection is not addressed.</S>
			<S sid ="58" ssid = "11">Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).</S>
			<S sid ="59" ssid = "12">For generating inflected forms based on stems and morphological features, we use an extended version of the finite-state morphology FRMOR (Zhou, 2007).</S>
			<S sid ="60" ssid = "13">Additionally, we use a manually compiled list of abbreviations and named entities (names of countries) and their respective grammatical gender.</S>
			<S sid ="61" ssid = "14">Stemming For building the SMT system, the French data (parallel and monolingual) is transformed into a stemmed representation.</S>
			<S sid ="62" ssid = "15">Nouns, i.e. the heads of NPs or PPs, are marked with inflection-relevant features: gender is considered as part of the stem, whereas number is determined by the source-side input: for example, we expect source-language words in plural to be translated by translated by stems with plural markup.</S>
			<S sid ="63" ssid = "16">This stem- markup is necessary in order to guarantee that the number information is not lost during translation.</S>
			<S sid ="64" ssid = "17">For a better generalization, portmanteaus are split into separate parts: au → a` +le (meaning, “to the”).</S>
			<S sid ="65" ssid = "18">Predicting morphological features For predicting the morphological features of the SMT output (number and gender), we use a linear chain CRF (Lavergne et al., 2010) trained on data annotated with these features using n-grams of stems and part- of-speech tags within a window of 4 positions to each side of the current word.</S>
			<S sid ="66" ssid = "19">Through the CRF, the values specified in the stem-markup (number and gender on nouns) are propagated over the rest of the linguistic phrase, as shown in column 2 of table 4.</S>
			<S sid ="67" ssid = "20">Based on the stems and the morphological features, inflected forms can be generated using FRMOR (column 3).</S>
			<S sid ="68" ssid = "21">Post-processing As the French data has been normalized, a post-processing step is needed in order to generate correct French surface forms: split portmanteaus are merged into their regular forms based on a simple rule set.</S>
			<S sid ="69" ssid = "22">Furthermore, apostrophes are reintroduced for words like le, la, ne, ... if they are followed by a vowel.</S>
			<S sid ="70" ssid = "23">Column 4 in table 4 shows post-processing including portmanteau formation.</S>
			<S sid ="71" ssid = "24">Since we work on lowercased data, an additional recasing step is required.</S>
			<S sid ="72" ssid = "25">Experiments and evaluation We use the same set of reduced parallel data as the FREN system; the language model is built on 32M French sentences.</S>
			<S sid ="73" ssid = "26">Results for the WMT2012 test set are given in table 5.</S>
			<S sid ="74" ssid = "27">Variant 1 shows the results for a small system trained only on a part of the training data (Europarl+News Commentary), whereas variant 2 corresponds to the submitted system.</S>
			<S sid ="75" ssid = "28">A small-scale analysis indicated that the inflection prediction system tends to have problems with subject-verb agreement.</S>
			<S sid ="76" ssid = "29">We trained a factored system using additional PoS-tags with number information which lead to a small improvement on both variants.</S>
			<S sid ="77" ssid = "30">While the small model is significantly better than the baseline2 as it benefits more from the generalization, the result for the full system is worse than the baseline3.</S>
			<S sid ="78" ssid = "31">Here, given the large amount of data, the generalization effect has less influence.</S>
			<S sid ="79" ssid = "32">However, we assume that the more general model from the inflection prediction system produces better translations than a regular model containing a large amount of irrelevant inflectional information, particularly when considering that it can produce well-formed inflected sequences that are inaccessible to the baseline.</S>
			<S sid ="80" ssid = "33">Even though this is not reflected in terms of BLEU, humans might prefer the inflection prediction system.</S>
			<S sid ="81" ssid = "34">For the WMT2013 set, we obtain BLEU scores of 29.6 (ci) and 28.30 (cs) with the inflection prediction system mes-inflection (marked in table 5).</S>
	</SECTION>
	<SECTION title="Russian-English. " number = "5">
			<S sid ="82" ssid = "1">The preparation of the Russian data includes the following stages: (1) tokenization and tagging and (2) morphological reduction.</S>
			<S sid ="83" ssid = "2">Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) 2 Pairwise bootstrap resampling with 1000 samples..</S>
			<S sid ="84" ssid = "3">3 However, the large inflection-prediction system has a slightly better NIST score than the baseline (7.63 vs. 7.61).</S>
			<S sid ="85" ssid = "4">S MT ou tp ut wit h stem ma rku p in bol d pri nt pr ed ict ed fea tur es ge ne rat ed for m s aft er post pr oce ssi ng glo ss av ert is se m en t&lt; Ma sc &gt;&lt; Pl &gt;[ N] si ni st re [A D J] d e [ P ] l e [ A R T ] p e n t a g o n e &lt; M a s c &gt; &lt; S g &gt; [ N ] s u r [ P ] de [P ] ´ r´ d e [ P ] l e [ A R T ] bu dg et &lt;M as c&gt; &lt;S g&gt; [N ] d e [ P ] l e [ A R T ] d´ Ma sc.</S>
			<S sid ="86" ssid = "5">Pl Ma sc.</S>
			<S sid ="87" ssid = "6">Pl – M a s c . S g M a s c . S g – – F e m . P l F e m . P l – M a s c . S g M a s c . S g – F e m . S g F e m . S g ave rtis se me nts s i n i s t r e s d e l e p e n t a g o n e s u r d e e ´ v e n t u e l l e s r e ´ d u c t i o n s d e l e b u d g e t d e l a d e ´ f e n s e ave rtis se me nts s i n i s t r e s d u p e n t a g o n e s u r d ’ e ´ v e n t u e l l e s r e ´ d u c t i o n s d u b u d g e t d e l a d e ´ f e n s e wa rni ng s d i r e f r o m t h e p e n t a g o n o v e r of p ot e nt ia l re d u ct io n s of t h e b u d g e t o f th e de´f en se Table 4: Processing steps for the input sentence dire warnings from pentagon over potential defence cuts.</S>
			<S sid ="88" ssid = "7">that has been developed based on data tagged with TreeTagger (Schmid, 1994) using a model from Sharoff et al.</S>
			<S sid ="89" ssid = "8">(2008).</S>
			<S sid ="90" ssid = "9">The data processed by Tree- Tagger contained errors such as wrong definition of PoS for adverbs, wrong selection of gender for adjectives in plural and missing features for pronouns and adverbs.</S>
			<S sid ="91" ssid = "10">In order to train RFTagger, the output of TreeTagger was corrected with a set of empirical rules.</S>
			<S sid ="92" ssid = "11">In particular, the morphological features of nominal phrases were made consistent to train RFTagger: in contrast to TreeTagger, where morphological features are regarded as part of the PoS-tag, RFTagger allows for a separate handling of morphological features and POS tags.</S>
			<S sid ="93" ssid = "12">Despite a generally good tagging quality, some errors seem to be unavoidable due to the ambiguity of certain grammatical forms in Russian.</S>
			<S sid ="94" ssid = "13">A good example of this are neuter nouns that have the same form in all cases, or feminine nouns, which have identical forms in singular genitive and plural nominative (Sharoff et al., 2008).</S>
			<S sid ="95" ssid = "14">Since Russian has no binding word order, and the case of nouns cannot be determined on that basis, such errors cannot be corrected with empirical rules implemented as post Sy st e m BL EU (ci) BL EU (cs ) 1 Ba sel ine Infl Pr ed Infl Pr ed fac tor ed 2 4 . 9 1 2 5 . 3 1 2 5 . 5 3 2 3 . 4 0 2 3 . 8 1 2 4 . 0 4 2 Ba sel ine Infl Pr ed * Infl Pr ed fac tor ed 2 9 . 3 2 2 9 . 0 7 2 9 . 1 7 2 7 . 6 5 2 7 . 4 0 2 7 . 4 6 Table 5: Results for French inflection prediction on the WMT2012 test set.</S>
			<S sid ="96" ssid = "15">The marked system (*) corresponds to the system submitted for manual evaluation.</S>
			<S sid ="97" ssid = "16">processing.</S>
			<S sid ="98" ssid = "17">Similar errors occur when specifying the case of adjectives, since the suffixes of adjectives are even less varied as compared to the nouns.</S>
			<S sid ="99" ssid = "18">In our application, we hope that this type of error does not affect the result due to the following suppression of a number of morphological attributes including the case of adjectives.</S>
			<S sid ="100" ssid = "19">Morphological reduction In comparison to Slavic languages, English is morphologically poor.</S>
			<S sid ="101" ssid = "20">For example, English has no morphological attributes for nouns and adjectives to express gender or case; verbs have no gender either.</S>
			<S sid ="102" ssid = "21">In contrast, Russian is morphologically very rich – there are e.g. 6 cases and 3 grammatical genders, which manifest themselves in different suffixes for nouns, pronouns, adjectives and some verb forms.</S>
			<S sid ="103" ssid = "22">When translating from Russian into English, many of these attributes are (hopefully) redundant and are therefore deleted from the training data.</S>
			<S sid ="104" ssid = "23">The morphological reduction in our system was applied to nouns, pronouns, verbs, adjectives, prepositions and conjunctions.</S>
			<S sid ="105" ssid = "24">The rest of the POS (adverbs, particles, interjections and abbreviations) have no morphological attributes.</S>
			<S sid ="106" ssid = "25">The list of the original and the reduced attributes is given in Table 6.</S>
			<S sid ="107" ssid = "26">Transliteration mining to handle OOVs The machine translation system fails to translate out-of- vocabulary words (OOVs) as they are unknown to the training data.</S>
			<S sid ="108" ssid = "27">Most of the OOVs are named entities and transliterating them to the target language script could solve this problem.</S>
			<S sid ="109" ssid = "28">The transliteration system requires a list of transliteration pairs for training.</S>
			<S sid ="110" ssid = "29">As we do not have such a list, we use the unsupervised transliteration mining system of Sajjad et al.</S>
			<S sid ="111" ssid = "30">(2012) that takes a list of word pairs for Original corpus SY S W MT 20 12 W MT 20 13 GI ZA ++ TA GI ZA ++ 3 2 . 5 1 3 3 . 4 0 2 5 . 5 2 5 . 9 * Morph-reduced SY S W MT 20 12 W MT 20 13 GI ZA ++ TA GI ZA ++ 3 1 . 2 2 3 1 . 4 0 2 4 . 3 2 4 . 4 5 Table 7: Russian to English machine translation system evaluated on WMT2012 and WMT2013.</S>
			<S sid ="112" ssid = "31">Human evaluation in WMT13 is performed on the system trained using the original corpus with TAGIZA++ for alignment (marked with *).</S>
			<S sid ="113" ssid = "32">Table 6: Rules for simplifying the morphological complexity for RU.</S>
			<S sid ="114" ssid = "33">training and extracts transliteration pairs that can be used for the training of the transliteration system.</S>
			<S sid ="115" ssid = "34">The procedure of mining transliteration pairs and transliterating OOVs is described as follows: We word-align the parallel corpus using GIZA++ and symmetrize the alignments using the grow-diag- final-and heuristic.</S>
			<S sid ="116" ssid = "35">We extract all word pairs which occur as 1-to-1 alignments (Sajjad et al., 2011) and later refer to them as a list of word pairs.</S>
			<S sid ="117" ssid = "36">We train the unsupervised transliteration mining system on the list of word pairs and extract transliteration pairs.</S>
			<S sid ="118" ssid = "37">We use these mined pairs to build a transliteration system using the Moses toolkit.</S>
			<S sid ="119" ssid = "38">The transliteration system is applied as a post-processing step to transliterate OOVs.</S>
			<S sid ="120" ssid = "39">The morphological reduction of Russian (cf.</S>
			<S sid ="121" ssid = "40">section 5) does not process most of the OOVs as they are also unknown to the POS tagger.</S>
			<S sid ="122" ssid = "41">So OOVs that we get are in their original form.</S>
			<S sid ="123" ssid = "42">When translit erating them, the inflected forms generate wrong English transliterations as inflectional suffixes get transliterated too, specially OOV named entities.</S>
			<S sid ="124" ssid = "43">We solved this problem by stemming the OOVs based on a list of suffixes ( , , , , , ) and transliterating the stemmed forms.</S>
			<S sid ="125" ssid = "44">Experiments and results We trained the systems separately on GIZA++ and transliteration augmented-GIZA++ (TAGIZA++) to compare their results; for more details see Sajjad et al.</S>
			<S sid ="126" ssid = "45">(2013).</S>
			<S sid ="127" ssid = "46">All systems are tuned using PROv1 (Nakov et al., 2012).</S>
			<S sid ="128" ssid = "47">The translation output is post- processed to transliterate OOVs.</S>
			<S sid ="129" ssid = "48">Table 7 summarizes the results of RUEN translation systems trained on the original corpus and on the morph-reduced corpus.</S>
			<S sid ="130" ssid = "49">Using TAGIZA++ alignment gives the best results for both WMT 2012 and WMT2013, leading to an improvement of 0.4 BLEU points.</S>
			<S sid ="131" ssid = "50">The system built on the morph-reduced data leads to decreased BLEU results.</S>
			<S sid ="132" ssid = "51">However, the percentage of OOVs is reduced for both test sets when using the morph-reduced data set compared to the original data.</S>
			<S sid ="133" ssid = "52">An analysis of the output showed that the morph-reduced system makes mistakes in choosing the right tense of the verb, which might be one reason for this outcome.</S>
			<S sid ="134" ssid = "53">In the future, we would like to investigate this issue in detail.</S>
	</SECTION>
	<SECTION title="German to English and English to. " number = "6">
			<S sid ="135" ssid = "1">German We submitted systems for DEEN and ENDE which used constituent parses for pre-reordering.</S>
			<S sid ="136" ssid = "2">For DEEN we also deal with word formation issues such as compound splitting.</S>
			<S sid ="137" ssid = "3">We did not perform inflectional normalization or generation for German due to time constraints, instead focusing our efforts on these issues for French and Russian as previously described.</S>
			<S sid ="138" ssid = "4">German to English German has a wider diversity of clausal orderings than English, all of which need to be mapped to the English SVO order.</S>
			<S sid ="139" ssid = "5">This is a difficult problem to solve during inference, as shown for hierarchical SMT by Fabienne Braune and Fraser (2012) and for phrase-based SMT by Bisazza and Federico (2012).</S>
			<S sid ="140" ssid = "6">We syntactically parsed all of the source side sentences of the parallel German to English data available, and the tuning, test and blindtest sets.</S>
			<S sid ="141" ssid = "7">We then applied reordering rules to these parses.</S>
			<S sid ="142" ssid = "8">We use the rules for reordering German constituent parses of Collins et al.</S>
			<S sid ="143" ssid = "9">(2005) together with the additional rules described by Fraser (2009).</S>
			<S sid ="144" ssid = "10">These are applied as a preprocess to all German data.</S>
			<S sid ="145" ssid = "11">For parsing the German sentences, we used the generative phrase-structure parser BitPar with optimizations of the grammar, as described by Fraser et al.</S>
			<S sid ="146" ssid = "12">(2013).</S>
			<S sid ="147" ssid = "13">The parser was trained on the Tiger Treebank (Brants et al., 2002) along with utilizing the Europarl corpus as unlabeled data.</S>
			<S sid ="148" ssid = "14">At the training of Bitpar, we followed the targeted self-training approach (Katz-Brown et al., 2011) as follows.</S>
			<S sid ="149" ssid = "15">We parsed the whole Europarl corpus using a grammar trained on the Tiger corpus and extracted the 100- best parse trees for each sentence.</S>
			<S sid ="150" ssid = "16">We selected the parse tree among the 100 candidates which got the highest usefulness scores for the reordering task.</S>
			<S sid ="151" ssid = "17">Then we trained a new grammar on the concatenation of the Tiger corpus and the automatic parses from Europarl.</S>
			<S sid ="152" ssid = "18">The usefulness score estimates the value of a parse tree for the reordering task.</S>
			<S sid ="153" ssid = "19">We calculated this score as the similarity between the word order achieved by applying the parse tree-based reordering rules of Fraser (2009) and the word order indicated by the automatic word alignment between the German and English sentences in Europarl.</S>
			<S sid ="154" ssid = "20">We used the Kendall’s Tau Distance as the similarity metric of two word orderings (as suggested by Birch and Osborne (2010)).</S>
			<S sid ="155" ssid = "21">Following this, we performed linguistically- informed compound splitting, using the system of Fritzinger and Fraser (2010), which disambiguates competing analyses from the high-recall Stuttgart Morphological Analyzer SMOR (Schmid et al., 2004) using corpus statistics.</S>
			<S sid ="156" ssid = "22">We also split German portmanteaus like zum → zu dem (meaning to the).</S>
			<S sid ="157" ssid = "23">Table 8: Results on WMT2013 (blindtest) English to German The task of mapping English SVO order to the different clausal orders in German is difficult.</S>
			<S sid ="158" ssid = "24">For our English to German systems, we solved this by parsing the English and applying the system of Gojun and Fraser (2012) to reorder English into the correct German clausal order (depending on the clause type which is detected using the English parse, see (Gojun and Fraser, 2012) for further details).</S>
			<S sid ="159" ssid = "25">We primarily used the CharniakJohnson generative parser (Charniak and Johnson, 2005) to parse the English Europarl data and the test data.</S>
			<S sid ="160" ssid = "26">However, due to time constraints we additionally used Berkeley parses of about 400K Europarl sentences and the other English parallel training data.</S>
			<S sid ="161" ssid = "27">We also left a small amount of the English parallel training data unparsed, which means that it was not reordered.</S>
			<S sid ="162" ssid = "28">For tune, test and blindtest (WMT 2013), we used the CharniakJohnson generative parser.</S>
			<S sid ="163" ssid = "29">Experiments and results We used all available training data for constrained systems; results for the WMT2013 set are given in table 8.</S>
			<S sid ="164" ssid = "30">For the contrastive BitPar results, we reparsed WMT 2013.</S>
			<S sid ="165" ssid = "31">7 Conclusion We presented 5 systems dealing with complex morphology.</S>
			<S sid ="166" ssid = "32">For two language pairs with a morpho- logically rich source language (FR and RU), the input was reduced to a simplified representation containing only translation relevant morphological information (e.g. number on nouns).</S>
			<S sid ="167" ssid = "33">We also used reordering techniques for DEEN and ENDE.</S>
			<S sid ="168" ssid = "34">For translating into a language with rich morphology (ENFR), we applied a two-step method that first translates into a stemmed representation of the target language and then generates inflected forms based on morphological features predicted on monolingual data.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="169" ssid = "35">We would like to thank the anonymous reviewers for their helpful feedback and suggestions, Daniel Quernheim for providing Berkeley parses of some of the English data, Stefan Ru¨ d for help with the manual evalution, and Philipp Koehn and Barry Haddow for providing data and alignments.</S>
			<S sid ="170" ssid = "36">Nadir Durrani was funded by the EuropeanUnion Seventh Framework Programme (FP7/2007 2013) under grant agreement n. 287658.</S>
			<S sid ="171" ssid = "37">Alexander Fraser was funded by Deutsche Forschungsgemeinschaft grant Models of Morphosyntax for Statistical Machine Translation and from the European Community’s Seventh Framework Pro- gramme (FP7/20072013) under Grant Agreement n. 248005.</S>
			<S sid ="172" ssid = "38">Marion Weller was funded from the European Community’s Seventh Framework Pro- gramme (FP7/20072013) under Grant Agreement n. 248005.</S>
			<S sid ="173" ssid = "39">Svetlana Smekalova was funded by Deutsche Forschungsgemeinschaft grant Models of Morphosyntax for Statistical Machine Translation.</S>
			<S sid ="174" ssid = "40">Helmut Schmid and Max Kisselew were supported by Deutsche Forschungsgemeinschaft grant SFB 732.</S>
			<S sid ="175" ssid = "41">Richa´rd Farkas was supported by the European Union and the European Social Fund through project FuturICT.hu (grant n. TA´ MOP 4.2.2.C-11/1/KONV2012-0013).</S>
			<S sid ="176" ssid = "42">This publication only reflects the authors’ views.</S>
	</SECTION>
</PAPER>
