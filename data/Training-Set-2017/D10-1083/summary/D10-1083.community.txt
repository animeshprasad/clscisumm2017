<S sid ="6" ssid = "6">Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts.</S>
<S sid ="9" ssid = "9">Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.</S>
<S sid ="20" ssid = "20">The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.</S>
<S sid ="21" ssid = "21">Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.</S>
<S sid ="22" ssid = "22">In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.</S>
<S sid ="27" ssid = "27">First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).</S>
<S sid ="52" ssid = "1">We consider the unsupervised POS induction problem without the use of a tagging dictionary.</S>
<S sid ="85" ssid = "34">Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution ψ over tag assignments drawn from DIRICHLET(β, K ).</S>
<S sid ="97" ssid = "2">During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior:</S>
<S sid ="99" ssid = "4">TheFigure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration</S>
<S sid ="112" ssid = "3">For all languages we do not make use of a tagging dictionary.</S>
<S sid ="239" ssid = "100">Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.</S>
<S sid ="155" ssid = "16">5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5).</S>
<S sid ="236" ssid = "97">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S>
<S sid ="243" ssid = "104">We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.</S>
