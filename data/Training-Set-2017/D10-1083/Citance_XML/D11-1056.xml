<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource.</S>
		<S sid ="2" ssid = "2">Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria.</S>
		<S sid ="3" ssid = "3">To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation.</S>
		<S sid ="4" ssid = "4">We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text.</S>
		<S sid ="5" ssid = "5">For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum.</S>
		<S sid ="6" ssid = "6">Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Word segmentation is the first step of natural language processing for Japanese, Chinese and Thai because they do not delimit words by white-space.</S>
			<S sid ="8" ssid = "8">Segmentation for Japanese is a successful field of research, achieving the F-score of nearly 99% (Kudo et al., 2004).</S>
			<S sid ="9" ssid = "9">This success rests on a high-coverage dictionary.</S>
			<S sid ="10" ssid = "10">Unknown words, or words not covered by the dictionary, are often misidentified.</S>
			<S sid ="11" ssid = "11">Historically, researchers have devoted extensive human resources to build and maintain high coverage dictionaries (Yokoi, 1995).</S>
			<S sid ="12" ssid = "12">Since the orthography of Japanese does not specify a standard for segmentation, researchers define their own criteria before constructing lexical resources.</S>
			<S sid ="13" ssid = "13">For this reason, it is difficult to exploit existing external resources, such as dictionaries and encyclopedias for human readers, where entry words are not segmented according to the criteria.</S>
			<S sid ="14" ssid = "14">Among them, encyclopedias are especially important in that they contain a lot of terms that a morphological dictionary fails to cover.</S>
			<S sid ="15" ssid = "15">Most of these terms are noun phrases and consist of more than one word (morpheme).</S>
			<S sid ="16" ssid = "16">For example, an encyclopedia has an en try “常山城” (tsuneyamajou, “Tsuneyama Castle”).</S>
			<S sid ="17" ssid = "17">According to our segmentation criteria, it consists of two words “常山” (tsuneyama) and “城” (jou).</S>
			<S sid ="18" ssid = "18">However, the morphological analyzer wrongly segments it into “常” (tsune) and “山城” (yamashiro) because “常山” (tsuneyama) is an unknown word.</S>
			<S sid ="19" ssid = "19">In this paper, we present the first attempt to utilize encyclopedias for word segmentation.</S>
			<S sid ="20" ssid = "20">We segment each entry noun phrase into words.</S>
			<S sid ="21" ssid = "21">To do this, we examine the main text of the entry, on the assumption that if the noun phrase in question consists of more than one word, its constituents appear in the main text either freely or as part of other noun phrases.</S>
			<S sid ="22" ssid = "22">For “常山城” (tsuneyamajou), its constituent “常山” (tsune) appears by itself and as constituents of other nouns phrases such as “常山山 頂” (peak of Tsuneyama) and “常山駅” (Tsuneyama Station) while “山城” (yamashiro) does not.</S>
			<S sid ="23" ssid = "23">To segment each noun phrase, we use non- parametric Bayesian language models (Goldwater et al., 2009; Mochihashi et al., 2009).</S>
			<S sid ="24" ssid = "24">Our approach 605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 605–615, Edinburgh, Scotland, UK, July 27–31, 2011.</S>
			<S sid ="25" ssid = "25">Qc 2011 Association for Computational Linguistics is based on two key factors: the bigram model and type-based block sampling.</S>
			<S sid ="26" ssid = "26">The bigram model alleviates a problem of the unigram model, that is, a tendency to misidentify a sequence of words in common collocations as a single word.</S>
			<S sid ="27" ssid = "27">Type-based sampling (Liang et al., 2010) has the ability to directly escape a local optimum, making inference very efficient.</S>
			<S sid ="28" ssid = "28">However, type-based sampling is not easily applicable to the bigram model owing to sparsity and its dependence on latent assignments.</S>
			<S sid ="29" ssid = "29">We propose a hybrid type-based sampling procedure, which combines the Metropolis-Hastings algorithm with Gibbs sampling.</S>
			<S sid ="30" ssid = "30">We circumvent the sparsity problem by joint sampling of unigram-level type.</S>
			<S sid ="31" ssid = "31">Also, instead of calculating the probability of every possible state of the jointly sampled random variables, we only compare the current state with a proposed state.</S>
			<S sid ="32" ssid = "32">This greatly eases the sampling procedure while retaining the efficiency of type- based sampling.</S>
			<S sid ="33" ssid = "33">Experiments show that the proposed method quickly corrects the initial segmentation given by a morphological analyzer.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="34" ssid = "1">Japanese Morphological Analysis and Lexical Acquisition Word segmentation for Japanese is usually solved as the joint task of segmentation and part-of-speech tagging, which is called morphological analysis (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004).</S>
			<S sid ="35" ssid = "2">The standard approach in Japanese morphological analysis is lattice-based path selection instead of character- based IOB tagging.</S>
			<S sid ="36" ssid = "3">Given a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using predefined parameters.</S>
			<S sid ="37" ssid = "4">This approach enables fast decoding and achieves accuracy high enough for practical use.</S>
			<S sid ="38" ssid = "5">This success, however, depends on a high- coverage dictionary, and unknown words are often misidentified.</S>
			<S sid ="39" ssid = "6">Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004), it by no means provides a definitive solution because it suffers from locality of contextual information available for identification (Nakagawa and Matsumoto, 2006).</S>
			<S sid ="40" ssid = "7">Therefore we like to perform separate lexical acquisition processes in which wider context can be examined.</S>
			<S sid ="41" ssid = "8">Our approach in this paper has a complementary relationship with unknown word acquisition from text, which we previously proposed (Murawaki and Kurohashi, 2008).</S>
			<S sid ="42" ssid = "9">Since, unlike Chinese and Thai, Japanese is rich in morphology, morphological regularity can be used to determine if an unknown word candidate in text is indeed the word to be acquired.</S>
			<S sid ="43" ssid = "10">In general, this method works pretty well, but one exception is noun phrases.</S>
			<S sid ="44" ssid = "11">Noun phrases can hardly be distinguished from single nouns because in Japanese, no morphological marker is attached to join nouns to form a noun phrase.</S>
			<S sid ="45" ssid = "12">We previously resort to a heuristic measure to segment noun phrases.</S>
			<S sid ="46" ssid = "13">The new statistical method provides a straightforward solution to this problem.</S>
			<S sid ="47" ssid = "14">Meanwhile, our language models have their own problem.</S>
			<S sid ="48" ssid = "15">The assumption that language is a sequence of invariant words fails to capture rich morphology, as our segmentation criteria specify that each verb or adjective consists of an invariant stem and an ending that changes its form according to its grammatical roles.</S>
			<S sid ="49" ssid = "16">For this reason, we limit our scope to noun phrases in this paper.</S>
			<S sid ="50" ssid = "17">Use of Noun Phrases Named entity recognition (NER) is a field where encyclopedic knowledge plays an important role.</S>
			<S sid ="51" ssid = "18">Kazama and Torisawa (2008) encode information extracted from a gazetteer (e.g. Wikipedia) as features of a CRF- based Japanese NE tagger.</S>
			<S sid ="52" ssid = "19">They formalize the NER task as the character-based labeling of IOB tags.</S>
			<S sid ="53" ssid = "20">Noun phrases extracted from a gazetteer are also straightforwardly represented as IOB tags.</S>
			<S sid ="54" ssid = "21">However, this does not fully solve the knowledge bottleneck problem.</S>
			<S sid ="55" ssid = "22">They also used the output of a morphological analyzer, which does not utilize encyclopedic knowledge.</S>
			<S sid ="56" ssid = "23">NER performance may be affected by segmentation errors in morphological analysis involving unknown words.</S>
			<S sid ="57" ssid = "24">Chinese word segmentation is often formalized as a character tagging problem (Xue, 2003).</S>
			<S sid ="58" ssid = "25">In this setting, it is easy to incorporate external resources into the model.</S>
			<S sid ="59" ssid = "26">Low et al.</S>
			<S sid ="60" ssid = "27">(2005) introduce an external dictionary as features of a discriminative model.</S>
			<S sid ="61" ssid = "28">However, they only use words up to 4 characters in length.</S>
			<S sid ="62" ssid = "29">We conjecture that words in their dictionary are not noun phrases.</S>
			<S sid ="63" ssid = "30">External resources used by Peng et al.</S>
			<S sid ="64" ssid = "31">(2004) are also lists of short words and characters.</S>
			<S sid ="65" ssid = "32">Non-parametric Language Models Non- parametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009).</S>
			<S sid ="66" ssid = "33">It does not compete with supervised segmentation, however.</S>
			<S sid ="67" ssid = "34">Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010).</S>
			<S sid ="68" ssid = "35">In this paper we demonstrate that non-parametric models can complement supervised segmentation.</S>
	</SECTION>
	<SECTION title="Japanese Noun Phrase Segmentation. " number = "3">
			<S sid ="69" ssid = "1">Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers.</S>
			<S sid ="70" ssid = "2">In our settings, we are given a list of entries from external resources.</S>
			<S sid ="71" ssid = "3">Almost all of them are noun phrases and each entry consists of one or more words.</S>
			<S sid ="72" ssid = "4">A na¨ıve implementation would be to use noun phrases as they are.</S>
			<S sid ="73" ssid = "5">In fact, ipadic1 regards as single words a large number of long proper nouns like “関西国際空港会社連絡橋” (literally, Kansai Interna tional Airport Company Connecting Bridge).</S>
			<S sid ="74" ssid = "6">However, this approach has various drawbacks.</S>
			<S sid ="75" ssid = "7">For example, in information retrieval, the query “Kansai International Airport” does not match the “single” word for the bridge.</S>
			<S sid ="76" ssid = "8">So we apply segmentation.</S>
			<S sid ="77" ssid = "9">Each entry is associated with text, which is usually the main text of the entry.2 We assume the text as the key to segmenting the noun phrase.</S>
			<S sid ="78" ssid = "10">If the noun phrase in question consists of more than one word, its constituents would appear in the text either freely or as part of other noun phrases.</S>
			<S sid ="79" ssid = "11">We obtain the segmentation of an entry noun phrase by considering the segmentation of the whole 1 http://sourceforge.jp/projects/ipadic/ 2 We may augment the text with related documents if the main text is not large enough.</S>
			<S sid ="80" ssid = "12">text.</S>
			<S sid ="81" ssid = "13">One may instead consider a pipeline approach in which we first extract noun phrases in text and then identify boundaries within these noun phrases.</S>
			<S sid ="82" ssid = "14">However, noun phrases in text are not trivially identifiable in the case that they contain unknown words as their constituents.</S>
			<S sid ="83" ssid = "15">For example, the analyzer erroneously segments the word “ちん すこう” (chiNsukou) into “ちん” (chiN) and “すこ う” (sukou), and since the latter is misidentified as a verb, the incorrect noun phrase “ちん” (chiN) is extracted.</S>
			<S sid ="84" ssid = "16">We have a morphological analyzer with a dictionary that covers frequent words.</S>
			<S sid ="85" ssid = "17">Although it often misidentifies unknown words, the overall accuracy is reasonably high.</S>
			<S sid ="86" ssid = "18">For this reason, we like to use the segmentation given by the analyzer as the initial state and to make small changes to them to get a desired output.</S>
			<S sid ="87" ssid = "19">We also use an annotated corpus, which was used to build the analyzer.</S>
			<S sid ="88" ssid = "20">As the annotated corpus encodes our segmentation criteria, it can be used to force the models to stick with our segmentation criteria.</S>
			<S sid ="89" ssid = "21">We concentrate on segmentation in this paper, but we also need to assign a POS tag to each constituent word and to incorporate segmented noun phrases into the dictionary of the morphological analyzer.</S>
			<S sid ="90" ssid = "22">We leave them for future work.3</S>
	</SECTION>
	<SECTION title="Non-parametric  Bayesian Language. " number = "4">
			<S sid ="91" ssid = "1">Models To correct the initial segmentation given by the analyzer, we use non-parametric Bayesian language models that have been applied to unsupervised word segmentation (Goldwater et al., 2009).</S>
			<S sid ="92" ssid = "2">Specifically, we adopt unigram and bigram models.</S>
			<S sid ="93" ssid = "3">We propose a small modification to these models in order to exploit an annotated corpus when it is much larger than raw text.</S>
			<S sid ="94" ssid = "4">4.1 Unigram Model.</S>
			<S sid ="95" ssid = "5">In the unigram model, a word in the corpus wi is generated as follows: G|α0, P0 ∼ DP(α0, P0) wi|G ∼ G 3 Fortunately, the morphological analyzer JUMAN is capa-.</S>
			<S sid ="96" ssid = "6">ble of handling phrases, each of which consists of more than one word.</S>
			<S sid ="97" ssid = "7">All we need to do is POS tagging.</S>
			<S sid ="98" ssid = "8">where G is a distribution over a countably infinite set of words, and DP(α0, P0) is a Dirichlet process (Ferguson, 1973) with the concentration parameter α0 and the base distribution P0, for which we use a zerogram model described in Section 4.3.</S>
			<S sid ="99" ssid = "9">Marginalizing out G, we can interpret the model as a Chinese restaurant process.</S>
			<S sid ="100" ssid = "10">Suppose that we have observed i − 1 words w−i = w1, · · · , wi−1, 4.3 Zerogram Model.</S>
			<S sid ="101" ssid = "11">Following Nagata (1996) and Mochihashi et al.</S>
			<S sid ="102" ssid = "12">(2009), we model the zerogram distribution P0 with the word length k and the character sequence w =c1, · · · , ck . Specifically, we define P0 as the combi nation of a Poisson distribution with mean λ and a bigram distribution over characters.</S>
			<S sid ="103" ssid = "13">P (c1, · · · , ck , k|Θ) the probability of wi is given by w + α0P0 (1) P0(w) = P (k; λ) λk P (k|Θ) P1(wi = w|w−i) = , i − 1 + α0 P (k; λ) = e−λ k ! where nw−i is the number of word label w observed in w−i. The unigram model is known for its tendency to P (c1, · · · , ck , k|Θ) = k+1 ∏ i=1 P (ci|ci−1) misidentify a sequence of words in common collocations as a single word (Goldwater et al., 2009).</S>
			<S sid ="104" ssid = "14">In preliminary experiments, we found that the unigram model often interpreted a noun phrase as a single word, even in the case that its constituents frequently appeared in text.</S>
			<S sid ="105" ssid = "15">4.2 Bigram Model.</S>
			<S sid ="106" ssid = "16">The problem of the unigram model can be alleviated by the bigram model based on a hierarchical Dirichlet process (Goldwater et al., 2009).</S>
			<S sid ="107" ssid = "17">In the bigram model, word wi is generated as follows: G|α0, P0 ∼ DP(α0, P0) Hl |α1, G ∼ DP(α1, G) wi|wi−1 = l, Hl ∼ Hl Marginalizing out G and Hl , we can again explainthe model with the Chinese restaurant process.</S>
			<S sid ="108" ssid = "18">Un like the unigram model, however, the bigram model depends on the latent table assignments z−i.</S>
			<S sid ="109" ssid = "19">(wi−1 ,wi ) + α1P1(wi|h−i) Θ is the zerogram model, and c0 and ck+1 are a wordboundary marker.</S>
			<S sid ="110" ssid = "20">P (k|Θ) can be estimated by ran domly generating words from the model.</S>
			<S sid ="111" ssid = "21">We use different λ for different scripts.</S>
			<S sid ="112" ssid = "22">The Japanese writing system uses several scripts, and each word can be classified by script such as hiragana, katakana, kanji, the mixture of hiragana and kanji, etc. The optimal value for λ depends on scripts.</S>
			<S sid ="113" ssid = "23">For example, katakana, which predominantly denotes loan words, is longer on average than hiragana, which is often used for short function words.</S>
			<S sid ="114" ssid = "24">We obtain the parameters and counts from an annotated corpus and fix them during noun phrase segmentation.</S>
			<S sid ="115" ssid = "25">This greatly simplifies inference but may make the model fragile with unknown words.</S>
			<S sid ="116" ssid = "26">For this reason, we set a hierarchical PitmanYor process prior (Teh, 2006; Goldwater et al., 2006) for the bigram probability P (ci|ci−1) with the base distribu tion of character unigrams.</S>
			<S sid ="117" ssid = "27">Note that even character bigrams are sparse because thousands of characters are used in Japanese.</S>
			<S sid ="118" ssid = "28">P2(wi|h−i) = th−i (wi−1 ,∗) + α1 (2) 4.4 Mixing an.</S>
			<S sid ="119" ssid = "29">Annotated Corpus An annotated corpus can be used to force the mod P1(wi|h−i) = wi + α0P0(wi) ∗ + α0 h−i (3) els to stick with our segmentation criteria.</S>
			<S sid ="120" ssid = "30">A straightforward way to do this is to mix it with raw text while fixing the segmentation during infer where h−i = (w−i, z−i), twi is the number of tables labeled with wi and th−i is the total number of tables.</S>
			<S sid ="121" ssid = "31">Thanks to exchangeability, we do not need to track the exact seating assignments.</S>
			<S sid ="122" ssid = "32">Still, we need to maintain a histogram for each w that consists of frequencies of table customers (Blunsom et al., 2009).</S>
			<S sid ="123" ssid = "33">ence (Mochihashi et al., 2009).</S>
			<S sid ="124" ssid = "34">A word found in the annotated corpus is generally preferred because it has fixed counts obtained from the annotated corpus.</S>
			<S sid ="125" ssid = "35">We call this method direct mixing.</S>
			<S sid ="126" ssid = "36">Direct mixing is problematic when raw text is much smaller than the annotated corpus.</S>
			<S sid ="127" ssid = "37">With this situation, the role of raw text associated with the noun phrase in question is marginalized by the annotated corpus.</S>
			<S sid ="128" ssid = "38">As a solution to this problem, we propose another mixing method called back-off mixing.</S>
			<S sid ="129" ssid = "39">In back-off mixing, the annotated corpus is used as part of the base distribution.</S>
			<S sid ="130" ssid = "40">In the unigram model, P0 in (1) is replaced by a local optimum although it is not too distant from the global optimum.</S>
			<S sid ="131" ssid = "41">The collapsed Gibbs sampler is easily entrapped by this local optimum.</S>
			<S sid ="132" ssid = "42">For this reason, the initial segmentation is usually chosen at random (Goldwater et al., 2009).</S>
			<S sid ="133" ssid = "43">Sentence-based block sampling is also susceptible to consistent initialization (Liang et al., 2010).</S>
			<SUBSECTION>5.2 Type-based Sampling.</SUBSECTION>
			<S sid ="134" ssid = "44">P BM REF 0 = λIPP0 + (1 − λIP)P1 , where λIP is a parameter for linear interpolation and 1 is the unigram probability obtained from the annotated text.</S>
			<S sid ="135" ssid = "45">The loose coupling makes the models robust to an imbalanced pair of texts.</S>
			<S sid ="136" ssid = "46">Similarly, the back-off mixing bigram model replaces P1 in (2) withTo achieve fast convergence, we adopt a block sam pling algorithm named type-based sampling (Liang et al., 2010).</S>
			<S sid ="137" ssid = "47">For the unigram model, a type-based sampler jointly samples multiple positions that share the same type.</S>
			<S sid ="138" ssid = "48">Two positions have the same type if the corresponding areas are both of the form w1 or w2w3.</S>
			<S sid ="139" ssid = "49">Type-based sampling takes advantage of the exchangeability of multiple positions with the P BM REF same type.</S>
			<S sid ="140" ssid = "50">Given n positions with the same type, 1 = λIPP1 + (1 − λIP)P2 .</S>
	</SECTION>
	<SECTION title="Inference. " number = "5">
			<S sid ="141" ssid = "1">Collapsed Gibbs sampling is widely used to find an optimal segmentation (Goldwater et al., 2009).</S>
			<S sid ="142" ssid = "2">In this section, we first show that simple collapsed sampling can hardly escape the initial segmentation.</S>
			<S sid ="143" ssid = "3">To address this problem, we apply a block sampling algorithm named type-based sampling (Liang et al., 2010) to the unigram model.</S>
			<S sid ="144" ssid = "4">Since type-based sampling is not applicable to the bigram model, we propose a novel sampling procedure for the bigram model, which we call hybrid type-based sampling.</S>
			<S sid ="145" ssid = "5">5.1 Collapsed Sampling.</S>
			<S sid ="146" ssid = "6">In collapsed Gibbs sampling, the sampler repeatedly samples every possible boundary position, conditioned on the current state of the rest of the corpus.</S>
			<S sid ="147" ssid = "7">It stochastically decides whether the corresponding local area consists of a single word w1 or two words w2w3 (w1 = w2.w3).</S>
			<S sid ="148" ssid = "8">The conditional probabilities can be derived from (1).</S>
			<S sid ="149" ssid = "9">Collapsed sampling is known for slow convergence.</S>
			<S sid ="150" ssid = "10">This property is especially problematic in our settings where the initial segmentation is given by a morphological analyzer.</S>
			<S sid ="151" ssid = "11">Since the analyzer deterministically segments text using predefined parameters, the resultant segmentation is fairly consistent.</S>
			<S sid ="152" ssid = "12">Segmentation errors involving unknown words also occur in a regular way.</S>
			<S sid ="153" ssid = "13">Intuitively, we start withthe sampler first samples the number of new bound aries m′ (0 ≤ m′ ≤ n), and then uniformly arranges m′ boundaries out of n positions.</S>
			<S sid ="154" ssid = "14">Type-based sampling has the ability to jump from a local optimum (e.g. consistently segmented) to another stable state (consistently unsegmented).</S>
			<S sid ="155" ssid = "15">While Liang et al.</S>
			<S sid ="156" ssid = "16">(2010) used random initialization, we take particular note of the possibility of efficiently correcting the consistent segmentation by the analyzer.</S>
			<S sid ="157" ssid = "17">Type-based sampling is, however, not applicable to the bigram model for two reasons.</S>
			<S sid ="158" ssid = "18">The first problem is sparsity.</S>
			<S sid ="159" ssid = "19">For the bigram model, we need to consider adjacent words, wl on the left and wr on the right.</S>
			<S sid ="160" ssid = "20">This means that each type consists of three or four words, wl w1wr or wl w2w3wr . Consequently, few positions share the same type and we fail to change closely-related areas wl′ w1wr′ and wl′ w2w3wr′ , making inference inefficient.</S>
			<S sid ="161" ssid = "21">The second and more fundamental problem arises from the hierarchical settings.</S>
			<S sid ="162" ssid = "22">Since the bigram model depends on latent table assignments, the joint distribution of multiple positions is no longer a closed-form function of counts.</S>
			<S sid ="163" ssid = "23">Strictly speaking, we need to update the model counts even when sampling one position because the observation of the bigram ⟨wl w1⟩, for example, may affect the probability P2(w2|h−, ⟨wl w1⟩).</S>
			<S sid ="164" ssid = "24">Goldwater et al.</S>
			<S sid ="165" ssid = "25">(2009) approximate the probability by not updating the model counts in collapsed Gibbs sampling (i.e. P2(w2|h−, ⟨wl w1⟩) ≈ P2(w2|h−)).</S>
			<S sid ="166" ssid = "26">They rely on the assumption that repeated bigrams are rare.</S>
			<S sid ="167" ssid = "27">Obviously this does not hold true for type- based sampling.</S>
			<S sid ="168" ssid = "28">Hence for type-based sampling, we have to update the model counts whenever we observe a new word.</S>
			<S sid ="169" ssid = "29">One way to obtain the joint probability is to explicitly simulate the updates of histograms and other model counts.</S>
			<S sid ="170" ssid = "30">This is very cumbersome as we need to simulate n + 1 ways of model updates.</S>
			<S sid ="171" ssid = "31">5.3 Hybrid Type-based Sampling.</S>
			<S sid ="172" ssid = "32">0.4 0.3 0.2 0.1 0 0 2 4 6 8 10 m’ 0.4 0.3 0.2 0.1 0 To address these problems, we propose a hybrid sampler which incorporates the Metropolis-Hastings algorithm into blocked Gibbs sampling.</S>
			<S sid ="173" ssid = "33">Metropolis- Hastings is another technique for sampling from a Figure 1: Probability of # of boundaries f10 (m′ ; 3).</S>
			<S sid ="174" ssid = "34">is defined as follows: Markov chain.</S>
			<S sid ="175" ssid = "35">It first draws a proposed next state h′ based on the current state h according to some fn(m′; m) Q(h′; h) = nCm′ − In(m, m′) , (5) proposal distribution Q(h′; h).</S>
			<S sid ="176" ssid = "36">Then it accepts the proposal with the probability of where In(m, m′) is 1 if m ∈/ {0, n} and m′ = m; otherwise 0.</S>
			<S sid ="177" ssid = "37">min { P (h′)Q(h; h′) } , 1 P (h)Q(h′; h) .</S>
			<S sid ="178" ssid = "38">(4) We construct fn(m′; m) by discretizing a beta distribution (α = β &lt; 1) and a normal distribution with mean m, as shown in Figure 1.</S>
			<S sid ="179" ssid = "39">The former.</S>
			<S sid ="180" ssid = "40">fa If the proposal is not accepted, the current state is used as the next state.</S>
			<S sid ="181" ssid = "41">Metropolis-Hastings is useful when it is difficult to directly sample from P . We use the Metropolis-Hastings algorithm within Gibbs sampling.</S>
			<S sid ="182" ssid = "42">Instead of calculating the n + 1 probabilities of the number of boundaries, we only compare the current state with a proposed boundary arrangement.</S>
			<S sid ="183" ssid = "43">Also, the set of positions sampled jointly is chosen at unigram-level type instead of bigram-level type.</S>
			<S sid ="184" ssid = "44">The positions are no longer exchangeable.</S>
			<S sid ="185" ssid = "45">Therefore we calculate the conditional probability of one specific boundary arrangement.When n = 1, the only choice is to flip the current state (i.e.</S>
			<S sid ="186" ssid = "46">(m, m′) ∈ {(0, 1), (1, 0)}).</S>
			<S sid ="187" ssid = "47">This re duces to simple collapsed sampling.</S>
			<S sid ="188" ssid = "48">Otherwise we draw a proposed state in two steps.</S>
			<S sid ="189" ssid = "49">Given the n positions and the number of current boundaries m, we first draw the number of proposed boundaries m′ from a probability distribution fn(m′; m).</S>
			<S sid ="190" ssid = "50">We then randomly arrange m′ boundaries.</S>
			<S sid ="191" ssid = "51">The probability mass is uniformly divided by nCm′ arrangements.</S>
			<S sid ="192" ssid = "52">vors extreme values while the latter prefers smaller moves.</S>
			<S sid ="193" ssid = "53">The sampling of each type is done in the following steps.</S>
			<S sid ="194" ssid = "54">1.</S>
			<S sid ="195" ssid = "55">Collect n positions that share a unigram-level.</S>
			<S sid ="196" ssid = "56">type.</S>
			<S sid ="197" ssid = "57">2.</S>
			<S sid ="198" ssid = "58">Propose a new boundary arrangement.</S>
			<S sid ="199" ssid = "59">In what.</S>
			<S sid ="200" ssid = "60">follows, we only focus on flipped boundaries because the rest does not change the likelihood ratio of the current and proposed states.</S>
			<S sid ="201" ssid = "61">3.</S>
			<S sid ="202" ssid = "62">Calculate the current conditional probability..</S>
			<S sid ="203" ssid = "63">This can be done by repeatedly applying (2) while removing words one-by-one and updating the model counts accordingly.</S>
			<S sid ="204" ssid = "64">4.</S>
			<S sid ="205" ssid = "65">Calculate the proposed conditional probability.</S>
			<S sid ="206" ssid = "66">while adding words one-by-one.</S>
			<S sid ="207" ssid = "67">5.</S>
			<S sid ="208" ssid = "68">Decide whether to accept the proposal accord-.</S>
			<S sid ="209" ssid = "69">ing to (4).</S>
			<S sid ="210" ssid = "70">If the proposal is accepted, we finalize the arrangement; otherwise we revert to the current state.</S>
			<S sid ="211" ssid = "71">One exception is the case when m ∈/ {0, n} and We implement skip approximation (Liang et al., m′ = m. In this case we perform permutation to obtain h′ ̸= h. To sum up, the proposal distribution 2010) and sample each type once per iteration.</S>
			<S sid ="212" ssid = "72">This is motivated by the observation that although the joint sampling of a large number of positions is computationally expensive, the proposal is accepted very infrequently.</S>
			<S sid ="213" ssid = "73">5.4 Additional Constraints.</S>
			<S sid ="214" ssid = "74">Partial annotations (Tsuboi et al., 2008; Neubig and Mori, 2010) can be used for inference.</S>
			<S sid ="215" ssid = "75">If we know in advance that a certain position is a boundary or non- boundary, we simply keep it unaltered.</S>
			<S sid ="216" ssid = "76">As partially- annotated text, we can use markup.</S>
			<S sid ="217" ssid = "77">Suppose that the original text is written with wiki markup as follows: *JR[[宇野線]][[常山駅]] [gloss] JR Ube Line Tsuneyama Station It is clear that the position between “線” (line) and “常” (tsune) is a boundary.</S>
			<S sid ="218" ssid = "78">Similarly, we can impose our trivial rules of segmentation on the model.</S>
			<S sid ="219" ssid = "79">For example, we can keep punctuation markers (Li and Sun, 2009) separate from others.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "6">
			<S sid ="220" ssid = "1">6.1 Settings.</S>
			<S sid ="221" ssid = "2">Data Set We evaluated our approach on Japanese Wikipedia.</S>
			<S sid ="222" ssid = "3">For each entry of Wikipedia, we regarded the title as a noun phrase and used both the title and main text for segmentation.</S>
			<S sid ="223" ssid = "4">We separately applied our segmentation procedure to each entry.</S>
			<S sid ="224" ssid = "5">We constructed the data set as follows.</S>
			<S sid ="225" ssid = "6">We extracted each entry from an XML dump of Japanese Wikipedia.4 We normalized the title by dropping trailing parentheses that disambiguate entries with similar names (e.g. “赤城 (空母)” for Akagi (aircraft carrier)).</S>
			<S sid ="226" ssid = "7">We extracted the main text from wikitext and used wiki markup as boundary markers.</S>
			<S sid ="227" ssid = "8">We applied both the title and main text to the morphological analyzer JUMAN5 to get an initial segmentation.</S>
			<S sid ="228" ssid = "9">If the resultant segmentation conflicted with markup information, we overrode the former.</S>
			<S sid ="229" ssid = "10">The initial segmentation was also used as the baseline.</S>
			<S sid ="230" ssid = "11">We only used entries that satisfied all of the following conditions.</S>
			<S sid ="231" ssid = "12">1.</S>
			<S sid ="232" ssid = "13">The (normalized) title is longer than one char-.</S>
			<S sid ="233" ssid = "14">acter and contains hiragana, katakana and/or kanji.</S>
			<S sid ="234" ssid = "15">4 http://download.wikimedia.org/jawiki/ 5 http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?JUMAN 2.</S>
			<S sid ="235" ssid = "16">The main text is longer than 1,000 characters..</S>
			<S sid ="236" ssid = "17">3.</S>
			<S sid ="237" ssid = "18">The title appears at least 5 times in the main.</S>
			<S sid ="238" ssid = "19">text.</S>
			<S sid ="239" ssid = "20">The first condition ensures that there are segmentation ambiguities.</S>
			<S sid ="240" ssid = "21">The second and third conditions exclude entries unsuitable for statistical methods.</S>
			<S sid ="241" ssid = "22">14% of the entries satisfied these conditions.</S>
			<S sid ="242" ssid = "23">We randomly selected 500 entries and manually segmented their titles for evaluation.</S>
			<S sid ="243" ssid = "24">The 2-person inter-annotator Kappa score was 0.95.</S>
			<S sid ="244" ssid = "25">As an annotated corpus, we used Kyoto Text Corpus.6 It contained 1,675,188 characters.</S>
			<S sid ="245" ssid = "26">Models We compared the unigram and bigram models.</S>
			<S sid ="246" ssid = "27">As for inference procedures, we used collapsed Gibbs sampling (CL) for both models, type- based sampling (TB) for the unigram model and hybrid type-based sampling (HTB) for the bigram model.</S>
			<S sid ="247" ssid = "28">We tested two mixing methods of the annotated corpus, direct mixing (DM) and back-off mixing (BM).</S>
			<S sid ="248" ssid = "29">To investigate the effect of initialization, we also tried randomly segmented text as the initial state (RAND).</S>
			<S sid ="249" ssid = "30">For random initialization, we placed a boundary with probability 0.5 on each position unless it was a fixed boundary.</S>
			<S sid ="250" ssid = "31">The unigram model has one Dirichlet process concentration hyperparameter α0 and the bigram model has α0 and α1.</S>
			<S sid ="251" ssid = "32">For each model, we experimented with the following values.</S>
			<S sid ="252" ssid = "33">α0 : 0.1, 0.5, 1 5 10, 50, 100, 500, 1,000 and 5,000 α1 : 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100 and 500 For comparison, we also performed hyperparameter sampling.</S>
			<S sid ="253" ssid = "34">Following Escobar and West (1995), we set a gamma prior and introduced auxiliary variables to infer concentration parameters from data.</S>
			<S sid ="254" ssid = "35">For back-off mixing, we used the linear interpolation parameter λIP = 0.5.</S>
			<S sid ="255" ssid = "36">The zerogram model was trained on the annotated corpus.</S>
			<S sid ="256" ssid = "37">In each run, we performed 10 burn-in iterations.</S>
			<S sid ="257" ssid = "38">We then performed another 10 iterations to collect samples.</S>
			<S sid ="258" ssid = "39">6 http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?Kyoto%20University%20Text% 20Corpus Table 1: Results of segmentation of entry titles (F-score (precision/recall)).</S>
			<S sid ="259" ssid = "40">m o d e l b e s t m e d i a n i n f e r r e d uni gr am + CL u n i g r a m + T B b i g r a m + C L b i g r a m + H T B 81 .3 5 (77.78/85.27)** 55 .8 7 (66.71/48.06) 80 .6 5 (76.73/84.99) 83 .2 3 (85.25/81.30)** 80 .0 9 (75.80/84.89) 51 .0 4 (62.64/43.06) 79 .9 6 (75.50/84.99) 74 .5 2 (71.33/78.00) 80 .8 6 (76.81/85.36) 42 .6 3 (54.91/34.84) 80 .5 4 (76.84/84.61) 34 .5 2 (46.69/27.38) uni gr am + CL + D M u n i g r a m + T B + D M b i g r a m + C L + D M b i g r a m + H T B + D M 85 .2 9 (83.14/87.54)** 35 .2 6 (47.74/29.95) 80 .3 7 (76.01/85.27) 69 .6 6 (67.68/71.77) 81 .6 2 (77.93/85.70)** 33 .8 1 (46.20/26.66) 79 .8 8 (75.42/84.89) 67 .3 9 (64.35/70.73) 80 .9 1 (82.87/79.04) 31 .9 0 (44.30/24.93) 73 .7 7 (78.49/69.59) 31 .5 4 (43.79/24.64) uni gr am + CL + B M u n i g r a m + T B + B M b i g r a m + C L + B M b i g r a m + H T B + B M 81 .2 8 (77.48/85.46) 57 .2 2 (68.01/49.39) 81 .3 3 (77.34/85.74) 86 .3 2 (85.67/86.97)** 80 .2 3 (76.06/84.89) 52 .9 8 (64.50/44.95) 80 .0 7 (75.69/84.99) 76 .3 5 (71.89/81.40) 81 .4 2 (77.75/85.46) 42 .4 3 (54.69/34.66) 81 .4 6 (77.82/85.46)** 40 .8 1 (53.35/33.05) uni gr am + TB + R A N D bi gr a m + H T B + R A N D u ni gr a m + T B + B M + R A N D bi gr a m + H T B + B M + R A N D 56 .0 1 (66.93/48.16) 79 .6 8 (80.13/79.23) 57 .4 4 (67.91/49.76) 84 .0 3 (83.10/84.99) 50 .8 9 (62.21/43.06) 68 .1 6 (63.64/73.37) 50 .8 6 (61.92/43.15) 70 .4 6 (65.25/76.58) 42 .6 8 (54.81/34.94) 34 .9 9 (47.05/27.86) 42 .3 1 (54.55/34.56) 40 .1 6 (52.60/32.48) ba sel ine (J U M A N) 80 .0 9 (75.80/84.89) ** Statistically significant improvement with p &lt; 0.01.</S>
			<S sid ="260" ssid = "41">Evaluation Metrics We evaluated the segmentation accuracy of 500 entry titles.</S>
			<S sid ="261" ssid = "42">Specifically we evaluated the performance of a model with precision, recall and the F-score, all of which were based on tokens.</S>
			<S sid ="262" ssid = "43">We report the score of the most frequent segmentation among 10 samples.</S>
			<S sid ="263" ssid = "44">Following Lee et al.</S>
			<S sid ="264" ssid = "45">(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.</S>
			<S sid ="265" ssid = "46">In order to evaluate the degree of difference between a pair of segmentations, we employed character-based evaluation.</S>
			<S sid ="266" ssid = "47">Following Kudo et al.</S>
			<S sid ="267" ssid = "48">(2004), we converted a word sequence into character-based BI labels and examined labeling disagreements.</S>
			<S sid ="268" ssid = "49">McNemar’s test of significance was based on this metric.</S>
			<S sid ="269" ssid = "50">6.2 Results.</S>
			<S sid ="270" ssid = "51">Table 1 shows segmentation accuracy of various models.</S>
			<S sid ="271" ssid = "52">One would notice that the baseline score is much lower than the score previously reported regarding newspaper articles (Kudo et al., 2004).</S>
			<S sid ="272" ssid = "53">It is because unlike newspaper articles, the titles of Wikipedia entries contain an unusually high proportion of unknown words.</S>
			<S sid ="273" ssid = "54">As suggested by relatively low precision, unknown words tend to be over-segmented by the morphological analyzer.</S>
			<S sid ="274" ssid = "55">In the best hyperparameter settings, the back-off mixing bigram model with hybrid type-based sam pling (bigram + HTB + BM) significantly outperformed the baseline and achieved the best F-score.</S>
			<S sid ="275" ssid = "56">It did not performed well in the median setting as it was sensitive to the value of α1.</S>
			<S sid ="276" ssid = "57">Hyperparameter estimation led to catastrophic decreases in bigram models as it made the hyperparameters much larger than those in the best settings.</S>
			<S sid ="277" ssid = "58">Collapsed sampling (+CL) returned scores comparable to that of the baseline.</S>
			<S sid ="278" ssid = "59">It is simply because it did not change the initial segmentation a lot.</S>
			<S sid ="279" ssid = "60">In contrast, type-based sampling (+TB) brought large moves to the unigram model and significantly hurt accuracy.</S>
			<S sid ="280" ssid = "61">As suggested by relatively low recall, the unigram model prefers under-segmentation.</S>
			<S sid ="281" ssid = "62">When combined with (hybrid) type-based sampling (+TB/+HTB), back-off mixing (+BM) increased accuracy from the corresponding non- mixing models.</S>
			<S sid ="282" ssid = "63">By contrast, direct mixing (+DM) drastically decreased accuracy from the non-mixing models.</S>
			<S sid ="283" ssid = "64">We can confirm that when the main text is orders of magnitude smaller than the annotated text, the role of constituent words in the main text is underestimated.</S>
			<S sid ="284" ssid = "65">To our surprise, collapsed sampling with mixing models (+CL, +DM/+BM) outperformed the baseline.</S>
			<S sid ="285" ssid = "66">However, the scores of type- based sampling (+TB) suggest that with much more iterations, the models would converge to undesired states.</S>
			<S sid ="286" ssid = "67">The unigram model with random initialization was indifferent from that with default initialization.</S>
			<S sid ="287" ssid = "68">By contrast, the performance of the bigram model slightly degenerated with random initialization.</S>
			<S sid ="288" ssid = "69">6.3 Convergence.</S>
			<S sid ="289" ssid = "70">Figure 2 shows how segmentations differed from the initial state in the course of inference.7 A diff is defined as the number of character-based disagreements between the baseline segmentation and a model output.</S>
			<S sid ="290" ssid = "71">Hyperparameters used were those of the best model with (hybrid) type-based sampling.</S>
			<S sid ="291" ssid = "72">900 800 700 600 500 400 300 200 100 0 unigram + CL unigram + TB bigram + CL bigram + HTB unigram + TB + RAND bigram + HTB + RAND 0 5 10 15 20 iteration We can see that collapsed sampling was almost unable to escape the initial state.</S>
			<S sid ="292" ssid = "73">With type-based sampling (+TB), the unigram model went further Figure 2: Diffs in the course of iteration.</S>
			<S sid ="293" ssid = "74">All models were with back-off mixing (+BM).</S>
			<S sid ="294" ssid = "75">than the bigram model, but to an undesired direction.</S>
			<S sid ="295" ssid = "76">The bigram model with hybrid type-based sampling (bigram + HTB) converged in few iterations.</S>
			<S sid ="296" ssid = "77">Although the model with random initialization (+RAND) converged to a nearby point, the initial segmentation by the morphological analyzer realized a bit faster convergence and better accuracy.</S>
			<S sid ="297" ssid = "78">Figure 2 shows how acceptance rates changed during inference.</S>
			<S sid ="298" ssid = "79">For comparison, a sample by a type-based Gibbs sampler was treated as “accepted” if the number of new boundaries was different from 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 unigram + TB bigram + HTB unigram + TB + RAND bigram + HTB + RAND 0 5 10 15 20 iteration that of the current boundaries (i.e. m′ ̸= m).</S>
			<S sid ="299" ssid = "80">The acceptance rates were low and samplers seemingly stayed around modes.</S>
			<S sid ="300" ssid = "81">6.4 Approximation.</S>
			<S sid ="301" ssid = "82">Up to this point, we consider every possible boundary position.</S>
			<S sid ="302" ssid = "83">However, this seems wasteful, given that a large portion of text has only marginal influence on the segmentation of the noun phrase in question.</S>
			<S sid ="303" ssid = "84">For this reason, we implemented approximation named matching skip.</S>
			<S sid ="304" ssid = "85">We sampled a boundary only if the corresponding local area contained a sub- string of the noun phrase in question.</S>
			<S sid ="305" ssid = "86">Table 2 shows the result of approximation.</S>
			<S sid ="306" ssid = "87">Hyperparameters used were those of the best models with full sampling.</S>
			<S sid ="307" ssid = "88">Matching skip steadily worsened performance although not to a large extent.</S>
			<S sid ="308" ssid = "89">Mean 7 For a fair comparison, we might need to report changes over time instead of iterations.</S>
			<S sid ="309" ssid = "90">However, the difference of convergence speed is obvious in the iteration-based comparison although (hybrid) type-based sampling takes several times longer than collapsed sampling in the current na¨ıve implementation.</S>
			<S sid ="310" ssid = "91">Figure 3: Acceptance rates for a noun phrase in the course of iteration.</S>
			<S sid ="311" ssid = "92">All models were with back-off mixing (+BM).</S>
			<S sid ="312" ssid = "93">while it drastically reduced the number of sampled positions.</S>
			<S sid ="313" ssid = "94">The median skip rate was 90.87%, with a standard deviation of 8.5.</S>
			<S sid ="314" ssid = "95">6.5 Discussion.</S>
			<S sid ="315" ssid = "96">Figure 4 shows some segmentations corrected by the back-off mixing bigram model with hybrid type based sampling.</S>
			<S sid ="316" ssid = "97">“市比野” (ichihino) is a rare place name but can be identified by the model because it is frequently used in the article.</S>
			<S sid ="317" ssid = "98">“こなみるく” (konamiruku in hiragana) seems a pun on “粉ミル ク” (kona miruku, “powdered milk”) and “コナミ” (konami in katakana, a company).</S>
			<S sid ="318" ssid = "99">We consider it as a single word because we cannot reconstruct the etymology solely based on the main text.</S>
			<S sid ="319" ssid = "100">Note the different scripts.</S>
			<S sid ="320" ssid = "101">In Japanese, people often change the script to derive a proper noun from a common noun, which a na¨ıve analyzer fails to recognize.</S>
			<S sid ="321" ssid = "102">It is Table 2: Effect of matching skip (F-score (precision/recall)).</S>
			<S sid ="322" ssid = "103">m o d e l f u l l m a t c h i n g s k i p big ra m + H TB big ra m + H TB + B M 83 .2 3 (85.25/81.30)** 86 .3 2 (85.67/86.97)** 82 .8 6 (84.27/81.49) 83 .8 7 (82.60/85.17)** big ra m + H TB + R A N D big ra m + H TB + B M + R A N D 79 .6 8 (80.13/79.23) 84 .0 3 (83.10/84.99) 78 .8 1 (78.64/75.07) 81 .0 8 (80.22/81.96) ba sel ine (J U M A N) 80 .0 9 (75.80/84.89) ** Statistically significant improvement with p &lt; 0.01.</S>
			<S sid ="323" ssid = "104">樋 + 脇 + 町 + 市 + 比 + 野 ⇒ 樋脇 hiwaki + 町 chou + 市比野 ichihino tify the constituents.</S>
			<S sid ="324" ssid = "105">On the other hand, the segmentation failed when our assumption about con (Ichihino, Hiwaki Town, an address) り + そな + カード ⇒ りそな + カード stituents does not hold.</S>
			<S sid ="325" ssid = "106">For example, the person name “菊池俊吉” (kikuchi shuNkichi) is two words risona (Risona Card, a company) kaRdo but was erroneously combined into a single word by the model because unfortunately he was always referred to by the full name.</S>
			<S sid ="326" ssid = "107">ちり + とて + ちん ⇒ ちりとてちん chiritotechiN (name of a play) こな + みる + く ⇒ こなみるく konamiruku (a shop affiliated with Konami Corporation) はい + じぃ ⇒ はいじぃ haiziI (stage name of a comedian) ちん + すこう ⇒ ちんすこう chiNsukou (a traditional sweet) コントラアルトクラリネット ⇒ コントラ + アルト 7 Conclusions In this paper, we proposed a new task of Japanese noun phrase segmentation.</S>
			<S sid ="327" ssid = "108">We adopted non- parametric Bayesian language models and proposed hybrid type-based sampling that can efficiently correct segmentation given by the morphological analyzer.</S>
			<S sid ="328" ssid = "109">Although supervised segmentation is very competitive, we showed that it can be supplemented + クラリネット koNtora aruto with our unsupervised approach.</S>
			<S sid ="329" ssid = "110">We applied the proposed method to encyclopedic kurarineQto (Contra-alto clarinet) Figure 4: Examples of improved segmentations.</S>
			<S sid ="330" ssid = "111">very important to identify hiragana words correctly.</S>
			<S sid ="331" ssid = "112">As hiragana is mainly used to write function words and other basic words, segmentation errors concerning hiragana often bring disastrous effects on applications of morphological analysis.</S>
			<S sid ="332" ssid = "113">For example,the analyzer over-segments “ちりとてちん” (chiri totechiN) into three shorter words among which the second word “とて” (tote) is a particle, and this se quence of words is transformed into a terrible parse tree.</S>
			<S sid ="333" ssid = "114">Most improvements come from correction of over-segmentation because the initial segmentation by the analyzer shows a tendency of over- segmentation.</S>
			<S sid ="334" ssid = "115">An example of corrected under- segmentation is “contra-alto clarinet.” The presence of “clarinet,” “alto” and “contrabass” and others in the main text allowed the model to iden text to segment noun phrases in it.</S>
			<S sid ="335" ssid = "116">The proposed method can be applied to other tasks.</S>
			<S sid ="336" ssid = "117">For example, in unknown word acquisition (Murawaki and Kurohashi, 2008), noun phrases are often acquired from text as single words.</S>
			<S sid ="337" ssid = "118">We can now segment them into words in a more sophisticated way.</S>
			<S sid ="338" ssid = "119">In the future we will assign a POS tag to each word in order to use segmented noun phrases in morphological analysis.</S>
			<S sid ="339" ssid = "120">We assume that the meaning of constituents in a noun phrase rarely depends on outer context.</S>
			<S sid ="340" ssid = "121">So it would be helpful to augment them with rich semantic information in advance instead of disambiguating their meaning every time we analyze given text.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="341" ssid = "122">This work was partly supported by JST CREST.</S>
	</SECTION>
</PAPER>
