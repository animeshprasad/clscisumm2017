<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Children learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process.</S>
		<S sid ="2" ssid = "2">Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically during learning.</S>
		<S sid ="3" ssid = "3">Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks.</S>
		<S sid ="4" ssid = "4">We test on child-directed utterances in English and Spanish and compare to single-task baselines.</S>
		<S sid ="5" ssid = "5">In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization.</S>
		<S sid ="6" ssid = "6">These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Models of language acquisition seek to infer linguistic structure from data with minimal amounts of prior knowledge, in order to discover which characteristics of the input data are useful for learning, and thus potentially utilised by human learners.</S>
			<S sid ="8" ssid = "8">Most previous work has focused on learning individual aspects of linguistic structure.</S>
			<S sid ="9" ssid = "9">However, children clearly learn multiple aspects in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning.</S>
			<S sid ="10" ssid = "10">Joint models investigate the interaction between different levels of linguistic structure during learning.</S>
			<S sid ="11" ssid = "11">These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources.</S>
			<S sid ="12" ssid = "12">Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012).</S>
			<S sid ="13" ssid = "13">In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process.</S>
			<S sid ="14" ssid = "14">Both morphology and word order depend on categorising words based on their morpho- syntactic function.</S>
			<S sid ="15" ssid = "15">However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues.</S>
			<S sid ="16" ssid = "16">Our joint model integrates both sources of information, allowing the model to flexibly weigh them according to their utility.</S>
			<S sid ="17" ssid = "17">Languages differ in the richness of their morphology and strictness of word order.</S>
			<S sid ="18" ssid = "18">These characteristics appear to be (anti)correlated, with rich morphology co-occurring with free word order and vice versa (Blake, 2001; McFadden, 2003).</S>
			<S sid ="19" ssid = "19">The timecourse of acquisition is also influenced by language typology: learners of morphologically rich languages become productive in morphology earlier (Xanthos et al., 2011), suggesting that richer morphology may be more salient for learners than impoverished morphology.</S>
			<S sid ="20" ssid = "20">Sentence comprehension in children also shows cross-linguistic differences in the cues used to make sense of non-canonical sentence structure: learners of a morphologically rich language (Turkish) disregard word order in 30 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30–41, Seattle, Washington, USA, 1821 October 2013.</S>
			<S sid ="21" ssid = "21">Qc 2013 Association for Computational Linguistics favour of morphology, whereas learners of English favour word order (Slobin, 1982; MacWhinney et al., 1984).</S>
			<S sid ="22" ssid = "22">These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength (rich morphology versus strict word order), and thus be more language-general, than single-task models.</S>
			<S sid ="23" ssid = "23">Both syntactic category and morphology induction have been the focus of much recent work.</S>
			<S sid ="24" ssid = "24">(See Hammarstro¨ m and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al.</S>
			<S sid ="25" ssid = "25">(2010) for a comparison of part of speech/syntactic category induction systems.)</S>
			<S sid ="26" ssid = "26">However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories.</S>
			<S sid ="27" ssid = "27">Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly.</S>
			<S sid ="28" ssid = "28">Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach.</S>
			<S sid ="29" ssid = "29">Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010).</S>
			<S sid ="30" ssid = "30">These models find morphological categories on the basis of word- internal features, without taking syntactic context into account (which is of course not available in a lexicon).</S>
			<S sid ="31" ssid = "31">Lee et al.</S>
			<S sid ="32" ssid = "32">(2011) and Sirts and Aluma¨e (2012) present models that infer morphological segmentations and syntactic categories jointly, although Lee et al.</S>
			<S sid ="33" ssid = "33">(2011) do not evaluate the inferred syntactic categories.</S>
			<S sid ="34" ssid = "34">Both make use of a word-type constraint which limits each word form to a single analysis (i.e., all instances of ducks are assigned to a single category and will have the same morpheme analysis, ignoring the gold standard distinction between a plural noun and third person singular verb).</S>
			<S sid ="35" ssid = "35">This can make inference more tractable, and often increases performance, but does not respect the ambiguity in herent in natural language, both over syntactic categories and morphological analyses.</S>
			<S sid ="36" ssid = "36">The degree of ambiguity is language dependent, so that even if a type-constraint is perhaps relatively unproblematic in English, it will pose problems in morphologically richer languages.</S>
			<S sid ="37" ssid = "37">Furthermore, these two models make use of an array of heuristics that may not allow them to be easily generalisable across languages and datasets (e.g., likelihood scaling (Sirts and Aluma¨e, 2012), sequential suffix matching (Lee et al., 2011)).</S>
			<S sid ="38" ssid = "38">In this paper, we present a joint model composed of two well-known individual models.</S>
			<S sid ="39" ssid = "39">This allows us to cleanly investigate the effects of joint learning and its potential benefits over the single task models.</S>
			<S sid ="40" ssid = "40">The simplicity of our models also allows us to avoid modelling and inference heuristics.</S>
			<S sid ="41" ssid = "41">Previous models have used adult-directed written texts, which differs significantly from the type of language available to child learners.</S>
			<S sid ="42" ssid = "42">We test our joint model on child-directed utterances in English (a morphologically poor language) and Spanish (with richer morphology)1.</S>
			<S sid ="43" ssid = "43">Our results indicate that our joint model is able to flexibly accommodate languages with differing levels of morphological richness.</S>
			<S sid ="44" ssid = "44">The joint model matches the performance of single task models on both tasks, demonstrating that the additional complexity is not a problem (i.e., it does not add noise).</S>
			<S sid ="45" ssid = "45">Moreover, the joint model improves performance significantly on the task corresponding to the language’s weaker cue, indicating a transfer of information from the stronger cue.</S>
			<S sid ="46" ssid = "46">The fact that the nature of this improvement varies by language provides evidence that joint learning can effectively accommodate typological diversity.</S>
	</SECTION>
	<SECTION title="Model. " number = "2">
			<S sid ="47" ssid = "1">The task is to assign word tokens to part of speech categories and simultaneously segment the tokens into morphemes.</S>
			<S sid ="48" ssid = "2">We assume a relatively simple yet commonly used concatenative morphology which models a word as a stem plus (possibly null) suffix2.</S>
			<S sid ="49" ssid = "3">1 There are languages with much richer morphology than Spanish, but none with a child-directed corpus suitably annotated for evaluation.</S>
			<S sid ="50" ssid = "4">2 Fullwood and O’Donnell (2013) recently presented a model of non-concatenative morphology that could be integrated into this model; however, it does not perform well on English (and presumably other mostly concatenative languages).</S>
			<S sid ="51" ssid = "5">Since this is an unsupervised model, the inferred categories and morphemes lack meaningful labels, but ideally will correspond to gold standard categories and morphemes.</S>
			<S sid ="52" ssid = "6">2.1 Word Order.</S>
			<S sid ="53" ssid = "7">We model a sequence of words as a Hidden Markov Model (HMM) with a non-parametric emission distribution.</S>
			<S sid ="54" ssid = "8">As usual, the latent states of the HMM represent syntactic categories.</S>
			<S sid ="55" ssid = "9">The tag sequence is generated by a trigram Dirichletmultinomial distribu tion, where transition parameters τ are drawn from a symmetric Dirichlet distribution with the hyperparameter αt . Each tag ti in the sequence is then drawn from the transition distribution conditioned on the previous two tags: τ(t,tf) ∼ Dir(αt ) ti = t|ti−1 = tf, ti−2 = tff, τ ∼ Mult(τ(tf,tff)) generated from Dirichletmultinomials conditioned on the tag t: κ ∼ Dir(ακ ) t| κ ∼ Mult(κ) σ ∼ Dir(αs) s|t, σ ∼ Mult(σt ) φ ∼ Dir(α f ) f |t, φ ∼ Mult(φt ) The αs are hyperparameters governing the Dirichlet distributions from which the multinomials κ, σ, φ are drawn.</S>
			<S sid ="56" ssid = "10">In turn, t, s, and f are drawn from these multinomials.</S>
			<S sid ="57" ssid = "11">The probability of a word under this model is the sum of the probabilities of all possible analyses l = (t, s, f ): P0(w) = ∑ P0(l) = ∑ P(s|t)P( f |t)P(t) (1) This model is token-based, permitting different tokens of the same word type to have different syntactic categories.</S>
			<S sid ="58" ssid = "12">Most recent models have included a constraint forcing all tokens of a given type into the same category, which improves performance but often complicates inference.</S>
			<S sid ="59" ssid = "13">The Bayesian HMM’s performance is therefore not state- of-the-art, but is comparable to other token-based models (Christodoulopoulos et al., 2010) and the model is easy to extend within the Bayesian framework, allowing us to compare multiple versions.</S>
			<S sid ="60" ssid = "14">This part of the model is parametric, operating over a fixed number of tags T , and is identical to the formulation of tag transitions in the Bayesian HMM (Goldwater and Griffiths, 2007).</S>
			<S sid ="61" ssid = "15">However, we replace the BHMM’s emission distribution with the morphologically-informed distributions described below.</S>
			<S sid ="62" ssid = "16">As in the BHMM, the emission distributions are conditioned on the tag, i.e., each tag has its own morphology.</S>
			<S sid ="63" ssid = "17">2.2 Morphology.</S>
			<S sid ="64" ssid = "18">The morphology model introduced by Goldwater et al.</S>
			<S sid ="65" ssid = "19">(2006) generates morphological analyses for a set of tokens.</S>
			<S sid ="66" ssid = "20">These analyses consist of a tag plus a stem and suffix pair, which are concatenated to form the observed words.</S>
			<S sid ="67" ssid = "21">Both stem s and suffix f are l t,s, f s.t. s⊕ f =w where s ⊕ f = w denotes that the concatenation of stem and suffix results in the word w. On its own, this distribution over morphological analyses makes independence assumptions that are too strong: most word tokens of a word type have the same analysis, but P0 will regenerate that analysis for every token.</S>
			<S sid ="68" ssid = "22">To resolve this problem, a PitmanYor process (PYP) is placed over the generating distribution above.</S>
			<S sid ="69" ssid = "23">The PitmanYor process has been found to be useful for representing the power-law distributions common in natural language (Teh, 2006; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011).</S>
			<S sid ="70" ssid = "24">The distribution of draws from a PitmanYor process (which, in our case, determines the distribution of word tokens with each morphological analysis) is commonly described using the metaphor of a Chinese restaurant.</S>
			<S sid ="71" ssid = "25">A series of customers (tokens z = z1 . . .</S>
			<S sid ="72" ssid = "26">zN ) enter a restaurant with an infinite number of initially empty tables.</S>
			<S sid ="73" ssid = "27">Upon entering, each customer is seated at a table k with probability p(zi = k|z1 . . .</S>
			<S sid ="74" ssid = "28">zi−1, a, b) = (2) ( nk −a i−1+b ≤ ≤ i−1+b if k = K + 1 ti−2 ti−1 ti zi Figure 1: Plate diagram depicting the morphology model (adapted from Goldwater et al.</S>
			<S sid ="75" ssid = "29">(2006)).</S>
			<S sid ="76" ssid = "30">Hyperparameters have been omitted for clarity.</S>
			<S sid ="77" ssid = "31">The left-hand plate depicts the base distribution P0 ; note that the morphological analyses lk are generated deterministically as (tk , sk , fk ).</S>
			<S sid ="78" ssid = "32">The observed words wi are also deterministic given zi = k and lk , since wi = sk ⊕ fk . where nk is the number of customers already sitting at table k, K is the total number of tables occupied by the i − 1 previous customers, and 0 ≤ a &lt; 1 and b ≥ 0 are hyperparameters of the process.</S>
			<S sid ="79" ssid = "33">The probability of being seated at a table increases with the number of customers already seated at that table, creating a ‘rich-get-richer’ power-law distribution of tokens to tables; a and b control the amount of reuse of existing tables, with smaller values leading to more reuse.</S>
			<S sid ="80" ssid = "34">Crucially, each table serves a dish generated by the base distribution P0—i.e., the dish is a morphological analysis lk = (t, s, f )—and all the customers seated at the same table share the same dish, which is generated only once (at the point when that table sk fk lk wi Kt N T Figure 2: Plate diagram depicting the joint model.</S>
			<S sid ="81" ssid = "35">Hyper- parameters have been omitted for clarity.</S>
			<S sid ="82" ssid = "36">The L-shaped plate contains the tokens, while the square plates contain the morphological analyses.</S>
			<S sid ="83" ssid = "37">The t are latent tags, zi is an assignment to a morphological analysis lk = (sk , fk ), and wi is the observed word.</S>
			<S sid ="84" ssid = "38">T is the number of distinct tags, and Kt the number of tables used by tag type t. in each of the tag-specific restaurants is still determined by Equation 2, except that all of the counts and indices are with respect to only the tokens and tables assigned to that tag.</S>
			<S sid ="85" ssid = "39">Each tag-specific PYP (restaurant) also has a separate base distribution, P(t), resulting in distinct distributions over stems and suffixes for each tag.</S>
			<S sid ="86" ssid = "40">The analyses generated by the base distributions consist of (stem, suffix) pairs; the tag is given by the identity of the generating PYP.is first occupied).</S>
			<S sid ="87" ssid = "41">The model can thus reuse the anal (t) (t) ysis for a particular word and avoid regenerating the P0 (w) = ∑ P0 (l = (s, f )) = ∑ P(s|t)P( f |t) same analysis multiple times.</S>
			<S sid ="88" ssid = "42">Note that multiple ta- l bles may have identical analyses, lk = lkf . Figure 1 s, f s.t. s⊕ f =w (3)illustrates how the full PYP morphology model gen erates the observed sequence of word tokens.</S>
			<S sid ="89" ssid = "43">2.3 Combined Model.</S>
			<S sid ="90" ssid = "44">The full model (Figure 2) combines the latent tag sequence with the morphology model.</S>
			<S sid ="91" ssid = "45">Tag tokens are generated conditioned on local context, not the base distribution, as in the morphology model.</S>
			<S sid ="92" ssid = "46">Instead of a single PYP generating morphological analyses for all tokens, as in the Goldwater et al.</S>
			<S sid ="93" ssid = "47">(2006) model, we have a separate PYP for each tag type, i.e., each tag has its own restaurant with its own customers (the tokens labeled with that tag) and its own morphological analyses.</S>
			<S sid ="94" ssid = "48">The distribution of customers The full joint posterior distribution of a sequence of words, tags, and morpheme analyses is shown in Figure 3.</S>
			<S sid ="95" ssid = "49">Note that all tag-specific morphology models share the same PitmanYor parameters a and b.</S>
	</SECTION>
	<SECTION title="Inference. " number = "3">
			<S sid ="96" ssid = "1">We use Gibbs sampling for inference over the three sets of discrete variables: tags t, their assignments to morphological analyses (tables) z, and the analyses themselves l. Each iteration of the sampler has two stages: First the morphological analyses l are sampled, and then each token samples a new tag and a new assignment to an analysis/table.</S>
			<S sid ="97" ssid = "2">Because the table assignments P(t, l, z|αt , a, b, αs, α f ) =P(t|αt )P(l|t, αs, α f )P(z|a, b) (4) N T Γ(T αt ) T Γ(nttftff + αt ) P(t|αt ) = ∏ P(ti|ti−1, ti−2, t1...i−1, αt ) = ∏ Γ(n + T α ) ∏ Γ(α ) (5) i=2 T Kt t,tf=1 ttf t tff=1 t P(l|t, αs, α f ) = ∏ ∏ Pt (lk = (s, f )|l1...k−1, αs, α f ) (6) t=1 k=1 T Γ(Sαs) Γ(F α f ) S Γ(mts + αs) F Γ(mt f + α f ) = ∏ Γ(m + Sα ) Γ(m + F α ) ∏ Γ(α ) ∏ Γ(α ) (7) t=1 t s t T Nt f s=1 s f =1 f P(z|a, b) = ∏ ∏ P(zi|t, z1...i−1, a, b) (8) t=1 i=1 T Γ(1 + b) Kt Γ(nk − a) = ∏ Γ(n + b) ∏(ka + b) Γ(1 a) (9) t=1 t k=1 − Figure 3: The posterior distribution of our joint model.</S>
			<S sid ="98" ssid = "3">Because the sequence of words w is deterministic given analyses l and assignments to analyses (tables) z, the joint posterior over all variables P(w, t, l, z|αt , a, b, αs , α f ) is equal to P(t, l, z|αt , a, b, αs , α f ) when lzi = wi for all i, and 0 otherwise.</S>
			<S sid ="99" ssid = "4">We give equations for the nonzero case.</S>
			<S sid ="100" ssid = "5">ns refer to token counts, ms to table counts.</S>
			<S sid ="101" ssid = "6">We add two dummy tokens at the start, end, and between sentences to pad the context history.</S>
			<S sid ="102" ssid = "7">are conditioned on tags (i.e., a token must be assigned to a table in the correct PYP restaurant) re- sampling the tag requires immediate resampling of multinomial posteriors as follows: m\k + α m\k + α f the table assignment as well.</S>
			<S sid ="103" ssid = "8">p(lk = (s, f )|t, l\k ) = s s f (10) 3.1 Initialization.</S>
			<S sid ="104" ssid = "9">The tags are initialized uniformly at random.</S>
			<S sid ="105" ssid = "10">For each token, a segmentation point is chosen uniformly at random (we disallow segmentations with a null stem).</S>
			<S sid ="106" ssid = "11">If this segmentation is new within the PYP associated with that token’s tag, a new table is created for the token in that PYP.</S>
			<S sid ="107" ssid = "12">If it matches an existing analysis, zi is sampled from the existing tables k plus a possible new table kf.</S>
			<S sid ="108" ssid = "13">3.2 Morphological Analyses.</S>
			<S sid ="109" ssid = "14">Each lk represents the morphological analysis for the set of tokens assigned to table k. Resampling the segmentation point (stem and suffix identity) of the analysis changes the segmentation of all of the word tokens assigned to that analysis.</S>
			<S sid ="110" ssid = "15">Note that the tag is not included in lk in the combined model, because the tag identity is dependent on the local contexts of all the tokens seated at the table.Analyses are sampled from a product of Dirichlet m\k + Sαs m\k + F α f where ms and m f are the number of analyses for this tag that share a stem or suffix with lk , and m is the total number of analyses for this tag.</S>
			<S sid ="111" ssid = "16">S and F are the total number of stems and suffixes in the model.</S>
			<S sid ="112" ssid = "17">l\k indicates that the current analysis lk has been removed from the distribution and the appropriate counts, to create the correct conditioning distribution for the Gibbs sampler.</S>
			<S sid ="113" ssid = "18">3.3 Tags.</S>
			<S sid ="114" ssid = "19">Tags are sampled from the product of posteriors of the transition and emission distributions.</S>
			<S sid ="115" ssid = "20">The transition distribution is a standard Dirichletmultinomial posterior.</S>
			<S sid ="116" ssid = "21">Calculating the emission distribution probability, i.e. the marginal probability of the word given the tag, involves summing over the probability of all the existing tables in the given PYP that emit the correct word, plus the probability of a new table being created, which also includes the probability of a new analysis from P(t).</S>
			<S sid ="117" ssid = "22">More precisely, tags are sampled from the following distribution: p(ti = t|wi = w, t\i, z\i, l, αt , a, b) (11) Language A abdc fefh pomo rtut usst cdcc bcba gghh npop npoo cdca aaaa fefh hfeg pnon Language B ∝ p(ti = t|ti 1, t , t\i, α ) × p(w|t, z\i = p(ti = t|ti 1, t , t\i, α ) , l) noom.no usrs.st bbdb.ac cbab.cc cdaa.cc rttt.uu cbab.aa mnom.oo ccda.bc onmm.om rruu.ts npop.mm gehg.fh trrt.uu tssu.uu × ( ∑ k s.t. lk =w p(zi = k|t, w, z\i) + p(zi = knew|t, w, z\i)) Table 1: Example sentences in the synthetic languages.</S>
			<S sid ="118" ssid = "23">= nti 2t i−1 t + αt Words in Category 1 are made of characters ad, Cate − i−1 + T α gory 2 eh, Category 3 m-p, Category 4 r-u.</S>
			<S sid ="119" ssid = "24">Suffixes in nti 2t t nk − a Kt a + b (t) Language B are separated with periods (.)</S>
			<S sid ="120" ssid = "25">for illustrativ e × ( ∑ k s.t. lk =w + nt + b P (w)) nt + b 0 purposes only.</S>
			<S sid ="121" ssid = "26">where lk = w matches tables compatible with w, i.e., the concatenation of stem and suffix form theword, slk ⊕ flk = w. nk is the number of words as signed to the table k and Kt is the total number of tables in the PYP for tag t. Note that all counts are obtained after the removal of the current ti and zi, i.e., from t\i and z\i. 3.4 Table Assignments.</S>
			<S sid ="122" ssid = "27">Once a new tag has been sampled for a token, the table assignment must be resampled conditioned on the new tag.</S>
			<S sid ="123" ssid = "28">The assignment zi is drawn over all compatible tables in the tag’s PYP (that is, where lk = w), plus a possible new table: p(zi = k|ti = t, w, z\i, a, b) ∝ (12) nt +b if 1 ≤ k ≤ Kt will be able to make use of the different cues in both language types in a flexible way.</S>
			<S sid ="124" ssid = "29">In order to test the proposed model, we run two experiments on synthetic languages, which simulate languages in which either word order or morphology is the sole cue.</S>
			<S sid ="125" ssid = "30">Most natural languages fall between these extremes, but these experiments show that our model can capture the full spectrum.</S>
			<S sid ="126" ssid = "31">Language A is a strict word order language lacking morphology.</S>
			<S sid ="127" ssid = "32">It has a vocabulary of 200 word types, split into four different categories.</S>
			<S sid ="128" ssid = "33">The 50 word types in each category are created by combining four letters, with replacement, into four-letter words, with a different set of letters used in each cat- egory3.</S>
			<S sid ="129" ssid = "34">Words within a category may thus share beginning or ending characters, which could be posited Kt a+b (t) as stems or suffixes by the model, but since only P(t) nt +b P0 (w) if k = Kt + 1 50 of 25 6 pos sibl e stri ngs are use d, ther e will be no stro ng evi den ce for con sist ent ste m and suff ixes 0 is calculated by summing over the probability of all possible segmentations for a new analysis for word wi, using Equation 3.</S>
			<S sid ="130" ssid = "35">If a new table is drawn (k &gt; Kt ) then we also sample a new analysis for that table from P(t).</S>
	</SECTION>
	<SECTION title="Preliminary Experiments. " number = "4">
			<S sid ="131" ssid = "1">An important argument for joint learning is that it affords increased flexibility and robustness across a (i.e. stems appearing with multiple suffixes and vice versa).</S>
			<S sid ="132" ssid = "2">Each sentence in Language A consists of five words in one of twenty possible category sequences.</S>
			<S sid ="133" ssid = "3">In these sequences, each category is either followed by itself or the next category (i.e. [2,2,2,3,4] is valid but [2,4,3,1,4] is not).</S>
			<S sid ="134" ssid = "4">Word order is thus strongly constrained by category membership.</S>
			<S sid ="135" ssid = "5">Language B has free word order, with category membership signalled by suffixes.</S>
			<S sid ="136" ssid = "6">Words are cre wider range of input data.</S>
			<S sid ="137" ssid = "7">A model that relies on word order cannot learn syntactic categories from a morphologically complex language with free word order; likewise a model attempting to categorise words using morphology alone will fail on a language without morphology.</S>
			<S sid ="138" ssid = "8">An effective joint model 3 We achieved the same results with a language using the.</S>
			<S sid ="139" ssid = "9">same four characters in all categories, but using different characters makes the categories human-readable.</S>
			<S sid ="140" ssid = "10">The model does not have a orthographic/phonological component and so will not recognise the within-category similarity, other than possibly positing spurious stems or suffixes.</S>
			<S sid ="141" ssid = "11">010000 -2000030000 -4000050000 -60000 LangA Total LangA Transitions LangA Morphology able to corre ctly identi fy the two langu age extre mes indica tes that the mode l is robus t to hyper para meter value s. Th ese experi ments demo nstrat e that our joint mode l is able to learn corre ctly even when only either morp holog y or word order is infor mativ e in a langu age.</S>
			<S sid ="142" ssid = "12">We now turn to acqui sition data from natural langu ages in which both morp holog y and word order are useful cues but to varyi ng degr ees.</S>
			<S sid ="143" ssid = "13">-70000 L a n g B T o t a l L a n g B T r a n s i t i o n s LangB Morpholo gy 0 200 400 600 800 1000 5 C DS Expe rime nts Iteration Figure 4: Log probability of the sampler state over 1000 iterations on Languages A and B. ated by the concatenation of a stem and a suffix, where the stems are the same as the words in language A (50 stems in each of four categories).</S>
			<S sid ="144" ssid = "14">One of six category-specific suffixes is appended to each stem, resulting in 300 word types per category.</S>
			<S sid ="145" ssid = "15">Each suffix is two letters long, created by combining three possible letters (the same letters used to create the stems), thus making mis-segmentation possible (for instance, up to three of the suffixes could have the same final letter).</S>
			<S sid ="146" ssid = "16">Sentences are again five words long, but the sequence of categories is drawn at random, resulting in uniformly random word order.</S>
			<S sid ="147" ssid = "17">See Table 1 for example sentences in both languages.</S>
			<S sid ="148" ssid = "18">We create a 5000 word corpus for each language, and run our model on these corpora.</S>
			<S sid ="149" ssid = "19">Hyperparameters are set to the same values in both languages4.</S>
			<S sid ="150" ssid = "20">We run the sampler on each dataset for 1000 iterations with simulated annealing.</S>
			<S sid ="151" ssid = "21">In both cases, the correct solution is found by iteration 500.</S>
			<S sid ="152" ssid = "22">Figure 4 shows that the morphology component continues to increase the log probability by increasing the number of tokens seated at a table.</S>
			<S sid ="153" ssid = "23">Note that the correct solution in Language A involves learning a very peaked transition distribution as well as an even more extreme distribution over suffixes (where only the null suffix has high probability), whereas the same distributions in Language B are much flat 5.1 Data.</S>
			<S sid ="154" ssid = "24">We use two corpora, Eve (Brown, 1973) and Or- nat (Ornat, 1994), from the CHILDES database (MacWhinney, 2000).</S>
			<S sid ="155" ssid = "25">These corpora consist of the child-directed utterances heard by two children, the former learning English and the latter Spanish.</S>
			<S sid ="156" ssid = "26">These have been annotated for part of speech categories and morphemes.</S>
			<S sid ="157" ssid = "27">The CHILDES corpora are tagged with a very rich set of part of speech tags (74 tags), which we collapse to a smaller set of tags5.</S>
			<S sid ="158" ssid = "28">The Eve corpus has 61224 tokens and is thus larger than the Spanish corpus, which has 40497 tokens.</S>
			<S sid ="159" ssid = "29">However, the English corpus has only 17 gold suffix types, while Spanish has 83.</S>
			<S sid ="160" ssid = "30">The increased richness of Spanish morphology also has an effect on the number of word types in the corpus: the Spanish dataset has 3046 word types, whereas the larger English dataset has only 1957.</S>
			<S sid ="161" ssid = "31">Morphology is annotated using a stem-affix encoding which does not directly correspond to our segmentation-based model.</S>
			<S sid ="162" ssid = "32">The word running is annotated as run-ING, jumping as jumpING; the annotation is thus agnostic about orthomorphemic segmentation (i.e., whether to segment as run.ning or runn.ing), whereas the model is forced to choose a segmentation point.</S>
			<S sid ="163" ssid = "33">Syncretic suffixes (sharing an identical surface form) are disambiguated: sings is annotated as sing-3S, plums as plum-PL. Conversely, the annotation scheme merges allomorphs into a single suffix: infinitive verbs in Spanish, for instance, are encoded as ending with -INF, corresponding to -ar, -er, and -ir surface forms.</S>
			<S sid ="164" ssid = "34">ter.</S>
			<S sid ="165" ssid = "35">The fact that the same hyperparameter setting is</S>
	</SECTION>
	<SECTION title="These are 13 for English (ADJ,  ADV, AUX, CONJ, DET,. " number = "5">
			<S sid ="166" ssid = "1">4 The PYP parameters are set to a = 0.1, b = 1.0 and the HMM transition parameter αt = 1.0; the parameters in the base distribution are αs , α f = 0.001, αk = 0.5.</S>
			<S sid ="167" ssid = "2">INF, NOUN, NEG, OTH, PART, PREP, PRO, VERB) and 10 for Spanish, since the gold standard does not distinguish AUX, PART or INF.</S>
			<S sid ="168" ssid = "3">We ignore irregular/non-affixing forms annotated with &amp; (e.g. was, annotated as be&amp;PAST ) and use only hyphen-separated suffixes to evaluate.</S>
			<S sid ="169" ssid = "4">Where multiple suffixes are concatenated together (e.g., dog-DIM-PL) we treat this as a single suffix (-DIM-PL) for evaluation purposes.</S>
			<S sid ="170" ssid = "5">In Spanish, many words are annotated as having a suffix of effectively zero length, e.g. the imperative gusta is annotated as gusta2S&amp;IMP.</S>
			<S sid ="171" ssid = "6">We replace these suffixes (where the stem is equal to the word) with a null suffix, excluding them from evaluation, as they are impossible for a segmentation- based model to find.</S>
			<S sid ="172" ssid = "7">5.2 Evaluation.</S>
			<S sid ="173" ssid = "8">Tags are evaluated using VM (Rosenberg and Hirschberg, 2007), as has become standard for this task (Christodoulopoulos et al., 2010).</S>
			<S sid ="174" ssid = "9">VM is a measure of the normalised cross-entropy between gold and proposed clusters; it ranges between 0 and 100, with higher scores being better.</S>
			<S sid ="175" ssid = "10">We also use VM to evaluate the morphological segmentation: all tokens with a common suffix are clustered together, and these clusters are compared against the gold suffix clusters6.</S>
			<S sid ="176" ssid = "11">Using a clustering metric avoids the need to evaluate against a gold segmentation point (which the annotation lacks).</S>
			<S sid ="177" ssid = "12">Tag membership is added to the non-null model suffixes, so that a finals suffix found in tag 2 is distinguished from the same suffix found in tag 8 (creating suffixes -s-T8 and -s-T2), analogous to the gold annotation distinction between syncretic morphemes -PL and -3S.</S>
			<S sid ="178" ssid = "13">Note that ceiling performance of our model on Suffix VM will be below 100, since our model cannot cluster allomorphs, which are represented by a single abstract morpheme in the gold standard.</S>
			<S sid ="179" ssid = "14">5.3 Baselines.</S>
			<S sid ="180" ssid = "15">We test the full model, MORTAG, against a number of variations to investigate the advantages of jointly modelling the two tasks.</S>
			<S sid ="181" ssid = "16">Two variants remove the transition distributions, and thus local syntactic context, from the model.</S>
			<S sid ="182" ssid = "17">6 We also evaluated stem morpheme clusters and found near- ceiling performance due to the high number of null-suffix words MORTAGNOTRANS is the full model without transitions between tag tokens; morphology PYP draws remain conditioned on token tags.</S>
			<S sid ="183" ssid = "18">We add a Dirichlet prior over tags (αt = 0.1) to encourage tag spar sity (analogous to the transition distribution in the full model).</S>
			<S sid ="184" ssid = "19">MORCLUSTERS is the original model of Goldwater et al.</S>
			<S sid ="185" ssid = "20">(2006), in which tags (called clusters in the original) are drawn by P0.</S>
			<S sid ="186" ssid = "21">MORTAGNOSEG is a variant in which the only available suffix is the null suffix; thus segmentations are trivial and only tags are inferred.</S>
			<S sid ="187" ssid = "22">This model is approximately equivalent to a simple Bayesian HMM but with the addition of PYPs within the emission distribution.</S>
			<S sid ="188" ssid = "23">We also evaluate against tags found by the BHMM, with a Dirichletmultinomial emission distribution and no morphology.</S>
			<S sid ="189" ssid = "24">MORTAGTRUETAGS is the full model but with all tags fixed to their gold values.</S>
			<S sid ="190" ssid = "25">This model gives us oracle-type results for morphology.</S>
			<S sid ="191" ssid = "26">(Due to the annotation scheme used in CHILDES, oracle morphological segmentations are unavailable, so we were unable to test a model with gold morphology and inferred tags.)</S>
			<S sid ="192" ssid = "27">5.4 Experimental Procedure.</S>
			<S sid ="193" ssid = "28">Hyperparameter values for the PitmanYor process were found using grid search on a development set (Section 10 of Eve and Section 8 of Ornat; these sections are removed from the dataset we report results on).</S>
			<S sid ="194" ssid = "29">We use the values which give the best Suffix VM performance on the development data; however we stress that the development results did not vary greatly over a wide range of hyperparameter values, and only deteriorated significantly at extreme values of a. There are a number of other hyperparameters in the model which we set to fixed values.</S>
			<S sid ="195" ssid = "30">The transi tion hyperparameter αt is set to 0.1 in all models.</S>
			<S sid ="196" ssid = "31">We set the hyperparameters for the stem and suffix distributions in the morphology base distribution P0 to 0.001 for both αs and α f ; αk over tags in the MORCLUSTERS model is set to 0.5.</S>
			<S sid ="197" ssid = "32">The number of possible stems and suffixes is given by the dataset: in the Eve dataset there are 5339 candidate stems and 6617 candidate suffixes; in the Ornat dataset these numbers are 8649 and 6598, respectively.</S>
			<S sid ="198" ssid = "33">The number of tags available to the model is set to the number Tag VM Suffix VM MORTAG 59.1(1.9) 41.9(10.0) MORCLUSTERS 22.4(1.0)∗ 28.0(11.9)∗ MORTAGNOTRANS 19.3(1.2)∗ 24.4(5.2)∗ MORTAGNOSEG 59.4(1.7) − BHMM 56.2(2.3)∗ − MORTAGTRUETAGS − 42.5(5.2) Tag VM Suffix VM MORTAG 43.4(2.6) 41.4(2.5) MORCLUSTERS 20.3(2.5)∗ 46.5(3.2) MORTAGNOTRANS 14.4(1.7)∗ 36.4(2.0)∗ MORTAGNOSEG 39.6(3.7)∗ − BHMM 36.4(0.7)∗ − MORTAGTRUETAGS − 59.8(0.4)∗ Table 2: English Eve corpus results.</S>
			<S sid ="199" ssid = "34">Standard deviations are in parentheses; ∗ denotes a significant difference from the MORTAG model.</S>
			<S sid ="200" ssid = "35">Sampling is run for 5000 iterations with annealing.</S>
			<S sid ="201" ssid = "36">Inspection of the posterior log-likelihood indicates that the models converge after about 1000 iterations.</S>
			<S sid ="202" ssid = "37">We run inference over all models ten times and report the average performance.</S>
			<S sid ="203" ssid = "38">Significance is reported using the non-parametric Wilcoxon rank sum test with a significance level of ρ &lt; 0.05.</S>
			<S sid ="204" ssid = "39">5.5 Results: English.</S>
			<S sid ="205" ssid = "40">Results on the English Eve corpus are shown in Table 2.</S>
			<S sid ="206" ssid = "41">We use PYP parameters a = 0.3 and b = 10, though we found similar performance over a wide range of values of a and b. Our results show a clear improvement in the morphological segmentations found by the joint model and stable tagging performance across all models with context information.</S>
			<S sid ="207" ssid = "42">The syntactic clusters found by models using only morphological patterns, MORTAGNOTRANS and MORCLUSTERS, are clearly inferior and lead to low Tag VM results.</S>
			<S sid ="208" ssid = "43">The models with local syntactic context all perform approximately equally well in terms of finding tags.</S>
			<S sid ="209" ssid = "44">We find no improvement on tagging performance in English when adding morphology, compared to the MORTAGNOSEG baseline in which words are not segmented.</S>
			<S sid ="210" ssid = "45">However, we do see a small but significant improvement over the BHMM for both of these models, due to the replacement of the multinomial emission distribution in the BHMM with the PYP.</S>
			<S sid ="211" ssid = "46">Morphological segmentations, as measured by Suffix VM, clearly improve with the addition of local contexts (and the ensuing better tags): the full model outperforms the baselines without syntactic contexts.</S>
			<S sid ="212" ssid = "47">On this dataset, the joint MORTAG model even matches the performance of the model us Table 3: Spanish Ornat corpus results.</S>
			<S sid ="213" ssid = "48">Standard deviations are in parentheses; ∗ denotes a significant difference from the MORTAG model.</S>
			<S sid ="214" ssid = "49">ing oracle tags.</S>
			<S sid ="215" ssid = "50">The standard deviation over Suffix VM scores is quite large for MORTAG and MORCLUSTERS; this is due to frequent words having two high probability segmentations (most notably is, which in some runs was segmented as i.s).</S>
			<S sid ="216" ssid = "51">5.6 Results: Spanish.</S>
			<S sid ="217" ssid = "52">For the Spanish Ornat corpus, we found slightly different optimal PYP hyperparameters and set a = 0.1 and b = 0.1.</S>
			<S sid ="218" ssid = "53">Results are shown in Table 3.</S>
			<S sid ="219" ssid = "54">The Spanish results pattern in the opposite way as English.</S>
			<S sid ="220" ssid = "55">Here we see a statistically significant improvement in tagging performance of the full joint model over both models without morphology (MORTAGNOSEG and BHMM).</S>
			<S sid ="221" ssid = "56">Models without context information again find much worse tags, mainly because (as in English) function words are not identifiable by suffixes.</S>
			<S sid ="222" ssid = "57">However, the full model does not find better morphological segmentations than the MORCLUSTERS model, despite better tags (the two models’ Suffix VM scores are not statistically significantly different).</S>
			<S sid ="223" ssid = "58">We also see that the difference between the segmentations found by the model using gold tags and estimated tags is quite large.</S>
			<S sid ="224" ssid = "59">This is due to the oracle model finding the rarer suffixes which were not distinguished by the models with noisier tags.</S>
			<S sid ="225" ssid = "60">This demonstrates the importance of syntactic categorisation for the morpheme induction task, and suggests that a more sophisticated tagging model (with better performance) may yet improve morpheme segmentation performance in Spanish.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="226" ssid = "1">We have presented a model of joint syntactic category and morphology induction.</S>
			<S sid ="227" ssid = "2">Operating within a generative Bayesian framework means that combining single-task components is straightforward and well-founded.</S>
			<S sid ="228" ssid = "3">Our model is token-based, allowing for syntactic and morphemic ambiguity.</S>
			<S sid ="229" ssid = "4">To our knowledge, this is the first joint model to be tested on child-directed speech data, which is less complex than the newswire corpora used by previous joint models.</S>
			<S sid ="230" ssid = "5">Child-directed speech may be simple enough for joint learning not to be necessary: our results indicate the contrary, namely that joint learning is indeed helpful when learning from realistic acquisition data.</S>
			<S sid ="231" ssid = "6">We tested this model on two languages with different morphological characteristics.</S>
			<S sid ="232" ssid = "7">On English, a language with relatively little morphology, especially in child directed speech, we found that better categorisation of words yielded much better morphology in terms of suffixes learned.</S>
			<S sid ="233" ssid = "8">Conversely, in Spanish we saw less difference on the morphology task between models with categories inferred solely from morphemic patterns and models that also used local syntactic context for categorisation.</S>
			<S sid ="234" ssid = "9">However, in Spanish we saw an improvement in the tagging task when morphology information was included.</S>
			<S sid ="235" ssid = "10">This suggests that English and Spanish make different word-order and morphology trade-offs.</S>
			<S sid ="236" ssid = "11">In English, local context provides at least as much information as morphology in terms of determining the correct syntactic category, but knowing a good estimate of the correct syntactic category is useful for determining a word’s morphology.</S>
			<S sid ="237" ssid = "12">In Spanish, a word’s morphology can more easily be determined simply by looking at frequent suffixes within a purely morphological system.</S>
			<S sid ="238" ssid = "13">On the other hand, word order is freer, making local syntactic context unreliable, so taking morphological information into account can improve tagging.</S>
			<S sid ="239" ssid = "14">These differences between languages demonstrate the benefits of joint learning, which enables the learner to more flexibly utilise the information available in the input data.</S>
	</SECTION>
</PAPER>
