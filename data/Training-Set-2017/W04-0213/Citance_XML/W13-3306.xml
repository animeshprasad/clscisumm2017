<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">The paper presents machine translation experiments from English to Czech with a large amount of manually annotated discourse connectives.</S>
		<S sid ="2" ssid = "2">The gold-standard discourse relation annotation leads to better translation performance in ranges of 4–60% for some ambiguous English connectives and helps to find correct syntactical constructs in Czech for less ambiguous connectives.</S>
		<S sid ="3" ssid = "3">Automatic scoring confirms the stability of the newly built discourse- aware translation systems.</S>
		<S sid ="4" ssid = "4">Error analysis and human translation evaluation point to the cases where the annotation was most and where less helpful.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Recently, research in statistical machine translation (SMT) has renewed interest in the fact that for a variety of linguistic phenomena one needs information from a longer-range context.</S>
			<S sid ="6" ssid = "6">Current statistical translation models and decoding algorithms operate at the sentence and/or phrase level only, not considering already translated context from previous sentences.</S>
			<S sid ="7" ssid = "7">This local distance is in many cases too restrictive to correctly model lexical cohesion, referential expressions (noun phrases, pronouns), and discourse markers, all of which relate to the sentence(s) before the one to be translated.</S>
			<S sid ="8" ssid = "8">Discourse relations between sentences are often conveyed by explicit discourse connectives (DC), such as although, because, but, since, while.</S>
			<S sid ="9" ssid = "9">DCs play a significant role in coherence and readability of a text.</S>
			<S sid ="10" ssid = "10">Likewise, if a wrong connective is used in translation, the target text can be fully incomprehensible or not conveying the same meaning as was established by the discourse relations in the source text.</S>
			<S sid ="11" ssid = "11">In English, about 100 types of such explicit connectives have been annotated in the Penn Discourse TreeBank (PDTB, see Section 4), signaling discourse relations such as temporality or contrast between two spans of text.</S>
			<S sid ="12" ssid = "12">Depending on the set of relations used, there can be up to 130 such relations and combinations thereof.</S>
			<S sid ="13" ssid = "13">Discourse relations can also be present implicitly (inferred from the context), without any explicit marker being present.</S>
			<S sid ="14" ssid = "14">Although annotation for implicit DCs exists as well, we only deal with explicit DCs in this paper.</S>
			<S sid ="15" ssid = "15">DCs are difficult to translate mainly because a same English connective can signal different discourse relations in different contexts and when the target language has either different connectives according to the source relations signaled or uses different lexical or syntactical constructs in place of the English connective.</S>
			<S sid ="16" ssid = "16">In this paper, we present MT experiments from English (EN) to Czech (CZ) with a large amount of manually annotated DCs.</S>
			<S sid ="17" ssid = "17">The corpus, the parallel Prague CzechEnglish Dependency Treebank (PCEDT) (Section 4), is directly usable for MT experiments: the entire discourse annotation in EN is paralleled with a human CZ translation.</S>
			<S sid ="18" ssid = "18">This means that we can build and evaluate, against the CZ reference, a translation system, that learns from the EN gold standard discourse relations.</S>
			<S sid ="19" ssid = "19">These then have no distortion from wrongly labeled connectives as it is given in related work (Section 3) where automatic classifiers have been used to label the connectives with a certain error rate.</S>
			<S sid ="20" ssid = "20">Furthermore, we can use the sense labels for 100 types of EN connectives, whereas related work only focused on a few highly ambiguous connectives that are especially problematic for translation.</S>
			<S sid ="21" ssid = "21">The paper starts by illustrating difficult translations involving connectives (Section 2) and discusses related work in Section 3.</S>
			<S sid ="22" ssid = "22">The resources and data used are introduced in Section 4.</S>
			<S sid ="23" ssid = "23">The MT experiments are explained in Section 5 and 43 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 43–50, Sofia, Bulgaria, August 9, 2013.</S>
			<S sid ="24" ssid = "24">Qc 2013 Association for Computational Linguistics automatic evaluation is given in Section 6.</S>
			<S sid ="25" ssid = "25">We further provide a detailed manual evaluation and error analysis for the CZ translations generated by our SMT systems (Section 7).</S>
			<S sid ="26" ssid = "26">Future work described in Section 8 concludes the paper.</S>
	</SECTION>
	<SECTION title="Motivation. " number = "2">
			<S sid ="27" ssid = "1">The following example shows a CZ translation of the English DC meanwhile.</S>
			<S sid ="28" ssid = "2">The previous sentences to the example were about other computer producers expected to report disappointing financial results.</S>
			<S sid ="29" ssid = "3">The interpretation of meanwhile plied to SMT (Eidelman et al., 2012) or decoding with document-wide features (Hardmeier et al., 2012).</S>
			<S sid ="30" ssid = "4">A recently published article summarizes most of the work on SMT with the broader perspective of discourse, lexical cohesion and co- reference (Hardmeier, 2013).</S>
			<S sid ="31" ssid = "5">For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004), Arabic (AlSaif, 2012), Chinese (Zhou and and the discourse relation (or sense) signaled is Xue, 2012) and Czech (Mladova´ et al., 2009).</S>
			<S sid ="32" ssid = "6">therefore CONTRASTIVE and not TEMPORAL: SOURCE: Apple Computer Inc., meanwhile&lt;COMPARISONCONTRAST&gt;, is expected to show improved earnings for the period ended September.</S>
			<S sid ="33" ssid = "7">BASELINE: Spolecˇnost Apple Computer Inc., mezit´ım by meˇla uka´zat lepsˇ´ı pˇr´ıjmy za obdob´ı koncˇ´ıc´ı v za´ˇr´ı.</S>
			<S sid ="34" ssid = "8">SYSTEM2: Spolecˇnost Apple Computer Inc., naopak by meˇla uka´zat lepsˇ´ı pˇr´ıjmy za obdob´ı koncˇ´ıc´ı v za´ˇr´ı.</S>
			<S sid ="35" ssid = "9">A baseline SMT system for EN/CZ generated the incorrect CZ connective mezit´ım which signals a temporal relation only.</S>
			<S sid ="36" ssid = "10">The translation marked SYSTEM2 in the example was output by one of the systems we trained on manual DC annotations (cf.</S>
			<S sid ="37" ssid = "11">Section 5).</S>
			<S sid ="38" ssid = "12">The system correctly generated the CZ connective naopak signaling a contrastive sense.</S>
			<S sid ="39" ssid = "13">The example sentence is taken from the Wall Street Journal corpus, section 2365.</S>
			<S sid ="40" ssid = "14">The sense tag for meanwhile was manually annotated in the Penn Discourse TreeBank, see Section 4.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "3">
			<S sid ="41" ssid = "1">The disambiguation of DCs can be seen as a special form of Word Sense Disambiguation (WSD), that has been applied to SMT for content words with slight improvements to translation quality (Chan et al., 2007; Carpuat and Wu, 2007).</S>
			<S sid ="42" ssid = "2">DCs however form a class of procedural function words that relate text spans from an arbitrarily long context and their disambiguation needs features from that longer-range context.</S>
			<S sid ="43" ssid = "3">Only few studies address function word disambiguation for SMT: Chang et al.</S>
			<S sid ="44" ssid = "4">(2009) disambiguate a multifunctional Chinese particle for Chinese/English translation and Ma et al.</S>
			<S sid ="45" ssid = "5">(2011) use tagging of English collocational particles for translation into Chinese.</S>
			<S sid ="46" ssid = "6">Lexical cohesion at the document level has recently also come into play, with studies on lexical consistency in SMT (Carpuat, 2009; Carpuat and Simard, 2012), topic modeling apThese resources however remain mostly monolin gual, i.e. translations or parallel texts in other languages do normally not exist.</S>
			<S sid ="47" ssid = "7">This makes these resources not directly usable for MT experiments.</S>
			<S sid ="48" ssid = "8">Recent work has shown that more adequate and coherent translations can be generated for English/French when ambiguous connectives in the source language are annotated with the discourse relation they signal (PopescuBelis et al., 2012).</S>
			<S sid ="49" ssid = "9">SMT systems for European language pairs are most often trained on Europarl corpus data (Koehn, 2005), where only a small amount of discourse-annotated instances is available (8 connectives with about 300500 manual annotations each).</S>
			<S sid ="50" ssid = "10">Meyer and PopescuBelis (2012) therefore used these few examples to train automatic classifiers that introduce the sense labels for the connectives in the entire English text of the Europarl corpus.</S>
			<S sid ="51" ssid = "11">Although these classifiers are state- of-the-art, they can have an error rate of up to 30% when labeling unseen instances of connectives.</S>
			<S sid ="52" ssid = "12">The discourse-aware SMT systems nevertheless improved about 810% of the connective translations.</S>
			<S sid ="53" ssid = "13">When integrating into SMT directly the small manually-labeled data, without training classifiers, hardly any translation improvement was measurable, cf.</S>
			<S sid ="54" ssid = "14">(Meyer and PopescuBelis, 2012).</S>
	</SECTION>
	<SECTION title="The Parallel Prague Czech-English. " number = "4">
			<S sid ="55" ssid = "1">Dependency Treebank With the EnglishCzech parallel text provided in the Prague CzechEnglish Dependency Treebank2.0 (PCEDT) (Hajicˇ et al., 2011)1, comes a hu man CZ translation of the entire Wall Street Journal Corpus in EN (WSJ, sections 0024, approxi 1 http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2012T08 mately 50k sentences).</S>
			<S sid ="56" ssid = "2">The syntactical annotation of WSJ, the Penn TreeBank (Marcus et al., 1993), has been followed by a discourse annotation project, the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), over the same sections of the corpus.</S>
			<S sid ="57" ssid = "3">In the PDTB version 2.0, 18,459 instances of explicit DCs, among other discourse-related phenomena (implicit relations, alternative lexicalizations), are labeled along with the text spans they connect (discourse arguments) and the discourse relation they signal (sense tags).</S>
			<S sid ="58" ssid = "4">The sense tags are organized in a three-level sense hierarchy with four top semantic classes, 16 sub-senses on the second and further 23 sub- senses on the third hierarchy level.</S>
			<S sid ="59" ssid = "5">The annotators were not forced to make the finest distinction (on the sub-sense level).</S>
			<S sid ="60" ssid = "6">A token can also be annotated with two senses, forming a composite sense with a label combination from wherever in the hierarchy, resulting in 129 theoretically possible distinct sense tags (see Section 5 for the sense levels we use).</S>
			<S sid ="61" ssid = "7">For the latter reason, some of the sense labels are very scarcely used and although they make for important and fine-grained distinctions in English, this granularity level might not be useful for translation, where only certain ambiguities have to be resolved to obtain a correct target language connective, see Section 7.</S>
			<S sid ="62" ssid = "8">The PCEDT is a 1:1 sentence-aligned parallel resource with a manual multilayer dependency analysis of both original Penn TreeBankWSJ texts and their translations to Czech.</S>
			<S sid ="63" ssid = "9">Despite the manually annotated parallel dependency trees which are very valuable in other linguistic studies, for translation we only used the plain CZ texts provided with the treebank.</S>
	</SECTION>
	<SECTION title="Experimental Setup. " number = "5">
			<S sid ="64" ssid = "1">In the following, we describe a series of SMT experiments that made direct use of the EN/CZ text as provided with the PCEDT.</S>
			<S sid ="65" ssid = "2">The SMT models were all phrase-based and trained with the Moses decoder (Koehn et al., 2007), either on plain text for the BASELINE or on text where the EN connective word-forms have been concatenated with the PDTB sense labels.</S>
			<S sid ="66" ssid = "3">All texts have been tokenized and lowercased with the Moses tools before training SMT.</S>
			<S sid ="67" ssid = "4">In future work, we will build factored translation models (Koehn and Hoang, 2007) as well, as this would reduce the label scarcity that was likely a problem when just concatenating word-forms and labels (see Sections 7 and 8).</S>
			<S sid ="68" ssid = "5">For SYSTEM1 in the following, we inserted, into the English side of the PCEDT data, the full sense labels from the PDTB, which can be, as already mentioned, as detailed as containing 3 sense levels and allowing for composite tags (where annotators chose that two senses hold at the same time).</S>
			<S sid ="69" ssid = "6">SYSTEM1 therefore operates on a total of 63 distinct and observed sense tags for all DCs.</S>
			<S sid ="70" ssid = "7">For SYSTEM2, we reduced the sense labels to contain only senses from PDTB sense hierarchy level 2 and 1, not allowing for composite senses, i.e. for those instances that were annotated with two senses we discarded the secondary (but not less important) sense.</S>
			<S sid ="71" ssid = "8">This reduced the set of senses for SYSTEM2 to 22.</S>
			<S sid ="72" ssid = "9">The procedure is exemplified in the example below with an EN sentence 1 (WSJ section 2300) containing a complex PDTB sense tag that has been kept for SYSTEM1.</S>
			<S sid ="73" ssid = "10">For SYSTEM2 we have reduced the sense of when to: &lt;CONTINGENCYCONDITIONGENERAL&gt;.</S>
			<S sid ="74" ssid = "11">Sentence 2 (WSJ section 2341) contains two already simplified sense tags.</S>
			<S sid ="75" ssid = "12">The original PDTB sense tags for meanwhile and as were respectively &lt;COMPARISONCONTRASTJUXTAPOSITION&gt; and &lt;CONTINGENCYPRAGMATICCAUSE- JUSTIFICATION&gt;, where JUXTAPOSITION and JUSTIFICATION were dropped because they stem from the third level of the PDTB sense hierarchy: 1.</S>
			<S sid ="76" ssid = "13">Selling snowballed because of waves of au-.</S>
			<S sid ="77" ssid = "14">tomatic “stop-loss” orders, which are triggered by computer when&lt;CONTINGENCYCONDITIONGENERALTEMPORALASYNCHRONOUSSUCCESSION&gt; prices fall to certain levels.</S>
			<S sid ="78" ssid = "15">2.</S>
			<S sid ="79" ssid = "16">Meanwhile&lt;COMPARISONCONTRAST&gt;, analysts said.</S>
			<S sid ="80" ssid = "17">Pfizer’s recent string of lackluster quarterly performances continued, as&lt;CONTINGENCYPRAGMATICCAUSE&gt; earnings in the quarter were expected to decline by about 5%.</S>
			<S sid ="81" ssid = "18">In order to build SMT systems of reasonable quality, we still need to combine the PCEDT texts (50k sentences) with other resources such as the EN/CZ parts of the Europarl corpus.</S>
			<S sid ="82" ssid = "19">This results in a mixture of labeled and unlabeled DCs in the data and estimates might be noisy.</S>
			<S sid ="83" ssid = "20">We however also checked system performance on the PDTB test set (section 23) with labeled DCs only (see Section 6) for which the unlabeled ones in the model do not pose a problem, as they are not considered as valid target phrases by the SMT decoder.</S>
			<S sid ="84" ssid = "21">The following list gives an overview of the data used to build three SMT systems.</S>
			<S sid ="85" ssid = "22">No modi fications have been done to the texts of the BASELINE system, that uses exactly the same amount of sentences, but no sense labels.</S>
			<S sid ="86" ssid = "23">• BASELINE: no tags for connectives • SYSTEM1: complex PDTB sense tags • SYSTEM2: simplified PDTB sense tags • training: Europarlv7 (645,155 sentences) + PDTB sections 0221 (41,532 sentences; 15,402 connectives) • tuning: newstest2011 (3,003 sentences) + PDTB sections 00,01,22,24 (5,260 sentences; 2,134 connectives) • testing: newstest2012 (3,001 sentences) + PDTB section 23 (2,416 sentences; 923 connectives)2 The language model, the same for BASELINE, SYSTEM1 and SYSTEM2, was built using SRILM (Stolcke et al., 2011) with 5-grams over Europarl and the news data sets 20072011 in CZ, as distributed by the Workshop on Machine Translation3.</S>
			<S sid ="87" ssid = "24">All systems were tuned by MERT (Och, 2003) as implemented in Moses.</S>
	</SECTION>
	<SECTION title="Automatic Evaluation. " number = "6">
			<S sid ="88" ssid = "1">Most automatic MT scoring relies on n-gram matching of a system’s candidate translation against (usually) only one human reference translation.</S>
			<S sid ="89" ssid = "2">For DCs therefore, automatic scores do not reveal much of a system’s performance, as often only one or two words, i.e. the DC is changed.</S>
			<S sid ="90" ssid = "3">When a candidate translation however contains a more accurate and correct connective, the translation output is often more coherent and readable than the baseline’s output, see Section 7.</S>
			<S sid ="91" ssid = "4">Automatic evaluation has been done using the MultEval tool, version 0.5.1 (Clark et al., 2011).</S>
			<S sid ="92" ssid = "5">The BLEU scores are computed by jBLEU V0.1.1 (an exact reimplementation of NIST’s mtevalv13.pl without tokenization).</S>
			<S sid ="93" ssid = "6">Table 1 provides an overview of the BLEU scores for the BASELINE and systems 1 and 2 on the full test set (new- stest2012 + PDTB section 23), and on PDTB section 23 only, the latter containing 2,416 sentences and 923 labeled DCs.</S>
			<S sid ="94" ssid = "7">In order to gain reliable automatic evaluation scores, we executed 5 runs of MERT for each 2 Note that this PDTB section division for training, development and testing is the same as is used for automatic classification experiments, as recommended in the PDTB annotation manual.</S>
			<S sid ="95" ssid = "8">3 http://www.statmt.org/wmt12/ translation model configuration.</S>
			<S sid ="96" ssid = "9">MERT is implemented as a randomized, non-deterministic optimization process, so that each run leads to different feature weights and as a consequence, to different BLEU scores when translating unseen text.</S>
			<S sid ="97" ssid = "10">The scores from the 5 runs were then averaged and with a t-test we calculated the confidence p-values for the score differences.</S>
			<S sid ="98" ssid = "11">When these are below 0.05, they confirm that it is statistically likely, that such scores would occur again in other tuning runs.</S>
			<S sid ="99" ssid = "12">In terms of BLEU, neither SYSTEM1 nor SYSTEM2 therefore performs significantly better or worse than the BASELINE.</S>
			<S sid ="100" ssid = "13">In order to show how little the DC labeling actually affects the BLEU score, we randomized all connective sense tags in PDTB test section 23 and translated again 5 times (with the weights from each tuning run) with both, SYSTEM1 and SYSTEM2.</S>
			<S sid ="101" ssid = "14">With randomized labels, both systems perform statistically significantly worse (p = 0.01, marked with a star in Table 1) than the BASELINE, but only with an average performance loss of −0.6 BLEU points.</S>
			<S sid ="102" ssid = "15">Note that some sense tags might still have been correct due to randomization.</S>
			<S sid ="103" ssid = "16">Te st se t S ys te m B L E U nt 20 12 + P D T B 23 B A S E L I N E S Y S T E M 1 SY ST E M 2 17 .6 17 .6 17 .6 P D T B 23 B A S E L I N E S Y S T E M 1 SY ST E M 2 21 .4 21 .4 21 .4 P D T B 23 ra nd o m SY ST E M 1 SY ST E M 2 20 .8 * 20 .8 * Table 1: BLEU scores when testing on the combined test set (newstest2012 + PDTB 23); on PDTB section 23 only (2416 sentences, 923 connectives); and when randomizing the sense tags (PDTB 23 random), for the BASELINE system and the two systems using PDTB connective labels: SYSTEM1: complex labels, SYSTEM2: simplified labels.</S>
			<S sid ="104" ssid = "17">When testing on randomized sense labels (PDTB 23 random), the BLEU scores are statistically significantly lower than the ones on the correctly labeled test set (PDTB 23), which is indicated by starred values.</S>
			<S sid ="105" ssid = "18">Automatic MT scoring does therefore not reveal actual changes in translation quality due to DC usage.</S>
			<S sid ="106" ssid = "19">In the next section, we manually analyze samples of the translation output by SYSTEM2 that reached the highest scores observed in some of the single tuning runs before averaging.</S>
	</SECTION>
	<SECTION title="Manual Evaluation and Error Analysis. " number = "7">
			<S sid ="107" ssid = "1">Two human judges went both through two random samples of SYSTEM2 translations from WSJ section 23, namely sentences 1300 and 10002416.</S>
			<S sid ="108" ssid = "2">In these sentences, there were 630 observed connectives.</S>
			<S sid ="109" ssid = "3">The judges counted the translations that were better, equal and worse in terms of the DCs as output by SYSTEM2 versus the BASELINE system.</S>
			<S sid ="110" ssid = "4">We then summarized the counts over the two samples and give the scores as ∆(%) in Table 2.</S>
			<S sid ="111" ssid = "5">To further test if we just had bad samples, the judges went through another set of translations (1024– 1138), containing 50 DCs, for which the counts are summarized in Table 2 as well.</S>
			<S sid ="112" ssid = "6">A translation was counted as being correct when it generated a valid CZ connective for the corresponding context, without grading the rest of the sentences.</S>
			<S sid ="113" ssid = "7">Overall, it was found that the number of better translations is only slightly higher for SYSTEM2 than the ones from the BASELINE system.</S>
			<S sid ="114" ssid = "8">The vast majority of DCs was translated correctly by both the BASELINE and SYSTEM2, and in very few cases, both systems translated the DCs incorrectly.</S>
			<S sid ="115" ssid = "9">SYSTEM2 appeared to systematically repeat one mistake, namely translating the very frequent connective but preferably with jenzˇe, which is correct but rare in CZ (the primary and default equivalent for but in CZ is ale).</S>
			<S sid ="116" ssid = "10">This ‘mis-learning’ likely happened to a frequent correspondence of but– jenzˇe in the SMT training data, which then does not necessarily scale to and be of appropriate style in the testing data.</S>
			<S sid ="117" ssid = "11">If one disregards these occurrences, SYSTEM2 translates between about 8 and 20% of all connectives better than the BASELINE test data.</S>
			<S sid ="118" ssid = "12">In relation to that, simply concatenating the sense tags onto the connective word-forms leads to scarcity of the latter, whereas other ways to include linguistic labels in SMT, such as factored translation models, would account for the labels as additional translation features, which will be investigated in future work (Section 8).</S>
			<S sid ="119" ssid = "13">In the following, we analyze cases where SYSTEM2 translates the connectives better and more appropriately than the BASELINE.</S>
			<S sid ="120" ssid = "14">These cases include highly ambiguous connectives, temporal DCs with verbal ing-forms and conditionals.</S>
			<S sid ="121" ssid = "15">In general, for the very ambiguous EN connectives (e.g. as, when, while), disambiguated for SYSTEM2 with the PDTB sense tags, we indeed obtained more accurate translations than those generated by the BASELINE.</S>
			<S sid ="122" ssid = "16">One of the human judges had a close look at 25 randomly sampled instances of as, taken from the manually evaluated sets mentioned above.</S>
			<S sid ="123" ssid = "17">In these test cases, 68% of all occurrences of as were better translated by SYSTEM2 and only 4% of the translations were degraded when compared to the BASELINE.</S>
			<S sid ="124" ssid = "18">For details, see Table 34.</S>
			<S sid ="125" ssid = "19">In the following translation example (WSJ section 2365), and often elsewhere, the BASELINE system treats the connective as as a preposition jako with the meaning She worked as a teacher.</S>
			<S sid ="126" ssid = "20">This frequent interpretation seems to be learned quite reasonably from the SMT training data, it is however incorrect where as actually functions as a DC.</S>
			<S sid ="127" ssid = "21">SYSTEM2, in agreement with the tagging, then correctly generates the causal connective protozˇe: SOURCE: In the occupied lands, underground leaders of the Arab uprising rejected a U.S. plan to arrange IsraeliPalestinian talks as&lt;CONTINGENCYCAUSE&gt; Shamir opposed holding such discussions in Cairo.</S>
			<S sid ="128" ssid = "22">BASELINE: *Na okupovany´ ch u´ zem´ıch, podzemn´ı vu˚ dcu˚ arabsky´ ch povsta´n´ı odm´ıtl americky´ pla´n uspoˇra´dat (discounted percentages for jenzˇe in Table 2).</S>
			<S sid ="129" ssid = "23">The results seem therefore to be dependent on the parts of the test set evaluated and the DCs occurring in izraelskopalestinske´ rozhovory jako Sˇ amira proti poˇra´da´n´ı takovy´ ch diskus´ı v Ka´hiˇre.</S>
			<S sid ="130" ssid = "24">SYSTEM2: Na okupovany´ ch u´ zem´ıch, podzemn´ı vu˚ dcu˚ arabske´ho povsta´n´ı odm´ıtl americky´ pla´n uspoˇra´dat them.</S>
			<S sid ="131" ssid = "25">izraelsko palestins ke´ rozhovo ry, protozˇe Sˇ amira proti The only slight quantitative improvements and cases were SYSTEM2 performed worse are most likely due to the overall scarcity of the PDTB sense tags (cf.</S>
			<S sid ="132" ssid = "26">Section 4).</S>
			<S sid ="133" ssid = "27">Especially for SYSTEM1 but to some extent also for SYSTEM2, rare sense tags such as CONTINGENCYPRAGMATIC- CAUSE might not be seen often or even not at all in the SMT training data and therefore not be learned appropriately to provide good translations for the poˇra´da´n´ı takovy´ ch diskus´ı v Ka´hiˇre.</S>
			<S sid ="134" ssid = "28">DCs can also be translated to other syntactical constructs available in the target language that convey the same discourse relation without any 4 We included simple occurrences only, i.e. not compound connectives like as if, as soon as or translations were the connective was dropped.</S>
			<S sid ="135" ssid = "29">In the PDTB, as can have up to 17 distinct senses, ranging from temporal, causal to concessive relations.</S>
			<S sid ="136" ssid = "30">Configuration ∆(%) vs. BASELINE Total (%) Improved Equal Degraded se nt en ce s 1– 30 0 / 1 0 0 0 – 2 4 1 6 6 3 0 l a b e l e d D C s SY ST E M 2 7 . 9 7 5 . 2 9 . 4 9 2 . 5 no t co un tin g 25 x bu t– je nzˇ e 8 . 2 8 0 . 3 4 . 0 9 2 . 5 bo th sy ste ms wr on g 7 . 5 1 0 0 s e n t e n c e s 1 0 2 4 – 1 1 3 8 5 0 l a b e l e d D C s SY ST E M 2 1 6 7 6 6 9 8 no t co un tin g 2 x bu t– je nzˇ e 1 9 7 7 2 9 8 bo th sy ste ms wr on g 2 1 0 0 Table 2: Performance of SYSTEM2 (simplified PDTB tags) when manually counting for improved, equal and degraded translations compared to the BASELINE, in samples from the PDTB section 23 test set.</S>
			<S sid ="137" ssid = "31">explicit DC.</S>
			<S sid ="138" ssid = "32">For EN/CZ this occurs for DCs such as before/after/since + Verb in Present Continuous.</S>
			<S sid ="139" ssid = "33">In CZ, these either should be rendered as a verbal clause or a nominalization.</S>
			<S sid ="140" ssid = "34">We accounted for translations as being well-formed, if the SMT systems generated one of these possibilities correctly, i.e. not only the connective/preposition but also the verb/noun.</S>
			<S sid ="141" ssid = "35">In CZ, it must be decided between using a preposition (e.g. prˇed) or a connective (e.g. nezˇ).</S>
			<S sid ="142" ssid = "36">A good translation would for example be: before climbing = PREP+NP or DC+V, and a bad translation: before climbing = PREP+V/ADJ or DC+NP.</S>
			<S sid ="143" ssid = "37">The following example (WSJ section 2381) is a SYSTEM2 output where the sense tag in English helped to translate the connective before more correctly by DC+V, whereas the BASELINE renders this wrongly by using PREP+ADJ: SOURCE: Mr. Weisman predicts stocks will appear to stabilize in the next few days before&lt;TEMPORALASYNCHRONOUS&gt; declining again, trapping more investors.</S>
			<S sid ="144" ssid = "38">BASELINE: *Pan Weisman pˇredpov´ıda´, zˇe akcie budou stabilizovat v pˇr´ısˇt´ıch neˇkolika dnech pˇred/PREP klesaj´ıc´ım/ADJ opeˇt odchytu v´ıce investoru˚ . BASELINE system completely missed to render the personal pronoun (but still generated the correct conditional connective if–pokud), whereas SYSTEM2 outputs the much better if I–kdybych.</S>
			<S sid ="145" ssid = "39">However, apart from the better connective, SYSTEM2’s translation is worse than the BASELINE’s, because the first verb form is misconjugated and the second verb (will take) is missing: SOURCE: If&lt;CONTINGENCYCONDITION&gt; I sell now, I’ll take a big loss.</S>
			<S sid ="146" ssid = "40">BASELINE: *Pokud chteˇl prodat, tedˇ budu bra´t s velkou ztra´tou.</S>
			<S sid ="147" ssid = "41">LIT.: If he-wanted to-sell, now I-will take with big- Instrumental loss-Instrumental.</S>
			<S sid ="148" ssid = "42">SYSTEM2: Kdybych se nyn´ı proda´vaj´ı, se z tohohle velkou ztra´tu.</S>
			<S sid ="149" ssid = "43">LIT.: If-I themselves-ReflexPron now they-are selling, Re- flexPron out-of this big-Accusative loss-Accusative.</S>
			<S sid ="150" ssid = "44">From the automatic and manual translation evaluation, we conclude that using the sense tags for all 100 connectives in EN is not the most ap SYSTEM2: Pan Weisman pˇredpov´ıda´, zˇe akcie bude propriate method, and that only certain connec stabilizovat, jak se zda´, v pˇr´ısˇt´ıch neˇkolika dn´ı, nezˇ/DC opeˇt klesat/V, zablokova´n´ı v´ıce investoru˚ . A further difficult case in CZ is the binding of conditionals with personal pronouns, e.g. if I = kdybych, if you = kdybys, if he/she = kdyby etc. In the following example (WSJ section 2386), thetives such as as, when, while, yet and a few oth ers are very problematic in translation due to the many discourse relations they can signal.</S>
			<S sid ="151" ssid = "45">In future work, we will therefore analyze in more detail which connectives and which sense labels from the PDTB should actually be included in the data to train SMT.</S>
			<S sid ="152" ssid = "46">test sets.</S>
			<S sid ="153" ssid = "47">In future work we plan to more thoroughly analyze which connectives need to be dis- ambiguated at which sense granularity level before implementing them into an SMT system.</S>
			<S sid ="154" ssid = "48">For label implementation there also are other ways worth examining, such as factored translation models that handle the supplementary linguistic information as separate features and alternative decoding paths.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="155" ssid = "49">We are grateful for the funding of this work to the Swiss National Science Foundation (SNSF) under the COMTIS Sinergia project, n. CRSI22 127510 (see www.idiap.ch/comtis/), to the Grant Agency of the Czech Republic (project n. P406/12/0658) and to the SVV of the Charles University (project n. 267 314).</S>
			<S sid ="156" ssid = "50">We would like to thank Lenka Sa´ndor for her help with manual translation evaluation.</S>
			<S sid ="157" ssid = "51">Table 3: Translation outputs for the EN connective as, which was translated more correctly by SYSTEM2 thanks to the disambiguating sense tags compared to the BASELINE that often just produces the prepositional as – jako.</S>
			<S sid ="158" ssid = "52">The erroneous translations are marked in bold.</S>
			<S sid ="159" ssid = "53">The PDTB sense tags indicate the meaning of the CZ translations and are encoded as follows: Synchrony (Sy), Asynchrony (Asy), Contingency (Co), Cause (Ca).</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "8">
			<S sid ="160" ssid = "1">We presented experiments for EN/CZ SMT with a large amount of hand-labeled discourse connectives that are disambiguated in the source language and training material for MT systems by their sense tags or discourse relations they signal.</S>
			<S sid ="161" ssid = "2">This leads to improved translations in cases where the source DC is highly ambiguous or where the target language uses other syntactical constructs than a connective to convey the discourse relation.</S>
			<S sid ="162" ssid = "3">Using all 100 types of EN DCs in the corpus and/or all the detailed sense tags from the manual annotation most probably lead to the only very slight improvements for the discourse-aware systems when measured quantitatively over the whole</S>
	</SECTION>
</PAPER>
