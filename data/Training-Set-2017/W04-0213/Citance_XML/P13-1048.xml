<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We propose a novel approach for developing a two-stage document-level discourse parser.</S>
		<S sid ="2" ssid = "2">Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing.</S>
		<S sid ="3" ssid = "3">We present two approaches to combine these two stages of discourse parsing effectively.</S>
		<S sid ="4" ssid = "4">A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the state- of-the-art, often by a wide margin.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units.</S>
			<S sid ="6" ssid = "6">Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007).</S>
			<S sid ="7" ssid = "7">Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1.</S>
			<S sid ="8" ssid = "8">The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example).</S>
			<S sid ="9" ssid = "9">Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by internal ∗This work was conducted at the University of British Columbia, Vancouver, Canada.</S>
			<S sid ="10" ssid = "10">nodes), which in turn are also subject to this relation linking.</S>
			<S sid ="11" ssid = "11">Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one.</S>
			<S sid ="12" ssid = "12">Discourse analysis in RST involves two sub- tasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree.</S>
			<S sid ="13" ssid = "13">While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard.</S>
			<S sid ="14" ssid = "14">This paper aims to reduce this performance gap and take discourse parsing one step further.</S>
			<S sid ="15" ssid = "15">To this end, we address three key limitations of existing parsers as follows.</S>
			<S sid ="16" ssid = "16">First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequential dependencies between the DT constituents, which has been recently shown to be critical (Feng and Hirst, 2012).</S>
			<S sid ="17" ssid = "17">To address this limitation, as the first contribution, we propose a novel document-level discourse parser based on probabilistic discriminative parsing models, represented as Conditional Random Fields (CRFs) (Sutton et al., 2007), to infer the probability of all possible DT constituents.</S>
			<S sid ="18" ssid = "18">The CRF models effectively represent the structure and the label of a DT constituent jointly, and whenever possible, capture the sequential dependencies between the constituents.</S>
			<S sid ="19" ssid = "19">Second, existing parsers apply greedy and sub- optimal parsing algorithms to build the DT for a document.</S>
			<S sid ="20" ssid = "20">To cope with this limitation, our CRF models support a probabilistic bottom-up parsing Attribution But he added: (1) Contrast ContrastSame Unit &quot;Some people use the purchasers’ index as a leading indicator, some use it as a coincident indicator.</S>
			<S sid ="21" ssid = "21">Elaboration it missed altogether last month.&quot;</S>
			<S sid ="22" ssid = "22">&lt;P&gt; (2) (3) But the thing it’s supposed to measure-- manufacturing strength - (6) (4) (5) Figure 1: Discourse tree for two sentences in RSTDT.</S>
			<S sid ="23" ssid = "23">Each of the sentences contains three EDUs.</S>
			<S sid ="24" ssid = "24">The second sentence has a well-formed discourse tree, but the first sentence does not have one.</S>
			<S sid ="25" ssid = "25">algorithm which is non-greedy and optimal.</S>
			<S sid ="26" ssid = "26">Third, existing discourse parsers do not discriminate between intrasentential (i.e., building the DTs for the individual sentences) and multisentential parsing (i.e., building the DT for the document).</S>
			<S sid ="27" ssid = "27">However, we argue that distinguishing between these two conditions can result in more effective parsing.</S>
			<S sid ="28" ssid = "28">Two separate parsing models could exploit the fact that rhetorical relations are distributed differently intrasententially vs. multisententially.</S>
			<S sid ="29" ssid = "29">Also, they could independently choose their own informative features.</S>
			<S sid ="30" ssid = "30">As another key contribution of our work, we devise two different parsing components: one for intrasentential parsing, the other for multisentential parsing.</S>
			<S sid ="31" ssid = "31">This provides for scalable, modular and flexible solutions, that can exploit the strong correlation observed between the text structure (sentence boundaries) and the structure of the DT.</S>
			<S sid ="32" ssid = "32">In order to develop a complete and robust discourse parser, we combine our intrasentential and multisentential parsers in two different ways.</S>
			<S sid ="33" ssid = "33">Since most sentences have a well-formed discourse sub-tree in the full document-level DT (for example, the second sentence in Figure 1), our first approach constructs a DT for every sentence using our intrasentential parser, and then runs the multisentential parser on the resulting sentence- level DTs.</S>
			<S sid ="34" ssid = "34">However, this approach would disregard those cases where rhetorical structures violate sentence boundaries.</S>
			<S sid ="35" ssid = "35">For example, consider the first sentence in Figure 1.</S>
			<S sid ="36" ssid = "36">It does not have a well-formed sub-tree because the unit containing EDUs 2 and 3 merges with the next sentence and only then is the resulting unit merged with EDU 1.</S>
			<S sid ="37" ssid = "37">Our second approach, in an attempt of dealing.</S>
			<S sid ="38" ssid = "38">with these cases, builds sentence-level sub-trees by applying the intrasentential parser on a sliding window covering two adjacent sentences and by then consolidating the results produced by over lapping windows.</S>
			<S sid ="39" ssid = "39">After that, the multisentential parser takes all these sentence-level sub-trees and builds a full rhetorical parse for the document.</S>
			<S sid ="40" ssid = "40">While previous approaches have been tested on only one corpus, we evaluate our approach on texts from two very different genres: news articles and instructional how-to-do manuals.</S>
			<S sid ="41" ssid = "41">The results demonstrate that our contributions provide consistent and statistically significant improvements over previous approaches.</S>
			<S sid ="42" ssid = "42">Our final result compares very favorably to the result of state-of-the-art models in document-level discourse parsing.</S>
			<S sid ="43" ssid = "43">In the rest of the paper, after discussing related work in Section 2, we present our discourse parsing framework in Section 3.</S>
			<S sid ="44" ssid = "44">In Section 4, we describe the intra- and multisentential parsing components.</S>
			<S sid ="45" ssid = "45">Section 5 presents the two approaches to combine the two stages of parsing.</S>
			<S sid ="46" ssid = "46">The experiments and error analysis, followed by future directions are discussed in Section 6.</S>
			<S sid ="47" ssid = "47">Finally, we summarize our contributions in Section 7.</S>
	</SECTION>
	<SECTION title="Related work. " number = "2">
			<S sid ="48" ssid = "1">The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004).</S>
			<S sid ="49" ssid = "2">These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on.</S>
			<S sid ="50" ssid = "3">However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004).</S>
			<S sid ="51" ssid = "4">Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features.</S>
			<S sid ="52" ssid = "5">Hernault et al., (2010) presents the publicly available HILDA parser.</S>
			<S sid ="53" ssid = "6">Given the EDUs in a doc 30 Multisentential.</S>
			<S sid ="54" ssid = "7">Algorithm Algorithm 25 Intrasentential.</S>
			<S sid ="55" ssid = "8">20 15 10 5 0 Sentences segmented into EDUs model Intrasentential parser model Multisentential parser Document-level discourse tree Elaboration Joint Attribution Same-Unit Contrast Explanation Figure 2: Distributions of six most frequent relations in intrasentential and multisentential parsing scenarios.</S>
			<S sid ="56" ssid = "9">ument, HILDA iteratively employs two Support Vector Machine (SVM) classifiers in pipeline to build the DT.</S>
			<S sid ="57" ssid = "10">In each iteration, a binary classifier first decides which of the adjacent units to merge, then a multi-class classifier connects the selected units with an appropriate relation label.</S>
			<S sid ="58" ssid = "11">They evaluate their approach on the RSTDT corpus (Carlson et al., 2002) of news articles.</S>
			<S sid ="59" ssid = "12">On a different genre of instructional texts, Subba and DiEugenio (2009) propose a shift-reduce parser that relies on a classifier for relation labeling.</S>
			<S sid ="60" ssid = "13">Their classifier uses Inductive Logic Programming (ILP) to learn first-order logic rules from a set of features including compositional semantics.</S>
			<S sid ="61" ssid = "14">In this work, we address the limitations of these models (described in Section 1) introducing our novel discourse parser.</S>
	</SECTION>
	<SECTION title="Our Discourse Parsing Framework. " number = "3">
			<S sid ="62" ssid = "1">Given a document with sentences already segmented into EDUs, the discourse parsing problem is determining which discourse units (EDUs or larger units) to relate (i.e., the structure), and how to relate them (i.e., the labels or the discourse relations) in the resulting DT.</S>
			<S sid ="63" ssid = "2">Since we already have an accurate sentence-level discourse parser (Joty et al., 2012), a straightforward approach to document-level parsing could be to simply apply this parser to the whole document.</S>
			<S sid ="64" ssid = "3">However this strategy would be problematic because of scalability and modeling issues.</S>
			<S sid ="65" ssid = "4">Note that the number of valid trees grows exponentially with the number of EDUs in a document.1 Therefore, an exhaustive search over the valid trees is often unfeasible, even for relatively small documents.</S>
			<S sid ="66" ssid = "5">For modeling, the problem is twofold.</S>
			<S sid ="67" ssid = "6">On the one hand, it appears that rhetorical relations are distributed differently intrasententially vs. multisententially.</S>
			<S sid ="68" ssid = "7">For example, Figure 2 shows a comparison between the two distributions of six most 1 For n + 1 EDUs, the number of valid discourse trees is actually the Catalan number Cn . Figure 3: Discourse parsing framework.</S>
			<S sid ="69" ssid = "8">frequent relations on a development set containing 20 randomly selected documents from RSTDT.</S>
			<S sid ="70" ssid = "9">Notice that relations Attribution and Same-Unit are more frequent than Joint in intrasentential case, whereas Joint is more frequent than the other two in multisentential case.</S>
			<S sid ="71" ssid = "10">On the other hand, different kinds of features are applicable and informative for intrasentential vs. multisentential parsing.</S>
			<S sid ="72" ssid = "11">For example, syntactic features like dominance sets (Soricut and Marcu, 2003) are extremely useful for sentence-level parsing, but are not even applicable in multisentential case.</S>
			<S sid ="73" ssid = "12">Likewise, lexical chain features (Sporleder and Lascarides, 2004), that are useful for multisentential parsing, are not applicable at the sentence level.</S>
			<S sid ="74" ssid = "13">Based on these observations, our discourse parsing framework comprises two separate modules: an intrasentential parser and a multisentential parser (Figure 3).</S>
			<S sid ="75" ssid = "14">First, the intrasentential parser produces one or more discourse sub-trees for each sentence.</S>
			<S sid ="76" ssid = "15">Then, the multisentential parser generates a full DT for the document from these sub-trees.</S>
			<S sid ="77" ssid = "16">Both of our parsers have the same two components: a parsing model assigns a probability to every possible DT, and a parsing algorithm identifies the most probable DT among the candidate DTs in that scenario.</S>
			<S sid ="78" ssid = "17">While the two models are rather different, the same parsing algorithm is shared by the two modules.</S>
			<S sid ="79" ssid = "18">Staging multisentential parsing on top of intrasentential parsing in this way allows us to exploit the strong correlation between the text structure and the DT structure as explained in detail in Section 5.</S>
			<S sid ="80" ssid = "19">Before describing our parsing models and the parsing algorithm, we introduce some terminology that we will use throughout the paper.</S>
			<S sid ="81" ssid = "20">Following (Joty et al., 2012), a DT can be formally represented as a set of constituents of the form R[i, m, j], referring to a rhetorical relation R between the discourse unit containing EDUs i through m and the unit containing EDUs m+1 through j. For example, the DT for the second sentence in Figure 1 can be represented as {Elaboration-NS[4,4,5], Same-Unit-NN[4,5,6]}.</S>
			<S sid ="82" ssid = "21">R 2Notice that a relation R also specifies the nuclear ity statuses of the discourse units involved, which S2can be one of Nucleus-Satellite (NS), Satellite Nucleus (SN) and Nucleus-Nucleus (NN).</S>
			<S sid ="83" ssid = "22">U U 2 R3 R j S S j U3 U j R t-1 St1 U t-1 R t Relation sequence S Structure t sequence Unit U t sequence at level i</S>
	</SECTION>
	<SECTION title="Parsing Models and Parsing Algorithm. " number = "4">
			<S sid ="84" ssid = "1">The job of our intrasentential and multisentential parsing models is to assign a probability to each of the constituents of all possible DTs at the sentence level and at the document level, respectively.</S>
			<S sid ="85" ssid = "2">Formally, given the model parameters Θ, for each possible constituent R[i, m, j] in a candidate DT at the sentence or document level, the parsing model estimates P (R[i, m, j]|Θ), which specifiesa joint distribution over the label R and the struc ture [i, m, j] of the constituent.</S>
			<S sid ="86" ssid = "3">4.1 IntraSentential Parsing Model.</S>
			<S sid ="87" ssid = "4">Recently, we proposed a novel parsing model for sentence-level discourse parsing (Joty et al., 2012), that outperforms previous approaches by effectively modeling sequential dependencies along with structure and labels jointly.</S>
			<S sid ="88" ssid = "5">Below we briefly describe the parsing model, and show how it is applied to obtain the probabilities of all possible DT constituents at the sentence level.</S>
			<S sid ="89" ssid = "6">Figure 4 shows the intrasentential parsing model expressed as a Dynamic Conditional Random Field (DCRF) (Sutton et al., 2007).</S>
			<S sid ="90" ssid = "7">The observed nodes Uj in a sequence represent the discourse units (EDUs or larger units).</S>
			<S sid ="91" ssid = "8">The first layer Figure 4: A chain-structured DCRF as our intrasentential parsing model.</S>
			<S sid ="92" ssid = "9">process, let us assume that the sentence contains four EDUs.</S>
			<S sid ="93" ssid = "10">At the first (bottom) level, when all the units are the EDUs, there is only one possible unit sequence to which we apply our DCRF model (Figure 5(a)).</S>
			<S sid ="94" ssid = "11">We compute the posterior marginals P (R2, S2=1|e1, e2, e3, e4, Θ), P (R3, S3=1|e1, e2, e3, e4, Θ) and P (R4, S4=1|e1, e2, e3,e4, Θ) to obtain the probability of the con stituents R[1, 1, 2], R[2, 2, 3] and R[3, 3, 4], respectively.</S>
			<S sid ="95" ssid = "12">At the second level, there are three possible unit sequences (e1:2, e3, e4), (e1,e2:3, e4) and (e1,e2,e3:4).</S>
			<S sid ="96" ssid = "13">Figure 5(b) shows their corresponding DCRFs.</S>
			<S sid ="97" ssid = "14">The posterior marginals P (R3, S3=1|e1:2,e3,e4,Θ), P (R2:3 S2:3=1|e1,e2:3,e4,Θ), P (R4, S4=1|e1,e2:3,e4,Θ) and P (R3:4, S3:4=1|e1,e2,e3:4,Θ) computed from the three sequences correspond to the probability of the constituents R[1, 2, 3], R[1, 1, 3], R[2, 3, 4] and R[2, 2, 4], respectively.</S>
			<S sid ="98" ssid = "15">Similarly, we attain the probability of the constituents R[1, 1, 4], R[1, 2, 4] and R[1, 3, 4] by computing their respective posterior marginals from the three possible sequences at the third (top) level.</S>
			<S sid ="99" ssid = "16">of hidden nodes are the structure nodes, where Sj ∈{0, 1} denotes whether two adjacent discourse units Uj−1 and Uj should be connected or not.</S>
			<S sid ="100" ssid = "17">R R R 2 3 4 S S S 2 R R 4 R R 3 2:3 4 S S S 3 2:3 The second layer of hidden nodes are the relation e e e e 1 2 3 4 e e 1:2 3 e e 1 2:3 4 nodes, with Rj ∈{1 . . .</S>
			<S sid ="101" ssid = "18">M } denoting the relation between two adjacent units Uj−1 and Uj , where M is the total number of relations in the relation set.</S>
			<S sid ="102" ssid = "19">R 2:4 (a) R 3:4 (i) (ii) R2 R 4 S2 R 3:4 S 3:4 The connections between adjacent nodes in a hidden layer encode sequential dependencies between S 2:4 e e e S3:4 S e e e 1 e 2 e 3:4the respective hidden nodes, and can enforce con 1 2:4 1:2 (c) 3:4 1:3 e 4 (iii) (b) straints such as the fact that a Sj = 1 must not follow a Sj−1= 1.</S>
			<S sid ="103" ssid = "20">The connections between the two hidden layers model the structure and the relation of a DT (sentence-level) constituent jointly.</S>
			<S sid ="104" ssid = "21">To obtain the probability of the constituents of all candidate DTs for a sentence, we apply the parsing model recursively at different levels of the DT and compute the posterior marginals over the relation-structure pairs.</S>
			<S sid ="105" ssid = "22">To illustrate the (i) (ii) (iii) Figure 5: Our parsing model applied to the sequences at different levels of a sentence-level DT.</S>
			<S sid ="106" ssid = "23">(a) Only possible sequence at the first level, (b) Three possible sequences at the second level, (c) Three possible sequences at the third level.</S>
			<S sid ="107" ssid = "24">At this point what is left to be explained is how we generate all possible sequences for a given number of EDUs in a sentence.</S>
			<S sid ="108" ssid = "25">Algorithm 1 demonstrates how we do that.</S>
			<S sid ="109" ssid = "26">More specifically, to compute the probabilities of each DT con stituent R[i, k, j], we need to generate sequences like (e1, · · · , ei−1, ei:k , ek+1:j , ej+1, · · · , en) for 1 ≤ i ≤ k &lt; j ≤ n. In doing so, we may R t Relation S Structure generate some duplicate sequences.</S>
			<S sid ="110" ssid = "27">Clearly, the sequence (e1, · · · , ei−1, ei:i, ei+1:j , ej+1, · · · , en) for 1 ≤ i ≤ k &lt; j &lt; n is already considered U t-1 t Adjacent Units U t at level i for computing the probability of R[i + 1, j, j + 1].</S>
			<S sid ="111" ssid = "28">Therefore, it is a duplicate sequence that we exclude from our list of all possible sequences.</S>
			<S sid ="112" ssid = "29">Input: Sequence of EDUs: (e1, e2, · · · , en) Output: List of sequences: L for i = 1 → n − 1 do for j = i + 1 → n do if j == n then for k = i → j − 1 do L.append ((e1 , .., ei−1 , ei:k , ek+1:j , ej+1 , .., en )) end else for k = i + 1 → j − 1 do L.append Figure 6: A CRF as a multisentential parsing model.</S>
			<S sid ="113" ssid = "30">we could use forwards-backwards algorithm for exact inference in this model (Sutton and McCallum, 2012).</S>
			<S sid ="114" ssid = "31">However, forwards-backwards on a sequence containing T units costs O(T M 2) time, where M is the number of relations in our relation set.</S>
			<S sid ="115" ssid = "32">This makes the chain-structured DCRF model impractical for multisentential parsing of long documents, since learning requires to run inference on every training sequence with an overall time complexity of O(T M 2n3) per document.</S>
			<S sid ="116" ssid = "33">Our model for multisentential parsing is shown in Figure 6.</S>
			<S sid ="117" ssid = "34">The two observed nodes Ut−1 and Ut are two adjacent discourse units.</S>
			<S sid ="118" ssid = "35">The (hidden) ((e1 , .., ei−1 , ei:k , ek+1:j , ej+1 , .., en )) end structure node S ∈{0, 1} denotes whether the two end end end Algorithm 1: Generating all possible sequences for a sentence with n EDUs.</S>
			<S sid ="119" ssid = "36">Once we obtain the probability of all possible units should be connected or not.</S>
			<S sid ="120" ssid = "37">The hidden node R∈{1 . . .</S>
			<S sid ="121" ssid = "38">M } represents the relation between the two units.</S>
			<S sid ="122" ssid = "39">Notice that like the previous model, this is also an undirected graphical model.</S>
			<S sid ="123" ssid = "40">It becomes a CRF if we directly model the hidden (output) variables by conditioning its clique potential (or factor) φ on the observed (input) variables: 1 DT constituents, the discourse sub-trees for the sentences are built by applying an optimal probabilistic parsing algorithm (Section 4.4) using one P (Rt, St|x, Θ) = Z (x, Θ) φ(Rt, St|x, Θ) (1) of the methods described in Section 5.</S>
			<S sid ="124" ssid = "41">4.2 MultiSentential Parsing Model.</S>
			<S sid ="125" ssid = "42">Given the discourse units (sub-trees) for all the sentences of a document, a simple approach to build the rhetorical tree of the document would be to apply a new DCRF model, similar to the one in Figure 4 (with different parameters), to all the possible sequences generated from these units to infer the probability of all possible higher-order constituents.</S>
			<S sid ="126" ssid = "43">However, the number of possible sequences and their length increase with the number of sentences in a document.</S>
			<S sid ="127" ssid = "44">For example, assuming that each sentence has a well-formed DT, for a document with n sentences, Algorithm 1 generates O(n3) sequences, where the sequence at the bottom level has n units, each of the sequences at the second level has n-1 units, and so on.</S>
			<S sid ="128" ssid = "45">Since the model in Figure 4 has a “fat” chain structure, where x represents input features extracted from the observed variables Ut−1 and Ut, and Z (x, Θ) is the partition function.</S>
			<S sid ="129" ssid = "46">We use a log-linear representation of the factor: φ(Rt, St|x, Θ) = exp(ΘT f (Rt, St, x)) (2) where f (Rt, St, x) is a feature vector derived from the input features x and the labels Rt and St, and Θ is the corresponding weight vector.</S>
			<S sid ="130" ssid = "47">Although, this model is similar in spirit to the model in Figure 4, we now break the chain structure, which makes the inference much faster (i.e., complexity of O(M 2)).</S>
			<S sid ="131" ssid = "48">Breaking the chain structure also allows us to balance the data for training (equal number instances with S=1 and S=0), which dramatically reduces the learning time of the model.</S>
			<S sid ="132" ssid = "49">We apply our model to all possible adjacent units at all levels for the multisentential case, and compute the posterior marginals of the relation- structure pairs P (Rt, St=1|Ut−1, Ut, Θ) to obtain the probability of all possible DT constituents.</S>
			<S sid ="133" ssid = "50">4.3 Features Used in our Parsing Models.</S>
			<S sid ="134" ssid = "51">Table 1 summarizes the features used in our parsing models, which are extracted from two adjacent units Ut−1 and Ut. Since most of these features are adopted from previous studies (Joty et al., 2012; Hernault et al., 2010), we briefly describe them.</S>
			<S sid ="135" ssid = "52">Organizational features include the length of the units as the number of EDUs and tokens.</S>
			<S sid ="136" ssid = "53">It also includes the distances of the units from the beginning and end of the sentence (or text in the multisentential case).</S>
			<S sid ="137" ssid = "54">Text structural features indirectly capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the units.</S>
			<S sid ="138" ssid = "55">Discourse markers (e.g., because, although) carry informative clues for rhetorical relations (Marcu, 2000a).</S>
			<S sid ="139" ssid = "56">Rather than using a fixed list of discourse markers, we use an empirically learned lexical N-gram dictionary following (Joty et al., 2012).</S>
			<S sid ="140" ssid = "57">This approach has been shown to be more robust and flexible across domains (Bi- ran and Rambow, 2011; Hernault et al., 2010).</S>
			<S sid ="141" ssid = "58">We also include part-of-speech (POS) tags for the beginning and end N tokens in a unit.</S>
			<S sid ="142" ssid = "59">8 Organizational features Intra &amp; MultiSentential.</S>
			<S sid ="143" ssid = "60">Number of EDUs in unit 1 (or unit 2).</S>
			<S sid ="144" ssid = "61">Number of tokens in unit 1 (or unit 2).</S>
			<S sid ="145" ssid = "62">Distance of unit 1 in EDUs to the beginning (or to the end).</S>
			<S sid ="146" ssid = "63">Distance of unit 2 in EDUs to the beginning (or to the end).</S>
			<S sid ="147" ssid = "64">4 Text structural features MultiSentential.</S>
			<S sid ="148" ssid = "65">Number of sentences in unit 1 (or unit 2).</S>
			<S sid ="149" ssid = "66">Number of paragraphs in unit 1 (or unit 2).</S>
			<S sid ="150" ssid = "67">8 N-gram features N ∈{1, 2, 3} Intra &amp; MultiSentential Beginning (or end) lexical N-grams in unit 1.</S>
			<S sid ="151" ssid = "68">Beginning (or end) lexical N-grams in unit 2.</S>
			<S sid ="152" ssid = "69">Beginning (or end) POS N-grams in unit 1.</S>
			<S sid ="153" ssid = "70">Beginning (or end) POS N-grams in unit 2.</S>
			<S sid ="154" ssid = "71">5 Dominance set features IntraSentential Syntactic labels of the head node and the attachment node.</S>
			<S sid ="155" ssid = "72">Lexical heads of the head node and the attachment node.</S>
			<S sid ="156" ssid = "73">Dominance relationship between the two units.</S>
			<S sid ="157" ssid = "74">8 Lexical chain features MultiSentential Number of chains start in unit 1 and end in unit 2.</S>
			<S sid ="158" ssid = "75">Number of chains start (or end) in unit 1 (or in unit 2).</S>
			<S sid ="159" ssid = "76">Number of chains skipping both unit 1 and unit 2.</S>
			<S sid ="160" ssid = "77">Number of chains skipping unit 1 (or unit 2).</S>
			<S sid ="161" ssid = "78">2 Contextual features Intra &amp; MultiSentential Previous and next feature vectors.</S>
			<S sid ="162" ssid = "79">2 Substructure features Intra &amp; MultiSentential.</S>
			<S sid ="163" ssid = "80">Root nodes of the left and right rhetorical sub-trees.</S>
			<S sid ="164" ssid = "81">Table 1: Features used in our parsing models.</S>
			<S sid ="165" ssid = "82">Lexico-syntactic features dominance sets (Soricut and Marcu, 2003) are very effective for intrasentential parsing.</S>
			<S sid ="166" ssid = "83">We include syntactic labels and lexical heads of head and attachment nodes along with their dominance relationship as features.</S>
			<S sid ="167" ssid = "84">Lexical chains (Morris and Hirst, 1991) are sequences of semantically related words that can indicate topic shifts.</S>
			<S sid ="168" ssid = "85">Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004).</S>
			<S sid ="169" ssid = "86">We compute lexical chains for a document following the approach proposed in (Galley and McKeown, 2003), that extracts lexical chains after performing word sense disambiguation.</S>
			<S sid ="170" ssid = "87">Following (Joty et al., 2012), we also encode contextual and rhetorical substructure features in our models.</S>
			<S sid ="171" ssid = "88">The rhetorical substructure features incorporate hierarchical dependencies between DT constituents.</S>
			<S sid ="172" ssid = "89">4.4 Parsing Algorithm.</S>
			<S sid ="173" ssid = "90">Given the probability of all possible DT constituents in the intrasentential and multisentential scenarios, the job of the parsing algorithm is to find the most probable DT for that scenario.</S>
			<S sid ="174" ssid = "91">Following (Joty et al., 2012), we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse using dynamic programming.</S>
			<S sid ="175" ssid = "92">Specifically, with n discourse units, we use the upper-triangular portion of the n×n dynamic programming table D. Given Ux(0) and Ux(1) are the start and end EDU Ids of unit Ux: D[i, j] = P (R[Ui(0), Uk (1), Uj (1)]) (3) where, k = argmax P (R[Ui(0), Up(1), Uj (1)]).</S>
			<S sid ="176" ssid = "93">i≤p≤j Note that, in contrast to previous studies on document-level parsing (Hernault et al., 2010; Subba and DiEugenio, 2009; Marcu, 2000b), which use a greedy algorithm, our approach finds a discourse tree that is globally optimal.</S>
	</SECTION>
	<SECTION title="Document-level Parsing Approaches. " number = "5">
			<S sid ="177" ssid = "1">Now that we have presented our intrasentential and our multisentential parsers, we are ready to describe how they can be effectively combined to perform document-level discourse analysis.</S>
			<S sid ="178" ssid = "2">Recall that a key motivation for a two-stage parsing is that it allows us to capture the correlation between text structure and discourse structure in a scalable, modular and flexible way.</S>
			<S sid ="179" ssid = "3">Below we describe two different approaches to model this correlation.</S>
			<S sid ="180" ssid = "4">5.1 1S1S (1 Sentence-1 Sub-tree) A key finding from several previous studies on sentence-level discourse analysis is that most sentences have a well-formed discourse sub-tree in the full document-level DT (Joty et al., 2012; Fisher and Roark, 2007).</S>
			<S sid ="181" ssid = "5">For example, Figure 7(a) shows 10 EDUs in 3 sentences (see boxes), where the DTs for the sentences obey their respective sentence boundaries.</S>
			<S sid ="182" ssid = "6">The 1S1S approach aims to maximally exploit this finding.</S>
			<S sid ="183" ssid = "7">It first constructs a DT for every sentence using our intrasentential parser, and then it provides our multisentential parser with the sentence-level DTs to build the rhetorical parse for the whole document.</S>
			<S sid ="184" ssid = "8">intrasentential parser constructs a DT for S1S2 and a DT for S2S3.</S>
			<S sid ="185" ssid = "9">In this process, each sentence in a document except the first and the last will be associated with two DTs: one with the previous sentence (say DTp) and one with the next (say DTn).</S>
			<S sid ="186" ssid = "10">In other words, for each non-boundary sentence, we will have two decisions: one from DTp and one from DTn.</S>
			<S sid ="187" ssid = "11">Our parser consolidates the two decisions and generates one or more sub-trees for each sentence by checking the following three mutually exclusive conditions one after another: • Same in both: If the sentence has the same (in terms of both structure and labels) well-formed sub-tree in both DTp and DTn, we take this sub- tree for the sentence.</S>
			<S sid ="188" ssid = "12">For example, in Figure 8(a), S2 has the same sub-tree in the two DTs, i.e. a DT for S1S2 and a DT for S2S3.</S>
			<S sid ="189" ssid = "13">The two decisions agree on the DT for the sentence.</S>
			<S sid ="190" ssid = "14">1 2 3 S 1 4 5 6 7 S 2 8 9 10 S 3 1 2 3 S 1 4 5 6 7 S 2 8 9 10 S 3 • Differe nt but no cross: If the sentenc e has a (a) (b) Figure 7: Two possible DTs for three sentences.</S>
			<S sid ="191" ssid = "15">5.2 Sliding Window.</S>
			<S sid ="192" ssid = "16">While the assumption made by 1S1S clearly simplifies the parsing process, it totally ignores the cases where discourse structures violate sentence boundaries.</S>
			<S sid ="193" ssid = "17">For example, in the DT shown in Figure 7(b), sentence S2 does not have a well-formed well-formed sub-tree in both DTp and DTn, but the two sub-trees vary either in structure or in labels, we pick the most probable one.</S>
			<S sid ="194" ssid = "18">For example, consider the DT for S1S2 in Figure 8(a) and the DT for S2S3 in Figure 8(b).</S>
			<S sid ="195" ssid = "19">In both cases S2 has a well-formed sub-tree, but they differ in structure.</S>
			<S sid ="196" ssid = "20">We pick the sub-tree which has the higher probability in the two dynamic programming tables.</S>
			<S sid ="197" ssid = "21">sub-tree because some of its units attach to the left (45, 6) and some to the right (7).</S>
			<S sid ="198" ssid = "22">Vliet and Redeker (2011) call these cases as ‘leaky’ boundaries.</S>
			<S sid ="199" ssid = "23">Even though less than 5% of the sentences S1 S2 S2 (a) 8 9 10 S 3 4 5 6 7 S 2 (b) 8 9 10 S 3 have leaky boundaries in RSTDT, in other corporathis can be true for a larger portion of the sen 1 2 3 S 1 4 5 6 7 S 2 4 5 6 7 S 2 8 9 10 S 3 tences.</S>
			<S sid ="200" ssid = "24">For example, we observe over 12% sentences with leaky boundaries in the Instructional corpus of (Subba and DiEugenio, 2009).</S>
			<S sid ="201" ssid = "25">However, we notice that in most cases where discourse structures violate sentence boundaries, its units are merged with the units of its adjacent sentences, as in Figure 7(b).</S>
			<S sid ="202" ssid = "26">For example, this is true for 75% cases in our development set containing 20 news articles from RSTDT and for 79% cases in our development set containing 20 how-to-do manuals from the Instructional corpus.</S>
			<S sid ="203" ssid = "27">Based on this observation, we propose a sliding window approach.</S>
			<S sid ="204" ssid = "28">In this approach, our intrasentential parser works with a window of two consecutive sentences, and builds a DT for the two sentences.</S>
			<S sid ="205" ssid = "29">For example, given the three sentences in Figure 7, our (i) (ii) (c) Figure 8: Extracting sub-trees for S2.</S>
			<S sid ="206" ssid = "30">• Cross: If either or both of DTp and DTn segment the sentence into multiple sub-trees, we pick the one with more sub-trees.</S>
			<S sid ="207" ssid = "31">For example, consider the two DTs in Figure 8(c).</S>
			<S sid ="208" ssid = "32">In the DT for S1S2, S2 has three sub-trees (45,6,7), whereas in the DT for S2S3, it has two (46,7).</S>
			<S sid ="209" ssid = "33">So, we extract the three sub-trees for S2 from the first DT.</S>
			<S sid ="210" ssid = "34">If the sentence has the same number of sub-trees in both DTp and DTn, we pick the one with higher probability in the dynamic programming tables.</S>
			<S sid ="211" ssid = "35">At the end, the multisentential parser takes all these sentence-level sub-trees for a document, and builds a full rhetorical parse for the document.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "6">
			<S sid ="212" ssid = "1">6.1 Corpora.</S>
			<S sid ="213" ssid = "2">While previous studies on document-level parsing only report their results on a particular corpus, to show the generality of our method, we experiment with texts from two very different genres.</S>
			<S sid ="214" ssid = "3">Our first corpus is the standard RSTDT (Carlson et al., 2002), which consists of 385 Wall Street Journal articles, and is partitioned into a training set of 347 documents and a test set of 38 documents.</S>
			<S sid ="215" ssid = "4">53 documents, selected from both sets were annotated by two annotators, based on which we measure human agreement.</S>
			<S sid ="216" ssid = "5">In RSTDT, the original 25 rhetorical relations defined by (Mann and Thompson, 1988) are further divided into a set of 18 coarser relation classes with 78 finer-grained relations.</S>
			<S sid ="217" ssid = "6">Our second corpus is the Instructional corpus prepared by (Subba and DiEugenio, 2009), which contains 176 how-to-do manuals on home- repair.</S>
			<S sid ="218" ssid = "7">The corpus was annotated with 26 informational relations (e.g., Preparation-Act, Act-Goal).</S>
			<S sid ="219" ssid = "8">6.2 Experimental Setup.</S>
			<S sid ="220" ssid = "9">We experiment with our discourse parser on the two datasets using our two different parsing approaches, namely 1S1S and the sliding window.</S>
			<S sid ="221" ssid = "10">We compare our approach with HILDA (Hernault et al., 2010) on RSTDT, and with the ILP-based approach of (Subba and DiEugenio, 2009) on the Instructional corpus, since they are the state-of- the-art on the respective genres.</S>
			<S sid ="222" ssid = "11">On RSTDT, the standard split was used for training and testing purposes.</S>
			<S sid ="223" ssid = "12">The results for HILDA were obtained by running the system with default settings on the same inputs we provided to our system.</S>
			<S sid ="224" ssid = "13">Since we could not run the ILP-based system of (Subba and DiEugenio, 2009) (not publicly available) on the Instructional corpus, we report the performances presented in their paper.</S>
			<S sid ="225" ssid = "14">They used 151 documents for training and 25 documents for testing.</S>
			<S sid ="226" ssid = "15">Since we did not have access to their particular split, we took 5 random samples of 151 documents for training and 25 documents for testing, and report the average performance over the 5 test sets.</S>
			<S sid ="227" ssid = "16">To evaluate the parsing performance, we use the standard unlabeled (i.e., hierarchical spans) SN, NN) to these relations, we get 41 distinct relations.</S>
			<S sid ="228" ssid = "17">Following (Subba and DiEugenio, 2009) on the Instructional corpus, we use 26 relations, and treat the reversals of non-commutative relations as separate relations.</S>
			<S sid ="229" ssid = "18">That is, Goal-Act and Act-Goal are considered as two different relations.</S>
			<S sid ="230" ssid = "19">Attaching the nuclearity statuses to these relations gives 76 distinct relations.</S>
			<S sid ="231" ssid = "20">Analogous to previous studies, we map the nary relations (e.g., Joint) into nested right-branching binary relations.</S>
			<S sid ="232" ssid = "21">6.3 Results and Error Analysis.</S>
			<S sid ="233" ssid = "22">Table 2 presents F-score parsing results for our parsers and the existing systems on the two corpora.2 On both corpora, our parser, namely, 1S1S (TSP 11) and sliding window (TSP SW), outperform existing systems by a wide margin (p&lt;7.1e 05).3 On RSTDT, our parsers achieve absolute F-score improvements of 8%, 9.4% and 11.4% in span, nuclearity and relation, respectively, over HILDA.</S>
			<S sid ="234" ssid = "23">This represents relative error reductions of 32%, 23% and 21% in span, nuclearity and relation, respectively.</S>
			<S sid ="235" ssid = "24">Our results are also close to the upper bound, i.e. human agreement on this corpus.</S>
			<S sid ="236" ssid = "25">On the Instructional genre, our parsers deliver absolute F-score improvements of 10.5%, 13.6% and 8.14% in span, nuclearity and relations, respectively, over the ILP-based approach.</S>
			<S sid ="237" ssid = "26">Our parsers, therefore, reduce errors by 36%, 27% and 13% in span, nuclearity and relations, respectively.</S>
			<S sid ="238" ssid = "27">If we compare the performance of our parsers on the two corpora, we observe higher results on RSTDT.</S>
			<S sid ="239" ssid = "28">This can be explained in at least two ways.</S>
			<S sid ="240" ssid = "29">First, the Instructional corpus has a smaller amount of data with a larger set of relations (76 when nuclearity attached).</S>
			<S sid ="241" ssid = "30">Second, some frequent relations are (semantically) very similar (e.g., Preparation-Act, Step1Step2), which makes it difficult even for the human annotators to distinguish them (Subba and DiEugenio, 2009).</S>
			<S sid ="242" ssid = "31">Comparison between our two models reveals that TSP SW significantly outperforms TSP 11 only in finding the right structure on both corpora (p&lt;0.01).</S>
			<S sid ="243" ssid = "32">Not surprisingly, the improvement is higher on the Instructional corpus.</S>
			<S sid ="244" ssid = "33">A likely explanation is that the Instructional corpus contains more leaky boundaries (12%), allowing the sliding and labeled (i.e., nuclearity and relation) precision, recall and F-score as described in (Marcu, 2000b).</S>
			<S sid ="245" ssid = "34">To compare with previous studies, our experiments on RSTDT use the 18 coarser relations.</S>
			<S sid ="246" ssid = "35">After attaching the nuclearity statuses (NS, 2 Precision, Recall and F-score are the same when manual.</S>
			<S sid ="247" ssid = "36">segmentation is used (see Marcu, (2000b), page 143).</S>
			<S sid ="248" ssid = "37">3 Since we did not have access to the output or to the system of (Subba and DiEugenio, 2009), we were not able to perform a significance test on the Instructional corpus.</S>
			<S sid ="249" ssid = "38">R S T D T Inst ruct ion al M etr ics HI L D A TS P 1 1 TS P S W Hu m an IL P TS P 1 1 TS P S W Sp an Nu cle ari ty Re lati on 7 4.</S>
			<S sid ="250" ssid = "39">6 8 5 8.</S>
			<S sid ="251" ssid = "40">9 9 4 4.</S>
			<S sid ="252" ssid = "41">3 2 8 2.</S>
			<S sid ="253" ssid = "42">4 7* 6 8.</S>
			<S sid ="254" ssid = "43">4 3* 5 5.</S>
			<S sid ="255" ssid = "44">7 3* 8 2.</S>
			<S sid ="256" ssid = "45">7 4* † 6 8.</S>
			<S sid ="257" ssid = "46">4 0* 5 5.</S>
			<S sid ="258" ssid = "47">7 1* 8 8.</S>
			<S sid ="259" ssid = "48">7 0 7 7.</S>
			<S sid ="260" ssid = "49">7 2 6 5.</S>
			<S sid ="261" ssid = "50">7 5 70 .3 5 49 .4 7 35 .4 4 7 9 . 6 7 6 3 . 0 3 4 3 . 5 2 8 0.</S>
			<S sid ="262" ssid = "51">8 8 † 6 3 . 1 0 4 3 . 5 8 Table 2: Parsing results of different models using manual (gold) segmentation.</S>
			<S sid ="263" ssid = "52">Performances significantly superior to HILDA (with p&lt;7.1e05) are denoted by *.</S>
			<S sid ="264" ssid = "53">Significant differences between TSP 11 and TSP SW (with p&lt;0.01) are denoted by †. T-C TO T-CM M-M CMP EV SU CND EN CA TE EX BA CO JO S-U AT EL T-C 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 TO 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 T-CM 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 7 M-M 0 0 0 10 0 0 0 0 0 0 0 1 1 0 0 0 1 3 CMP 0 0 0 1 4 0 0 1 0 1 0 3 3 0 1 1 0 2 EV 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 2 11 SU 0 0 0 0 0 0 8 0 0 0 0 0 0 0 1 0 0 12 CND 0 0 0 0 0 0 0 22 0 0 0 0 1 3 0 0 3 2 EN 0 0 0 0 0 0 0 1 24 1 0 0 0 0 0 0 1 7 CA 0 0 0 0 0 0 0 0 2 3 0 4 2 2 7 0 3 11 TE 0 0 0 1 0 0 0 1 2 0 7 1 9 1 9 0 3 4 EX 0 0 0 1 0 0 0 0 1 5 0 12 0 1 3 0 3 12 BA 0 0 0 1 0 0 0 1 0 1 4 1 19 2 6 1 5 12 CO 0 0 0 1 2 0 0 2 0 1 3 2 2 33 7 0 0 9 JO 0 0 0 0 0 0 1 2 0 1 1 1 1 2 57 1 0 13 S-U 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 85 1 0 AT 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 272 9 EL 0 1 0 0 0 0 0 0 14 6 1 8 1 0 8 2 2 359 Figure 9: Confusion matrix for relation labels on the RSTDT test set.</S>
			<S sid ="265" ssid = "54">Y-axis represents true and X-axis represents predicted relations.</S>
			<S sid ="266" ssid = "55">The relations are Topic-Change (T-C), Topic-Comment (T-CM), Textual Organization (TO), Manner-Means (M-M), Comparison (CMP), Evaluation (EV), Summary (SU), Condition (CND), Enablement (EN), Cause (CA), Temporal (TE), Explanation (EX), Background (BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribution (AT) and Elaboration (EL).</S>
			<S sid ="267" ssid = "56">window approach to be more effective in finding those, without inducing much noise for the labels.</S>
			<S sid ="268" ssid = "57">This clearly demonstrates the potential of TSP SW for datasets with even more leaky boundaries e.g., the Dutch (Vliet and Redeker, 2011) and the German Potsdam (Stede, 2004) corpora.</S>
			<S sid ="269" ssid = "58">Error analysis reveals that although TSP SW finds more correct structures, a corresponding improvement in labeling relations is not present because in a few cases, it tends to induce noise from the neighboring sentences for the labels.</S>
			<S sid ="270" ssid = "59">For example, when parsing was performed on the first sentence in Figure 1 in isolation using 1S1S, our parser rightly identifies the Contrast relation between EDUs 2 and 3.</S>
			<S sid ="271" ssid = "60">But, when it is considered with its neighboring sentences by the sliding window, the parser labels it as Elaboration.</S>
			<S sid ="272" ssid = "61">A promising strategy to deal with this and similar problems that we plan to explore in future, is to apply both approaches to each sentence and combine them by consolidating three probabilistic decisions, i.e. the one from 1S1S and the two from sliding window.</S>
			<S sid ="273" ssid = "62">To further analyze the errors made by our parser on the hardest task of relation labeling, Figure 9 presents the confusion matrix for TSP 11 on the RSTDT test set.</S>
			<S sid ="274" ssid = "63">The relation labels are ordered according to their frequency in the RSTDT training set.</S>
			<S sid ="275" ssid = "64">In general, the errors are produced by two different causes acting together: (i) imbalanced distribution of the relations, and (ii) semantic similarity between the relations.</S>
			<S sid ="276" ssid = "65">The most frequent relation Elaboration tends to mislead others especially, the ones which are semantically similar (e.g., Explanation, Background) and less frequent (e.g., Summary, Evaluation).</S>
			<S sid ="277" ssid = "66">The relations which are semantically similar mislead each other (e.g., Temporal:Background, Cause:Explanation).</S>
			<S sid ="278" ssid = "67">These observations suggest two ways to improve our parser.</S>
			<S sid ="279" ssid = "68">We would like to employ a more robust method (e.g., ensemble methods with bagging) to deal with the imbalanced distribution of relations, along with taking advantage of a richer semantic knowledge (e.g., compositional semantics) to cope with the errors caused by semantic similarity between the rhetorical relations.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "7">
			<S sid ="280" ssid = "1">In this paper, we have presented a novel discourse parser that applies an optimal parsing algorithm to probabilities inferred from two CRF models: one for intrasentential parsing and the other for multisentential parsing.</S>
			<S sid ="281" ssid = "2">The two models exploit their own informative feature sets and the distributional variations of the relations in the two parsing conditions.</S>
			<S sid ="282" ssid = "3">We have also presented two novel approaches to combine them effectively.</S>
			<S sid ="283" ssid = "4">Empirical evaluations on two different genres demonstrate that our approach yields substantial improvement over existing methods in discourse parsing.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="284" ssid = "5">We are grateful to Frank Tompa and the anonymous reviewers for their comments, and the NSERC BIN and CGS-D for financial support.</S>
	</SECTION>
</PAPER>
