<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes the IMSSZEGED-CIS contribution to the SPMRL 2013 Shared Task.</S>
		<S sid ="2" ssid = "2">We participate in both the constituency and dependency tracks, and achieve state-of-the- art for all languages.</S>
		<S sid ="3" ssid = "3">For both tracks we make significant improvements through high quality preprocessing and (re)ranking on top of strong baselines.</S>
		<S sid ="4" ssid = "4">Our system came out first for both tracks.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">In this paper, we present our contribution to the 2013 Shared Task on Parsing Morphologically Rich Languages (MRLs).</S>
			<S sid ="6" ssid = "6">MRLs pose a number of interesting challenges to today’s standard parsing algorithms, for example a free word order and, due to their rich morphology, greater lexical variation that aggravates out-of-vocabulary problems considerably (Tsarfaty et al., 2010).</S>
			<S sid ="7" ssid = "7">Given the wide range of languages encompassed by the term MRL, there is, as of yet, no clear consensus on what approaches and features are generally important for parsing MRLs.</S>
			<S sid ="8" ssid = "8">However, developing tailored solutions for each language is time- consuming and requires a good understanding of the language in question.</S>
			<S sid ="9" ssid = "9">In our contribution to the SPMRL 2013 Shared Task (Seddah et al., 2013), we therefore chose an approach that we could apply to all languages in the Shared Task, but that would also allow us to fine-tune it for individual languages by varying certain components.</S>
			<S sid ="10" ssid = "10">∗Authors in alphabetical order.</S>
			<S sid ="11" ssid = "11">For the dependency track, we combined the n- best output of multiple parsers and subsequently ranked them to obtain the best parse.</S>
			<S sid ="12" ssid = "12">While this approach has been studied for constituency parsing (Zhang et al., 2009; Johnson and Ural, 2010; Wang and Zong, 2011), it is, to our knowledge, the first time this has been applied successfully within dependency parsing.</S>
			<S sid ="13" ssid = "13">We experimented with different kinds of features in the ranker and developed feature models for each language.</S>
			<S sid ="14" ssid = "14">Our system ranked first out of seven systems for all languages except French.</S>
			<S sid ="15" ssid = "15">For the constituency track, we experimented with an alternative way of handling unknown words and applied a products of Context Free Grammars with Latent Annotations (PCFG-LA) (Petrov et al., 2006), whose output was reranked to select the best analysis.</S>
			<S sid ="16" ssid = "16">The additional reranking step improved results for all languages.</S>
			<S sid ="17" ssid = "17">Our system beats various baselines provided by the organizers for all languages.</S>
			<S sid ="18" ssid = "18">Unfortunately, no one else participated in this track.</S>
			<S sid ="19" ssid = "19">For both settings, we made an effort to automatically annotate our data with the best possible pre- processing (POS, morphological information).</S>
			<S sid ="20" ssid = "20">We used a multi-layered CRF (Mu¨ ller et al., 2013) to annotate each data set, stacking with the information provided by the organizers when this was beneficial.</S>
			<S sid ="21" ssid = "21">The high quality of our preprocessing considerably improved the performance of our systems.</S>
			<S sid ="22" ssid = "22">The Shared Task involved a variety of settings as to whether gold or predicted part-of-speech tags and morphological information were available, as well as whether the full training set or a smaller (5k sen 135 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 135–145, Seattle, Washington, USA, 18 October 2013.</S>
			<S sid ="23" ssid = "23">Qc 2013 Association for Computational Linguistics Arabic Basque French German Hebrew Hungarian Korean Polish Swedish MarMoT 97.38/92.22 97.02/87.08 97.61/90.92 98.10/91.80 97.09/97.67 98.72/97.59 94.03/87.68 98.12/90.84 97.27/97.13 Stacked 98.23/89.05 98.56/92.63 97.83/97.62 Table 1: POS/morphological feature accuracies on the development sets.</S>
			<S sid ="24" ssid = "24">tences) training set was used for training.</S>
			<S sid ="25" ssid = "25">Throughout this paper we focus on the settings with predicted preprocessing information with gold segmentation and the full1 training sets.</S>
			<S sid ="26" ssid = "26">Unless stated otherwise, all given numbers are drawn from experiments in this setting.</S>
			<S sid ="27" ssid = "27">For all other settings, we refer the reader to the Shared Task overview paper (Seddah et al., 2013).</S>
			<S sid ="28" ssid = "28">The remainder of the paper is structured as follows: We present our preprocessing in Section 2 and afterwards describe both our systems for the constituency (Section 3) and for the dependency tracks (Section 4).</S>
			<S sid ="29" ssid = "29">Section 5 discusses the results on the Shared Task test sets.</S>
			<S sid ="30" ssid = "30">We conclude with Section 6.</S>
	</SECTION>
	<SECTION title="Preprocessing. " number = "2">
			<S sid ="31" ssid = "1">We first spent some time on preparing the data sets, in particular we automatically annotated the data with high-quality POS and morphological information.</S>
			<S sid ="32" ssid = "2">We consider this kind of preprocessing to be an essential part of a parsing system, since the quality of the automatic preprocessing strongly affects the performance of the parsers.</S>
			<S sid ="33" ssid = "3">Because our tools work on CoNLL09 format, we first converted the training data from the CoNLL06 format to CoNLL09.</S>
			<S sid ="34" ssid = "4">We thus had to decide whether to use coarse or fine part-of-speech (POS) tags.</S>
			<S sid ="35" ssid = "5">In a preliminary experiment we found that fine tags are the better option for all languages but Basque and Korean.</S>
			<S sid ="36" ssid = "6">For Korean the reason seems to be that the fine tag set is huge (&gt; 900) and that the same information is also provided in the feature column.</S>
			<S sid ="37" ssid = "7">We predict POS tags and morphological features jointly using the Conditional Random Field (CRF) tagger MarMoT2 (Mu¨ ller et al., 2013).</S>
			<S sid ="38" ssid = "8">MarMoT incrementally creates forward- backward lattices of increasing order to prune the sizable space of possible morphological analyses.</S>
			<S sid ="39" ssid = "9">We use MarMoT with the default parameters.</S>
			<S sid ="40" ssid = "10">1 Although, for Hebrew and Swedish only 5k sentences were available for training, and the two settings thus coincide.</S>
			<S sid ="41" ssid = "11">2 https://code.google.com/p/cistern/ Since morphological dictionaries can improve automatic POS tagging considerably, we also created such dictionaries for each language.</S>
			<S sid ="42" ssid = "12">For this, we analyzed the word forms provided in the data sets with language-specific morphological analyzers except for Hebrew and German where we just extracted the morphological information from the lattice files provided by the organizers.</S>
			<S sid ="43" ssid = "13">For the other languages we used the following tools: Arabic: AraMorph a reimplementation of Buckwalter (2002), Basque: Apertium (Forcada et al., 2011), French: an IMS internal tool,3 Hungarian: Magyarlanc (Zsibrita et al., 2013), Korean: HanNanum (Park et al., 2010), Polish: Morfeusz (Wolin´ ski, 2006), and Swedish: Granska (Domeij et al., 2000).</S>
			<S sid ="44" ssid = "14">The created dictionaries were shared with the other Shared Task participants.</S>
			<S sid ="45" ssid = "15">We used these dictionaries as additional features for MarMoT.</S>
			<S sid ="46" ssid = "16">For some languages we also integrated the predicted tags provided by the organizers into the feature model.</S>
			<S sid ="47" ssid = "17">These stacked models gave improvements for Swedish, Polish and Basque (cf.</S>
			<S sid ="48" ssid = "18">Table 1 for accuracies).</S>
			<S sid ="49" ssid = "19">For the full setting the training data was annotated using 5-fold jackknifing.</S>
			<S sid ="50" ssid = "20">In the 5k setting, we additionally added all sentences not present in the parser training data to the training data sets of the tagger.</S>
			<S sid ="51" ssid = "21">This is similar to the predicted 5k files provided by the organizers, where more training data than the 5k was also used for prediction.</S>
			<S sid ="52" ssid = "22">Table 3 presents a comparison between our graph- based baseline parser using the preprocessing explained in this section (denoted mate) and the preprocessing provided by the organizers (denoted mate’).</S>
			<S sid ="53" ssid = "23">Our preprocessing yields improvements for all languages but Swedish.</S>
			<S sid ="54" ssid = "24">The worse performance for Swedish is due to the fact that the predictions provided by the organizers were produced by models that were trained on a much larger data 3 The French morphology was written by Zhenxia Zhou, Max Kisselew and Helmut Schmid.</S>
			<S sid ="55" ssid = "25">It is an extension of Zhou (2007) and implemented in SFST (Schmid, 2005).</S>
			<S sid ="56" ssid = "26">Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Ber kele y 78.</S>
			<S sid ="57" ssid = "27">24 69.</S>
			<S sid ="58" ssid = "28">17 79.</S>
			<S sid ="59" ssid = "29">74 81.</S>
			<S sid ="60" ssid = "30">74 87.</S>
			<S sid ="61" ssid = "31">83 83 .9 0 70 .9 7 84.</S>
			<S sid ="62" ssid = "32">11 74.</S>
			<S sid ="63" ssid = "33">50 Repl aced 78.</S>
			<S sid ="64" ssid = "34">70 84.</S>
			<S sid ="65" ssid = "35">33 79.</S>
			<S sid ="66" ssid = "36">68 82.</S>
			<S sid ="67" ssid = "37">74 89.</S>
			<S sid ="68" ssid = "38">55 89 .0 8 82 .8 4 87.</S>
			<S sid ="69" ssid = "39">12 75.</S>
			<S sid ="70" ssid = "40">52 Pro duc t 80.</S>
			<S sid ="71" ssid = "41">30 86.</S>
			<S sid ="72" ssid = "42">21 81.</S>
			<S sid ="73" ssid = "43">42 84.</S>
			<S sid ="74" ssid = "44">56 90.</S>
			<S sid ="75" ssid = "45">49 89 .8 0 84 .1 5 88.</S>
			<S sid ="76" ssid = "46">32 79.</S>
			<S sid ="77" ssid = "47">25 Rer ank ed 81.</S>
			<S sid ="78" ssid = "48">24 87.</S>
			<S sid ="79" ssid = "49">35 82.</S>
			<S sid ="80" ssid = "50">49 85.</S>
			<S sid ="81" ssid = "51">01 90.</S>
			<S sid ="82" ssid = "52">49 91 .0 7 84 .6 3 88.</S>
			<S sid ="83" ssid = "53">40 79.</S>
			<S sid ="84" ssid = "54">53 Table 2: PARSEVAL scores on the development sets.</S>
			<S sid ="85" ssid = "55">set.</S>
			<S sid ="86" ssid = "56">The comparison with other parsers demonstrates that for some languages (e.g., Hebrew or Korean) the improvements due to better preprocessing can be greater than the improvements due to a better parser.</S>
			<S sid ="87" ssid = "57">For instance, for Hebrew the parser trained on the provided preprocessing is more than three points (LAS) behind the three parsers trained on our own preprocessing.</S>
			<S sid ="88" ssid = "58">However, the difference between these three parsers is less than a point.</S>
	</SECTION>
	<SECTION title="Constituency Parsing. " number = "3">
			<S sid ="89" ssid = "1">The phrase structure parsing pipeline is based on products of Context Free Grammars with Latent Annotations (PCFG-LA) (Petrov et al., 2006) and dis- criminative reranking.</S>
			<S sid ="90" ssid = "2">We further replace rare words by their predicted morphological analysis.</S>
			<S sid ="91" ssid = "3">We preprocess the treebank trees by removing the morphological annotation of the POS tags and the function labels of all non-terminals.</S>
			<S sid ="92" ssid = "4">We also reduce the 177 compositional Korean POS tags to their first atomic tag, which results in a POS tag set of 9 tags.</S>
			<S sid ="93" ssid = "5">PCFGLAs are incrementally built by splitting non-terminals, refining parameters using EM- training and reversing splits that only cause small increases in likelihood.Running the Berkeley Parser4 – the reference im plementation of PCFGLAs – on the data sets results in the PARSEVAL scores given in Table 2 (Berkeley).</S>
			<S sid ="94" ssid = "6">The Berkeley parser only implements a simple signature-based unknown word model that seems to be ineffective for some of the languages, especially Basque and Korean.</S>
			<S sid ="95" ssid = "7">We thus replace rare words (frequency &lt; 20) by the predicted morphological tags of Section 2 (or the true morphological tag for the gold setup).</S>
			<S sid ="96" ssid = "8">The intuition is that our discriminative tagger has a more sophisticated unknown word treatment than the Berkeley parser, taking for example prefixes, suffixes and 4 http://code.google.com/p/ berkeleyparser/ the immediate lexical context into account.</S>
			<S sid ="97" ssid = "9">Furthermore, the morphological tag contains most of the necessary syntactic information.</S>
			<S sid ="98" ssid = "10">An exception, for instance, might be the semantic information needed to disambiguate prepositional attachment.</S>
			<S sid ="99" ssid = "11">We think that replacing rare words by tags has an advantage over constraining the pre-terminal layer of the parser, because the parser can still decide to assign a different tag, for example in cases were the tag- ger produces errors due to long-distance dependencies.</S>
			<S sid ="100" ssid = "12">The used frequency threshold of 20 results in token replacement rates of 18% (French) to 57% (Korean and Polish), which correspond to 209 (for Polish) to 3221 (for Arabic) word types that are not replaced.</S>
			<S sid ="101" ssid = "13">The PARSEVAL scores for the described method are again given in Table 2 (Replaced).</S>
			<S sid ="102" ssid = "14">The method yields improvements for all languages except for French where we observe a drop of 0.06.</S>
			<S sid ="103" ssid = "15">The improvements range from 0.46 for Arabic to 1.02 for Swedish, 3.1 for Polish and more than 10 for Basque and Korean.</S>
			<S sid ="104" ssid = "16">To further improve results, we employ the product-of-grammars procedure (Petrov, 2010), where different grammars are trained on the same data set but with different initialization setups.</S>
			<S sid ="105" ssid = "17">We trained 8 grammars and used tree-level inference.</S>
			<S sid ="106" ssid = "18">In Table 2 (Product) we can see that this leads to improvements from 0.72 for Hungarian to 3.73 for Swedish.</S>
			<S sid ="107" ssid = "19">On the 50-best output of the product parser, we also carry out discriminative reranking.</S>
			<S sid ="108" ssid = "20">The reranker is trained for the maximum entropy objective function of Charniak and Johnson (2005) and use the standard feature set – without language- specific feature engineering – from Charniak and Johnson (2005) and Collins (2000).</S>
			<S sid ="109" ssid = "21">We use a slightly modified version of the Mallet toolkit (McCallum, 2002) for reranking.</S>
			<S sid ="110" ssid = "22">Improvements range from negligible differences (&lt; .1) for Hebrew and Polish to substantial differences (&gt; 1.)</S>
			<S sid ="111" ssid = "23">for Basque, French, and Hungarian.</S>
			<S sid ="112" ssid = "24">Parsing Ranking mate parser IN best-first parser scores merged list of 50100 best trees/sentence scores merged list scored by all parsers ranker features OUT turboparser scores ptb trees Figure 1: Architecture of the dependency ranking system.</S>
			<S sid ="113" ssid = "25">For our final submission, we used the reranker output for all languages except French, Hebrew, Polish, and Swedish.</S>
			<S sid ="114" ssid = "26">This decision was based on an earlier version of the evaluation setting provided by the organizers.</S>
			<S sid ="115" ssid = "27">In this setup, reranking did not help or was even harmful for these four languages.</S>
			<S sid ="116" ssid = "28">The figures in Table 2 use the latest evaluation script and are thus consistent with the test set results presented in Section 5.</S>
			<S sid ="117" ssid = "29">After the submission deadline the Shared Task organizers made us aware that we had surprisingly low exact match scores for Polish (e.g., 1.22 for the reranked setup).</S>
			<S sid ="118" ssid = "30">The reason seems to be that the Berkeley parser cannot produce unary chains of length &gt; 2.</S>
			<S sid ="119" ssid = "31">The gold development set contains 1783 such chains while the prediction of the reranked system contains none.</S>
			<S sid ="120" ssid = "32">A particularly frequent unary chain with 908 occurences in the gold data is ff →fwe → formaczas.</S>
			<S sid ="121" ssid = "33">As this chain cannot be pro duced the parser leaves out the fwe phrase.</S>
			<S sid ="122" ssid = "34">Inserting new fwe nodes between ff and formacszas nodes raises the PARSEVAL scores of the reranked model from 88.40 to 90.64 and the exact match scores to 11.34.</S>
			<S sid ="123" ssid = "35">This suggests that the Polish results could be improved substantially if unary chains were properly dealt with, for example by collapsing unary chains.5</S>
	</SECTION>
	<SECTION title="Dependency Parsing. " number = "4">
			<S sid ="124" ssid = "1">The core idea of our dependency parsing system is the combination of the n-best output of several parsers followed by a ranking step on the combined list.</S>
			<S sid ="125" ssid = "2">Specifically, we first run two parsers that each output their 50-best analyses for each sentence.</S>
			<S sid ="126" ssid = "3">These 50-best analyses are merged together into one single n-best list of between 50 and 100 analyses (depending on the overlap between the n-best lists of the two parsers).</S>
			<S sid ="127" ssid = "4">We then use the two parsers plus an additional one to score each tree in the n- best lists according to their parsing model, thus providing us with three different scores for each tree in the n-best lists.</S>
			<S sid ="128" ssid = "5">The n-best lists are then given to a ranker, which ranks the list using the three scores and a small set of additional features in order to find the best overall analysis.</S>
			<S sid ="129" ssid = "6">Figure 1 shows a schematic of the process.</S>
			<S sid ="130" ssid = "7">As a preprocessing step, we reduced the dependency label set for the Hungarian training data.</S>
			<S sid ="131" ssid = "8">The Hungarian dependency data set encodes ellipses through composite edge labels which leads to a proliferation of edge labels (more than 400).</S>
			<S sid ="132" ssid = "9">Since many of these labels are extremely rare and thus hard to learn for the parsers, we reduced the set of edge labels during the conversion.</S>
			<S sid ="133" ssid = "10">Specifically, we retained the 50 most frequent labels, while reducing the composite labels to their base label.</S>
			<S sid ="134" ssid = "11">For producing the initial n-best lists, we use the mate parser6 (Bohnet, 2010) and a variant of the EasyFirst parser (Goldberg and Elhadad, 2010), which we here call best-first parser.</S>
			<S sid ="135" ssid = "12">The mate parser is a state-of-the-art graph-based dependency parser that uses second-order features.</S>
			<S sid ="136" ssid = "13">length limit.</S>
			<S sid ="137" ssid = "14">6 https://co de.googl e.com/p/mate tools The parser works in two steps.</S>
			<S sid ="138" ssid = "15">First, it uses dynamic programming to find the optimal projective tree using the Carreras (2007) decoder.</S>
			<S sid ="139" ssid = "16">It then applies the non-projective approximation algorithm proposed by McDonald and Pereira (2006) in order to produce non-projective parse trees.</S>
			<S sid ="140" ssid = "17">The non- projective approximation algorithm is a greedy hill climbing algorithm that starts from the optimal pro- jective parse and iteratively tries to reattach all tokens, one at a time, everywhere in the sentence as long as the tree property holds.</S>
			<S sid ="141" ssid = "18">It halts when the increase in the score of the tree according to the parsing model is below a certain threshold.</S>
			<S sid ="142" ssid = "19">n-best lists are obtained by applying the non- projective approximation algorithm in a non-greedy manner, exploring multiple possibilities.</S>
			<S sid ="143" ssid = "20">All trees are collected in a list, and when no new trees are found, or newer trees have a significantly lower score than the currently best one, search halts.</S>
			<S sid ="144" ssid = "21">The n best trees are then retrieved from the list.</S>
			<S sid ="145" ssid = "22">It should be noted that, in the standard case, the non- projective approximation algorithm may find a local optimum, and that there may be other trees that have a higher score which were not explored.</S>
			<S sid ="146" ssid = "23">Thus the best parse in the greedy case may not necessarily be the one with the highest score in the n-best list.</S>
			<S sid ="147" ssid = "24">Since the parser is trained with the greedy version of the non-projective approximation algorithm, the greedily chosen output parse tree is of special interest.</S>
			<S sid ="148" ssid = "25">We thus flag this tree as the baseline mate parse, in order to use that for features in the ranker.</S>
			<S sid ="149" ssid = "26">The baseline mate parse is also our overall baseline in the dependency track.</S>
			<S sid ="150" ssid = "27">The best-first parser deviates from the EasyFirst parser in several small respects: The EasyFirst decoder creates dependency links between the roots of adjacent substructures, which gives an O(n log n) complexity, but restricts the output to projective trees.</S>
			<S sid ="151" ssid = "28">The best-first parser is allowed to choose as head any node of an adjacent substructure instead of only the root, which increases complexity to O(n2), but accounts for a big part of possible non-projective structures.</S>
			<S sid ="152" ssid = "29">We additionally implemented a swap- operation (Nivre, 2009; Tratz and Hovy, 2011) to account for the more complex structures.</S>
			<S sid ="153" ssid = "30">The best- first parser relies on a beam-search strategy7 to pur 7 Due to the nature of the decoder, the parser can produce.</S>
			<S sid ="154" ssid = "31">sue multiple derivations, which we also use to produce the n-best output.</S>
			<S sid ="155" ssid = "32">In the scoring step, we additionally apply the turboparser8 (Martins et al., 2010), which is based on linear programming relaxations.9 We changed all three parsers such that they would return a score for a given tree.</S>
			<S sid ="156" ssid = "33">We use this to extract scores from each parser for all trees in the n-best lists.</S>
			<S sid ="157" ssid = "34">It is important to have a score from every parser for every tree, as previously observed by Zhang et al.</S>
			<S sid ="158" ssid = "35">(2009) in the context of constituency reranking.</S>
			<S sid ="159" ssid = "36">4.1 Ranking.</S>
			<S sid ="160" ssid = "37">Table 3 shows the performance of the individual parsers measured on the development sets.</S>
			<S sid ="161" ssid = "38">It also displays the oracle scores over the different n-best lists, i.e., the maximal possible score over an n-best list if the best tree is always selected.</S>
			<S sid ="162" ssid = "39">The mate parser generally performs best followed by turboparser, while the best-first parser comes last.</S>
			<S sid ="163" ssid = "40">But we can see from the oracle scores that the best- first parser often shows comparable or even higher oracle scores than mate, and that the combination of the n-best lists always adds substantial improvements to the oracle scores.</S>
			<S sid ="164" ssid = "41">These findings show that the mate and best-first parsers are providing different sets of n-best lists.</S>
			<S sid ="165" ssid = "42">Moreover, all three parsers rely on different parsing algorithms and feature sets.</S>
			<S sid ="166" ssid = "43">For these reasons, we hypothesized that the parsers contribute different views on the parse trees and that their combination would result in better overall performance.</S>
			<S sid ="167" ssid = "44">In order to leverage the diversity between the parsers we experimented with ranking10 on the n-best lists.</S>
			<S sid ="168" ssid = "45">We used the same ranking model introduced in Section 3 here as well.</S>
			<S sid ="169" ssid = "46">The model is trained to select the best parse according to the labeled attachment score (LAS).</S>
			<S sid ="170" ssid = "47">The training data for the ranker was created by 5-fold jackknifing on the training sets.</S>
			<S sid ="171" ssid = "48">The feature sets for the ranker for spurious ambiguities in the beam.</S>
			<S sid ="172" ssid = "49">If this occurs, only the one with the higher score is kept.</S>
			<S sid ="173" ssid = "50">8 http://www.ark.cs.cmu.edu/TurboParser/ 9 Ideally we would also extract n-best lists from the turboparser, however time prevented us from making the necessary modifications.</S>
			<S sid ="174" ssid = "51">10 We refrain from calling it reranking in this setting, since we are using merged n-best lists and the initial ranking is not entirely clear to begin with.</S>
			<S sid ="175" ssid = "52">Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Baseline results for individual parsers mate’ 88.50/83.50 88.18/84.49 92.71/90.85 83.63/75.89 87.07/82.84 86.06/82.39 91.17/85.81 83.65/77.16 mate 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05 bf 87.61/85.32 84.07/75.90 87.45/83.92 92.90/91.10 86.10/79.57 83.85/75.94 86.54/83.97 90.10/83.75 82.27/75.36 turbo 87.82/85.35 88.88/83.84 88.24/84.57 93.59/91.54 85.74/78.95 86.86/82.80 88.35/86.23 90.97/85.55 83.24/76.15 Oracle scores for n-best lists mate 90.85/88.74 93.39/89.85 90.99/87.81 97.14/95.84 89.05/83.03 91.41/88.19 94.86/92.96 95.19/91.67 87.19/81.66 bf 91.47/89.46 91.68/86.46 91.38/88.68 97.40/96.60 91.04/85.67 87.64/81.79 94.90/92.94 96.25/93.74 87.60/82.46 merged 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96 Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets.</S>
			<S sid ="176" ssid = "53">mate’ uses the preprocessing provided by the organizers, the other parsers use the preprocessing described in Section 2.</S>
			<S sid ="177" ssid = "54">each language were optimized manually via cross- validation on the training sets.</S>
			<S sid ="178" ssid = "55">The features used for each language, as well as a default (baseline) feature set, are shown in Table 4.</S>
			<S sid ="179" ssid = "56">We now outline the features we used in the ranker: Score from the base parsers – denoted B, M, T, for the best-first, mate, and turbo parsers, respectively.</S>
			<S sid ="180" ssid = "57">We also have indicator features whether a certain parse was the best according to a given parser, denoted GB, GM, GT, respectively.</S>
			<S sid ="181" ssid = "58">Since the mate parser does not necessarily assign the highest score to the baseline mate parse, the GM feature is a ternary feature which indicates whether a parse is the same as the baseline mate parse, or better, or worse.</S>
			<S sid ="182" ssid = "59">We also experimented with transformations and combinations of the scores from the parsers.</S>
			<S sid ="183" ssid = "60">Specifically, BMProd denotes the product of B and M; BMeProd denotes the sum of B and M in e-space, i.e., eB+M ; reBMT, reBT, reMT denote the normalized product of the corresponding scores, where scores are normalized in a softmax fashion such that all features take on values in the interval (0, 1).</S>
			<S sid ="184" ssid = "61">Projectivity features (Hall et al., 2007) – the number of non-projective edges in a tree, denoted np.</S>
			<S sid ="185" ssid = "62">Whether a tree is ill-nested, denoted I. Since ill- nested trees are extremely rare in the treebanks, this helps the ranker filter out unlikely candidates from the n-best lists.</S>
			<S sid ="186" ssid = "63">For a definition and further discussion of ill-nestedness, we refer to (Havelka, 2007).</S>
			<S sid ="187" ssid = "64">Constituent features – from the constituent track we also have constituent trees of all sentences which can be used for feature extraction.</S>
			<S sid ="188" ssid = "65">Specifically, for every head-dependent pair, we extract the path in the constituent tree between the nodes, denoted ptbp.</S>
			<S sid ="189" ssid = "66">Case agreement – on head-dependent pairs that both have a case value assigned among their morphological features, we mark whether it is the same case or not, denoted case.</S>
			<S sid ="190" ssid = "67">Function label uniqueness – on each training set we extracted a list of function labels that generally occur at most once as the dependent of a node, e.g., subjects or objects.</S>
			<S sid ="191" ssid = "68">Features are then extracted from all nodes that have one or more dependents of each label aimed at capturing mistakes such as double subjects on a verb.</S>
			<S sid ="192" ssid = "69">This template is denoted FL.</S>
			<S sid ="193" ssid = "70">In addition to the features mentioned above, we experimented with a variety of feature templates, including features drawn from previous work on dependency reranking (Hall, 2007), e.g., lexical and POS-based features over edges, “subcategorization” frames (i.e., the concatenation of POS-tags that are headed by a certain node in the tree), etc, although these features did not seem to help.</S>
			<S sid ="194" ssid = "71">For German we created feature templates based on the constraints used in the constraint-based parser by Seeker and Kuhn (2013).</S>
			<S sid ="195" ssid = "72">This includes, e.g., violations in case or number agreement between heads and dependents, as well as more complex features that consider labels on entire verb complexes.</S>
			<S sid ="196" ssid = "73">None of these features yielded any clear improvements though.</S>
			<S sid ="197" ssid = "74">We also experimented with features that target some specific constructions (and specifics of annotation schemes) which the parsers typically cannot fully see, such as coordination, however, also here we saw no clear improvements.</S>
			<S sid ="198" ssid = "75">4.2 Effects of Ranking.</S>
			<S sid ="199" ssid = "76">In Table 5, we show the improvements from using the ranker, both with the baseline and optimized features sets for the ranker.</S>
			<S sid ="200" ssid = "77">For the sake of comparison, Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Baseline 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05 Ranked-dflt 88.54/86.32 89.99/85.43 88.85/85.39 94.06/92.36 87.28/80.44 88.16/84.54 88.71/86.65 92.26/87.12 84.51/77.83 Ranked 88.93/86.74 89.95/85.61 89.37/85.96 94.20/92.68 87.63/81.02 88.38/84.77 89.20/87.12 93.02/87.69 85.04/78.57 Oracle 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96 Table 5: Performance (UAS/LAS) of the reranker on the development sets.</S>
			<S sid ="201" ssid = "78">Baseline denotes our baseline.</S>
			<S sid ="202" ssid = "79">Ranked-dflt and Ranked denote the default and optimized ranker feature sets, respectively.</S>
			<S sid ="203" ssid = "80">Oracle denotes the oracle scores.</S>
			<S sid ="204" ssid = "81">default B, M, T, GB, GM, GT, I Arabic B, M, T, GB, GM, I, ptbp, reBMT Basque B, M, T, GB, GM, GT, I, ptbp, I, reMT, case French B, M, T, GB, GM, GT, I, ptbp German B, M, T, GM, I, BMProd, FL Hebrew B, M, T, GB, GM, GT, I, ptbp, FL, BMeProd Hungarian B, M, T, GB, GM, GT, I, ptbp, reBM, FL Korean B, M, T, GB, GM, GT, I, ptbp, reMT, FL Polish B, M, T, GB, GM, GT, I, ptbp, np Swedish B, M, T, GB, GM, GT, I, ptbp, reBM, FL Table 4: Feature sets for the dependency ranker for each language.</S>
			<S sid ="205" ssid = "82">default denotes the default ranker feature set.</S>
			<S sid ="206" ssid = "83">the baseline mate parses as well as the oracle parses on the merged n-best lists are repeated from Table 3.</S>
			<S sid ="207" ssid = "84">We see that ranking clearly helps, both with a tailored feature set, as well as the default feature set.</S>
			<S sid ="208" ssid = "85">The improvement in LAS between the baseline and the tailored ranking feature sets ranges from 1.1% (French) to 1.6% (Hebrew) absolute, with the exception of Hungarian, where improvements on the dev set are more modest (contrary to the test set results, cf.</S>
			<S sid ="209" ssid = "86">Section 5).</S>
			<S sid ="210" ssid = "87">Even with the default feature set, the improvements range from 0.5% (French) to 1.1% (Hebrew) absolute, again setting Hungarian aside.</S>
			<S sid ="211" ssid = "88">We believe that this is an interesting result considering the simplicity of the default feature set.</S>
	</SECTION>
	<SECTION title="Test Set Results. " number = "5">
			<S sid ="212" ssid = "1">In this section we outline our final results on the test sets.</S>
			<S sid ="213" ssid = "2">As previously, we focus on the setting with predicted tags in gold segmentation and the largest training set.</S>
			<S sid ="214" ssid = "3">We also present results on Arabic and Hebrew for the predicted segmentation setting.</S>
			<S sid ="215" ssid = "4">For the gold preprocessing and all 5k settings, we refer the reader to the Shared Task overview paper (Seddah et al., 2013).11In Table 7, we present our results in the con 11 Or the results page online: http://www.spmrl.org/.</S>
			<S sid ="216" ssid = "5">spmrl2013sharedtask-results.html stituency track.</S>
			<S sid ="217" ssid = "6">Since we were the only participating team in the constituency track, we compare ourselves with the best baseline12 provided by the organizers.</S>
			<S sid ="218" ssid = "7">Our system outperforms the baseline for all languages in terms of PARSEVAL F1.</S>
			<S sid ="219" ssid = "8">Following the trend on the development sets, reranking is consistently helping across languages.13 Despite the lack of other submissions in the shared task, we believe our numbers are generally strong and hope that they can serve as a reference for future work on constituency parsing on these data sets.</S>
			<S sid ="220" ssid = "9">Table 8 displays our results in the dependency track.</S>
			<S sid ="221" ssid = "10">We submitted two runs: a baseline, which is the baseline mate parse, and the reranked trees.</S>
			<S sid ="222" ssid = "11">The table also compares our results to the best performing other participant in the shared task (denoted Other) as well as the MaltParser (Nivre et al., 2007) baseline provided by the shared task organizers (denoted ST Baseline).</S>
			<S sid ="223" ssid = "12">We obtain the highest scores for all languages, with the exception of French.</S>
			<S sid ="224" ssid = "13">It is also clear that we make considerable gains over our baseline, confirming our results on the development sets reported in Section 4.</S>
			<S sid ="225" ssid = "14">It is also noteworthy that our baseline (i.e., the mate parser with our own pre- processing) outperforms the best other system for 5 languages.</S>
			<S sid ="226" ssid = "15">Arabic Hebrew Other 90.75/8.48 88.33/12.20 Dep . Ba seli ne 91 .1 3/ 9.</S>
			<S sid ="227" ssid = "16">10 89.</S>
			<S sid ="228" ssid = "17">27/ 15.</S>
			<S sid ="229" ssid = "18">01 Dep . Ra nke d 91 .7 4/ 9.</S>
			<S sid ="230" ssid = "19">83 89.</S>
			<S sid ="231" ssid = "20">47/ 16.</S>
			<S sid ="232" ssid = "21">97 Co nsti tue ncy 92 .0 6/ 9.</S>
			<S sid ="233" ssid = "22">49 89.</S>
			<S sid ="234" ssid = "23">30/ 13.</S>
			<S sid ="235" ssid = "24">60 Table 6: Unlabeled TedEval scores (accuracy/exact match) for the test sets in the predicted segmentation set ting.</S>
			<S sid ="236" ssid = "25">Only sentences of length ≤ 70 are evaluated.</S>
			<S sid ="237" ssid = "26">12 It should be noted that the Shared Task organizers computed 2 different baselines on the test sets.</S>
			<S sid ="238" ssid = "27">The best baseline results for each language thus come from different parsers.</S>
			<S sid ="239" ssid = "28">13 We remind the reader that our submission decisions are not.</S>
			<S sid ="240" ssid = "29">based on figures in Table 2, cf.</S>
			<S sid ="241" ssid = "30">Section 3.</S>
			<S sid ="242" ssid = "31">Arabic Basque French German Hebrew Hungarian Korean Polish Swedish ST Bas eline 79.</S>
			<S sid ="243" ssid = "32">19 74.</S>
			<S sid ="244" ssid = "33">74 80.</S>
			<S sid ="245" ssid = "34">38 78.</S>
			<S sid ="246" ssid = "35">30 86 .9 6 85 .2 2 78 .5 6 86.</S>
			<S sid ="247" ssid = "36">75 80.</S>
			<S sid ="248" ssid = "37">64 Pro duc t 80.</S>
			<S sid ="249" ssid = "38">81 87.</S>
			<S sid ="250" ssid = "39">18 81.</S>
			<S sid ="251" ssid = "40">83 80.</S>
			<S sid ="252" ssid = "41">70 89 .4 6 90 .5 8 83 .4 9 87.</S>
			<S sid ="253" ssid = "42">55 83.</S>
			<S sid ="254" ssid = "43">99 Rer ank ed 81.</S>
			<S sid ="255" ssid = "44">32 87.</S>
			<S sid ="256" ssid = "45">86 82.</S>
			<S sid ="257" ssid = "46">86 81.</S>
			<S sid ="258" ssid = "47">27 89.</S>
			<S sid ="259" ssid = "48">49 91 .8 5 84 .2 7 87.</S>
			<S sid ="260" ssid = "49">76 84.</S>
			<S sid ="261" ssid = "50">88 Table 7: Final PARSEVAL F1 scores for constituents on the test set for the predicted setting.</S>
			<S sid ="262" ssid = "51">ST Baseline denotes the best baseline (out of 2) provided by the Shared Task organizers.</S>
			<S sid ="263" ssid = "52">Our submission is underlined.</S>
			<S sid ="264" ssid = "53">Arabic Basque French German Hebrew Hungarian Korean Polish Swedish ST Baseline 83.18/80.36 79.77/70.11 82.49/77.98 81.51/77.81 76.49/69.97 80.72/70.15 85.72/82.06 82.19/75.63 80.29/73.21 Oth er 85 .7 8/ 83 .2 0 89 .1 9/ 84 .2 5 89 .1 9/ 85 .8 6 90 .8 0/ 88 .6 6 81 .0 5/ 73 .6 3 88 .9 3/ 84 .9 7 85 .8 4/ 82 .6 5 88 .1 2/ 82 .5 6 87 .2 8/ 80 .8 8 Bas elin e 86 .9 6/ 84 .8 1 89 .3 2/ 84 .2 5 87 .8 7/ 84 .3 7 90 .5 4/ 88 .3 7 85 .8 8/ 79 .6 7 89 .0 9/ 85 .3 1 87 .4 1/ 85 .5 1 90 .3 0/ 85 .5 1 86 .8 5/ 80 .6 7 Ran ked 88 .3 2/ 86 .2 1 89 .8 8/ 85 .1 4 88 .6 8/ 85 .2 4 91 .6 4/ 89 .6 5 86 .7 0/ 80 .8 9 89 .8 1/ 86 .1 3 88 .4 7/ 86 .6 2 91 .7 5/ 87 .0 7 88 .0 6/ 82 .1 3 Table 8: Final UAS/LAS scores for dependencies on the test sets for the predicted setting.</S>
			<S sid ="265" ssid = "54">Other denotes the highest scoring other participant in the Shared Task.</S>
			<S sid ="266" ssid = "55">ST Baseline denotes the MaltParser baseline provided by the Shared Task organizers.</S>
			<S sid ="267" ssid = "56">Table 6 shows the unlabeled TedEval (Tsarfaty et al., 2012) scores (accuracy/exact match) on the test sets for the predicted segmentation setting for Arabic and Hebrew.</S>
			<S sid ="268" ssid = "57">Note that these figures only include sentences of length less than or equal to 70.</S>
			<S sid ="269" ssid = "58">Since TedEval enables cross-framework comparison, we compare our submissions from the dependency track to our submission from the constituency track.</S>
			<S sid ="270" ssid = "59">In these runs we used the same systems that were used for the gold segmentation with predicted tags track.</S>
			<S sid ="271" ssid = "60">The predicted segmentation was provided by the Shared Task organizers.</S>
			<S sid ="272" ssid = "61">We also compare our results to the best other system from the Shared Task (denoted Other).</S>
			<S sid ="273" ssid = "62">Also here we obtain the highest results for both languages.</S>
			<S sid ="274" ssid = "63">However, it is unclear what syntactic paradigm (dependencies or constituents) is better suited for the task.</S>
			<S sid ="275" ssid = "64">All in all it is difficult to assess whether the differences between the best and second best systems for each language are meaningful.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="276" ssid = "1">We have presented our contribution to the 2013 SPMRL Shared Task.</S>
			<S sid ="277" ssid = "2">We participated in both the constituency and dependency tracks.</S>
			<S sid ="278" ssid = "3">In both tracks we make use of a state-of-the-art tagger for POS and morphological features.</S>
			<S sid ="279" ssid = "4">In the constituency track, we use the tagger to handle unknown words and employ a product-of-grammars-based PCFG-LA parser and parse tree reranking.</S>
			<S sid ="280" ssid = "5">In the dependency track, we combine multiple parsers output as input for a ranker.</S>
			<S sid ="281" ssid = "6">Since there were no other participants in the constituency track, it is difficult to draw any conclusions from our results.</S>
			<S sid ="282" ssid = "7">We do however show that the application of product grammars, our handling of rare words, and a subsequent reranking step outperforms a baseline PCFG-LA parser.</S>
			<S sid ="283" ssid = "8">In the dependency track we obtain the best results for all languages except French among 7 participants.</S>
			<S sid ="284" ssid = "9">Our reranking approach clearly outperforms a baseline graph-based parser.</S>
			<S sid ="285" ssid = "10">This is the first time multiple parsers have been used in a dependency reranking setup.</S>
			<S sid ="286" ssid = "11">Aside from minor decisions made on the basis of each language, our approach is language agnostic and does not target morphology in any particular way as part of the parsing process.</S>
			<S sid ="287" ssid = "12">We show that with a strong baseline and with no language specific treatment it is possible to achieve state-of- the-art results across all languages.</S>
			<S sid ="288" ssid = "13">Our architecture for the dependency parsing track enables the use of language-specific features in the ranker, although we only had minor success with features that target morphology.</S>
			<S sid ="289" ssid = "14">However, it may be the case that approaches from previous work on parsing MRLs, or the approaches taken by other teams in the Shared Task, can be successfully combined with ours and improve parsing accuracy even more.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="290" ssid = "15">Richa´rd Farkas is funded by the European Union and the European Social Fund through the project FuturICT.hu (grant no.: TA´ MOP-4.2.2.C-11/1/KONV 20120013).</S>
			<S sid ="291" ssid = "16">Thomas Mu¨ ller is supported by a Google Europe Fellowship in Natural Language Processing.</S>
			<S sid ="292" ssid = "17">The remaining authors are funded by the Deutsche Forschungsgemeinschaft (DFG) via the SFB 732, projects D2 and D8 (PI: Jonas Kuhn).</S>
			<S sid ="293" ssid = "18">We also express our gratitude to the treebank providers for each language: Arabic (Maamouri et al., 2004; Habash and Roth, 2009; Habash et al., 2009; Green and Manning, 2010), Basque (Aduriz Key-Sun Choi, Young S Han, Young G Han, and Oh W Kwon.</S>
			<S sid ="294" ssid = "19">1994.</S>
			<S sid ="295" ssid = "20">Kaist tree bank project for korean: Present and future development.</S>
			<S sid ="296" ssid = "21">In Proceedings of the International Workshop on Sharable Natural Language Resources, pages 7–14.</S>
			<S sid ="297" ssid = "22">Citeseer.</S>
			<S sid ="298" ssid = "23">Jinho D. Choi.</S>
			<S sid ="299" ssid = "24">2013.</S>
			<S sid ="300" ssid = "25">Preparing korean data for the shared task on parsing morphologically rich languages.</S>
			<S sid ="301" ssid = "26">ArXiv e-prints.</S>
			<S sid ="302" ssid = "27">Michael Collins.</S>
			<S sid ="303" ssid = "28">2000.</S>
			<S sid ="304" ssid = "29">Discriminative Reranking forNatural Language Parsing.</S>
			<S sid ="305" ssid = "30">In Proceedings of the Sev et al., 2003), French (Abeille´et al., 2003), He enteenth International Conference on Machine Learn brew (Sima’an et al., 2001; Tsarfaty, 2010; Goldberg, 2011; Tsarfaty, 2013), German (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian (Csendes et al., 2005; Vincze et al., 2010), Korean (Choi et al., 1994; Choi, 2013), Polish (S´ widzin´ ski and Wolin´ ski, 2010), and Swedish (Nivre et al., 2006).</S>
	</SECTION>
</PAPER>
