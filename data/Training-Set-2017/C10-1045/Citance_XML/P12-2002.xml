<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance.</S>
		<S sid ="2" ssid = "2">The protocol uses distance-based metrics defined for the space of trees over lattices.</S>
		<S sid ="3" ssid = "3">Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags).</S>
		<S sid ="4" ssid = "4">Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">A parser takes a sentence in natural language as input and returns a syntactic parse tree representing the sentence’s human-perceived interpretation.</S>
			<S sid ="6" ssid = "6">Current state-of-the-art parsers assume that the space- delimited words in the input are the basic units of syntactic analysis.</S>
			<S sid ="7" ssid = "7">Standard evaluation procedures and metrics (Black et al., 1991; Buchholz and Marsi, 2006) accordingly assume that the yield of the parse tree is known in advance.</S>
			<S sid ="8" ssid = "8">This assumption breaks down when parsing morphologically rich languages (Tsarfaty et al., 2010), where every space-delimited word may be effectively composed of multiple morphemes, each of which having a distinct role in the syntactic parse tree.</S>
			<S sid ="9" ssid = "9">In order to parse such input the text needs to undergo morphological segmentation, that is, identifying the morphological segments of each word and assigning the corresponding part-of- speech (PoS) tags to them.</S>
			<S sid ="10" ssid = "10">Morphologically complex words may be highly ambiguous and in order to segment them correctly their analysis has to be disambiguated.</S>
			<S sid ="11" ssid = "11">The multiple morphological analyses of input words may be represented via a lattice that encodes the different segmentation possibilities of the entire word sequence.</S>
			<S sid ="12" ssid = "12">One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).</S>
			<S sid ="13" ssid = "13">If the selected segmentation is different from the gold segmentation, the gold and parse trees are rendered incomparable and standard evaluation metrics break down.</S>
			<S sid ="14" ssid = "14">Evaluation scenarios restricted to gold input are often used to bypass this problem, but, as shall be seen shortly, they present an overly optimistic upper- bound on parser performance.</S>
			<S sid ="15" ssid = "15">This paper presents a full treatment of evaluation in different parsing scenarios, using distance-based measures defined for trees over a shared common denominator defined in terms of a lattice structure.</S>
			<S sid ="16" ssid = "16">We demonstrate the informativeness of our metrics by evaluating joint segmentation and parsing performance for the Semitic language Modern Hebrew, using the best performing systems, both constituency- based and dependency-based (Tsarfaty, 2010; Goldberg, 2011a).</S>
			<S sid ="17" ssid = "17">Our experiments demonstrate that, for all parsers, significant performance gaps between realistic and non-realistic scenarios crucially depend on the kind of information initially provided to the parser.</S>
			<S sid ="18" ssid = "18">The tool and metrics that we provide are completely general and can straightforwardly apply to other languages, treebanks and different tasks.</S>
			<S sid ="19" ssid = "19">6 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 6–10, Jeju, Republic of Korea, 814 July 2012.</S>
			<S sid ="20" ssid = "20">Qc 2012 Association for Computational Linguistics (tree1) TOP (tree2) TOP PP PP IN 0B1 “in” NP N P DEF NP ADJP DEF JJ IN 0B1 “in” NP NN 1CL2 NP V B 4 H N E I M 5PP “made pleasant” 1H2 “the” NN 2CL3 “shadow” POSS 3FL4 PP PRN 4HM5 5H6 “the” 6NEIM7 “pleasant” “shadow” POSS 2FL3 “of” PRN 3HM4 “them” of “them” Figure 1: A correct tree (tree1) and an incorrect tree (tree2) for “BCLM HNEIM”, indexed by terminal boundaries.</S>
			<S sid ="21" ssid = "21">Erroneous nodes in the parse hypothesis are marked in italics.</S>
			<S sid ="22" ssid = "22">Missing nodes from the hypothesis are marked in bold.</S>
	</SECTION>
	<SECTION title="The Challenge: Evaluation for MRLs. " number = "2">
			<S sid ="23" ssid = "1">In morphologically rich languages (MRLs) substantial information about the grammatical relations between entities is expressed at word level using inflectional affixes.</S>
			<S sid ="24" ssid = "2">In particular, in MRLs such as Hebrew, Arabic, Turkish or Maltese, elements such as determiners, definite articles and conjunction markers appear as affixes that are appended to an open- class word.</S>
			<S sid ="25" ssid = "3">Take, for example the Hebrew word- token BCLM,1 which means “in their shadow”.</S>
			<S sid ="26" ssid = "4">This word corresponds to five distinctly tagged elements: B (“in”/IN), H (“the”/DEF), CL (“shadow”/NN), FL (”of”/POSS), HM (”they”/PRN).</S>
			<S sid ="27" ssid = "5">Note that morphological segmentation is not the inverse of concatenation.</S>
			<S sid ="28" ssid = "6">For instance, the overt definite article H and the possessor FL show up only in the analysis.</S>
			<S sid ="29" ssid = "7">The correct parse for the Hebrew phrase “BCLM HNEIM” is shown in Figure 1 (tree1), and it presupposes that these segments can be identified and assigned the correct PoS tags.</S>
			<S sid ="30" ssid = "8">However, morphological segmentation is nontrivial due to massive word- level ambiguity.</S>
			<S sid ="31" ssid = "9">The word BCLM, for instance, can be segmented into the noun BCL (“onion”) and M (a genitive suffix, “of them”), or into the prefix B (“in”) followed by the noun CLM (“image”).2 The multitude of morphological analyses may be encoded in a lattice structure, as illustrated in Figure 2.</S>
			<S sid ="32" ssid = "10">1 We use the Hebrew transliteration in Sima’an et al.</S>
			<S sid ="33" ssid = "11">(2001)..</S>
			<S sid ="34" ssid = "12">2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).</S>
			<S sid ="35" ssid = "13">Examples for similar phenomena in Arabic may be found in Green and Manning (2010).</S>
			<S sid ="36" ssid = "14">Figure 2: The morphological segmentation possibilities of BCLM HNEIM.</S>
			<S sid ="37" ssid = "15">Double-circles are word boundaries.</S>
			<S sid ="38" ssid = "16">In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.</S>
			<S sid ="39" ssid = "17">This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).</S>
			<S sid ="40" ssid = "18">Either way, an incorrect morphological segmentation hypothesis introduces errors into the parse hypothesis, ultimately providing a parse tree which spans a different yield than the gold terminals.</S>
			<S sid ="41" ssid = "19">In such cases, existing evaluation metrics break down.</S>
			<S sid ="42" ssid = "20">To understand why, consider the trees in Figure 1.</S>
			<S sid ="43" ssid = "21">Metrics like PARSEVAL (Black et al., 1991) calculate the harmonic means of precision and recallon labeled spans (i, label, j) where i, j are termi nal boundaries.</S>
			<S sid ="44" ssid = "22">Now, the NP dominating “shadow of them” has been identified and labeled correctly in tree2, but in tree1 it spans (2, NP, 5) and in tree2 it spans (1, NP, 4).</S>
			<S sid ="45" ssid = "23">This node will then be counted as an error for tree2, along with its dominated and dominating structure, and PARSEVAL will score 0.</S>
			<S sid ="46" ssid = "24">A generalized version of PARSEVAL which considers i, j character-based indices instead of terminal boundaries (Tsarfaty, 2006) will fail here too, since the missing overt definite article H will cause similar misalignments.</S>
			<S sid ="47" ssid = "25">Metrics for dependency- based evaluation such as ATTACHMENT SCORES (Buchholz and Marsi, 2006) suffer from similar problems, since they assume that both trees have the same nodes — an assumption that breaks down in the case of incorrect morphological segmentation.</S>
			<S sid ="48" ssid = "26">Although great advances have been made in parsing MRLs in recent years, this evaluation challenge Edit Scripts and Edit Costs We assume a set A={ADD(c, i, j),DEL(c, i, j),ADD((s, p), i, j), DEL((s, p), i, j)} of edit operations which can add or delete a labeled node c ∈ N or an entry (s, p) ∈ LEX which spans the states i, j in the lattice L. The operations in A are properly constrained by the lat tice, that is, we can only add and delete lexemes that belong to LEX, and we can only add and delete them where they can occur in the lattice.</S>
			<S sid ="49" ssid = "27">We assume a function C(a) = 1 assigning a unit cost to every op eration a ∈ A, and define the cost of a sequence(a1, . . .</S>
			<S sid ="50" ssid = "28">, am) as the sum of the costs of all operaremained unsolved.3 In this paper we present a solu tions in the sequence C((a1, ..., am)) = Lm C(ai).tion to this challenge by extending TEDEVAL (Tsar faty et al., 2011) for handling trees over lattices.</S>
	</SECTION>
	<SECTION title="The Proposal: Distance-Based Metrics. " number = "3">
			<S sid ="51" ssid = "1">Input and Output Spaces We view the joint taskAn edit script ES(y1, y2) = (a1, . . .</S>
			<S sid ="52" ssid = "2">, am) is a sequence of operations that turns y1 into y2.</S>
			<S sid ="53" ssid = "3">The tree edit distance is the minimum cost of any edit script that turns y1 into y2 (Bille, 2005).</S>
			<S sid ="54" ssid = "4">as a structured prediction function h : X → Y from input space X onto output space Y. Each element TED(y1, y2) = min ES(y1 ,y2 ) C(ES(y1, y2)) x ∈ X is a sequence x = w1, . . .</S>
			<S sid ="55" ssid = "5">, wn of space- delimited words from a set W . We assume a lexicon LEX, distinct from W , containing pairs of segments drawn from a set T of terminals and PoS categories drawn from a set N of nonterminals.</S>
			<S sid ="56" ssid = "6">LEX = {(s, p)|s ∈ T , p ∈ N } Each word wi in the input may admit multiple morphological analyses, constrained by a language- specific morphological analyzer MA.</S>
			<S sid ="57" ssid = "7">The morphological analysis of an input word MA(wi) can be represented as a lattice Li in which every arc corresponds to a lexicon entry (s, p).</S>
			<S sid ="58" ssid = "8">The morpholog ical analysis of an input sentence x is then a lattice L obtained through the concatenation of the lattices L1, . . .</S>
			<S sid ="59" ssid = "9">, Ln where MA(w1) = L1, . . .</S>
			<S sid ="60" ssid = "10">, MA(wn) = Ln.</S>
			<S sid ="61" ssid = "11">Now, let x = w1, . . .</S>
			<S sid ="62" ssid = "12">, wn be a sentence with a morphological analysis lattice MA(x) = L. We define the output space YMA(x)=L for h (abbreviated YL), as the set of linearly-ordered labeled trees such that the yield of LEX entries (s1, p1),.</S>
			<S sid ="63" ssid = "13">,(sk , pk ) in each tree (where si ∈ T and pi ∈ N , and possibly k /= n) corresponds to a path through the lattice L. 3 A tool that could potentially apply here is SParseval (Roark et al., 2006).</S>
			<S sid ="64" ssid = "14">But since it does not respect word-boundaries, it fails to apply to such lattices.</S>
			<S sid ="65" ssid = "15">Cohen and Smith (2007) aimed to fix this, but in their implementation syntactic nodes internal to word boundaries may be lost without scoring.</S>
			<S sid ="66" ssid = "16">Distance-Based Metrics The error of a predicted structure p with respect to a gold structure g is now taken to be the TED cost, and we can turn it into a score by normalizing it and subtracting from a unity: TED(p, g) TEDEVAL(p, g) = 1 − |p| + |g| − 2 The term |p| + |g| − 2 is a normalization factor defined in terms of the worst-case scenario, in which the parser has only made incorrect decisions.</S>
			<S sid ="67" ssid = "17">We would need to delete all lexemes and nodes in p and add all the lexemes and nodes of g, except for roots.</S>
			<S sid ="68" ssid = "18">An Example Both trees in Figure 1 are contained in YL for the lattice L in Figure 2.</S>
			<S sid ="69" ssid = "19">If we re place terminal boundaries with lattice indices from Figure 2, we need 6 edit operations to turn tree2 into tree1 (deleting the nodes in italic, adding the nodes in bold) and the evaluation score will be TEDEVAL(tree2,tree1) = 1 6 = 0.7273.</S>
			<S sid ="70" ssid = "20">14+10−2</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
			<S sid ="71" ssid = "1">We aim to evaluate state-of-the-art parsing architectures on the morphosyntactic disambiguation of Hebrew texts in three different parsing scenarios: (i) Gold: assuming gold segmentation and PoS-tags, (ii) Predicted: assuming only gold segmentation, and (iii) Raw: assuming unanalyzed input text.</S>
			<S sid ="72" ssid = "2">SE GE VAL PA RS EV AL TE DE VAL G o l d PS U: 10 0.0 0 L: 10 0.0 0 L: 88.</S>
			<S sid ="73" ssid = "3">75 U: 94.</S>
			<S sid ="74" ssid = "4">35 L: 93.</S>
			<S sid ="75" ssid = "5">39 Pre dict ed PS U: 10 0.0 0 L: 90.</S>
			<S sid ="76" ssid = "6">85 L: 82.</S>
			<S sid ="77" ssid = "7">30 U: 92.</S>
			<S sid ="78" ssid = "8">92 L: 86: 26 R a w PS U: 96.</S>
			<S sid ="79" ssid = "9">42 L: 84.</S>
			<S sid ="80" ssid = "10">54 N/ A U: 88.</S>
			<S sid ="81" ssid = "11">47 L: 80.</S>
			<S sid ="82" ssid = "12">67 G o l d RR U: 10 0.0 0 L: 10 0.0 0 L: 83.</S>
			<S sid ="83" ssid = "13">93 U: 94.</S>
			<S sid ="84" ssid = "14">34 L: 92.</S>
			<S sid ="85" ssid = "15">45 Pre dict ed RR U: 10 0.0 0 L: 91.</S>
			<S sid ="86" ssid = "16">69 L: 78.</S>
			<S sid ="87" ssid = "17">93 U: 92.</S>
			<S sid ="88" ssid = "18">82 L: 85.</S>
			<S sid ="89" ssid = "19">83 R a w RR U: 96.</S>
			<S sid ="90" ssid = "20">03 L: 86.</S>
			<S sid ="91" ssid = "21">10 N/ A U: 87.</S>
			<S sid ="92" ssid = "22">96 L: 79.</S>
			<S sid ="93" ssid = "23">46 Table 1: Phrase-Structure based results for the Berkeley Parser trained on bare-bone trees (PS) and relational- realizational trees (RR).</S>
			<S sid ="94" ssid = "24">We parse all sentences in the dev set.</S>
			<S sid ="95" ssid = "25">RR extra decoration is removed prior to evaluation.</S>
			<S sid ="96" ssid = "26">SE GE VAL AT TS CO RES TE DE VAL G old Pr edi cte d Ra w M P M P M P 10 0.0 0 10 0.0 0 95.</S>
			<S sid ="97" ssid = "27">07 U: 83.</S>
			<S sid ="98" ssid = "28">59 U: 82.</S>
			<S sid ="99" ssid = "29">00 N/ A U: 91.</S>
			<S sid ="100" ssid = "30">76 U: 91.</S>
			<S sid ="101" ssid = "31">20 U: 87.</S>
			<S sid ="102" ssid = "32">03 G old Pr edi cte d Ra w E F E F E F 10 0.0 0 10 0.0 0 95.</S>
			<S sid ="103" ssid = "33">07 U: 84.</S>
			<S sid ="104" ssid = "34">68 U: 83.</S>
			<S sid ="105" ssid = "35">97 N/ A U: 92.</S>
			<S sid ="106" ssid = "36">25 U: 92: 02 U: 87.</S>
			<S sid ="107" ssid = "37">75 Table 2: Dependency parsing results by MaltParser (MP) and EasyFirst (EF), trained on the treebank converted into unlabeled dependencies, and parsing the entire dev-set.</S>
			<S sid ="108" ssid = "38">For constituency-based parsing we use two models trained by the Berkeley parser (Petrov et al., 2006) one on phrase-structure (PS) trees and one on relational-realizational (RR) trees (Tsarfaty and Sima’an, 2008).</S>
			<S sid ="109" ssid = "39">In the raw scenario we let a lattice- based parser choose its own segmentation and tags (Goldberg, 2011b).</S>
			<S sid ="110" ssid = "40">For dependency parsing we use MaltParser (Nivre et al., 2007b) optimized for Hebrew by Ballesteros and Nivre (2012), and the Easy- First parser of Goldberg and Elhadad (2010) with the features therein.</S>
			<S sid ="111" ssid = "41">Since these parsers cannot choose their own tags, automatically predicted segments and tags are provided by Adler and Elhadad (2006).</S>
			<S sid ="112" ssid = "42">We use the standard split of the Hebrew tree- bank (Sima’an et al., 2001) and its conversion into unlabeled dependencies (Goldberg, 2011a).</S>
			<S sid ="113" ssid = "43">We use PARSEVAL for evaluating phrase-structure trees, ATTACHSCORES for evaluating dependency trees, and TEDEVAL for evaluating all trees in all scenarios.</S>
			<S sid ="114" ssid = "44">We implement SEGEVAL for evaluating segmentation based on our TEDEVAL implementation, replacing the tree distance and size with string terms.</S>
			<S sid ="115" ssid = "45">Table 1 shows the constituency-based parsing results for all scenarios.</S>
			<S sid ="116" ssid = "46">All of our results confirm that gold information leads to much higher scores.</S>
			<S sid ="117" ssid = "47">TEDEVAL allows us to precisely quantify the drop in accuracy from gold to predicted (as in PARSE- VAL) and than from predicted to raw on a single scale.</S>
			<S sid ="118" ssid = "48">TEDEVAL further allows us to scrutinize the contribution of different sorts of information.</S>
			<S sid ="119" ssid = "49">Unlabeled TEDEVAL shows a greater drop when moving from predicted to raw than from gold to predicted, and for labeled TEDEVAL it is the other way round.</S>
			<S sid ="120" ssid = "50">This demonstrates the great importance of gold tags which provide morphologically disambiguated information for identifying phrase content.</S>
			<S sid ="121" ssid = "51">Table 2 shows that dependency parsing results confirm the same trends, but we see a much smaller drop when moving from gold to predicted.</S>
			<S sid ="122" ssid = "52">This is due to the fact that we train the parsers for predicted on a treebank containing predicted tags.</S>
			<S sid ="123" ssid = "53">There is however a great drop when moving from predicted to raw, which confirms that evaluation benchmarks on gold input as in Nivre et al.</S>
			<S sid ="124" ssid = "54">(2007a) do not provide a realistic indication of parser performance.</S>
			<S sid ="125" ssid = "55">For all tables, TEDEVAL results are on a similar scale.</S>
			<S sid ="126" ssid = "56">However, results are not yet comparable across parsers.</S>
			<S sid ="127" ssid = "57">RR trees are flatter than bare-bone PS trees.</S>
			<S sid ="128" ssid = "58">PS and DEP trees have different label sets.</S>
			<S sid ="129" ssid = "59">Cross-framework evaluation may be conducted by combining this metric with the cross-framework protocol of Tsarfaty et al.</S>
			<S sid ="130" ssid = "60">(2012).</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="131" ssid = "1">We presented distance-based metrics defined for trees over lattices and applied them to evaluating parsers on joint morphological and syntactic disambiguation.</S>
			<S sid ="132" ssid = "2">Our contribution is both technical, providing an evaluation tool that can be straightforwardly applied for parsing scenarios involving trees over lattices,4 and methodological, suggesting to evaluate parsers in all possible scenarios in order to get a realistic indication of parser performance.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="133" ssid = "3">We thank Shay Cohen, Yoav Goldberg and Spence Green for discussion of this challenge.</S>
			<S sid ="134" ssid = "4">This work was supported by the Swedish Science Council.</S>
			<S sid ="135" ssid = "5">4 The tool can be downloaded http://stp.ling.uu.</S>
			<S sid ="136" ssid = "6">se/˜tsarfaty/unipar/index.html</S>
	</SECTION>
</PAPER>
