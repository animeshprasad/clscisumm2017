<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level.</S>
		<S sid ="2" ssid = "2">However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm.</S>
		<S sid ="3" ssid = "3">We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation.</S>
		<S sid ="4" ssid = "4">We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents.</S>
		<S sid ="5" ssid = "5">We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments.</S>
		<S sid ="6" ssid = "6">Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents.</S>
			<S sid ="8" ssid = "8">To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.</S>
			<S sid ="9" ssid = "9">Topic-specific lexicon translation models focus on word-level translations.</S>
			<S sid ="10" ssid = "10">Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗ Corresponding author by these probabilities.</S>
			<S sid ="11" ssid = "11">However, the state-of-the- art SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word by word.</S>
			<S sid ="12" ssid = "12">Since a synchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic model directly at the rule level rather than the word level.</S>
			<S sid ="13" ssid = "13">Consequently, we propose a topic similarity model for hierarchical phrase-based translation (Chiang, 2007), where each synchronous rule is associated with a topic distribution.</S>
			<S sid ="14" ssid = "14">In particular, • Given a document to be translated, we calculate the topic similarity between a rule and the document based on their topic distributions.</S>
			<S sid ="15" ssid = "15">We augment the hierarchical phrase-based system by integrating the proposed topic similarity model as a new feature (Section 3.1).</S>
			<S sid ="16" ssid = "16">• As we will discuss in Section 3.2, the similarity between a generic rule and a given source document computed by our topic similarity model is often very low.</S>
			<S sid ="17" ssid = "17">We don’t want to penalize these generic rules.</S>
			<S sid ="18" ssid = "18">Therefore we further propose a topic sensitivity model which rewards generic rules so as to complement the topic similarity model.</S>
			<S sid ="19" ssid = "19">• We estimate the topic distribution for a rule based on both the source and target side topic models (Section 4.1).</S>
			<S sid ="20" ssid = "20">In order to calculate sim ilarities between target-side topic distributions of rules and source-side topic distributions of given documents during decoding, we project 750 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 750–758, Jeju, Republic of Korea, 814 July 2012.</S>
			<S sid ="21" ssid = "21">Qc 2012 Association for Computational Linguistics 0.6 0.6 0.6 0.6 0.4 0.4 0.4 0.4 0.2 0.2 0.2 0.2 0 1 5 10 15 20 25 30 0 1 5 10 15 20 25 30 0 1 5 10 15 20 25 30 0 1 5 10 15 20 25 30(a) 作 战 能 力 ⇒ opera tional capability (b) 给予 X1 ⇒ grands X1 (c) 给予 X1 ⇒ give X1 (d) X1 举行 会谈 X2 ⇒ held talks X1 X2 Figure 1: Four synchronous rules with topic distributions.</S>
			<S sid ="22" ssid = "22">Each sub-graph shows a rule with its topic distribution, where the X-axis means topic index and the Y-axis means the topic probability.</S>
			<S sid ="23" ssid = "23">Notably, the rule (b) and rule (c) shares the same source Chinese string, but they have different topic distributions due to the different English translations.</S>
			<S sid ="24" ssid = "24">the target-side topic distributions of rules into the space of source-side topic model by one-to- many projection (Section 4.2).</S>
			<S sid ="25" ssid = "25">Experiments on ChineseEnglish translation tasks (Section 6) show that, our method outperforms the baseline hierarchial phrase-based system by +0.9 BLE U points.</S>
			<S sid ="26" ssid = "26">This result is also +0.5 points higher and 3 times faster than the previous topic-specific lexicon translation method.</S>
			<S sid ="27" ssid = "27">We further show that both the source-side and target-side topic distributions improve translation quality and their improvements are complementary to each other.</S>
	</SECTION>
	<SECTION title="Background: Topic Model. " number = "2">
			<S sid ="28" ssid = "1">A topic model is used for discovering the topics that occur in a collection of documents.</S>
			<S sid ="29" ssid = "2">Both Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) are types of topic models.</S>
			<S sid ="30" ssid = "3">LDA is the most common topic model currently in use, therefore we exploit it for mining topics in this paper.</S>
			<S sid ="31" ssid = "4">Here, we first give a brief description of LDA.</S>
			<S sid ="32" ssid = "5">LDA views each document as a mixture proportion of various topics, and generates each word by multinomial distribution conditioned on a topic.</S>
			<S sid ="33" ssid = "6">More specifically, as a generative process, LDA first samples a document-topic distribution for each document.</S>
			<S sid ="34" ssid = "7">Then, for each word in the document, it samples a topic index from the document-topic distribution and samples the word conditioned on the topic index according the topic-word distribution.</S>
			<S sid ="35" ssid = "8">Generally speaking, LDA contains two types of parameters.</S>
			<S sid ="36" ssid = "9">The first one relates to the document- topic distribution, which records the topic distribution of each document.</S>
			<S sid ="37" ssid = "10">The second one is used for topic-word distribution, which represents each topic as a distribution over words.</S>
			<S sid ="38" ssid = "11">Based on these parameters (and some hyper-parameters), LDA can infer a topic assignment for each word in the documents.</S>
			<S sid ="39" ssid = "12">In the following sections, we will use these parameters and the topic assignments of words to estimate the parameters in our method.</S>
	</SECTION>
	<SECTION title="Topic Similarity Model. " number = "3">
			<S sid ="40" ssid = "1">Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).</S>
			<S sid ="41" ssid = "2">In the hierarchical phrase based system, a synchronous rule may be related to some topics and unrelated to others.</S>
			<S sid ="42" ssid = "3">In terms of probability, a rule often has an uneven probability distribution over topics.</S>
			<S sid ="43" ssid = "4">The probability over a topic is high if the rule is highly related to the topic, otherwise the probability will be low.</S>
			<S sid ="44" ssid = "5">Therefore, we use topic distribution to describe the relatedness of rules to topics.</S>
			<S sid ="45" ssid = "6">Figure 1 shows four synchronous rules (Chiang, 2007) with topic distributions, some of which contain nonterminals.</S>
			<S sid ="46" ssid = "7">We can see that, although the source part of rule (b) and (c) are identical, their topic distributions are quite different.</S>
			<S sid ="47" ssid = "8">Rule (b) contains a highest probability on the topic about “China-U.S. relationship”, which means rule (b) is much more related to this topic.</S>
			<S sid ="48" ssid = "9">In contrast, rule (c) contains an even distribution over various topics.</S>
			<S sid ="49" ssid = "10">Thus, given a document about “China-U.S. relationship”, we hope to encourage the system to apply rule (b) but penalize the application of rule (c).</S>
			<S sid ="50" ssid = "11">We achieve this by calculating similarity between the topic distributions of a rule and a document to be translated.</S>
			<S sid ="51" ssid = "12">More formally, we associate each rule with a rule- topic distribution P (z|r), where r is a rule, and z is a topic.</S>
			<S sid ="52" ssid = "13">Suppose there are K topics, this distribution can be represented by a K -dimension vector.</S>
			<S sid ="53" ssid = "14">The k-th component P (z = k|r) means the probability of topic k given the rule r. The estimation of such distribution will be described in Section 4.</S>
			<S sid ="54" ssid = "15">Analogously, we represent the topic information of a document d to be translated by a documenttopic distribution P (z|d), which is also a K dimension vector.</S>
			<S sid ="55" ssid = "16">The k-th dimension P (z = k|d) means the probability of topic k given document d. Different from rule-topic distribution, the document- topic distribution can be directly inferred by an off- the-shelf LDA tool.</S>
			<S sid ="56" ssid = "17">Consequently, based on these two distributions, we select a rule for a document to be translated according to their topic similarity (Section 3.1), which measures the relatedness of the rule to the document.</S>
			<S sid ="57" ssid = "18">In order to encourage the application of generic rules which are often penalized by our similarity model, we also propose a topic sensitivity model (Section 3.2).</S>
			<S sid ="58" ssid = "19">3.1 Topic Similarity.</S>
			<S sid ="59" ssid = "20">By comparing the similarity of their topic distributions, we are able to decide whether a rule is suitable for a given source document.</S>
			<S sid ="60" ssid = "21">The topic similarity computes the distance of two topic distributions.</S>
			<S sid ="61" ssid = "22">We calculate the topic similarity by Hellinger function: Similarity(P (z|d), P (z|r)) K rules according to their topic distributions.</S>
			<S sid ="62" ssid = "23">Let’s revisit Figure 1.</S>
			<S sid ="63" ssid = "24">We can easily find that the topic distribution of rule (c) distribute evenly.</S>
			<S sid ="64" ssid = "25">This indicates that it is insensitive to topics, and can be applied in any topics.</S>
			<S sid ="65" ssid = "26">We call such a rule a topic- insensitive rule.</S>
			<S sid ="66" ssid = "27">In contrast, the distributions of the rest rules peak on a few topics.</S>
			<S sid ="67" ssid = "28">Such rules are called topic-sensitive rules.</S>
			<S sid ="68" ssid = "29">Generally speaking, a topic- insensitive rule has a fairly flat distribution, while a topic-sensitive rule has a sharp distribution.</S>
			<S sid ="69" ssid = "30">A document typically focuses on a few topics, and has a sharp topic distribution.</S>
			<S sid ="70" ssid = "31">In contrast, the distribution of topic-insensitive rule is fairly flat.</S>
			<S sid ="71" ssid = "32">Hence, a topic-insensitive rule is always less similar to documents and is punished by the similarity function.</S>
			<S sid ="72" ssid = "33">However, topic-insensitive rules may be more preferable than topic-sensitive rules if neither of them are similar to given documents.</S>
			<S sid ="73" ssid = "34">For a document about the “military” topic, the rule (b) and (c) in Figure 1 are both dissimilar to the document, because rule (b) relates to the “China-U.S. relationship” topic and rule (c) is topic-insensitive.</S>
			<S sid ="74" ssid = "35">Nevertheless, since rule (c) occurs more frequently across various topics, it may be better to apply rule (c).</S>
			<S sid ="75" ssid = "36">To address such issue of the topic similarity model, we further introduce a topic sensitivity model to describe the topic sensitivity of a rule using entropy as a metric: √ 2 Sensitivity(P (z|r)) = ∑ (√P (z = k|d) − k=1 P (z = k|r)) (1) K = − ∑ P (z = k|r) × log (P (z = k|r)) (2) Hellinger function is used to calculate distribution distance and is popular in topic model (Blei and Lafferty, 2007).1 By topic similarity, we aim to encourage or penalize the application of a rule for a given document according to their topic distributions, which then helps the SMT system make better translation decisions.</S>
			<S sid ="76" ssid = "37">3.2 Topic Sensitivity.</S>
			<S sid ="77" ssid = "38">Domain adaptation (Wu et al., 2008; Bertoldi and Federico, 2009) often distinguishes general-domain data from in-domain data.</S>
			<S sid ="78" ssid = "39">Similarly, we divide the rules into topic-insensitive rules and topic-sensitive 1 We also try other distance functions, including Euclidean distance, KullbackLeibler divergence and cosine function.</S>
			<S sid ="79" ssid = "40">They produce similar results in our preliminary experiments.</S>
			<S sid ="80" ssid = "41">k=1 According to the Eq.</S>
			<S sid ="81" ssid = "42">(2), a topic-insensitive rule has a large entropy, while a topic-sensitive rule has a smaller entropy.</S>
			<S sid ="82" ssid = "43">By incorporating the topic sensitivity model with the topic similarity model, we enable our SMT system to balance the selection of these two types of rules.</S>
			<S sid ="83" ssid = "44">Given rules with approximately equal values of Eq.</S>
			<S sid ="84" ssid = "45">(1), we prefer topic-insensitive rules.</S>
	</SECTION>
	<SECTION title="Estimation. " number = "4">
			<S sid ="85" ssid = "1">Unlike document-topic distribution that can be directly learned by LDA tools, we need to estimate the rule-topic distribution according to our requirement.</S>
			<S sid ="86" ssid = "2">In this paper, we try to exploit the topic information of both source and target language.</S>
			<S sid ="87" ssid = "3">To achieve this goal, we use both source-side and target-side monolingual topic models, and learn the correspondence between the two topic models from word-aligned bilingual corpus.</S>
			<S sid ="88" ssid = "4">Specifically, we use two types of rule-topic distributions: one is source-side rule-topic distribution and the other is target-side rule-topic distribution.</S>
			<S sid ="89" ssid = "5">These two rule-topic distributions are estimated by corresponding topic models in the same way (Section 4.1).</S>
			<S sid ="90" ssid = "6">Notably, only source language documents are available during decoding.</S>
			<S sid ="91" ssid = "7">In order to compute bution of every documents inferred by LDA tool.</S>
			<S sid ="92" ssid = "8">We first extract synchronous rules from training data in a traditional way.</S>
			<S sid ="93" ssid = "9">When a rule r is extracted from a document d with topic distribution P (z|d), we collect an instance (r, P (z|d), c), where c is the fraction count of an instance as described in Chiang, (2007).</S>
			<S sid ="94" ssid = "10">After extraction, we get a set of instances I = {(r, P (z|d), c)} with different document-topic distributions for each rule.</S>
			<S sid ="95" ssid = "11">Using these instances, we calculate the topic probability P (z = k|r) as follows: ∑ c P (z = k d)the similarity between the target-side topic distribu P (z = k|r) = I ∈I × | (3) tion of a rule and the source-side topic distribution of a given document，we need to project the target ∑K k′=1 ∑I ∈I c × P (z = k′|d) side topic distribution of a synchronous rule into the space of the source-side topic model (Section 4.2).</S>
			<S sid ="96" ssid = "12">A more principle way is to learn a bilingual topic model from bilingual corpus (Mimno et al., 2009).</S>
			<S sid ="97" ssid = "13">However, we may face difficulty during decoding, where only source language documents are available.</S>
			<S sid ="98" ssid = "14">It requires a marginalization to infer the monolingual topic distribution using the bilingual topic model.</S>
			<S sid ="99" ssid = "15">The high complexity of marginalization prohibits such a summation in practice.</S>
			<S sid ="100" ssid = "16">Previous work on bilingual topic model avoid this problem by some monolingual assumptions.</S>
			<S sid ="101" ssid = "17">Zhao and Xing (2007) assume that the topic model is generated in a monolingual manner, while Tam et al., (2007) construct their bilingual topic model by enforcing a one-to- one correspondence between two monolingual topic models.</S>
			<S sid ="102" ssid = "18">We also estimate our rule-topic distribution by two monolingual topic models, but use a different way to project target-side topics onto source-side topics.</S>
			<S sid ="103" ssid = "19">4.1 Monolingual Topic Distribution Estimation.</S>
			<S sid ="104" ssid = "20">We estimate rule-topic distribution from word- aligned bilingual training corpus with document boundaries explicitly given.</S>
			<S sid ="105" ssid = "21">The source and target side distributions are estimated in the same way.</S>
			<S sid ="106" ssid = "22">For simplicity, we only describe the estimation of source-side distribution in this section.</S>
			<S sid ="107" ssid = "23">The process of rule-topic distribution estimation is analogous to the traditional estimation of rule translation probability (Chiang, 2007).</S>
			<S sid ="108" ssid = "24">In addition to the word-aligned corpus, the input for estimation also contains the source-side topic-document distri By using both source-side and target-side document-topic distribution, we obtain two rule- topic distributions for each rule in total.</S>
			<S sid ="109" ssid = "25">4.2 Target-side Topic Distribution Projection.</S>
			<S sid ="110" ssid = "26">As described in the previous section, we also estimate the target-side rule-topic distribution.</S>
			<S sid ="111" ssid = "27">However, only source document-topic distributions are available during decoding.</S>
			<S sid ="112" ssid = "28">In order to calculate the similarity between the target-side rule-topic distribution of a rule and the source-side document- topic distribution of a source document, we need to project target-side topics into the source-side topic space.</S>
			<S sid ="113" ssid = "29">The projection contains two steps: • In the first step, we learn the topic-to-topic correspondence probability p(zf |ze ) from target side topic ze to source-side topic zf . • In the second step, we project the target-side topic distribution of a rule into source-side topic space using the correspondence probability.</S>
			<S sid ="114" ssid = "30">In the first step, we estimate the correspondence probability by the co-occurrence of the source-side and the target-side topic assignment of the word- aligned corpus.</S>
			<S sid ="115" ssid = "31">The topic assignments are output by LDA tool.</S>
			<S sid ="116" ssid = "32">Thus, we denotes each sentence pair by (zf , ze , a), where zf and ze are the topic assignments of source-side and target-side sentences respectively, and a is a set of links {(i, j)}.</S>
			<S sid ="117" ssid = "33">A link (i, j) means a source-side position i aligns to a target-side position j. Thus, the co-occurrence of a source-side topic with index kf and a target-sidee to pic f t o p i c 1 f t o p i c 2 f t o p i c 3 ente rpri ses rura l stat e agri cult ural mar ket refo rm pro duct ion peas ants own ed ente rpri se 农业 (agri cultu ral) 农 村 ( r u r a l ) 农 民 ( p e a s a n t ) 改 革 ( r e f o r m ) 财 政 ( f i n a n c e ) 社 会 ( s o c i a l ) 保 障 ( s a f e t y ) 调 整 ( a d j u s t ) 政 策 ( p o l i c y ) 收 入 ( i n c o m e ) 企 业( ente rpri se) 市 场( mar ket) 国 有(s tate) 公 司(c omp any) 金 融(f inan ce) 银 行(b ank) 投 资(i nves tme nt) 管 理( man age) 改 革(r efor m) 经 营(o pera tion ) 发 展 ( d e v e l o p ) 经 济(e con omi c) 科 技(t ech nolo gy ) 我 国( Chi na) 技 术(t ech niqu e) 产 业(i ndu stry) 结 构(s truct ure) 创 新(i nno vati on) 加 快(a ccel erat e) 改 革(r efor m) p(z f |ze ) 0 . 3 8 0 . 2 8 0 . 1 6 Table 1: Example of topic-to-topic correspondence.</S>
			<S sid ="118" ssid = "34">The last line shows the correspondence probability.</S>
			<S sid ="119" ssid = "35">Each column means a topic represented by its top-10 topical words.</S>
			<S sid ="120" ssid = "36">The first column is a target-side topic, while the rest three columns are source-side topics.</S>
			<S sid ="121" ssid = "37">topic ke is calculated by: ∑ ∑ δ(zf , kf ) ∗ δ(ze , ke ) (4) i j (zf ,ze ,a) (i,j)∈a where δ(x, y) is the Kronecker function, which is 1 if x = y and 0 otherwise.</S>
			<S sid ="122" ssid = "38">We then compute the probability of P (z = kf |z = ke ) by normalizing the co-occurrence count.</S>
			<S sid ="123" ssid = "39">Overall, after the first step, we obtain an correspondence matrix MKe ×Kf from target-side topic to source-side topic, where the item Mi,j represents the probability P (zf = i|ze = j).In the second step, given the correspondence ma trix MKe ×Kf , we project the target-side rule-topic distribution P (ze |r) to the source-side topic space by multiplication as follows: T (P (ze |r)) = P (ze |r) ⊗ MKe ×Kf (5) In this way, we get a second distribution for a rule in the source-side topic space, which we called projected target-side topic distribution T (P (ze |r)).</S>
			<S sid ="124" ssid = "40">Obviously, our projection method allows one target-side topic to align to multiple source-side topics.</S>
			<S sid ="125" ssid = "41">This is different from the one-to-one correspondence used by Tam et al., (2007).</S>
			<S sid ="126" ssid = "42">From the training result of the correspondence matrix MKe ×Kf , we find that the topic correspondence between source and target language is not necessarily one-to-one.</S>
			<S sid ="127" ssid = "43">Typically, the probability P (z = kf |z = ke ) of a target-side topic mainly distributes on two or three source-side topics.</S>
			<S sid ="128" ssid = "44">Table 1 shows an example of a target-side topic with its three mainly aligned source-side topics.</S>
	</SECTION>
	<SECTION title="Decoding. " number = "5">
			<S sid ="129" ssid = "1">We incorporate our topic similarity model as a new feature into a traditional hiero system (Chi- ang, 2007) under discriminative framework (Och and Ney, 2002).</S>
			<S sid ="130" ssid = "2">Considering there are a source- side rule-topic distribution and a projected target- side rule-topic distribution, we add four features in total: • Similarity (P (zf |d), P (zf |r)) • Similarity(P (zf |d), T (P (ze |r))) • Sensitivity(P (zf |r)) • Sensitivity(T (P (ze |r)) To calculate the total score of a derivation on each feature listed above during decoding, we sum up the correspondent feature score of each applied rule.2 The source-side and projected target-side rule- topic distribution are calculated before decoding.</S>
			<S sid ="131" ssid = "3">During decoding, we first infer the topic distribution P (zf |d) for a given document on source language.When applying a rule, it is straightforward to calcu late these topic features.</S>
			<S sid ="132" ssid = "4">Obviously, the computational cost of these features is rather small.</S>
			<S sid ="133" ssid = "5">In the topic-specific lexicon translation model, given a source document, it first calculates the topic- specific translation probability by normalizing the entire lexicon translation table, and then adapts the lexical weights of rules correspondingly.</S>
			<S sid ="134" ssid = "6">This makes the decoding slower.</S>
			<S sid ="135" ssid = "7">Therefore, comparing with the previous topic-specific lexicon translation method, our method provides a more efficient way for incorporating topic model into SMT.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "6">
			<S sid ="136" ssid = "1">We try to answer the following questions by experiments: 1.</S>
			<S sid ="137" ssid = "2">Is our topic similarity model able to improve.</S>
			<S sid ="138" ssid = "3">translation quality in terms of BLEU?</S>
			<S sid ="139" ssid = "4">Furthermore, are source-side and target-side rule-topic distributions complementary to each other?</S>
			<S sid ="140" ssid = "5">2 Since glue rule and rules of unknown words are not extracted from training data, here, we just ignore the calculation of the four features for them.</S>
			<S sid ="141" ssid = "6">S y s t e m M T0 6 M T0 8 A vg Sp eed B a s e l i n e T o p i c L e x 30 .2 0 30 .6 5 21.</S>
			<S sid ="142" ssid = "7">93 22.</S>
			<S sid ="143" ssid = "8">29 26.</S>
			<S sid ="144" ssid = "9">07 26.</S>
			<S sid ="145" ssid = "10">47 12 .6 3 . 3 S i m Sr c Si m T gt Si m Sr c + Si m T gt Si m + S en 30 .4 1 30 .5 1 30 .7 3 30 .9 5 22.</S>
			<S sid ="146" ssid = "11">69 22.</S>
			<S sid ="147" ssid = "12">39 22.</S>
			<S sid ="148" ssid = "13">69 22.</S>
			<S sid ="149" ssid = "14">92 26.</S>
			<S sid ="150" ssid = "15">55 26.</S>
			<S sid ="151" ssid = "16">45 26.</S>
			<S sid ="152" ssid = "17">71 26.</S>
			<S sid ="153" ssid = "18">94 11 .5 11 .7 11 .2 10 .2 Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (“Baseline”) and the topic-specific lexicon translation method (“TopicLex”).</S>
			<S sid ="154" ssid = "19">“SimSrc” and “SimTgt” denote similarity by source-side and target-side rule-distribution respectively, while “Sim+Sen” activates the two similarity and two sensitivity features.</S>
			<S sid ="155" ssid = "20">“Avg” is the average BL E U score on the two test sets.</S>
			<S sid ="156" ssid = "21">Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p &lt; 0.01).</S>
			<S sid ="157" ssid = "22">2.</S>
			<S sid ="158" ssid = "23">Is it helpful to introduce the topic sensitivi-.</S>
			<S sid ="159" ssid = "24">ty model to distinguish topic-insensitive and topic-sensitive rules?</S>
			<S sid ="160" ssid = "25">3.</S>
			<S sid ="161" ssid = "26">Is it necessary to project topics by one-to-many.</S>
			<S sid ="162" ssid = "27">correspondence instead of one-to-one correspondence?</S>
			<S sid ="163" ssid = "28">4.</S>
			<S sid ="164" ssid = "29">What is the effect of our method on various.</S>
			<S sid ="165" ssid = "30">types of rules, such as phrase rules and rules with non-terminals?</S>
			<S sid ="166" ssid = "31">6.1 Data.</S>
			<S sid ="167" ssid = "32">We present our experiments on the NIST ChineseEnglish translation tasks.</S>
			<S sid ="168" ssid = "33">The bilingual training data contains 239K sentence pairs with 6.9M Chinese words and 9.14M English words, which comes from the FBIS portion of LDC data.</S>
			<S sid ="169" ssid = "34">There are 10,947 documents in the FBIS corpus.</S>
			<S sid ="170" ssid = "35">The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words.</S>
			<S sid ="171" ssid = "36">We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets.</S>
			<S sid ="172" ssid = "37">The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively.</S>
			<S sid ="173" ssid = "38">We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003).</S>
			<S sid ="174" ssid = "39">The SCFG rules are extracted from this word-aligned training data.</S>
			<S sid ="175" ssid = "40">A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002).</S>
			<S sid ="176" ssid = "41">Case-insensitive NIST BLE U (Papineni et al., 2002) was used to mea sure translation performance.</S>
			<S sid ="177" ssid = "42">We used minimum error rate training (Och, 2003) for optimizing the feature weights.</S>
			<S sid ="178" ssid = "43">For the topic model, we used the open source L- DA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference.</S>
			<S sid ="179" ssid = "44">The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively.</S>
			<S sid ="180" ssid = "45">We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 During decoding, we first infer the topic distribution of given documents before translation according to the topic model trained on Chinese part of FBIS corpus.</S>
			<S sid ="181" ssid = "46">6.2 Effect of Topic Similarity Model.</S>
			<S sid ="182" ssid = "47">We compare our method with two baselines.</S>
			<S sid ="183" ssid = "48">In addition to the traditional hiero system, we also compare with the topic-specific lexicon translation method in Zhao and Xing (2007).</S>
			<S sid ="184" ssid = "49">The lexicon translation probability is adapted by: p(f |e, DF ) ∝ p(e|f, DF )P (f |DF ) (6) = ∑ p(e|f, z = k)p(f |z = k)p(z = k|DF ) (7) k However, we simplify the estimation of p(e|f, z = k) by directly using the word alignment corpus with 3 http://gibbslda.sourceforge.net/ 4 We determine K by testing {15, 30, 50, 100, 200} in our.</S>
			<S sid ="185" ssid = "50">preliminary experiments.</S>
			<S sid ="186" ssid = "51">We find that K = 30 produces a s lightly better performance than other values.</S>
			<S sid ="187" ssid = "52">T y p e C ou nt Src% Tgt% P hr ase ru le M o n ot o ne ru le R eo rd er ing ru le 3.</S>
			<S sid ="188" ssid = "53">9 M 83.4 84.4 19.</S>
			<S sid ="189" ssid = "54">2 M 85.3 86.1 5.</S>
			<S sid ="190" ssid = "55">7 M 85.9 86.8 A l l r u l e 28.</S>
			<S sid ="191" ssid = "56">8 M 85.1 86.0 Table 3: Percentage of topic-sensitive rules of various types of rule according to source-side (“Src”) and target- side (“Tgt”) topic distributions.</S>
			<S sid ="192" ssid = "57">Phrase rules are fully lexicalized, while monotone and reordering rules contain nonterminals (Section 6.5).</S>
			<S sid ="193" ssid = "58">topic assignment that is inferred by the GibbsLDA++.</S>
			<S sid ="194" ssid = "59">Despite the simplification of estimation, the improvement of our implementation is comparable with the improvement in Zhao et al.,(2007).</S>
			<S sid ="195" ssid = "60">Given a new document, we need to adapt the lexical translation weights of the rules based on topic model.</S>
			<S sid ="196" ssid = "61">The adapted lexicon translation model is added as a new feature under the discriminative framework.</S>
			<S sid ="197" ssid = "62">Table 2 shows the result of our method comparing with the traditional system and the topic-lexicon specific translation method described as above.</S>
			<S sid ="198" ssid = "63">By using all the features (last line in the table), we improve the translation performance over the baseline system by 0.87 BLEU point on average.</S>
			<S sid ="199" ssid = "64">Our method also outperforms the topic-lexicon specific translation method by 0.47 points.</S>
			<S sid ="200" ssid = "65">This verifies that topic similarity model can improve the translation quality significantly.</S>
			<S sid ="201" ssid = "66">In order to gain insights into why our model is helpful, we further investigate how many rules are topic-sensitive.</S>
			<S sid ="202" ssid = "67">As described in Section 3.2, we use entropy to measure the topic sensitivity.</S>
			<S sid ="203" ssid = "68">If the entropy of a rule is smaller than a certain threshold, then the rule is topic sensitive.</S>
			<S sid ="204" ssid = "69">Since documents often focus on some topics, we use the average entropy of document-topic distribution of all training documents as the threshold.</S>
			<S sid ="205" ssid = "70">We compare both source- side and target-side distribution shown in Table 3.</S>
			<S sid ="206" ssid = "71">We find that more than 80 percents of the rules are topic-sensitive, thus provides us a large space to improve the translation by exploiting topics.</S>
			<S sid ="207" ssid = "72">We also compare these methods in terms of the decoding speed (words/second).</S>
			<S sid ="208" ssid = "73">The baseline translates 12.6 words per second, while the topic-specific lexicon translation method only translates 3.3 words in one second.</S>
			<S sid ="209" ssid = "74">The overhead of the topic-specificTable 4: Effects of one-to-one and one-to-many topic pro jection.</S>
			<S sid ="210" ssid = "75">lexicon translation method mainly comes from the adaptation of lexical weights.</S>
			<S sid ="211" ssid = "76">It takes 72.8% of the time to do the adaptation, despite only lexical weights of the used rules are adapted.</S>
			<S sid ="212" ssid = "77">In contrast, our method has a speed of 10.2 words per second for each sentence on average, which is three times faster than the topic-specific lexicon translation method.</S>
			<S sid ="213" ssid = "78">Meanwhile, we try to separate the effects of source-side topic distribution from the target-side topic distribution.</S>
			<S sid ="214" ssid = "79">From lines 46 of Table 2.</S>
			<S sid ="215" ssid = "80">We clearly find that the two rule-topic distributions improve the performance by 0.48 and 0.38 BLE U points over the baseline respectively.</S>
			<S sid ="216" ssid = "81">It seems that the source-side topic model is more helpful.</S>
			<S sid ="217" ssid = "82">Furthermore, when combine these two distributions, the improvement is increased to 0.64 points.</S>
			<S sid ="218" ssid = "83">This indicates that the effects of source-side and target-side distributions are complementary.</S>
			<S sid ="219" ssid = "84">6.3 Effect of Topic Sensitivity Model.</S>
			<S sid ="220" ssid = "85">As described in Section 3.2, because the similarity features always punish topic-insensitive rules, we introduce topic sensitivity features as a complement.</S>
			<S sid ="221" ssid = "86">In the last line of Table 2, we obtain a further improvement of 0.23 points, when incorporating topic sensitivity features with topic similarity features.</S>
			<S sid ="222" ssid = "87">This suggests that it is necessary to distinguish topic-insensitive and topic-sensitive rules.</S>
			<S sid ="223" ssid = "88">6.4 One-to-One Vs. One-to-Many Topic.</S>
			<S sid ="224" ssid = "89">Projection In Section 4.2, we find that source-side topic and target-side topics may not exactly match, hence we use one-to-many topic correspondence.</S>
			<S sid ="225" ssid = "90">Yet another method is to enforce one-to-one topic projection (Tam et al., 2007).</S>
			<S sid ="226" ssid = "91">We achieve one-to-one projection by aligning a target topic to the source topic with the largest correspondence probability as calculated in Section 4.2.Table 4 compares the effects of these two method S y s t e m M T0 6 MT08 Avg.</S>
			<S sid ="227" ssid = "92">B a s e l i n e 30 .2 0 21.93 26.07 P hr ase ru le M o n ot o ne ru le R eo rd er ing ru le 30 .5 3 22.29 26.41 30 .7 2 22.62 26.67 30 .3 1 22.40 26.36 A l l r u l e 30 .9 5 22.92 26.94 Table 5: Effect of our topic model on three types of rules.</S>
			<S sid ="228" ssid = "93">Phrase rules are fully lexicalized, while monotone and reordering rules contain nonterminals.</S>
			<S sid ="229" ssid = "94">s. We find that the enforced one-to-one topic method obtains a slight improvement over the baseline system, while one-to-many projection achieves a larger improvement.</S>
			<S sid ="230" ssid = "95">This confirms our observation of the non-one-to-one mapping between source-side and target-side topics.</S>
			<S sid ="231" ssid = "96">6.5 Effect on Various Types of Rules.</S>
			<S sid ="232" ssid = "97">To get a more detailed analysis of the result, we further compare the effect of our method on different types of rules.</S>
			<S sid ="233" ssid = "98">We divide the rules into three types: phrase rules, which only contain terminals and are the same as the phrase pairs in phrase- based system; monotone rules, which contain non- terminals and produce monotone translations; reordering rules, which also contain non-terminals but change the order of translations.</S>
			<S sid ="234" ssid = "99">We define the monotone and reordering rules according to Chiang et al., (2008).</S>
			<S sid ="235" ssid = "100">Table 5 show the results.</S>
			<S sid ="236" ssid = "101">We can see that our method achieves improvements on all the three types of rules.</S>
			<S sid ="237" ssid = "102">Our topic similarity method on monotone rule achieves the most improvement which is 0.6 BLE U points, while the improvement on reorder-.</S>
			<S sid ="238" ssid = "103">ing rules is the smallest among the three types.</S>
			<S sid ="239" ssid = "104">This shows that topic information also helps the selections of rules with non-terminals.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "7">
			<S sid ="240" ssid = "1">In addition to the topic-specific lexicon translation method mentioned in the previous sections, researchers also explore topic model for machine translation in other ways.</S>
			<S sid ="241" ssid = "2">Foster and Kunh (2007) describe a mixture-model approach for SMT adaptation.</S>
			<S sid ="242" ssid = "3">They first split a training corpus into different domains.</S>
			<S sid ="243" ssid = "4">Then, they train separate models on each domain.</S>
			<S sid ="244" ssid = "5">Finally, they combine a specific domain translation model with a general domain translation model depending on various text distances.</S>
			<S sid ="245" ssid = "6">One way to calculate the distance is using topic model.</S>
			<S sid ="246" ssid = "7">Gong et al.</S>
			<S sid ="247" ssid = "8">(2010) introduce topic model for filtering topic-mismatched phrase pairs.</S>
			<S sid ="248" ssid = "9">They first assign a specific topic for the document to be translated.</S>
			<S sid ="249" ssid = "10">Similarly, each phrase pair is also assigned with one specific topic.</S>
			<S sid ="250" ssid = "11">A phrase pair will be discarded if its topic mismatches the document topic.</S>
			<S sid ="251" ssid = "12">Researchers also introduce topic model for cross- lingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011).</S>
			<S sid ="252" ssid = "13">They use bilingual topic model to project latent topic distribution across languages.</S>
			<S sid ="253" ssid = "14">Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side.</S>
			<S sid ="254" ssid = "15">Our topic similarity model uses the document topic information.</S>
			<S sid ="255" ssid = "16">From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009).</S>
			<S sid ="256" ssid = "17">Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number = "8">
			<S sid ="257" ssid = "1">We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system.</S>
			<S sid ="258" ssid = "2">Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topic- specific lexicon translation.</S>
			<S sid ="259" ssid = "3">This verifies the advantage of exploiting topic model at the rule level over the word level.</S>
			<S sid ="260" ssid = "4">Further improvement is achieved by distinguishing topic-sensitive and topic-insensitive rules using the topic sensitivity model.</S>
			<S sid ="261" ssid = "5">In the future, we are interesting to find ways to exploit topic model on bilingual data without document boundaries, thus to enlarge the size of training data.</S>
			<S sid ="262" ssid = "6">Furthermore, our training corpus mainly focus on news, it is also interesting to apply our method on corpus with more diverse topics.</S>
			<S sid ="263" ssid = "7">Finally, we hope to apply our method to other translation models, especially syntax-based models.</S>
	</SECTION>
	<SECTION title="Acknowledgement">
			<S sid ="264" ssid = "8">The authors were supported by High-Technology R&amp;D Program (863) Project No 2011AA01A207 and 2012BAH39B03.</S>
			<S sid ="265" ssid = "9">This work was done during Xinyan Xiao’s internship at I2 R. We would like to thank Yun Huang, Zhengxian Gong, Wenliang Chen, Jun lang, Xiangyu Duan, Jun Sun, Jinsong Su and the anonymous reviewers for their insightful comments.</S>
	</SECTION>
</PAPER>
