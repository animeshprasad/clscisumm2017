<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model.</S>
		<S sid ="2" ssid = "2">Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language.</S>
		<S sid ="3" ssid = "3">We design a generative model for word alignment that uses synonym information as a regularization term.</S>
		<S sid ="4" ssid = "4">The experimental results show that our proposed method significantly improves word alignment quality.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT).</S>
			<S sid ="6" ssid = "6">It is an inference problem of word correspondences between different languages given parallel sentence pairs.</S>
			<S sid ="7" ssid = "7">Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance.</S>
			<S sid ="8" ssid = "8">Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vo- gel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007).</S>
			<S sid ="9" ssid = "9">One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus.</S>
			<S sid ="10" ssid = "10">This monolingual knowledge makes it easier to determine corresponding words correctly.</S>
			<S sid ="11" ssid = "11">For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008).</S>
			<S sid ="12" ssid = "12">It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality.</S>
			<S sid ="13" ssid = "13">A large number of monolingual lexical semantic resources such as WordNet (Miller, 1995) have been constructed in more than fifty languages (Sagot and Fiser, 2008).</S>
			<S sid ="14" ssid = "14">They include word- level relations such as synonyms, hypernyms and hyponyms.</S>
			<S sid ="15" ssid = "15">Synonym information is particularly helpful for word alignment because we can expect a synonym to correspond to the same word in a different language.</S>
			<S sid ="16" ssid = "16">In this paper, we explore a method for using synonym information effectively to improve word alignment quality.</S>
			<S sid ="17" ssid = "17">In general, synonym relations are defined in terms of word sense, not in terms of word form.</S>
			<S sid ="18" ssid = "18">In other words, synonym relations are usually context or domain dependent.</S>
			<S sid ="19" ssid = "19">For instance, ‘head’ and ‘chief’ are synonyms in contexts referring to working environment, while ‘head’ and ‘forefront’ are synonyms in contexts referring to physical positions.</S>
			<S sid ="20" ssid = "20">It is difficult, however, to imagine a context where ‘chief’ and ‘forefront’ are synonyms.</S>
			<S sid ="21" ssid = "21">Therefore, it is easy to imagine that simply replacing all occurrences of ‘chief’ and ‘forefront’ with ‘head’ do sometimes harm with word alignment accuracy, and we have to model either the context or senses of words.</S>
			<S sid ="22" ssid = "22">We propose a novel method that incorporates synonyms from monolingual resources in a bilingual word alignment model.</S>
			<S sid ="23" ssid = "23">We formulate a synonym pair generative model with a topic variable and use this model as a regularization term with a bilingual word alignment model.</S>
			<S sid ="24" ssid = "24">The topic variable in our synonym model is helpful for disambiguating the meanings of synonyms.</S>
			<S sid ="25" ssid = "25">We extend HMBiTAM, which is a HMM-based word alignment model with a latent topic, with a novel synonym pair generative model.</S>
			<S sid ="26" ssid = "26">We applied the proposed method to an English-French word alignment task and successfully improved the word 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, Uppsala, Sweden, 1116 July 2010.</S>
			<S sid ="27" ssid = "27">Qc 2010 Association for Computational Linguistics translation probability from e to f under the kth topic: p (f |e, z = k ).</S>
			<S sid ="28" ssid = "28">T = {Ti,i′ } is a state tran sition probability of a first order Markov process.</S>
			<S sid ="29" ssid = "29">Fig.</S>
			<S sid ="30" ssid = "30">1 shows a graphical model of HMBiTAM.</S>
			<S sid ="31" ssid = "31">The total likelihood of bilingual sentence pairs{E, F } can be obtained by marginalizing out la tent variables z, a and θ, p (F, E; Ψ) = ∑ ∑ f p (F, E, z, a, θ; Ψ) dθ, (1) z a Figure 1: Graphical model of HMBiTAM alignment quality.</S>
	</SECTION>
	<SECTION title="Bilingual Word Alignment Model. " number = "2">
			<S sid ="32" ssid = "1">In this section, we review a conventional generative word alignment model, HMBiTAM (Zhao and Xing, 2008).</S>
			<S sid ="33" ssid = "2">HMBiTAM is a bilingual generative model with topic z, alignment a and topic weight vector θ as latent variables.</S>
			<S sid ="34" ssid = "3">Topic variables such as ‘science’ or ‘economy’ assigned to individual sentences help to disambiguate the meanings of words.</S>
			<S sid ="35" ssid = "4">HMBiTAM assumes that the nth bilingual sentence pair, (En , Fn ), is generated under a given latent topic zn ∈ {1, . . .</S>
			<S sid ="36" ssid = "5">, k, . . .</S>
			<S sid ="37" ssid = "6">, K }, where K is the number of latent topics.</S>
			<S sid ="38" ssid = "7">Let N be the number of sentence pairs, and In and Jn be the lengths of En and Fn , respectively.</S>
			<S sid ="39" ssid = "8">In this frame work, all of the bilingual sentence pairs {E, F } = {(En , Fn )}n=1 are generated as follows.</S>
			<S sid ="40" ssid = "9">1.</S>
			<S sid ="41" ssid = "10">θ ∼ Dirichlet (α): sample topic-weight vector 2.</S>
			<S sid ="42" ssid = "11">For each sentence pair (En , Fn ).</S>
			<S sid ="43" ssid = "12">(a) zn ∼ M ultinomial (θ): sample the topic (b) en,i:In |zn ∼ p (En |zn ; β ): sample English words from a monolingual unigram model given topic zn (c) For each position jn = 1, . . .</S>
			<S sid ="44" ssid = "13">, Jni.</S>
			<S sid ="45" ssid = "14">ajn ∼ p (ajn |ajn −1 ; T ): sample an align where Ψ = {α, β, T , B} is a parameter set.</S>
			<S sid ="46" ssid = "15">Inthis model, we can infer word alignment a by max imizing the likelihood above.</S>
	</SECTION>
	<SECTION title="Proposed Method. " number = "3">
			<S sid ="47" ssid = "1">3.1 Synonym Pair Generative Model.</S>
			<S sid ="48" ssid = "2">We design a generative model for synonym pairs {f, f ′ } in language F , which assumes that thesynonyms are collected from monolingual linguis tic resources.</S>
			<S sid ="49" ssid = "3">We assume that each synonym pair (f, f ′ ) is generated independently given the same ‘sense’ s. Under this assumption, the probability of synonym pair (f, f ′ ) can be formulated as, p (f, f ′ ) ∝ ∑ p (f |s ) p (f ′ |s ) p (s) .</S>
			<S sid ="50" ssid = "4">(2) s We define a pair (e, k) as a representation of the sense s, where e and k are a word in a different language E and a latent topic, respectively.</S>
			<S sid ="51" ssid = "5">It has been shown that a word e in a different language is an appropriate representation of s in synonym modeling (Bannard and CallisonBurch, 2005).</S>
			<S sid ="52" ssid = "6">We assume that adding a latent topic k for the sense is very useful for disambiguating word meaning, and thus that (e, k) gives us a good approximation of s. Under this assumption, the synonym pair generative model can be defined as follows.</S>
			<S sid ="53" ssid = "7">p ({f, f ′ } ; Ψ� )ment link ajn from a first order Markov pro ∝ ∏ ∑ p(f |e, k; Ψ� )p(f ′ |e, k; Ψ� )p(e, k; Ψ� ), (3) cess ii.</S>
			<S sid ="54" ssid = "8">fjn ∼ p (fjn |En , ajn , zn ; B ): sample a target word fjn given an aligned source word and topic where alignment ajn = i denotes source word ei and target word fjn are aligned.</S>
			<S sid ="55" ssid = "9">α is a parameter over the topic weight vector θ, β = {βk,e } is the source word probability given the kth topic: p (e |z = k ).</S>
			<S sid ="56" ssid = "10">B = {Bf,e,k } represents the word (f,f ′ ) e,k where Ψ� is the parameter set of our model.</S>
			<S sid ="57" ssid = "11">3.2 Word Alignment with Synonym.</S>
			<S sid ="58" ssid = "12">Regularization In this section, we extend the bilingual generative model (HMBiTAM) with our synonym pair model.</S>
			<S sid ="59" ssid = "13">Our expectation is that synonym pairs Figure 2: Graphical model of synonym pair generative process correspond to the same word in a different language, thus they make it easy to infer accurate word alignment.</S>
			<S sid ="60" ssid = "14">HMBiTAM and the synonym model share parameters in order to incorporate monolingual synonym information into the bilingual word alignment model.</S>
			<S sid ="61" ssid = "15">This can be achieved via reparameterizing Ψ� in eq. 3 as, ( ) p (e, k; Ψ� ) ≡ p (e |k; β ) p (k; α) .</S>
			<S sid ="62" ssid = "16">(5) Overall, we redefine the synonym pair model with the HMBiTAM parameter set Ψ, p({f, f ′ } ; Ψ)</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
			<S sid ="63" ssid = "1">4.1 Experimental Setting.</S>
			<S sid ="64" ssid = "2">For an empirical evaluation of the proposed method, we used a bilingual parallel corpus of English-French Hansards (Mihalcea and Pedersen, 2003).</S>
			<S sid ="65" ssid = "3">The corpus consists of over 1 million sentence pairs, which include 447 manually word- aligned sentences.</S>
			<S sid ="66" ssid = "4">We selected 100 sentence pairs randomly from the manually word-aligned sentences as development data for tuning the regularization weight ζ , and used the 347 remaining sentence pairs as evaluation data.</S>
			<S sid ="67" ssid = "5">We also randomly selected 10k, 50k, and 100k sized sentence pairs from the corpus as additional training data.</S>
			<S sid ="68" ssid = "6">We ran the unsupervised training of our proposed word alignment model on the additional training data and the 347 sentence pairs of the evaluation data.</S>
			<S sid ="69" ssid = "7">Note that manual word alignment of the 347 sentence pairs was not used for the unsupervised training.</S>
			<S sid ="70" ssid = "8">After the unsupervised training, we evaluated the word alignment performance of our proposed method by comparing the manual word alignment of the 347 sentence pairs with the prediction provided by the trained model.</S>
			<S sid ="71" ssid = "9">We collected English and French synonym pairs from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008), respectively.</S>
			<S sid ="72" ssid = "10">WOLF is a semantic resource constructed from the Princeton 1 ∏ ∑ ∝ ∑ αk βk,e Bf,e,k Bf ′ ,e,k .</S>
			<S sid ="73" ssid = "11">(6) WordNet and various multilingual resources.</S>
			<S sid ="74" ssid = "12">We selected synonym pairs where both words were in k′ αk′ (f,f ′ ) k,e Fig.</S>
			<S sid ="75" ssid = "13">2 shows a graphical model of the synonym pair generative process.</S>
			<S sid ="76" ssid = "14">We estimate the parameter values to maximize the likelihood of HMBiTAM with respect to bilingual sentences and that of the synonym model with respect to synonym pairs collected from monolingual resources.</S>
			<S sid ="77" ssid = "15">Namely, the parameter estimate, Ψˆ , is computed as Ψˆ = arg max {log p(F, E; Ψ) + ζ log p({f, f ′ } ; Ψ)} , Ψ (7) where ζ is a regularization weight that should be set for training.</S>
			<S sid ="78" ssid = "16">We can expect that the second term of eq. 7 to constrain parameter set Ψ and avoid overfitting for the bilingual word alignment model.</S>
			<S sid ="79" ssid = "17">We resort to the variational EM approach (Bernardo et al., 2003) to infer Ψˆ following HMBiTAM.</S>
			<S sid ="80" ssid = "18">We omit the parameter update equation due to lack of space.</S>
			<S sid ="81" ssid = "19">cluded in the bilingual training set.</S>
			<S sid ="82" ssid = "20">We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vo- gel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us.</S>
			<S sid ="83" ssid = "21">GIZA++ is an implementation of IBM-model 4 and HMM, and HMBiTAM corresponds to ζ = 0 in eq. 7.</S>
			<S sid ="84" ssid = "22">We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).</S>
			<S sid ="85" ssid = "23">We trained the word alignment in two directions: English to French, and French to English.</S>
			<S sid ="86" ssid = "24">The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).</S>
			<S sid ="87" ssid = "25">We evaluated these results for precision, recall, F- measure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000).</S>
			<S sid ="88" ssid = "26">1 http://fjoch.com/GIZA++.html 1 0 k Prec isio n Reca llF mea sure AE R GI Z A ++ sta ndar d with SR H 0.</S>
			<S sid ="89" ssid = "27">85 6 0.</S>
			<S sid ="90" ssid = "28">87 4 0.71 8 0.72 0 0.</S>
			<S sid ="91" ssid = "29">7 8 1 0.</S>
			<S sid ="92" ssid = "30">7 8 9 0.20 7 0.19 8HM BiT AM stan dar d with SR H 0.</S>
			<S sid ="93" ssid = "31">86 9 0.</S>
			<S sid ="94" ssid = "32">88 4 0.78 8 0.79 0 0.</S>
			<S sid ="95" ssid = "33">8 2 6 0.</S>
			<S sid ="96" ssid = "34">8 3 4 0.16 9 0.16 0 Pr op os ed 0.</S>
			<S sid ="97" ssid = "35">94 1 0.80 8 0.</S>
			<S sid ="98" ssid = "36">8 7 0 0.12 3 (a) 5 0 k Prec isio n Reca llF mea sure AE R GI Z A ++ sta ndar d with SR H 0.</S>
			<S sid ="99" ssid = "37">90 5 0.</S>
			<S sid ="100" ssid = "38">90 3 0.77 0 0.75 9 0.</S>
			<S sid ="101" ssid = "39">8 3 2 0.</S>
			<S sid ="102" ssid = "40">8 2 5 0.15 6 0.16 4HM BiT AM stan dar d with SR H 0.</S>
			<S sid ="103" ssid = "41">90 1 0.</S>
			<S sid ="104" ssid = "42">89 9 0.81 4 0.80 8 0.</S>
			<S sid ="105" ssid = "43">8 5 5 0.</S>
			<S sid ="106" ssid = "44">8 5 3 0.14 0 0.14 5 Pr op os ed 0.</S>
			<S sid ="107" ssid = "45">94 7 0.82 4 0.</S>
			<S sid ="108" ssid = "46">8 8 1 0.11 2 (b) 1 0 0 k Prec isio n Reca llF mea sure AE R GI Z A ++ sta ndar d with SR H 0.</S>
			<S sid ="109" ssid = "47">92 5 0.</S>
			<S sid ="110" ssid = "48">93 4 0.79 1 0.80 3 0.</S>
			<S sid ="111" ssid = "49">8 5 3 0.</S>
			<S sid ="112" ssid = "50">8 6 4 0.13 6 0.12 6HM BiT AM stan dar d with SR H 0.</S>
			<S sid ="113" ssid = "51">89 8 0.</S>
			<S sid ="114" ssid = "52">90 9 0.85 1 0.86 0 0.</S>
			<S sid ="115" ssid = "53">8 7 4 0.</S>
			<S sid ="116" ssid = "54">8 7 9 0.12 4 0.11 4 Pr op os ed 0.</S>
			<S sid ="117" ssid = "55">92 7 0.86 2 0.</S>
			<S sid ="118" ssid = "56">8 9 3 0.10 3 (c) Table 1: Comparison of word alignment accuracy.</S>
			<S sid ="119" ssid = "57">The best results are indicated in bold type.</S>
			<S sid ="120" ssid = "58">The additional data set sizes are (a) 10k, (b) 50k, (c) 100k.</S>
			<S sid ="121" ssid = "59">4.2 Results and Discussion.</S>
			<S sid ="122" ssid = "60">Table 1 shows the word alignment accuracy of the three methods trained with 10k, 50k, and 100k additional sentence pairs.</S>
			<S sid ="123" ssid = "61">For all settings, our proposed method outperformed other conventional methods.</S>
			<S sid ="124" ssid = "62">This result shows that synonym information is effective for improving word alignment quality as we expected.</S>
			<S sid ="125" ssid = "63">As mentioned in Sections 1 and 3.1, the main idea of our proposed method is to introduce latent topics for modeling synonym pairs, and then to utilize the synonym pair model for the regularization of word alignment models.</S>
			<S sid ="126" ssid = "64">We expect the latent topics to be useful for modeling polysemous words included in synonym pairs and to enable us to incorporate synonym information effectively into word alignment models.</S>
			<S sid ="127" ssid = "65">To confirm the effect of the synonym pair model with latent topics, we also tested GIZA++ and HMBiTAM with what we call Synonym Replacement Heuristics (SRH), where all of the synonym pairs in the bilingual training sentences were simply replaced with a representative word.</S>
			<S sid ="128" ssid = "66">For instance, the words ‘sick’ and ‘ill’ in the bilingual sentences Table 2: The number of vocabularies in the 10k, 50k and 100k data sets.</S>
			<S sid ="129" ssid = "67">were replaced with the word ‘sick’.</S>
			<S sid ="130" ssid = "68">As shown in Table 2, the number of vocabularies in the English and French data sets decreased as a result of employing the SRH.</S>
			<S sid ="131" ssid = "69">We show the performance of GIZA++ and HMBiTAM with the SRH in the lines entitled “with SRH” in Table 1.</S>
			<S sid ="132" ssid = "70">The GIZA++ and HMBiTAM with the SRH slightly outperformed the standard GIZA++ and HMBiTAM for the 10k and 100k data sets, but underperformed with the 50k data set.</S>
			<S sid ="133" ssid = "71">We assume that the SRH mitigated the over- fitting of these models into low-frequency word pairs in bilingual sentences, and then improved the word alignment performance.</S>
			<S sid ="134" ssid = "72">The SRH regards all of the different words coupled with the same word in the synonym pairs as synonyms.</S>
			<S sid ="135" ssid = "73">For instance, the words ‘head’, ‘chief’ and ‘forefront’ in the bilingual sentences are replaced with ‘chief’, since (‘head’, ‘chief’) and (‘head’, ‘forefront’) are synonyms.</S>
			<S sid ="136" ssid = "74">Obviously, (‘chief’, ‘forefront’) are not synonyms, which is detrimented to word alignment.</S>
			<S sid ="137" ssid = "75">The proposed method consistently outperformed GIZA++ and HMBiTAM with the SRH in 10k, 50k and 100k data sets in F-measure.</S>
			<S sid ="138" ssid = "76">The synonym pair model in our proposed method can automatically learn that (‘head’, ‘chief’) and (‘head’, ‘forefront’) are individual synonyms with different meanings by assigning these pairs to different topics.</S>
			<S sid ="139" ssid = "77">By sharing latent topics between the synonym pair model and the word alignment model, the synonym information incorporated in the synonym pair model is used directly for training word alignment model.</S>
			<S sid ="140" ssid = "78">The experimental results show that our proposed method was effective in improving the performance of the word alignment model by using synonym pairs including such ambiguous synonym words.</S>
			<S sid ="141" ssid = "79">Finally, we discuss the data set size used for unsupervised training.</S>
			<S sid ="142" ssid = "80">As shown in Table 1, using a large number of additional sentence pairs improved the performance of all the models.</S>
			<S sid ="143" ssid = "81">In all our experimental settings, all the additional sen tence pairs and the evaluation data were selected from the Hansards data set.</S>
			<S sid ="144" ssid = "82">These experimental results show that a larger number of sentence pairs was more effective in improving word alignment performance when the sentence pairs were collected from a homogeneous data source.</S>
			<S sid ="145" ssid = "83">However, in practice, it might be difficult to collect a large number of such homogeneous sentence pairs for a specific target domain and language pair.</S>
			<S sid ="146" ssid = "84">One direction for future work is to confirm the effect of the proposed method when training the word alignment model by using a large number of sentence pairs collected from various data sources including many topics for a specific language pair.</S>
	</SECTION>
	<SECTION title="Conclusions and Future Work. " number = "5">
			<S sid ="147" ssid = "1">We proposed a novel framework that incorporates synonyms from monolingual linguistic resources in a word alignment generative model.</S>
			<S sid ="148" ssid = "2">This approach utilizes both bilingual and monolingual synonym resources effectively for word alignment.</S>
			<S sid ="149" ssid = "3">Our proposed method uses a latent topic for bilingual sentences and monolingual synonym pairs, which is helpful in terms of word sense disambiguation.</S>
			<S sid ="150" ssid = "4">Our proposed method improved word alignment quality with both small and large data sets.</S>
			<S sid ="151" ssid = "5">Future work will involve examining the proposed method for different language pairs such as EnglishChinese and EnglishJapanese and evaluating the impact of our proposed method on SMT performance.</S>
			<S sid ="152" ssid = "6">We will also apply our proposed method to a larger data sets of multiple domains since we can expect a further improvement in word alignment accuracy if we use more bilingual sentences and more monolingual knowledge.</S>
	</SECTION>
</PAPER>
