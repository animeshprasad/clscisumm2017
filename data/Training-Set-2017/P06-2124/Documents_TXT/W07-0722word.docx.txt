Domain Adaptation in Statistical Machine Translation with Mixture
Modelling ∗



Jorge Civera and Alfons Juan 
Universidad Polite´cnica de Valencia 
Camino de Vera s/n
46022 Valencia, Spain
{jorcisai,ajuan}@iti.upv.es







Abstract

Mixture modelling is a standard technique 
for density estimation, but its use in sta- 
tistical machine translation (SMT) has just 
started to be explored.  One of the main 
advantages of this technique is its capabil- 
ity to learn specific probability distributions 
that better fit subsets of the training dataset. 
This feature is even more important in SMT 
given the difficulties to translate polysemic 
terms whose semantic depends on the con- 
text in which that term appears. In this pa- 
per, we describe a mixture extension of the 
HMM alignment model and the derivation of 
Viterbi alignments to feed a state-of-the-art 
phrase-based system.  Experiments carried 
out on the Europarl and News Commentary 
corpora show the potential interest and limi- 
tations of mixture modelling.

1   Introduction

Mixture modelling is a popular approach for density 
estimation in many scientific areas (G. J. McLach- 
lan and D. Peel, 2000).  One of the most interest- 
ing properties of mixture modelling is its capability 
to model multimodal datasets by defining soft parti- 
tions on these datasets, and learning specific proba- 
bility distributions for each partition, that better ex- 
plains the general data generation process.
   ∗Work supported by  the  EC  (FEDER) and  the  Spanish 
MEC under grant TIN2006-15694-CO2-01, the Conseller´ıa 
d’Empresa, Universitat i Cie`ncia - Generalitat Valenciana un- 
der contract GV06/252, the Universidad Polite´cnica de Valen- 
cia with ILETA project and Ministerio de Educacio´ n y Ciencia.
  

In Machine Translation (MT), it is common to 
encounter large parallel corpora devoted to hetero- 
geneous topics.   These topics usually define sets 
of topic-specific lexicons that need to be translated 
taking into the semantic context in which they are 
found.   This semantic dependency problem could 
be overcome by learning topic-dependent translation 
models that capture together the semantic context 
and the translation process.
  However, there have not been until very recently 
that the application of mixture modelling in SMT 
has received increasing attention.   In (Zhao and 
Xing, 2006), three fairly sophisticated bayesian top- 
ical translation models, taking IBM Model 1 as a 
baseline model, were presented under the bilingual 
topic admixture model formalism.  These models 
capture latent topics at the document level in order to 
reduce semantic ambiguity and improve translation 
coherence.  The models proposed provide in some 
cases better word alignment and translation quality 
than HMM and IBM models on an English-Chinese 
task.  In (Civera and Juan, 2006), a mixture exten- 
sion of IBM model 2 along with a specific dynamic- 
programming decoding algorithm were proposed. 
This IBM-2 mixture model offers a significant gain 
in translation quality over the conventional IBM 
model 2 on a semi-synthetic task.
  In this work, we present a mixture extension of the 
well-known HMM alignment model first proposed 
in (Vogel and others, 1996) and refined in (Och and 
Ney, 2003). This model possesses appealing proper- 
ties among which are worth mentioning, the simplic- 
ity of the first-order word alignment distribution that 
can be made independent of absolute positions while




177
Proceedings of the Second Workshop on Statistical Machine Translation, pages 177–180, 
Prague, June 2007. Qc 2007 Association for Computational Linguistics


taking advantage of the localization phenomenon 
of word alignment in European languages, and the 
efficient and exact computation of the E-step and 
Viterbi alignment by using a dynamic-programming 
approach.  These properties have made this model 
suitable  for  extensions  (Toutanova  et  al.,  2002) 
and integration in a phrase-based model (Deng and 
Byrne, 2005) in the past.


3	Mixture of HMM alignment models
Let us suppose that p(x | y) has been generated using 
a T-component mixture of HMM alignment models:

T
p(x | y) = ) p(t | y) p(x | y, t)
t=1
T


= ) p(t | y)	)


p(x, a | y, t) 	(6)


2	HMM alignment model


t=1


a∈A(x,y)


Given a bilingual pair (x, y), where x and y are mu- 
tual translation, we incorporate the hidden variable
a = a1a2 · · · a|x| to reveal, for each source word po- 
sition j, the target word position aj  ∈ {0, 1, . . . , |y|}
to which it is connected. Thus,


In Eq. 6, we introduce mixture coefficients p(t | y)
to weight the contribution of each HMM alignment 
model in the mixture. While the term p(x, a | y, t) is
decomposed as in the original HMM model.
The assumptions of the constituent HMM mod-


p(x | y) =	)
a∈A(x,y)


p(x, a | y) 	(1)


els are the same than those of 
the previous section, but we 
obtain topic-dependent 
statistical dictionaries and 
word alignments. Apropos of 
the mixture coef-


where A(x, y) denotes the set of all possible align- 
ments between x and y.  The alignment-completed
probability P (x, a | y) can be decomposed in terms
of source position-dependent probabilities as:


ficients, we simplify these terms dropping its depen- 
dency on y, leaving as future work its inclusion in 
the model. Formally, the assumptions are:

p(t | y) ≈ p(t) 	(7)


|x|


j−1


j−1


  p(aj | t) 	j = 1


p(x, a | y) =n p(aj | aj−,1xj−,1y) p(xj | aj , xj−,1y)


p(aj | a1  , x1  , y, t) ≈


p(a	a


t) j > 1	(8)


1	1	1	1
j=1


j − j−1 |


(2)


p(xj | aj , xj−1, y, t) ≈ p(xj | ya , t) 	(9)


1	1	j
The original formulation of the HMM alignment


model assumes that each source word is connected 
to exactly one target word. This connection depends 
on the target position to which was aligned the pre-


Replacing the assumptions in Eq. 6, we obtain the
(incomplete) HMM mixture model as follows:

T


vious source word and the length of the target sen-


p(x | y) = ) p(t) 	)


p(a1 | t)×


tence. Here, we drop both dependencies in order to
simplify to a jump width alignment probability dis- 
tribution:

   p(aj ) 	j = 1


t=1
|x|
n
×
j=2


a∈A(x,y)

p(aj −aj−1 | t)



|x|
n

j=1




p(xj |yaj , t)  (10)


p(aj | aj−1, xj−1, y) ≈


(3)


1	1	p(aj −aj−1)   j > 1


and the set of unknown parameters comprises:


p(xj | aj , xj−1, y) ≈ p(xj | ya ) 	(4)


1	1	j


 p(t) 	t = 1 . . . T



  Furthermore, the treatment of the NULL word is 
the same as that presented in (Och and Ney, 2003).
Finally, the HMM alignment model is defined as:


Θ�  =  p(i | t) 	j = 1
p(i − i | t)   j > 1
 p(u | v, t) 	∀u ∈ X and v ∈ Y



(11)





|x|


|x|


X and Y, being the 
source and target 
vocabular-


p(x | y) = )


p(a1) n p(aj 
−aj−1) n 
p(xj |ya  )


ies.


a∈A(x,y)


j=2


j=1



(5)


The  
estimation of  
the  unknown 
parameters in
Eq. 10 is 
troublesome, 
since topic and 
alignment



data are missing.  Here, we revert to the EM opti- 
misation algoritm to compute these parameters.
  In order to do that, we define the complete version 
of Eq. 10 incorporating the indicator variables zt and
  

The M step finds a new estimate of Θ� , by max- 
imising Eq. 12, using the expected value of the miss- 
ing data from Eqs. 13,14 and 15 over all sample n:

N


za, uncovering, the until now hidden variables. The


1
p(t) =


znt


variable zt is a T -dimensional bit vector with 1 in
the position corresponding to the component gener- 
ating (x, y) and zeros elsewhere, while the variable


N
n=1
N
p(i | t) ∝ ) zna   t


za = za1 . . . za


|x|


where zaj


is a |y|-
dimensional bit


n=1


vector with 1 in the position corresponding to the tar-


N   |xn |


get position to which position j is aligned and zeros
elsewhere. Then, the complete model is:


p(i − i | t) ∝ ) )(zna
n=1  j=1



j−1i 


zna


ji )t


T	|y|


N   |xn | |yn |


p(x, zt, za | y) ≈ n p(t)zt  n p(i | t)za1i zt ×


p(u | v, t) ∝ ) ) ) zna


t δ(xnj , u)δ(yni, v)



|x|



|y|


t=1


i=1
|y|


n=1  
j=1  
i=1


n n
×
j=1 i=1


p(xj | yi, 
t)zaji zt


n

i =1


z
a
j
−
1
i  
z
a
j
i 
z
t


(12)


3
.
1
   
W
o
r
d
 
a
l
i
g
n
m
e
n
t
 
e
x
t
r
a
c
t
i
o
n
The 
HMM 
mixture 
model 
described 
in the 
previous 
section 
was used 
to 
generate 
Viterbi 
alignment
s on the 
training 
dataset.  
These 
optimal 
alignment
s are


Given the complete model,  the EM algorithm
works in two basic steps in each iteration:  the 
E(xpectation) step and the M(aximisation) step. At 
iteration k, the E step computes the expected value 
of  the  hidden  variables given  the  observed data
(x, y) and the estimate of the parameters Θ� (k).
The E step reduces to the computation of the ex-


the basis for phrase-based systems.
  In the original HMM model, the Viterbi align- 
ment can be efficiently computed by a dynamic-
programming algorithm with a complexity O(|x| ·
|y| ). In the mixture HMM model, we approximate
the Viterbi alignment by maximising over the com-
ponents of the mixture:


pected value of zt, zaji zt and zaj
sample n:


zaji zt for each



aˆ ≈ arg max max p(t) p(x, a | y, 
t)


|y|
zt ∝ p(t) ) α


a
 
	
t
(13)



i=1


|x|it
  

So we have that the 
complexity of the compu- tation 
of the Viterbi alignment in a T-
component


zaji zt = zaji t zt	(14)


HMM mixture model is O(T


· |x


| · |


y|2).




where


zaj


zaji zt 
= (za

|y|
)


j−1i 


zaji )t zt	(15)



4
	Expe
rimenta
l results

The data 
that was 
employed 
in the 
experime
nts to


zaji t ∝



k=1


αjktβjkt


train the HMM 
mixture model 
corresponds to 
the 
concatenation of 
the Spanish-
English 
partitions of


(zaj


zaji 
)t ∝ 
αj−
1it 
p(i 
− i 
| t) 
p(x
j 
|yi, 
t) 
βjit


the 
Europ
arl 
and 
the 
News 
Comm
entary 
corpor
a.


and the recursive functions α and β defined as:


The idea behind this decision was to let the 
mixture model distinguish which bilingual pairs 
should con-



αjit  =  |y|

k=1

βjit  =  |y|

k=1


p(
i | 
t) 
p(
xj 
| 
yi
, 
t)   
j 
= 
1
αj−1kt p(i − 
k | t) p(xj | yi, 
t)   j > 1
1	j = |x|
p(k − i | t) 
p(xj+1 | yk , 
t)βj+1kt  j < 
|x|


tribute to 
learn a given 
HMM 
component 
in the mix- 
ture. Both 
corpora were 
preprocessed 
as suggested 
for the 
baseline 
system by 
tokenizing, 
filtering sen- 
tences longer 
than 40 
words and 
lowercasing.
  Regarding 
the 
components 
of the 
translation 
sys- tem, 5-
gram 
language 
models were 
trained on 
the 
monolingual 
version of 
the corpora 
for 
English(En)


and Spanish(Es), while phrase-based models with 
lexicalized reordering model were trained using the 
Moses toolkit (P. Koehn and others, 2007), but re- 
placing the Viterbi alignments, usually provided by 
GIZA++ (Och and Ney, 2003), by those of the HMM 
mixture  model  with  training  scheme  mix 15H 5. 
This configuration was used to translate both test de- 
velopment sets, Europarl and News Commentary.
  Concerning the weights of the different models, 
we tuned those weights by minimum error rate train- 
ing and we employed the same weighting scheme 
for all the experiments in the same language pair. 
Therefore, the same weighting scheme was used 
over different number of components.
  BLEU scores are reported in Tables 1 and 2 as a 
function of the number of components in the HMM 
mixture model on the preprocessed development test 
sets of the Europarl and News Commentary corpora.

Table 1: BLEU scores on the Europarl development 
test data
T
En-Es
Es-En

Table 2: BLEU scores on the News-Commentary 
development test data
T	1	2	3	4
En-Es	29.62	30.01	30.17	29.95
Es-En	29.15	29.22	29.11	29.02


  As observed in Table 1, if we compare the BLEU 
scores of the conventional single-component HMM 
model to those of the HMM mixture model, it seems 
that there is little or no gain from incorporating 
more topics into the mixture for the Europarl cor- 
pus.   However, in Table 2,  the BLEU scores on 
the English-Spanish pair significantly increase as the 
number of components is incremented. We believe 
that this is due to the fact that the News Commen- 
tary corpus seems to have greater influence on the 
mixture model than on the single-component model, 
specializing Viterbi alignments to favour this corpus.

5   Conclusions and future work

In this work, a novel mixture version of the HMM 
alignment model was introduced.  This model was 
employed to generate topic-dependent Viterbi align-


ments that were input into a state-of-the-art phrase- 
based system.  The preliminary results reported on 
the English-Spanish partitions of the Europarl and 
News-Commentary corpora may raise some doubts 
about the applicability of mixture modelling to SMT, 
nonetheless in the advent of larger open-domain cor- 
pora, the idea behind topic-specific translation mod- 
els seem to be more than appropriate, necessary. On 
the other hand, we are fully aware that indirectly 
assessing the quality of a model through a phrase- 
based system is a difficult task because of the differ- 
ent factors involved (Ayan and Dorr, 2006).
  Finally, the main problem in mixture modelling is 
the linear growth of the set of parameters as the num- 
ber of components increases. In the HMM, and also 
in IBM models, this problem is aggravated because 
of the use of statistical dictionary entailing a large 
number of parameters. A possible solution is the im- 
plementation of interpolation techniques to smooth 
sharp distributions estimated on few events (Och and 
Ney, 2003; Zhao and Xing, 2006).


References

N. F. Ayan and B. J. Dorr. 2006. Going beyond AER: an 
extensive analysis of word alignments and their impact 
on MT. In Proc. of ACL’06, pages 9–16.

J. Civera and A. Juan. 2006. Mixtures of IBM Model 2.
In Proc. of EAMT’06, pages 159–167.

Y. Deng and W. Byrne.  2005.  HMM word and phrase 
alignment for statistical machine translation. In Proc. 
of HLT-EMNLP’05, pages 169–176.

G. J. McLachlan and D. Peel. 2000. Finite Mixture Mod- 
els. Wiley.

F. J. Och and H. Ney. 2003. A systematic comparison of 
various statistical alignment models.  Computational 
Linguistics, 29(1):19–51.

P. Koehn and others.    2007.    Moses:   Open Source 
Toolkit for Statistical Machine Translation.  In Proc. 
of ACL’07 Demo Session, page To be published.

K. Toutanova, H. T. Ilhan, and C. D. Manning.  2002.
Extensions to HMM-based statistical word alignment 
models. In Proc. of EMNLP ’02, pages 87–94.

S. Vogel et al.  1996.  HMM-based word alignment in 
statistical translation. In Proc. of CL, pages 836–841.

B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual Topic 
AdMixture Models for Word Alignment.  In Proc. of 
COLING/ACL’06.

