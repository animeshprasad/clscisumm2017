<S sid ="2" ssid = "2">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S
<S sid ="3" ssid = "3">Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).</S>
<S sid ="7" ssid = "7">Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.</S>
<S sid ="10" ssid = "10">Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.</S>
<S sid ="13" ssid = "13">For example, the word shot in â€œIt was a nice shot.â€ should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.</S>
<S sid ="15" ssid = "15">In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.</S>
<S sid ="17" ssid = "17">Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.</S>
<S sid ="21" ssid = "21">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S>
<S sid ="37" ssid = "7">The translation lexicon p(f |e) is the key component in this generative process.</S>
<S sid ="39" ssid = "9">We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.</S>
<S sid ="45" ssid = "6">Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity</S>
<S sid ="83" ssid = "44">Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ).</S>
<S sid ="115" ssid = "25">The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).</S>
<S sid ="116" ssid = "26">To reduce the data sparsity problem, we introduce two remedies in our models.</S>
<S sid ="117" ssid = "27">First: Laplace smoothing.</S>
<S sid ="119" ssid = "29">Second: interpolation smoothing.</S>
<S sid ="120" ssid = "30">Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting:</S>
<S sid ="124" ssid = "34">Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA).</S>
<S sid ="194" ssid = "67">Inter takes the intersection of the two directions and generates high-precision alignments;</S>
<S sid ="152" ssid = "25">Topic-specific translation lexicons are learned by a 3-topic BiTAM1.</S>
<S sid ="192" ssid = "65">Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE).</S>
<S sid ="203" ssid = "76">As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1∼3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1.</S>



