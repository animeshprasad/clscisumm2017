<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Mixture modelling is a standard technique for density estimation, but its use in statistical machine translation (SMT) has just started to be explored.</S>
		<S sid ="2" ssid = "2">One of the main advantages of this technique is its capability to learn specific probability distributions that better fit subsets of the training dataset.</S>
		<S sid ="3" ssid = "3">This feature is even more important in SMT given the difficulties to translate polysemic terms whose semantic depends on the context in which that term appears.</S>
		<S sid ="4" ssid = "4">In this paper, we describe a mixture extension of the HMM alignment model and the derivation of Viterbi alignments to feed a state-of-the-art phrase-based system.</S>
		<S sid ="5" ssid = "5">Experiments carried out on the Europarl and News Commentary corpora show the potential interest and limitations of mixture modelling.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Mixture modelling is a popular approach for density estimation in many scientific areas (G. J. McLachlan and D. Peel, 2000).</S>
			<S sid ="7" ssid = "7">One of the most interesting properties of mixture modelling is its capability to model multimodal datasets by defining soft partitions on these datasets, and learning specific probability distributions for each partition, that better explains the general data generation process.</S>
			<S sid ="8" ssid = "8">∗Work supported by the EC (FEDER) and the Spanish MEC under grant TIN200615694-CO201, the Conseller´ıa d’Empresa, Universitat i Cie`nciaGeneralitat Valenciana under contract GV06/252, the Universidad Polite´cnica de Valencia with ILETA project and Ministerio de Educacio´ n y Ciencia.</S>
			<S sid ="9" ssid = "9">In Machine Translation (MT), it is common to encounter large parallel corpora devoted to heterogeneous topics.</S>
			<S sid ="10" ssid = "10">These topics usually define sets of topic-specific lexicons that need to be translated taking into the semantic context in which they are found.</S>
			<S sid ="11" ssid = "11">This semantic dependency problem could be overcome by learning topic-dependent translation models that capture together the semantic context and the translation process.</S>
			<S sid ="12" ssid = "12">However, there have not been until very recently that the application of mixture modelling in SMT has received increasing attention.</S>
			<S sid ="13" ssid = "13">In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.</S>
			<S sid ="14" ssid = "14">These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.</S>
			<S sid ="15" ssid = "15">The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.</S>
			<S sid ="16" ssid = "16">In (Civera and Juan, 2006), a mixture extension of IBM model 2 along with a specific dynamic- programming decoding algorithm were proposed.</S>
			<S sid ="17" ssid = "17">This IBM2 mixture model offers a significant gain in translation quality over the conventional IBM model 2 on a semi-synthetic task.</S>
			<S sid ="18" ssid = "18">In this work, we present a mixture extension of the well-known HMM alignment model first proposed in (Vogel and others, 1996) and refined in (Och and Ney, 2003).</S>
			<S sid ="19" ssid = "19">This model possesses appealing properties among which are worth mentioning, the simplicity of the first-order word alignment distribution that can be made independent of absolute positions while 177 Proceedings of the Second Workshop on Statistical Machine Translation, pages 177–180, Prague, June 2007.</S>
			<S sid ="20" ssid = "20">Qc 2007 Association for Computational Linguistics taking advantage of the localization phenomenon of word alignment in European languages, and the efficient and exact computation of the E-step and Viterbi alignment by using a dynamic-programming approach.</S>
			<S sid ="21" ssid = "21">These properties have made this model suitable for extensions (Toutanova et al., 2002) and integration in a phrase-based model (Deng and Byrne, 2005) in the past.</S>
			<S sid ="22" ssid = "22">3 Mixture of HMM alignment models.</S>
			<S sid ="23" ssid = "23">Let us suppose that p(x | y) has been generated using a T-component mixture of HMM alignment models: T p(x | y) = ) p(t | y) p(x | y, t) t=1 T = ) p(t | y) ) p(x, a | y, t) (6)</S>
	</SECTION>
	<SECTION title="HMM alignment model. " number = "2">
			<S sid ="24" ssid = "1">t=1 a∈A(x,y) Given a bilingual pair (x, y), where x and y are mutual translation, we incorporate the hidden variable a = a1a2 · · · a|x| to reveal, for each source word position j, the target word position aj ∈ {0, 1, . . .</S>
			<S sid ="25" ssid = "2">, |y|} to which it is connected.</S>
			<S sid ="26" ssid = "3">Thus, In Eq. 6, we introduce mixture coefficients p(t | y) to weight the contribution of each HMM alignment model in the mixture.</S>
			<S sid ="27" ssid = "4">While the term p(x, a | y, t) is decomposed as in the original HMM model.The assumptions of the constituent HMM mod p(x | y) = ) a∈A(x,y) p(x, a | y) (1) els are the same than those of the previous section, but we obtain topic-dependent statistical dictionaries and word alignments.</S>
			<S sid ="28" ssid = "5">Apropos of the mixture coef where A(x, y) denotes the set of all possible alignments between x and y. The alignment-completed probability P (x, a | y) can be decomposed in terms of source position-dependent probabilities as: ficients, we simplify these terms dropping its dependency on y, leaving as future work its inclusion in the model.</S>
			<S sid ="29" ssid = "6">Formally, the assumptions are: p(t | y) ≈ p(t) (7) |x| j−1 j−1 p(aj | t) j = 1 p(x, a | y) =n p(aj | aj−,1xj−,1y) p(xj | aj , xj−,1y) p(aj | a1 , x1 , y, t) ≈ p(a a t) j &gt; 1 (8) 1 1 1 1 j=1 j − j−1 | (2) p(xj | aj , xj−1, y, t) ≈ p(xj | ya , t) (9) 1 1 j The original formulation of the HMM alignment model assumes that each source word is connected to exactly one target word.</S>
			<S sid ="30" ssid = "7">This connection depends on the target position to which was aligned the pre Replacing the assumptions in Eq. 6, we obtain the (incomplete) HMM mixture model as follows: Tvious source word and the length of the target sen p(x | y) = ) p(t) ) p(a1 | t)× tence.</S>
			<S sid ="31" ssid = "8">Here, we drop both dependencies in order to simplify to a jump width alignment probability distribution: p(aj ) j = 1 t=1 |x| n × j=2 a∈A(x,y) p(aj −aj−1 | t) |x| n j=1 p(xj |yaj , t) (10) p(aj | aj−1, xj−1, y) ≈ (3) 1 1 p(aj −aj−1) j &gt; 1 and the set of unknown parameters comprises: p(xj | aj , xj−1, y) ≈ p(xj | ya ) (4) 1 1 j  p(t) t = 1 . . .</S>
			<S sid ="32" ssid = "9">T  Furthermore, the treatment of the NULL word is the same as that presented in (Och and Ney, 2003).</S>
			<S sid ="33" ssid = "10">Finally, the HMM alignment model is defined as: Θ� =  p(i | t) j = 1 p(i − i | t) j &gt; 1  p(u | v, t) ∀u ∈ X and v ∈ Y (11) |x| |x| X and Y, being the source and target vocabular p(x | y) = ) p(a1) n p(aj −aj−1) n p(xj |ya ) ies.</S>
			<S sid ="34" ssid = "11">a∈A(x,y) j=2 j=1 (5) The estimation of the unknown parameters in Eq. 10 is troublesome, since topic and alignment data are missing.</S>
			<S sid ="35" ssid = "12">Here, we revert to the EM optimisation algoritm to compute these parameters.</S>
			<S sid ="36" ssid = "13">In order to do that, we define the complete version of Eq. 10 incorporating the indicator variables zt and The M step finds a new estimate of Θ� , by maximising Eq. 12, using the expected value of the missing data from Eqs.</S>
			<S sid ="37" ssid = "14">13,14 and 15 over all sample n: N za, uncovering, the until now hidden variables.</S>
			<S sid ="38" ssid = "15">The 1 p(t) = znt variable zt is a T -dimensional bit vector with 1 in the position corresponding to the component generating (x, y) and zeros elsewhere, while the variable N n=1 N p(i | t) ∝ ) zna t za = za1 . . .</S>
			<S sid ="39" ssid = "16">za |x| where zajis a |y| dimensional bit n=1vector with 1 in the position corresponding to the tar N |xn | get position to which position j is aligned and zeros elsewhere.</S>
			<S sid ="40" ssid = "17">Then, the complete model is: p(i − i | t) ∝ ) )(zna n=1 j=1 j−1i zna ji )t T |y| N |xn | |yn | p(x, zt, za | y) ≈ n p(t)zt n p(i | t)za1i zt × p(u | v, t) ∝ ) ) ) zna t δ(xnj , u)δ(yni, v) |x| |y| t=1 i=1 |y| n=1 j=1 i=1 n n × j=1 i=1 p(xj | yi, t)zaji zt n i =1 z a j − 1 i z a j i z t (12) 3 . 1 W o r d a l i g n m e n t e x t r a c t i o n The HMM mixture model described in the previous section was used to generate Viterbi alignment s on the training dataset.</S>
			<S sid ="41" ssid = "18">These optimal alignment s are Given the complete model, the EM algorithm works in two basic steps in each iteration: the E(xpectation) step and the M(aximisation) step.</S>
			<S sid ="42" ssid = "19">At iteration k, the E step computes the expected value of the hidden variables given the observed data (x, y) and the estimate of the parameters Θ� (k).The E step reduces to the computation of the ex the basis for phrase-based systems.</S>
			<S sid ="43" ssid = "20">In the original HMM model, the Viterbi alignment can be efficiently computed by a dynamic programming algorithm with a complexity O(|x| · |y| ).</S>
			<S sid ="44" ssid = "21">In the mixture HMM model, we approximatethe Viterbi alignment by maximising over the com ponents of the mixture: pected value of zt, zaji zt and zaj sample n: zaji zt for each aˆ ≈ arg max max p(t) p(x, a | y, t) |y| zt ∝ p(t) ) α a t (13) i=1 |x|it So we have that the complexity of the computation of the Viterbi alignment in a T component zaji zt = zaji t zt (14) HMM mixture model is O(T · |x | · | y|2).</S>
			<S sid ="45" ssid = "22">where zaj zaji zt = (za |y| ) j−1i zaji )t zt (15) 4 Expe rimenta l results The data that was employed in the experime nts to zaji t ∝ k=1 αjktβjkt train the HMM mixture model corresponds to the concatenation of the Spanish English partitions of (zaj zaji )t ∝ αj− 1it p(i − i | t) p(x j |yi, t) βjit the Europ arl and the News Comm entary corpor a. and the recursive functions α and β defined as: The idea behind this decision was to let the mixture model distinguish which bilingual pairs should con  αjit =  |y|  k=1  βjit =  |y|  k=1 p( i | t) p( xj | yi , t) j = 1 αj−1kt p(i − k | t) p(xj | yi, t) j &gt; 1 1 j = |x| p(k − i | t) p(xj+1 | yk , t)βj+1kt j &lt; |x| tribute to learn a given HMM component in the mixture.</S>
			<S sid ="46" ssid = "23">Both corpora were preprocessed as suggested for the baseline system by tokenizing, filtering sentences longer than 40 words and lowercasing.</S>
			<S sid ="47" ssid = "24">Regarding the components of the translation system, 5 gram language models were trained on the monolingual version of the corpora for English(En) and Spanish(Es), while phrase-based models with lexicalized reordering model were trained using the Moses toolkit (P. Koehn and others, 2007), but replacing the Viterbi alignments, usually provided by GIZA++ (Och and Ney, 2003), by those of the HMM mixture model with training scheme mix 15H 5.</S>
			<S sid ="48" ssid = "25">This configuration was used to translate both test development sets, Europarl and News Commentary.</S>
			<S sid ="49" ssid = "26">Concerning the weights of the different models, we tuned those weights by minimum error rate training and we employed the same weighting scheme for all the experiments in the same language pair.</S>
			<S sid ="50" ssid = "27">Therefore, the same weighting scheme was used over different number of components.</S>
			<S sid ="51" ssid = "28">BLEU scores are reported in Tables 1 and 2 as a function of the number of components in the HMM mixture model on the preprocessed development test sets of the Europarl and News Commentary corpora.</S>
			<S sid ="52" ssid = "29">Table 1: BLEU scores on the Europarl development test data T En-Es Es-En Table 2: BLEU scores on the News-Commentary development test data T 1 2 3 4 En-Es 29.62 30.01 30.17 29.95 Es-En 29.15 29.22 29.11 29.02 As observed in Table 1, if we compare the BLEU scores of the conventional single-component HMM model to those of the HMM mixture model, it seems that there is little or no gain from incorporating more topics into the mixture for the Europarl corpus.</S>
			<S sid ="53" ssid = "30">However, in Table 2, the BLEU scores on the EnglishSpanish pair significantly increase as the number of components is incremented.</S>
			<S sid ="54" ssid = "31">We believe that this is due to the fact that the News Commentary corpus seems to have greater influence on the mixture model than on the single-component model, specializing Viterbi alignments to favour this corpus.</S>
			<S sid ="55" ssid = "32">5 Conclusions and future work.</S>
			<S sid ="56" ssid = "33">In this work, a novel mixture version of the HMM alignment model was introduced.</S>
			<S sid ="57" ssid = "34">This model was employed to generate topic-dependent Viterbi align ments that were input into a state-of-the-art phrase- based system.</S>
			<S sid ="58" ssid = "35">The preliminary results reported on the EnglishSpanish partitions of the Europarl and News-Commentary corpora may raise some doubts about the applicability of mixture modelling to SMT, nonetheless in the advent of larger open-domain corpora, the idea behind topic-specific translation models seem to be more than appropriate, necessary.</S>
			<S sid ="59" ssid = "36">On the other hand, we are fully aware that indirectly assessing the quality of a model through a phrase- based system is a difficult task because of the different factors involved (Ayan and Dorr, 2006).</S>
			<S sid ="60" ssid = "37">Finally, the main problem in mixture modelling is the linear growth of the set of parameters as the number of components increases.</S>
			<S sid ="61" ssid = "38">In the HMM, and also in IBM models, this problem is aggravated because of the use of statistical dictionary entailing a large number of parameters.</S>
			<S sid ="62" ssid = "39">A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).</S>
	</SECTION>
</PAPER>
