<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information.</S>
		<S sid ="2" ssid = "2">In this paper, we propose a cache-based approach to document-level translation.</S>
		<S sid ="3" ssid = "3">Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size.</S>
		<S sid ="4" ssid = "4">In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side.</S>
		<S sid ="5" ssid = "5">In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches.</S>
		<S sid ="6" ssid = "6">Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses.</S>
		<S sid ="7" ssid = "7">Especially, detailed analysis and discussion are presented to give new insights to document-level translation.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">During last decade, tremendous work has been done to improve the quality of statistical machine * Corresponding author.</S>
			<S sid ="9" ssid = "9">translation (SMT) systems.</S>
			<S sid ="10" ssid = "10">However, there is still a huge performance gap between the state-of-the- art SMT systems and human translators.</S>
			<S sid ="11" ssid = "11">Bond (2002) suggested nine ways to improve machine translation by imitating the best practices of human translators (Nida, 1964), with parsing the entire document before translation as the first priority.</S>
			<S sid ="12" ssid = "12">However, most SMT systems still treat parallel corpora as a list of independent sentence-pairs and ignore document-level information.</S>
			<S sid ="13" ssid = "13">Document-level information can and should be used to help document-level machine translation.</S>
			<S sid ="14" ssid = "14">At least, the topic of a document can help choose specific translation candidates, since when taken out of the context from their document, some words, phrases and even sentences may be rather ambiguous and thus difficult to understand.</S>
			<S sid ="15" ssid = "15">Another advantage of document-level machine translation is its ability in keeping a consistent translation.</S>
			<S sid ="16" ssid = "16">However, document-level translation has drawn little attention from the SMT research community.</S>
			<S sid ="17" ssid = "17">The reasons are manifold.</S>
			<S sid ="18" ssid = "18">First of all, most of parallel corpora lack the annotation of document boundaries (Tam, 2007).</S>
			<S sid ="19" ssid = "19">Secondly, although it is easy to incorporate a new feature into the classical log-linear model (Och, 2003), it is difficult to capture document-level information and model it via some simple features.</S>
			<S sid ="20" ssid = "20">Thirdly, reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts.</S>
			<S sid ="21" ssid = "21">This makes the evaluation of document-level SMT systems extremely difficult.</S>
			<S sid ="22" ssid = "22">Tiedemann (2010) showed that the repetition and consistency are very important when modeling natural language and translation.</S>
			<S sid ="23" ssid = "23">He proposed to employ cache-based language and translation models in a phrase-based SMT system for domain 909 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919, Edinburgh, Scotland, UK, July 27–31, 2011.</S>
			<S sid ="24" ssid = "24">Qc 2011 Association for Computational Linguistics adaptation.</S>
			<S sid ="25" ssid = "25">Especially, the cache in the translation model dynamically grows up by adding bilingual phrase pairs from the best translation hypotheses of previous sentences.</S>
			<S sid ="26" ssid = "26">One problem with the dynamic cache is that those initial sentences in a test document may not benefit from the dynamic cache.</S>
			<S sid ="27" ssid = "27">Another problem is that the dynamic cache may be prone to noise and cause error propagation.</S>
			<S sid ="28" ssid = "28">This explains why the dynamic cache fails to much improve the performance.</S>
			<S sid ="29" ssid = "29">This paper proposes a cache-based approach for document-level SMT using a static cache and a dynamic cache.</S>
			<S sid ="30" ssid = "30">While such a approach applies to both phrase-based and syntax-based SMT, this paper focuses on phrase-based SMT.</S>
			<S sid ="31" ssid = "31">In particular, the static cache is employed to store relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their target counterparts) in the training parallel corpus while the dynamic cache is employed to store bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document.</S>
			<S sid ="32" ssid = "32">In this way, our cache-based approach can provide useful data at the beginning of the translation process via the static cache.</S>
			<S sid ="33" ssid = "33">As the translation process continues, the dynamic cache grows and contributes more and more to the translation of subsequent sentences.</S>
			<S sid ="34" ssid = "34">Our motivation to employ similar bilingual document pairs in the training parallel corpus is simple: a human translator often collects similar bilingual document pairs to help translation.</S>
			<S sid ="35" ssid = "35">If there are translation pairs of sentences/phrases/words in similar bilingual document pairs, this makes the translation much easier.</S>
			<S sid ="36" ssid = "36">Given a test document, our approach imitates this procedure by first retrieving similar bilingual document pairs from the training parallel corpus, which has often been applied in IR-based adaptation of SMT systems (Zhao et al.2004; Hildebrand et al.2005; Lu et al.2007) and then extracting bilingual phrase pairs from similar bilingual document pairs to store them in a static cache.However, such a cache-based approach may in troduce many noisy/unnecessary bilingual phrase pairs in both the static and dynamic caches.</S>
			<S sid ="37" ssid = "37">In order to resolve this problem, this paper employs a topic model to weaken those noisy/unnecessary bilingual phrase pairs by recommending the decoder to choose most likely phrase pairs according to the topic words extracted from the target-side text of similar bilingual document pairs.</S>
			<S sid ="38" ssid = "38">Just like a human translator, even with a big bilingual dictionary, is often confused when he meets a source phrase which corresponds to several possible translations.</S>
			<S sid ="39" ssid = "39">In this case, some topic words can help reduce the perplexity.</S>
			<S sid ="40" ssid = "40">In this paper, the topic words are stored in a topic cache.</S>
			<S sid ="41" ssid = "41">In some sense, it has the similar effect of employing an adaptive language model with the advantage of avoiding the interpolation of a global language model with a specific domain language model.</S>
			<S sid ="42" ssid = "42">The rest of this paper is organized as follows.</S>
			<S sid ="43" ssid = "43">Section 2 reviews the related work.</S>
			<S sid ="44" ssid = "44">Section 3 presents our cache-based approach to document- level SMT.</S>
			<S sid ="45" ssid = "45">Section 4 presents the experimental results.</S>
			<S sid ="46" ssid = "46">Session 5 gives new insights on cache- based document-level translation.</S>
			<S sid ="47" ssid = "47">Finally, we conclude this paper in Section 6.</S>
	</SECTION>
	<SECTION title="Related work. " number = "2">
			<S sid ="48" ssid = "1">There are only a few studies on document-level SMT.</S>
			<S sid ="49" ssid = "2">Representative work includes Zhao et al.</S>
			<S sid ="50" ssid = "3">(2006), Tam et al.</S>
			<S sid ="51" ssid = "4">(2007), Carpuat (2009).</S>
			<S sid ="52" ssid = "5">Zhao et al.</S>
			<S sid ="53" ssid = "6">(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.</S>
			<S sid ="54" ssid = "7">It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.</S>
			<S sid ="55" ssid = "8">Tam et al.</S>
			<S sid ="56" ssid = "9">(2007) proposed a bilingual-LSA model on the basis of a parallel document corpus and built a topic-based language model for each language.</S>
			<S sid ="57" ssid = "10">By automatically building the correspondence between the source and target language models, this method can match the topic-based language model and improve the performance of SMT.</S>
			<S sid ="58" ssid = "11">Carpuat (2009) revisited the “one sense per discourse” hypothesis of Gale et al.</S>
			<S sid ="59" ssid = "12">(1992) and gave a detailed comparison and analysis of the “one translation per discourse” hypothesis.</S>
			<S sid ="60" ssid = "13">However, she failed to propose an effective way to integrate document-level information into a SMT system.</S>
			<S sid ="61" ssid = "14">For example, she simply recommended some translation candidates to replace some target words in the post-process stage.</S>
			<S sid ="62" ssid = "15">In principle, the cache-based approach can bewell suited for document-level translation.</S>
			<S sid ="63" ssid = "16">Basical ly, the cache is analogous to “cache memory” in hardware terminology, which tracks short-term fluctuation (Iyer et al., 1999).</S>
			<S sid ="64" ssid = "17">As the cache changes with different documents, the document- level information should be capable of influencing SMT.</S>
			<S sid ="65" ssid = "18">Previous cache-based approaches mainly point to cache-based language modeling (Kuhn and Mori, 1990), which uses a large global language model to mix with a small local model estimated from recent history data.</S>
			<S sid ="66" ssid = "19">However, applying such a language model in SMT is very difficult due to the risk of introducing extra noise (Raab, 2007).</S>
			<S sid ="67" ssid = "20">For cache-based translation modeling, Nepveu et al.</S>
			<S sid ="68" ssid = "21">(2004) explored user-edited translations in the context of interactive machine translation.</S>
			<S sid ="69" ssid = "22">Tie- demann (2010) proposed to fill the cache with bi lingual phrase pairs from the best translation choosing noisy/unnecessary bilingual phrase pairs in both the static and dynamic caches is wakened with the help of the topic words in the topic cache.</S>
			<S sid ="70" ssid = "23">In particular, only the most similar document pair is used to construct the static cache and the topic cache unless specified.</S>
			<S sid ="71" ssid = "24">In this section, we first introduce the basic phrase-based SMT system and then present our cache-based approach to achieve document-level SMT with focus on constructing the caches (static, dynamic and topic) and designing their corresponding features.</S>
			<SUBSECTION>3.1 Basic phrase-based SMT system.</SUBSECTION>
			<S sid ="72" ssid = "25">It is well known that the translation process of SMT can be modeled as obtaining the best translation e of the source sentence f by maximizing following posterior probability (Brown et al., 1993):hypotheses of previous sentences in the test docu ebest = arg max P(e | f ) = arg max P( f | e)Plm (e) (1) e e ment.</S>
			<S sid ="73" ssid = "26">Both Nepveu et al.</S>
			<S sid ="74" ssid = "27">(2004) and Tiedemann (2010) also explored traditional cache-based language models and found that a cache-based language model often contributes much more than a cache-based translation model.</S>
	</SECTION>
	<SECTION title="Cache-based document-level SMT. " number = "3">
			<S sid ="75" ssid = "1">Given a test document, our system works as follows: 1) clears the static, topic and dynamic caches when switching to a new test document dx; 2) retrieves a set of most similar bilingual document pairs dds for dx from the training parallel corpus using the cosine similarity with tfidf weighting; 3) fills the static cache with bilingual phrase pairs where P(e|f) is a translation model and Plm is a language model.</S>
			<S sid ="76" ssid = "2">Our system adopted Moses (a state-of-art phrase-based SMT system) as a baseline, which follows Koehn et al.</S>
			<S sid ="77" ssid = "3">(2003) and mainly adopts six groups of popular features: 1) two phrase translation probabilities (two directions): Pphr(e|f) and Pphr(f|e); 2) two word translation probabilities (two directions) : Pw(e|f) and Pw(f|e); 3) one language model (target language): LM(e); 4) one phrase penalty (target language): PP(f); 5) one word penalty (target language):WP(e); 6) a lexicalized reordering model.</S>
			<S sid ="78" ssid = "4">Besides, the log-linear model as described in (Och and Ney, 2003) is employed to linearly interpolate these features for obtaining the best translation according to the formula (2): M extracted from dds; e = argmax{ l h (e, f )} (2) 4) fills the topic cache with topic words extracted from the target-side documents of dds;5) for each sentence in the test document, trans lates it using cache-based SMT and continuously expands the dynamic cache with bilingual phrase pairs obtained from the best translation hypothesis of the previous sentences.</S>
			<S sid ="79" ssid = "5">In this way, our cache-based approach can provide useful data at the beginning of the translation process via the static cache.</S>
			<S sid ="80" ssid = "6">As the translation process continues, the dynamic cache grows and contributes more and more to the translation of subsequent sentences.</S>
			<S sid ="81" ssid = "7">Besides, the possibility of best Â m m m=1 where hm(e , f) is a feature function, and λm is the weight of hm(e , f) optimized by a discriminative training method on a held-out development data.</S>
			<S sid ="82" ssid = "8">In principle, a phrase-based SMT system can provide the best phrase segmentation and align ment that cover a bilingual sentence pair.</S>
			<S sid ="83" ssid = "9">Here, a segmentation of a sentence into K phrases is defined as: (f~e)≈ ∑ (f ,e , ~) (3) where tuple (f ,e ) refers to a phrase pair, and ~ indicates corresponding alignment information.</S>
			<S sid ="84" ssid = "10">3.2 Dynamic Cache.</S>
			<S sid ="85" ssid = "11">Our dynamic cache is mostly inspired by Tiedemann (2010), which adopts a dynamic cache to store relevant bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document.</S>
			<S sid ="86" ssid = "12">In particular, a specific feature isincorporated S to capture useful document level information in the dynamic cache: K -∂i a sub-phrase of ei and e for a sub-phrase of e , to allow partial matching.</S>
			<S sid ="87" ssid = "13">Finally, F measures the overall value of a target candidate f by summing over the scores of K phrase pairs.</S>
			<S sid ="88" ssid = "14">Obviously, F rewards both full matching and partial matching.</S>
			<S sid ="89" ssid = "15">In order to avoid too much noise, we put some constraints on the number of words in the target phrase of &lt;ec,fc&gt; or &lt;ei,fi&gt;, such as ∥ e ∥&gt; 3 , where &quot; ∥∥ &quot; measures the number of S cache (ec | f c I ( &lt; e , f &gt; = &lt; e , f &gt; ) ¥ e ) = Â K I ( f = f ) (4)non blank chara cters in a phras e. For exam ple, if phras e pair “, 减少|| | and reduc ed” occur s in the where e-∂i is a decay factor to avoid the depen cache, phrase pair “,|||and ” is not reward ed becaus e dence of the feature’s contribution on the cache size.</S>
			<S sid ="90" ssid = "16">Given &lt;ec, fc&gt; an existing phrase pair in the dynamic cache and &lt;ei,fi&gt; a phrase pair in a newhypothesis, if ( ei=ec ∧ fi=fc ) is true (i.e. full match ing), function I(.)</S>
			<S sid ="91" ssid = "17">returns 1 , otherwise 0.One problem with the dynamic cache in Tiede mann (2010) is that it continuously updates the weight of a phrase pair in the dynamic cache.</S>
			<S sid ="92" ssid = "18">This may cause noticeable computational burden with the increasing number of phrase pairs in the dynamic cache.</S>
			<S sid ="93" ssid = "19">In addition, as a source phrase (fc) may occur many times in the dynamic cache, the weights for related phrase pairs may degrade severely and thus his decoder needs a decay factor, which is difficult to optimize.</S>
			<S sid ="94" ssid = "20">Finally, Tiedemann (2010) only allowed full matching.</S>
			<S sid ="95" ssid = "21">This largely lowers down the probability of hitting the dynamic cache and thus much affects its effectiveness.</S>
			<S sid ="96" ssid = "22">To overcome above problems, we only employ the bilingual phrase pairs in the dynamic cache to inform the decoder whether one bilingual phrase pair exists in the dynamic cache or not, which is slightly similar to (Nepveu et al, 2004) ,thus avoiding extra computational burden and the fine-tuning of the decay factor.</S>
			<S sid ="97" ssid = "23">In particular, following new feature is incorporated to better explore the dynamic cache: = ∑ dpairmatch(e ,f ) (5) where dpairmatch( , ) ⎧ 1 (e = e ∧ f = f ) = ∨ e = e ∧ f = f ∧∥ e ∥&gt; 3 ⎨ ∨ e = e ∧ f = f ∧∥ e ∥&gt; 3 ⎩0 other Here, F is called the dynamic cache feature.</S>
			<S sid ="98" ssid = "24">Assume (ec,fc ) is a phrase pair in the dynamic cache and (ei,fi) is a phrase pair candidate for anew hypothesis.</S>
			<S sid ="99" ssid = "25">Besides full matching, we intro duce a symbol of “^” for sub-phrase, such as e for such shorter phrase pairs occur frequently and maylargely degrade the effect of the cache.</S>
			<S sid ="100" ssid = "26">In accor dance, the dynamic cache only contains phrase pairs whose target phrases contain 4 or more non- blank characters.</S>
			<S sid ="101" ssid = "27">3.3 Static Cache.</S>
			<S sid ="102" ssid = "28">In Tiedemann (2010), initial sentences in a test document fail to benefit from the dynamic cache due to the lack of contents in the dynamic cache at the beginning of the translation process.</S>
			<S sid ="103" ssid = "29">To overcome this problem, a static cache is included to store relevant bilingual phrase pairs extracted from similar bilingual document pairs in the training parallel corpus.</S>
			<S sid ="104" ssid = "30">In particular, a static cache feature F is designed to capture useful information in the static cache in the same way as the dynamic cache feature, shown in Formula (5).</S>
			<S sid ="105" ssid = "31">For this purpose, all the document pairs in the training parallel corpus are aligned at the phrase level using 2-fold cross-validation.</S>
			<S sid ="106" ssid = "32">That is, we adopt 50% of the training parallel corpus to train a model using Moses and apply the model to enforce phrase alignment of the remaining training data, and vice versa.</S>
			<S sid ="107" ssid = "33">Here, the enforcement is done by guaranteeing the occurrence of the target phrase candidate of a source phrase in the sentence pair.</S>
			<S sid ="108" ssid = "34">Besides, all the words pairs trained on the whole training parallel corpus are included in both folds to ensure at least one possible translation.</S>
			<S sid ="109" ssid = "35">Finally, the phrase pairs in the best translation hypothesis of a sentence pair is retrieved from the decoder.</S>
			<S sid ="110" ssid = "36">In this way, we can extract a set of phrase pairs for each bilingual document pairs.</S>
			<S sid ="111" ssid = "37">Given a test document, we first find a set of similar source documents by computing the Cosine similarity using the TFIDF weighting scheme and their corresponding target documents, from the training parallel corpus.</S>
			<S sid ="112" ssid = "38">Then, the phrase pairs ex tracted from these similar bilingual document pairs are collected into the static cache.</S>
			<S sid ="113" ssid = "39">To avoid noise, we filter out those phrase pairs which occur less than two times in the training parallel corpus.</S>
			<S sid ="114" ssid = "40">general language model and added additional two adaptive lexicon probabilities in his phrase table.</S>
			<S sid ="115" ssid = "41">In principle, LDA is a probabilistic model of text data, which provides a generative analog of PLSA (Blei et al., 2003), and is primarily meant to 出口 ||| exports 放慢 ||| slowdown 股市 ||| stock market 现行 ||| leading 出口 增幅 ||| export growth 多种 原因 ||| various reasons 汇率 ||| exchange 活力 ||| vitality 加快 ||| speed up the 经济学家 ||| economists reveal hidden topics in text documents.</S>
			<S sid ="116" ssid = "42">Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (i.e. the “bag-of-words” assumption).</S>
			<S sid ="117" ssid = "43">国家 著名 ||| a well-known international 议会 委员会 ||| congressional committee 不 乐观 的 预期 ||| pessimistic predictions 保持 一定 的 增长 ||| maintain a certain growth 美元 汇率 下跌 ||| a drop in the dollar exchange rate Table 1: Phrase pairs extracted from a document pair with an economic topic Similar to the dynamic cache, we only consider those phrase pairs whose target phrases contain 4 or more non-blank characters to avoid noise.</S>
			<S sid ="118" ssid = "44">We do not deliberately remove long phrase pairs.</S>
			<S sid ="119" ssid = "45">It is possible to use these long phrase pairs if our test document is very similar to one training document pair.</S>
			<S sid ="120" ssid = "46">Table 1 shows some bilingual phrase pairs extracted from a document pair, which reports a piece of news about “impact on slowdown in US economic growth”.</S>
			<S sid ="121" ssid = "47">Obviously, these phrase pairs are closely related to economics.</S>
			<S sid ="122" ssid = "48">3.4 Topic Cache.</S>
			<S sid ="123" ssid = "49">Both the dynamic and static caches may still introduce noisy/unnecessary bilingual phrase pairs even with constraints on the length of phrases and their occurrence frequency in the training parallel corpus.</S>
			<S sid ="124" ssid = "50">In order to resolve this problem, this paper adopts a topic cache to store relevant topic words and employs a topic cache feature to weaken those noisy/unnecessary phrase pairs.</S>
			<S sid ="125" ssid = "51">Given w is a topic word in the topic cache, the topic cache feature F is designed as follows: = topicexist(e ,f ) (6) where topicexist(e ,f ) = 1 (w ∈ e ) 0 other Here, the target phrase which contains a topic wordw will be rewarded.</S>
			<S sid ="126" ssid = "52">w is derived by a topic model, LDA (Latent Dirichlet Allocation).</S>
			<S sid ="127" ssid = "53">This is dif ferent from the previous work (Tam, 2007), which mainly interpolated a topic language model with a Figure 1 shows the principle of LDA, where α is the parameter of the uniform Dirichlet prior on the per-document topic distributions, β is the parameter of the uniform Dirichlet prior on the per-topic word distribution, θi is the topic distribution for document i, zij is the topic for the jth word in document i, and wij is the specific word.</S>
			<S sid ="128" ssid = "54">Among all variables, wij is the only observable variable with all the other variables latent.</S>
			<S sid ="129" ssid = "55">In particular, K denotes the number of topics considered in the model and φ is a K*V (V is the dimension of the vocabulary) Markov matrix each line of which denotes the word distribution of a topic.</S>
			<S sid ="130" ssid = "56">The inner plate over z and w illustrates the repeated sampling of topics and words until N words have been generated for document d. The plate surrounding θ illustrates the sampling of a distribution over topics for each document d for a total of M documents.</S>
			<S sid ="131" ssid = "57">The plate surrounding φ illustrates the repeated sampling of word distributions for each topic z until K topics have been generated.</S>
			<S sid ="132" ssid = "58">Figure 1: LDA We use a LDA tool1 to build a topic model using the target-side documents in the training parallel corpus.</S>
			<S sid ="133" ssid = "59">Using LDA, we can obtain the topic distribution of each word w, namely p(z|w) for topic zϵK.</S>
			<S sid ="134" ssid = "60">Moreover, using the obtained word topic dis tributions, we can infer the topic distribution of a new document, namely p(z|d) for each topic z ϵK.</S>
			<S sid ="135" ssid = "61">Given a test document, we first find the most similar source document from the training data in 1 http://www.arbylon.net/projects/ the same way as done in the static cache.</S>
			<S sid ="136" ssid = "62">After that, we retrieve its corresponding target document.</S>
			<S sid ="137" ssid = "63">Then, the topic of the target document is determined by its major topic, with the maximum p(z|d).</S>
			<S sid ="138" ssid = "64">Finally, we load some topic words corresponding to this topic z into the topic cache.</S>
			<S sid ="139" ssid = "65">In particular, our LDA model deploy the setting of K=15, α=0.5 and β=0.1.</S>
			<S sid ="140" ssid = "66">Besides, only top 1000 topic words are reserved for each topic.</S>
			<S sid ="141" ssid = "67">Table 2 shows top 10 topic words for five topics.</S>
			<S sid ="142" ssid = "68">In this paper, we use FBIS as the training data, the 2003 NIST MT evaluation test data as the de-.</S>
			<S sid ="143" ssid = "69">velopment data, and the 2005 NIST MT test data as the test data.</S>
			<S sid ="144" ssid = "70">Table 3 shows the statistics of these data sets (with document boundaries annotated).</S>
			<S sid ="145" ssid = "71">Table 2: Topic words extracted from target-side documents</S>
	</SECTION>
	<SECTION title="Experimentation. " number = "4">
			<S sid ="146" ssid = "1">We have systematically evaluated our cache-based approach to document-level SMT on the ChineseEnglish translation task.</S>
			<S sid ="147" ssid = "2">4.1 Experimental Setting.</S>
			<S sid ="148" ssid = "3">Here, we use SRI language modeling toolkit to train a trigram general language model on English newswire text, mostly from the Xinhua portion of the Gigaword corpus (2007) and performed word alignment on the training parallel corpus using GIZA++(Och and Ney,2000) in two directions.</S>
			<S sid ="149" ssid = "4">For evaluation, the NIST BLEU script (version 13) with the default setting is used to calculate the Bleu score (Papineni et al. 2002), which measures case-insensitive matching of n-grams with n up to 4.</S>
			<S sid ="150" ssid = "5">To see whether an improvement is statistically.</S>
			<S sid ="151" ssid = "6">significant, we also conduct significance tests using the paired bootstrap approach (Koehn, 2004)2.</S>
			<S sid ="152" ssid = "7">In this paper, ‘***’, ‘**’, and ‘*’ denote p-values less than or equal to 0.01, in-between (0.01, 0.05), and bigger than 0.05, which mean significantly better, moderately better and slightly better, respectively.</S>
			<S sid ="153" ssid = "8">2 http://www.ark.cs.cmu.edu/MT Table 3: Corpus statistics In particular, the sizes of the static, topic and dynamic caches are fine-tuned to 2000, 1000 and 5000 items, respectively.</S>
			<S sid ="154" ssid = "9">For the dynamic cache, we only keep those most recently-visited items, while for the static cache; we always keep the most frequently-occurring items.</S>
			<S sid ="155" ssid = "10">4.2 Experimental Results.</S>
			<S sid ="156" ssid = "11">Table 4 shows the contribution of various caches in our cache-based document-level SMT system.</S>
			<S sid ="157" ssid = "12">The column of “BLEU_W” means the BLEU score computed over the whole test set and “BLEU_D” corresponds to the average BLEU score over separated documents.</S>
			<S sid ="158" ssid = "13">Sys tem BL EU on De v( %) BLE U on Test( %) BL EU _W NI ST BL EU _D Mo ses 29.</S>
			<S sid ="159" ssid = "14">87 25.</S>
			<S sid ="160" ssid = "15">76 7.7 84 25.</S>
			<S sid ="161" ssid = "16">08 Fd 29.</S>
			<S sid ="162" ssid = "17">90 26.</S>
			<S sid ="163" ssid = "18">03 (*) 7.8 52 25.</S>
			<S sid ="164" ssid = "19">39 Fd+ Fs 30.</S>
			<S sid ="165" ssid = "20">29 26.</S>
			<S sid ="166" ssid = "21">30 (**) 7.8 84 25.</S>
			<S sid ="167" ssid = "22">86 Fd+ Ft 30.</S>
			<S sid ="168" ssid = "23">11 26.</S>
			<S sid ="169" ssid = "24">24 (**) 7.8 71 25.</S>
			<S sid ="170" ssid = "25">74 Fd+ Fs+ Ft 30.</S>
			<S sid ="171" ssid = "26">50 26.</S>
			<S sid ="172" ssid = "27">42 (** *) 7.8 96 26.</S>
			<S sid ="173" ssid = "28">11 Fd+ Fs+ Ft wi th me rg - in g 26.</S>
			<S sid ="174" ssid = "29">57 (** *) 7.9 01 26.</S>
			<S sid ="175" ssid = "30">32 Table 4: Contribution of various caches in our cache- based document-level SMT system.</S>
			<S sid ="176" ssid = "31">Note that significance tests are done against Moses.</S>
			<S sid ="177" ssid = "32">Contribution of dynamical cache (Fd) Table 4 shows that the dynamic cache slightly improves the performance by 0.27 (*) in BLEU_W.</S>
			<S sid ="178" ssid = "33">This is similar to Tiedemann (2010).</S>
			<S sid ="179" ssid = "34">However, detailed analysis indicates that the dynamic cache does have negative effect on about one third of documents, largely due to the instability of the dynamic cache at the beginning of translating a document.</S>
			<S sid ="180" ssid = "35">Figure 2 shows the distribution of the BLEU_D difference of 100 test documents (sorted by BLEU_D).</S>
			<S sid ="181" ssid = "36">It shows that about 55% of test documents benefit from the dynamic cache.</S>
			<S sid ="182" ssid = "37">8.00% effectiveness of combining the dynamic and topic caches (sorted by BLEU_D).</S>
			<S sid ="183" ssid = "38">10.00% 8.00% 6.00% 4.00% 2.00% 0.00% fdmoses 6.00% 4.00% 2.00% fd+ftmoses -2.00% 1 8 15 22 29 36 43 50 57 64 71 78 85 92 994.00% -6.00% -8.00% Figure 2: Contribution of employing the dynamic cache on different test documents Contribution of static cache (Fs) Table 4 shows that the combination of the static cache with the dynamic cache further improves the performance by 0.27(*) in BLEU_W.</S>
			<S sid ="184" ssid = "39">This suggests the effectiveness of the static cache in eliminating the instability of the dynamic cache when translating first few sentences of a test document.</S>
			<S sid ="185" ssid = "40">Together, the dynamic and static caches much improve the performance by 0.54 (**) in BLEU_W over Moses.</S>
			<S sid ="186" ssid = "41">Figure 3 shows the distribution of the BLEU_D difference of 100 test documents (sorted by BLEU_D), with more positive effect on those borderline documents, compared to Figure 2.</S>
			<S sid ="187" ssid = "42">0.00% 1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 972.00% -4.00% -6.00% Figure 4: Contribution of combining the dynamic and topic caches However, detailed analysis shows that the topic cache and the static cache are quite complementary by contributing on different test documents, largely due to that while the static cache tends to keep translation consistent, the topic cache plays like a document-specific language model.</S>
			<S sid ="188" ssid = "43">This is justified by Table 4 that the combination of the dynamic, static and topic caches significantly improve the performance by 0.66 (***) in BLEU_W, and by Figure 5 that about 75% of test documents benefit from the combination of the three caches (sorted by BLEU_D).</S>
			<S sid ="189" ssid = "44">10.00% 6.00% 4.00%fd+fs moses 8.00% 6.00% 4.00% fd+fs+ftmoses 2.00% 2.00% 0.00% -2.00% 1 8 15 22 29 36 43 50 57 64 71 78 85 92 99 0.00% -2.00% -4.00% 1 9 17 25 33 41 49 57 65 73 81 89 974.00% -6.00% Figure 3: Contribution of combining the dynamic and static cache on different test documents Contribution of topic cache (Ft) Table 4 shows that the topic cache has comparable effect on improving the performance as the static cache when combined with the dynamic cache (0.48 vs. 0.54 in BLEU_W).</S>
			<S sid ="190" ssid = "45">Figure 4 shows the Figure 5: Contribution of combining the three caches Contribution of merging phrase pairs of similar document pairs Here, the number of similar documents we adopt is different from previous experiments.</S>
			<S sid ="191" ssid = "46">In the previous experiments, we only cache bilingual phrase pairs extracted from the most similar document.</S>
			<S sid ="192" ssid = "47">Here, we merge phrase pairs for several most similar documents (5 at most) which have the same topic.</S>
			<S sid ="193" ssid = "48">Table 4 shows that employing this trick can further improve the performance by 0.15 in BLEU_W.</S>
			<S sid ="194" ssid = "49">As a result, the cache-based approach significantly improve the performance by 0.81 (***) in BLEU_W over Moses.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "5">
			<S sid ="195" ssid = "1">In this section, we explore in more depth why the static cache can help the dynamic cache, some constrained factors which impact the effectiveness of our cache-based approach.</S>
			<S sid ="196" ssid = "2">Effectiveness of the static cache We investigate why the static cache affects the performance.</S>
			<S sid ="197" ssid = "3">Basically, it is difficult for the dynamic cache to capture such similar information in the static cache.</S>
			<S sid ="198" ssid = "4">In principle, the static cache can both influence the initial and subsequent sentences; however subsequent ones can be affected by multiple caches.</S>
			<S sid ="199" ssid = "5">In order to give an insight of the static cache, we evaluate its effectiveness on the first sentence for each test document.</S>
			<S sid ="200" ssid = "6">Figure 6 shows the contribution of the static cache on translating these first sentences (y-axis shows BLEU value of the first sentence for each test document).</S>
			<S sid ="201" ssid = "7">It notes that the most BLEU scores of them are zeros because of the length limitation of first sentences.</S>
			<S sid ="202" ssid = "8">0.9 0.8 quency of the dynamic cache is 504 on whole test sets, this figure increases to 685 with the static cache.</S>
			<S sid ="203" ssid = "9">This means that the static cache significantly enlarges the effectiveness of the dynamic cache by including more relevant phrase pairs to the dynamic cache, largely due to the positive impact of the static cache on the initial sentences of each test document.</S>
			<S sid ="204" ssid = "10">Size of topic cache Table 5 shows the impact of the topic cache when the number of the retained topic words for each topic increases from 500 to 2000.</S>
			<S sid ="205" ssid = "11">It shows that too more topic words actually harm the performance, due to the increase of noise.</S>
			<S sid ="206" ssid = "12">1000 topic words seem a lot largely due to that we didn’t do stemming for our topic modeling since we hope to introduce some tense information of them in the future.</S>
			<S sid ="207" ssid = "13">Nu mb er of top ic wo rds B L E U _ W 50 0 26.</S>
			<S sid ="208" ssid = "14">27 70 0 26.</S>
			<S sid ="209" ssid = "15">31 10 00 26.</S>
			<S sid ="210" ssid = "16">42 15 00 26.</S>
			<S sid ="211" ssid = "17">23 20 00 26.</S>
			<S sid ="212" ssid = "18">19 Table 5: Impact of the topic cache size Influenced translations In order to explore how our cache-based system impacts on translation results, we manually in 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 F d F d F s 0 20 40 60 80 100 120 spe cte d 5 doc um ent s res pec tive ly whi ch is im- pro ved or deg rad ed in tra nsl atio n qua lity co mp are d to the bas elin e Mo ses out put.</S>
			<S sid ="213" ssid = "19">Th ose doc um ent s h a v e 1 0 7 s e n t e n c e s i n s u m . T he goo d effe ctiv enes s of eac h kind of cac he can be obs erve d by the exa mpl e 1 and 2 sho wed in Tab le 6.</S>
			<S sid ="214" ssid = "20">Bot h the exa mpl e 1 and 2 com e fro m the sam e doc ume nt who se “BL EU _D” scor e exc eeds Mo ses with 8.4 poin t. The exa mpl e 1 ben efits fro m the topi c cac he whi ch cont ains the ite m of “act ion” . Figure 6: Contribution of the static cache on the first sentence of each test document (i.e. with empty dynamic cache) Furthermore, we count the hit (matching) frequency of the static cache for each test documents.</S>
			<S sid ="215" ssid = "21">Since we use 1 or 0 for the static cache feature, it is easy to retrieve its effect for each test document.</S>
			<S sid ="216" ssid = "22">Our statistics shows that the hit frequency on static cache fluctuates between 5 and 18 for each test document.</S>
			<S sid ="217" ssid = "23">Without the static cache, the hit fre The example 2 benefits from the static cache which contains a phrase pair of “承诺||| promised to” while Moses use “commitment” for “承诺” , which may be the reason for missing the part of “prime minister” in Moses output.</S>
			<S sid ="218" ssid = "24">Furthermore, due to thephrase pair of “停火 协议||| the ceasefire agree ment” existing in our static cache, our decoder keeps using “ceasefire” to translate “停火” in the whole document while Moses randomly use “ceasefire” or “ceasefire” for this translation.</S>
			<S sid ="219" ssid = "25">1 官 员 预 测 “ 准 备 工 作 将 会 进 行 到 七 月 , 然 后 再 展 开 政 治 动 作 ” Mo ses : offi cial for eca sts sai d that pre par ato ry wo rk wil l be car rie d out in jul y and the n lau nch ed a pol itic al ma ne uve r . Ou rs: offi cial for eca sts sai d that pre par ato ry wo rk wil l be car rie d out in jul y , the n beg an a pol itic al act ion . Re fer enc e: offi cial s exp ect ed that &quot;pr epa rati ons wo uld tak e pla ce unt il Jul y, afte r whi ch pol itic al acti on wil l beg in&quot;.</S>
			<S sid ="220" ssid = "26">2 关 于 这 一 点 , 中 东 新 闻 社 说 , 以 色 列 总 理 夏 隆 承 诺 “ 只 要 巴 勒 斯 坦 当 局 尊 重 停 火 协 议 , 控 制 好 它 们 的 地 方 , 以 色 列 将 会 停 止 对 巴 勒 斯 坦 人 的 军 事 行 动 ” 。 Mo ses : on this poi nt , sai d that isra eli co m mit me nt to the pal esti nia n aut hor itie s to res pec t the cease fire agr ee me nt , wh ere the y are wel l und er con trol , isra el wil l sto p its mil itar y acti ons aga inst pal esti nia ns . Ou rs: on this poi nt , sai d that isra eli pri me mi nis ter pro mi sed to res pec t the cea sefi re agr ee me nt , the pal esti nia n au tho riti es to pro per ly con trol thei r are as and isra el wil l sto p its mil itar y acti ons aga inst pal esti nia ns . Re fer enc e:F or this poi nt , ME NA sai d Isra eli Pri me Mi nist er Sha ron has pro mis ed to &quot; sto p Isra eli mil itar y ope rati ons aga inst the Pal esti nia ns ins ofa r as the y con tin ue to res pec t the cea sefi re dea l and con trol thei r terr itor y . &quot; 3 17 日 晚 ， 近 300 0 多 名 市 民 在 市 中 心 的 武 器 广 场 观 看 了 由 市 政 府 举 办 的 精 彩 纷 呈 的 歌 舞 晚 会 ， 五 颜 六 色 的 灯 光 装 扮 着 广 场 周 围 的 古 老 建 筑 ， 著 名 歌 舞 艺 术 家 们 表 演 了 不 同 地 区 的 民 族 歌 舞 。 Mo ses : on the eve nin g , nea rly 3,0 00 resi den ts in the do wnt ow n squ are of the we apo ns hel d by the mu nici pal gov ern m en t , w at ch ed a so ng an d da nc e so ire e , ha vi ng co lo rf ul lig hti ng di sg ui se of an ci en t bu ild in gs ar ou nd th e sq ua re , si ng - in g an d da nc in g fa m ou s art ist s st ag ed di ffe re nt re gi on s of et hn ic so ng an d da nc e . Ou rs: late r on, nea rly 3,0 00 resi den ts in the do wnt ow n squ are to wat ch the gov ern me nt of hav ing a son g and dan ce pe rf or m an ce s w er e he ld un de r th e di sg ui se of co lor ful lig hti ng ar ou nd th e sq ua re , a fa m ou s an ci en t bu ild in gs an d lo ca l art ist s of dif fer en t et hn ic so ng an d da nc e . Re fer enc e: On the nig ht of the 17t h , nea rly 3,0 00 resi den ts wat che d a wo nde rful gal a of son gs and dan ces , org ani zed by th e m un ici pa l go ve rn m en t , at Pl az a da Ar m as . C ol or ful lig ht s lig ht ed up an ci en t ar ch ite ct ur e ar ou nd th e pl az a . Fa m ou s art ist s in cl ud in g si ng er s an d da nc er s sta ge d pe rf or m an ce s of na tio na l so ng s an d da nc es of dif fe re nt re gi on s . 4 利 马 的 城 市 面 积 已 从 建 城 之 初 的 2.1 4 平 方 公 里 发 展 到 260 0 多 平 方 公 里 ， 而 人 口 也 增 加 到 80 0 万 左 右 ， 约 占 全 国 总 人 口 的 31% 。 Mo ses : at lim a &apos;s urb an are a fro m the beg inn ing of 260 0 squ are to 2.1 4 mil lio n squ are kil om ete rs , whi le the pop ula tio n has inc rea sed to 8 per cen t of the cou ntr y &apos;s tota l , abo ut 31 % . Ou rs: lim a , the urb an are a fro m the beg inn ing of 260 0 squ are kil om ete rs to 2.1 4 mil lio n squ are kil om ete rs , but als o inc rea sed to abo ut 8 mil lio n pop ulat ion , the cou ntr y &apos;s tota l pop ulat ion of abo ut 31 % . Re fer enc e: Th e are a of Li ma city has exp and ed to mo re tha n 2,6 00 squ are kilo met ers fro m the ori gin al 2.1 4 squ are ki lo met ers wh en the city wa s fou nde d , whi le the pop ulat ion has inc rea sed to aro und 8 mil lio n , rou ghl y acc oun tin g for 31 % of the nati on&apos;s tota l . Table 6: Positive and negative examples The example 3 and 4 also come from the same document however whose performance degrades with 2.17 point.</S>
			<S sid ="221" ssid = "27">We don’t think the translation quality for example 4 in our system is worse than Moses.</S>
			<S sid ="222" ssid = "28">However, the translation quality for example 3 in our system is very bad and especially showed on “reordering”.</S>
			<S sid ="223" ssid = "29">We found this sentence did not match any item in our static cache and topic cache.</S>
			<S sid ="224" ssid = "30">Although this phenomenon also happens in other documents, but this is the most typical negative example among these documents.</S>
			<S sid ="225" ssid = "31">Document-specific characteristics It seems that using the same weight for the whole test sets (all documents) is not very reasonable.</S>
			<S sid ="226" ssid = "32">Actually, if we can determine those negative documents which are not suitable for the cache-based approach, our cache-based approach may gain much improvement.</S>
			<S sid ="227" ssid = "33">Tiedemann (2010) explored the correlation to document length, baseline performance and source document repetition.</S>
			<S sid ="228" ssid = "34">Howev er, it seems that there are no obvious rules to filter out those negative documents.</S>
			<S sid ="229" ssid = "35">Besides, there may be two more document-specific factors: repetition of the reference text and document style.</S>
			<S sid ="230" ssid = "36">Tiedemann (2010) only considered the repetition of the test text in the source side.</S>
			<S sid ="231" ssid = "37">Since BLEU score is computed against the reference text, the repetition in the reference text may greatly influence the performance of our cache-based approach to document-level SMT.</S>
			<S sid ="232" ssid = "38">As for document style, it is quite possible that a document may contain several topics.</S>
			<S sid ="233" ssid = "39">Therefore, it may be useful to track such change over topics and refresh various caches when there is a topic change.</S>
			<S sid ="234" ssid = "40">We will leave the above issues to the future work.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="235" ssid = "1">We have shown that our cache-based approach significantly improves the performance with the help of various caches, such as the dynamic, static and topic caches, although the cache-based ap proach may introduce some negative impact on BLEU scores for certain documents.In the future, we will further explore how to re flect document divergence during training and dynamically adjust cache weights according to different documents.</S>
			<S sid ="236" ssid = "2">There are many useful components in training documents, such as named entity, event and co- reference.</S>
			<S sid ="237" ssid = "3">In this experiment, we only adopt the flat data in our cache.</S>
			<S sid ="238" ssid = "4">However, the structured data may improve the correctness of matching and thus effectively avoid noise.</S>
			<S sid ="239" ssid = "5">We will explore more effective ways to pick up various kinds of useful information from the training parallel corpus to expand our cache-based approach.</S>
			<S sid ="240" ssid = "6">Besides, we will resort to comparable corpora to enlarge our cache- based approach to document-level SMT.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="241" ssid = "7">This research was supported by Projects 90920004, 60873150, and 61003155 under the National Natural Science Foundation of China, Project 20093201110006 under the Specialized Research Fund for the Doctoral Program of Higher Education of China.</S>
	</SECTION>
</PAPER>
