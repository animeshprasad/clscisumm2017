<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We propose a novel approach to crosslingual language model (LM) adaptation based on bilingual Latent Semantic Analysis (bLSA).</S>
		<S sid ="2" ssid = "2">A bLSA model is introduced which enables latent topic distributions to be efficiently transferred across languages by enforcing a one-to-one topic correspondence during training.</S>
		<S sid ="3" ssid = "3">Using the proposed bLSA framework crosslingual LM adaptation can be performed by, first, inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language N-gram LM via marginal adaptation.</S>
		<S sid ="4" ssid = "4">The proposed framework also enables rapid bootstrapping of LSA models for new languages based on a source LSA model from another language.</S>
		<S sid ="5" ssid = "5">On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27% for a unigram LM and up to 13.6% for a 4-gram LM.</S>
		<S sid ="6" ssid = "6">Furthermore, the proposed approach consistently improved machine translation quality on both speech and text based adaptation.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Language model adaptation is crucial to numerous speech and translation tasks as it enables higher- level contextual information to be effectively incorporated into a background LM improving recognition or translation performance.</S>
			<S sid ="8" ssid = "8">One approach is 520 to employ Latent Semantic Analysis (LSA) to capture in-domain word unigram distributions which are then integrated into the background N-gram LM.</S>
			<S sid ="9" ssid = "9">This approach has been successfully applied in automatic speech recognition (ASR) (Tam and Schultz, 2006) using the Latent Dirichlet Allocation (LDA) (Blei et al., 2003).</S>
			<S sid ="10" ssid = "10">The LDA model can be viewed as a Bayesian topic mixture model with the topic mixture weights drawn from a Dirichlet distribution.</S>
			<S sid ="11" ssid = "11">For LM adaptation, the topic mixture weights are estimated based on in-domain adaptation text (e.g. ASR hypotheses).</S>
			<S sid ="12" ssid = "12">The adapted mixture weights are then used to interpolate a topic- dependent unigram LM, which is finally integrated into the background N-gram LM using marginal adaptation (Kneser et al., 1997) In this paper, we propose a framework to perform LM adaptation across languages, enabling the adaptation of a LM from one language based on the adaptation text of another language.</S>
			<S sid ="13" ssid = "13">In statistical machine translation (SMT), one approach is to apply LM adaptation on the target language based on an initial translation of input references (Kim and Khudanpur, 2003; Paulik et al., 2005).</S>
			<S sid ="14" ssid = "14">This scheme is limited by the coverage of the translation model, and overall by the quality of translation.</S>
			<S sid ="15" ssid = "15">Since this approach only allows to apply LM adaptation after translation, available knowledge cannot be applied to extend the coverage.</S>
			<S sid ="16" ssid = "16">We propose a bilingual LSA model (bLSA) for crosslingual LM adaptation that can be applied before translation.</S>
			<S sid ="17" ssid = "17">The bLSA model consists of two LSA models: one for each side of the language trained on parallel document corpora.</S>
			<S sid ="18" ssid = "18">The key property of the bLSA model is that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 520–527, Prague, Czech Republic, June 2007.</S>
			<S sid ="19" ssid = "19">Qc 2007 Association for Computational Linguistics the latent topic of the source and target LSA models can be assumed to be a one-to-one correspondence and thus share a common latent topic space since the training corpora consist of bilingual parallel data.</S>
			<S sid ="20" ssid = "20">For instance, say topic 10 of the Chinese LSA model is about politics.</S>
			<S sid ="21" ssid = "21">Then topic 10 of the English LSA model is set to also correspond to politics and so forth.</S>
			<S sid ="22" ssid = "22">During LM adaptation, we first infer the topic mixture weights from the source text using the source LSA model.</S>
			<S sid ="23" ssid = "23">Then we transfer the inferred mixture weights to the target LSA model and thus obtain the target LSA marginals.</S>
			<S sid ="24" ssid = "24">The challenge is to enforce the one-to-one topic correspon dence.</S>
			<S sid ="25" ssid = "25">Our proposal is to share common variational ASR hypo Chinese ASR Chinese−&gt;English SMT Chinese N−gram LM English N−gram LM Adapt Adapt Topic distribution Chinese LSA English LSA Chinese text English text Chinese−English Parallel document corpus MT hypo Dirichlet posteriors over the topic mixture weights of a document pair in the LDA-style model.</S>
			<S sid ="26" ssid = "26">The beauty of the bLSA framework is that the model searches for a common latent topic space in an unsupervised fashion, rather than to require manual interaction.</S>
			<S sid ="27" ssid = "27">Since the topic space is language independent, our approach supports topic transfer in multiple language pairs in O(N) where N is the number of languages.</S>
			<S sid ="28" ssid = "28">Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).</S>
			<S sid ="29" ssid = "29">Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.</S>
			<S sid ="30" ssid = "30">On the other hand, the bLSA framework models P r(c|k) and P r(e|k) which is different from the BiTAM model.</S>
			<S sid ="31" ssid = "31">By their different modeling nature, the bLSA model usually supports more topics than the BiTAM model.</S>
			<S sid ="32" ssid = "32">Another work by (Kim and Khudanpur, 2004) employed crosslingual LSA using singular value decomposition which concatenates bilingual documents into a single input supervector before projection.</S>
			<S sid ="33" ssid = "33">We organize the paper as follows: In Section 2, we introduce the bLSA framework including Latent Dirichlet-Tree Allocation (LDTA) (Tam and Schultz, 2007) as a correlated LSA model, bLSA training and crosslingual LM adaptation.</S>
			<S sid ="34" ssid = "34">In Section 3, we present the effect of LM adaptation on word perplexity, followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3.</S>
			<S sid ="35" ssid = "35">Section 4 describes conclusions and fu Figure 1: Topic transfer in bilingual LSA model.</S>
			<S sid ="36" ssid = "36">ture works.</S>
	</SECTION>
	<SECTION title="Bilingual Latent Semantic Analysis. " number = "2">
			<S sid ="37" ssid = "1">The goal of a bLSA model is to enforce a one- to-one topic correspondence between monolingual LSA models, each of which can be modeled using an LDA-style model.</S>
			<S sid ="38" ssid = "2">The role of the bLSA model is to transfer the inferred latent topic distribution from the source language to the target language assuming that the topic distributions on both sides are identical.</S>
			<S sid ="39" ssid = "3">The assumption is reasonable for parallel document pairs which are faithful translations.</S>
			<S sid ="40" ssid = "4">Figure 1 illustrates the idea of topic transfer between monolingual LSA models followed by LM adaptation.</S>
			<S sid ="41" ssid = "5">One observation is that the topic transfer can be bidirectional meaning that the “flow” of topic can be from ASR to SMT or vice versa.</S>
			<S sid ="42" ssid = "6">In this paper, we only focus on ASR-to-SMT direction.</S>
			<S sid ="43" ssid = "7">Our target is to minimize the word perplexity on the target language through LM adaptation.</S>
			<S sid ="44" ssid = "8">Before we introduce the heuristic of enforcing a one-to-one topic correspondence, we describe the Latent Dirichlet- Tree Allocation (LDTA) for LSA.</S>
			<S sid ="45" ssid = "9">2.1 Latent Dirichlet-Tree Allocation.</S>
			<S sid ="46" ssid = "10">The LDTA model extends the LDA model in which correlation among latent topics are captured using a Dirichlet-Tree prior.</S>
			<S sid ="47" ssid = "11">Figure 2 illustrates a depth-two Dirichlet-Tree.</S>
			<S sid ="48" ssid = "12">A tree of depth one simply falls back to the LDA model.</S>
			<S sid ="49" ssid = "13">The LDTA model is a generative model with the following generative process: 1.</S>
			<S sid ="50" ssid = "14">Sample a vector of branch probabilities bj ∼.</S>
			<S sid ="51" ssid = "15">j=1 Dir(.)</S>
			<S sid ="52" ssid = "16">j=2 j=3 Similar to LDA training, we apply the variational Bayes approach by optimizing the lower bound of the marginalized document likelihood: p(wn , zn, bJ ; Λ) Dir(.)</S>
			<S sid ="53" ssid = "17">Dir(.)</S>
			<S sid ="54" ssid = "18">Dir(.)</S>
			<S sid ="55" ssid = "19">L(wn ; Λ, Γ) = Eq [log 1 1 1 ] 1 q(zn , bJ ; Γ) 1 1 p(zn|bJ ) = Eq [log p(wn |zn)] + Eq [log 1 1 ] Latent topics topic 1 topic 4 topic K 1 1 q(zn ) Figure 2: Dirichlet-Tree prior of depth two.Dir(αj ) for each node j = 1...J where αj de +Eq [log p(bJ ; {αj }) q(bJ ; {γj }) notes the parameter (aka the pseudo-counts ofwhere q(zn , bJ ; Γ) = Tin q(zi ) · TiJ q(bj ) is a fac 1 1 i jits outgoing branches) of the Dirichlet distribu tion at node j. 2.</S>
			<S sid ="56" ssid = "20">Compute the topic proportions as:.</S>
			<S sid ="57" ssid = "21">θk = bδjc (k) jc torizable variational posterior distribution over the latent variables parameterized by Γ which are determined in the E-step.</S>
			<S sid ="58" ssid = "22">Λ is the model parameters fora Dirichlet-Tree {αj } and the topic-dependent uni gram LM {βwk }.</S>
			<S sid ="59" ssid = "23">The LDTA model has an E-step similar to the LDA model: E-Step: where δjc(k) is an indicator function which sets n K to unity when the c-th branch of the j-th node γjc = αjc + ) ) qik · δjc(k) (2) leads to the leaf node of topic k and zero other- i k wise.</S>
			<S sid ="60" ssid = "24">The k-th topic proportion θk is computed as the product of branch probabilities from the root node to the leaf node of topic k. where qik ∝ βw k · eEq [log θk ] (3)</S>
	</SECTION>
	<SECTION title="Generate a document using the topic multino-. " number = "3">
			<S sid ="61" ssid = "1">mial for each word wi: Eq [log θk ] = ) δjc(k)Eq [log bjc] jc ( \ zi ∼ M ult(θ) = ) δjc(k) jc Ψ(γjc) − Ψ() γjc) c wi ∼ M ult(β.zi ) where qik denotes q(zi = k) meaning the variational where β.zi denotes the topic-dependent uni- gram LM indexed by zi.</S>
			<S sid ="62" ssid = "2">The joint distribution of the latent variables (topic topic posterior of word wi.</S>
			<S sid ="63" ssid = "3">Eqn 2 and Eqn 3 are executed iteratively until convergence is reached.</S>
			<S sid ="64" ssid = "4">M-Step: n sequence zn and the Dirichlet nodes over child βwk ∝ ) qik · δ(wi , w) (4) branches bj ) and an observed document wn can be written as follows: n i where δ(wi , w) is a Kronecker Delta function.</S>
			<S sid ="65" ssid = "5">The alpha parameters can be estimated with iterative p(wn, zn , bJ ) = p(bJ |{αj }) βw z · θz methods such as Newton-Raphson or simple gradi 1 1 1 1 J i i i i ent ascent procedure.</S>
			<S sid ="66" ssid = "6">where p(bJ |{αj }) = Dir(bj ; αj ) j 2.2 Bilingual LSA.</S>
			<S sid ="67" ssid = "7">training For the following explanations, we assume that our αjc −1 ∝ jc jc source and target languages are Chinese and English respectively.</S>
			<S sid ="68" ssid = "8">The bLSA model training is a two-stage procedure.</S>
			<S sid ="69" ssid = "9">At the first stage, we train a Chinese LSA model using the Chinese docu LM: ( P rldta(w) βments in parallel corpora.</S>
			<S sid ="70" ssid = "10">We applied the varia tional EM algorithm (Eqn 2–4) to train a Chinese P ra(w|h) ∝ P rbg (w) · P rbg (w|h) (7) LSA model.</S>
			<S sid ="71" ssid = "11">Then we used the model to compute the term eEq [log θk ] needed in Eqn 3 for each Chinese document in parallel corpora.</S>
			<S sid ="72" ssid = "12">At the second stage, we apply the same eEq [log θk ] to bootstrap an English LSA model, which is the key to enforce a one-to-one topic correspondence.</S>
			<S sid ="73" ssid = "13">Now the hyper-parameters of the variational Dirichlet posteriors of each node in the Dirichlet-Tree are shared among the Chinese and English model.</S>
			<S sid ="74" ssid = "14">Precisely, we apply only Eqn 3 with fixed eEq [log θk ] in the E-step and Eqn 4 in the M-stepon {βwk } to bootstrap an English LSA model.</S>
			<S sid ="75" ssid = "15">No tice that the E-step is non-iterative resulting in rapid LSA training.</S>
			<S sid ="76" ssid = "16">In short, given a monolingual LSA model, we can rapidly bootstrap LSA models of new languages using parallel document corpora.</S>
			<S sid ="77" ssid = "17">Notice that the English and Chinese vocabulary sizes do not need to be similar.</S>
			<S sid ="78" ssid = "18">In our setup, the Chinese vocabulary comes from the ASR system while the English vocabulary comes from the English part of the parallel corpora.</S>
			<S sid ="79" ssid = "19">Since the topic transfer can be bidirectional, we can perform the bLSA training in a reverse manner, i.e. training an English LSA model followed by bootstrapping a Chinese LSA model.</S>
			<S sid ="80" ssid = "20">2.3 Crosslingual LM adaptation.</S>
			<S sid ="81" ssid = "21">Given a source text, we apply the E-step to estimate variational Dirichlet posterior of each node in the Dirichlet-Tree.</S>
			<S sid ="82" ssid = "22">We estimate the topic weights on the source language using the following equation: δjc (k) Likewise, LM adaptation can take place on the source language as well due to the bidirectional nature of the bLSA framework when target-side adaptation text is available.</S>
			<S sid ="83" ssid = "23">In this paper, we focus on LM adaptation on the target language for SMT.</S>
			<S sid ="84" ssid = "24">3 Experimental Setup.</S>
			<S sid ="85" ssid = "25">We evaluated our bLSA model using the Chinese– English parallel document corpora consisting of the Xinhua news, Hong Kong news and Sina news.</S>
			<S sid ="86" ssid = "26">The combined corpora contains 67k parallel documents with 35M Chinese (CH) words and 43M English (EN) words.</S>
			<S sid ="87" ssid = "27">Our spoken language translation system translates from Chinese to English.</S>
			<S sid ="88" ssid = "28">The Chinese vocabulary comes from the ASR decoder while the English vocabulary is derived from the English portion of the parallel training corpora.</S>
			<S sid ="89" ssid = "29">The vocabulary sizes for Chinese and English are 108k and 69k respectively.</S>
			<S sid ="90" ssid = "30">Our background English LM is a 4-gram LM trained with the modified KneserNey smoothing scheme using the SRILM toolkit on the same training text.</S>
			<S sid ="91" ssid = "31">We explore the bLSA training in both directions: EN→CH and CH→EN meaning that an English LSA model is trained first and a Chinese LSA model is bootstrapped or vice versa.</S>
			<S sid ="92" ssid = "32">Experiments explore which bootstrapping direction yield best results measured in terms of English word perplexity.</S>
			<S sid ="93" ssid = "33">The number of latent topics is set to 200 and a balanced binary Dirichlet-Tree prior is used.With an increasing interest in the ASRSMT coupling for spoken language translation, we also eval ˆ(C H ) k ∝ ( γjc jc c′ γjc′ (5) uated our approach with Chinese ASR hypotheses and compared with Chinese manual transcriptions.</S>
			<S sid ="94" ssid = "34">We are interested to see the impact due to recog Then we apply the topic weights into the target LSA model to obtain an in-domain LSA marginals: K nition errors on the ASR hypotheses compared to the manual transcriptions.</S>
			<S sid ="95" ssid = "35">We employed the CMU- InterACT ASR system developed for the GALE P rEN (w) = ) β(EN ) · θˆ(C H ) (6) 2006 evaluation.</S>
			<S sid ="96" ssid = "36">We trained acoustic models with wk k k=1 We integrate the LSA marginal into the target background LM using marginal adaptation (Kneser et al., 1997) which minimizes the KullbackLeibler divergence between the adapted LM and the background over 500 hours of quickly transcribed speech data released by the GALE program and the LM with over 800M-word Chinese corpora.</S>
			<S sid ="97" ssid = "37">The character error rates on the CCTV, RFA and NTDTV shows in the RT04 test set are 7.4%, 25.5% and 13.1% respectively.</S>
			<S sid ="98" ssid = "38">Topi c inde x T o p w o r d s “ C H 4 0 ” “ E N 4 0 ” fl y i n g , s u b m a r i n e , a i r c r a f t , a i r , p i l o t , l a n d , m i s s i o n , b r a n d n e w a i r , s e a , s u b m a r i n e , a i r c r a f t , f l i g h t , f l y i n g , s h i p , t e s t “ C H 4 1 ” “ E N 4 1 ” sat e l l i t e , h a n t i a n , l a u n c h , s p a c e , c h i n a , t e c h n o l o g y , a s t r o n o m y s p a c e , s a t e l l i t e , c h i n a , t e c h n o l o g y , s a t e l l i t e s , s c i e n c e “ C H 4 2 ” “ E N 4 2 ” f i r e , a i r p o r t , s e r v i c e s , m a r i n e , a c c i d e n t , a i r f i r e , a i r p o r t , s e r v i c e s , d e p a r t m e n t , m a r i n e , a i r , s e r v i c e Table 1: Parallel topics extracted by the bLSA model.</S>
			<S sid ="99" ssid = "39">Top words on the Chinese side are translated into English for illustration purpose.</S>
			<S sid ="100" ssid = "40">-2.7e+082.75e+082.8e+082.85e+082.9e+082.95e+083e+083.05e+08 b o o t s t r a p p e d E N L S A m o n o l i n g u a l E N L S A 2 4 6 8 10 12 14 16 18 20 Table 2: Engli sh word perple xity (PPL) on the RT04 test set using a unigr am LM.</S>
			<S sid ="101" ssid = "41">3 . 2 L M a d a p t a t i o n r e s u l t s We traine d the bLSA model s on both CH→ EN and EN→ CH direct ions and comp ared their LM adapta tion perfor manc e using the Chine se ASR hypothe ses (hypo ) and the manu al transc riptio ns (ref) as input.</S>
			<S sid ="102" ssid = "42">We adapt ed the Engli sh backg round LM using the LSA margi nals descri bed in Secti on 2.3 for each # of training iterations Figure 3: Comparison of training log likelihood of English LSA models bootstrapped from a Chinese LSA and from a flat monolingual English LSA.</S>
			<S sid ="103" ssid = "43">3.1 Analysis of the bLSA model.</S>
			<S sid ="104" ssid = "44">By examining the top-words of the extracted parallel topics, we verify the validity of the heuristic described in Section 2.2 which enforces a one-to-one topic correspondence in the bLSA model.</S>
			<S sid ="105" ssid = "45">Table 1 shows the latent topics extracted by the CH→EN bLSA model.</S>
			<S sid ="106" ssid = "46">We can see that the ChineseEnglish topic words have strong correlations.</S>
			<S sid ="107" ssid = "47">Many of them are actually translation pairs with similar word rankings.</S>
			<S sid ="108" ssid = "48">From this viewpoint, we can interpret bLSA as a crosslingual word trigger model.</S>
			<S sid ="109" ssid = "49">The result indicates that our heuristic is effective to extract parallel latent topics.</S>
			<S sid ="110" ssid = "50">As a sanity check, we also examine the likelihood of the training data when an English LSA model is bootstrapped.</S>
			<S sid ="111" ssid = "51">We can see from Figure 3 that the likelihood increases monotonically with the number of training iterations.</S>
			<S sid ="112" ssid = "52">The figure also shows that by sharing the variational Dirichlet posteriors from the Chinese LSA model, we can bootstrap an English LSA model rapidly compared to monolingual English LSA training with both training procedures started from the same flat model.</S>
			<S sid ="113" ssid = "53">show on the test set.</S>
			<S sid ="114" ssid = "54">We first evaluated the English word perplexity using the EN unigram LM generated by the bLSA model.</S>
			<S sid ="115" ssid = "55">Table 2 shows that the bLSA-based LM adaptation reduces the word perplexity by over 27% relative compared to an unadapted EN unigram LM.</S>
			<S sid ="116" ssid = "56">The results indicate that the bLSA model successfully leverages the text from the source language and improves the word perplexity on the target language.</S>
			<S sid ="117" ssid = "57">We observe that there is almost no performance difference when either the ASR hypotheses or the manual transcriptions are used for adaptation.</S>
			<S sid ="118" ssid = "58">The result is encouraging since the bLSA model may be insensitive to moderate recognition errors through the projection of the input adaptation text into the latent topic space.</S>
			<S sid ="119" ssid = "59">We also apply an English translation reference for adaptation to show an oracle performance.</S>
			<S sid ="120" ssid = "60">The results using the Chinese hypotheses are not too far off from the oracle performance.</S>
			<S sid ="121" ssid = "61">Another observation is that the CH→EN bLSA model seems to give better performance than the EN→CH bLSA model.</S>
			<S sid ="122" ssid = "62">However, their differences are not signifi cant.</S>
			<S sid ="123" ssid = "63">The result may imply that the direction of the bLSA training is not important since the latent topic space captured by either language is similar when parallel training corpora are used.</S>
			<S sid ="124" ssid = "64">Table 3 shows the word perplexity when the background 4-gram English LM is adapted with the tuning parameter β set L M ( 4 3 M , β = 0 . 7 ) C C T V R F A N T D T V B G E N 4 g r a m 1 1 8 2 1 2 2 0 3 + C H → E N ( C H r e f) + E N → C H ( C H r e f) 1 0 2 1 0 2 1 9 1 1 9 8 1 7 9 1 7 9 + C H → E N (C H hy po ) + E N → C H (C H hy po ) 1 0 2 1 0 3 1 9 3 1 9 8 1 8 0 1 8 0 + C H → E N ( E N r e f) + E N → C H ( E N r e f) 1 0 0 1 0 1 1 8 6 1 9 0 1 7 6 1 7 6 Table 3: English word perplexity (PPL) on the RT04 test set using a 4-gram LM.</S>
			<S sid ="125" ssid = "65">CCTV (CER=7.4%) rated the proposed approach into our state-of-the-art phrase-based SMT system.</S>
			<S sid ="126" ssid = "66">Translation performance was evaluated on the RT04 broadcast news evaluation set when applied to both the manual transcriptions and 1-best ASR hypotheses.</S>
			<S sid ="127" ssid = "67">During evaluation two performance metrics, BLEU (Papineni et al., 2002) and NIST, were computed.</S>
			<S sid ="128" ssid = "68">In both cases, a single English reference was used during scoring.</S>
			<S sid ="129" ssid = "69">In the transcription case the original English references were used.</S>
			<S sid ="130" ssid = "70">For the ASR case, as utterance segmentation was performed automatically, the number of sentences generated by ASR and SMT differed from 125 120 B G 4 g r a m +bLSA (CH reference) +bLSA (CH ASR hypo) +bLSA (EN reference ) the nu mb er of Eng lish refe ren ces.</S>
			<S sid ="131" ssid = "71">In this cas e, Levens htei n alig nm ent was use d to alig n the tran slati on out put to the Eng lish refe ren ces bef ore scor ing.</S>
			<S sid ="132" ssid = "72">115 110 3.4 Baseline SMT Setup.</S>
			<S sid ="133" ssid = "73">105 100 0.1 0 .2 0 .3 0 .4 0 .5 0 .6 0 .7 0 .8 0 .9 1 B e t a The bas elin e SM T syst em con sist ed of a non ada p- tive syst em trai ned usin g the sam e Chi nese Eng lish par alle l doc um ent cor por a use d in the pre vio us ex- peri me nts (Se ctio ns 3.1 and 3.2) . For phr ase extrac Figure 4: Word perplexity with different β using manual reference or ASR hypotheses on CCTV.</S>
			<S sid ="134" ssid = "74">to 0.7.</S>
			<S sid ="135" ssid = "75">Figure 4 shows the change of perplexity with different β.</S>
			<S sid ="136" ssid = "76">We see that the adaptation performance using the ASR hypotheses or the manual transcriptions are almost identical on different β with an optimal value at around 0.7.</S>
			<S sid ="137" ssid = "77">The results show that the proposed approach successfully reduces the perplexity in the range of 9–13.6% relative compared to an unadapted baseline on different shows when ASR hypotheses are used.</S>
			<S sid ="138" ssid = "78">Moreover, we observe similar performance using ASR hypotheses or manual Chinese transcriptions which is consistent with the results on Table 2.</S>
			<S sid ="139" ssid = "79">On the other hand, it is interesting to see that the performance gap from the oracle adaptation is somewhat related to the degree of mismatch between the test show and the training condition.</S>
			<S sid ="140" ssid = "80">The gap looks wider on the RFA and NTDTV shows compared to the CCTV show.</S>
			<S sid ="141" ssid = "81">3.3 Incorporating bLSA into Spoken Language.</S>
			<S sid ="142" ssid = "82">Translation To investigate the effectiveness of bLSA LM adaptation for spoken language translation, we incorpo tion a cleaned subset of these corpora, consisting of 1M ChineseEnglish sentence pairs, was used.</S>
			<S sid ="143" ssid = "83">SMT decoding parameters were optimized using manual transcriptions and translations of 272 utterances from the RT04 development set (LDC2006E10).</S>
			<S sid ="144" ssid = "84">SMT translation was performed in two stages using an approach similar to that in (Vogel, 2003).</S>
			<S sid ="145" ssid = "85">First, a translation lattice was constructed by matching all possible bilingual phrase-pairs, extracted from the training corpora, to the input sentence.</S>
			<S sid ="146" ssid = "86">Phrase extraction was performed using the “PESA” (Phrase Pair Extraction as Sentence Splitting) approach described in (Vogel, 2005).</S>
			<S sid ="147" ssid = "87">Next, a search was performed to find the best path through the lattice, i.e. that with maximum translation-score.</S>
			<S sid ="148" ssid = "88">During search reordering was allowed on the target language side.</S>
			<S sid ="149" ssid = "89">The final translation result was that hypothesis with maximum translation-score, which is a log-linear combination of 10 scores consisting of Target LM probability, Distortion Penalty, Word-Count Penalty, Phrase-Count and six Phrase- Alignment scores.</S>
			<S sid ="150" ssid = "90">Weights for each component score were optimized to maximize BLEU-score on the development set using MER optimization as described in (Venugopal et al., 2005).</S>
			<S sid ="151" ssid = "91">S M T T a r g e t L M T ra n sl at io n Q u al it y - B L E U ( N I S T ) C C T V R F A N T D T V A L L M a n u a l T r a n s c r i p t i o n B a s e l i n e L M : 0.</S>
			<S sid ="152" ssid = "92">16 2 (5.</S>
			<S sid ="153" ssid = "93">21 2) 0.</S>
			<S sid ="154" ssid = "94">08 7 (3.</S>
			<S sid ="155" ssid = "95">85 4) 0.</S>
			<S sid ="156" ssid = "96">14 0 (4.</S>
			<S sid ="157" ssid = "97">85 9) 0.</S>
			<S sid ="158" ssid = "98">13 2 (5.</S>
			<S sid ="159" ssid = "99">14 6) b L S A (b L SA A da pt ed L M ): 0.</S>
			<S sid ="160" ssid = "100">16 4 (5.</S>
			<S sid ="161" ssid = "101">21 2) 0.</S>
			<S sid ="162" ssid = "102">08 7 (3.</S>
			<S sid ="163" ssid = "103">89 7) 0.</S>
			<S sid ="164" ssid = "104">14 3 (4.</S>
			<S sid ="165" ssid = "105">86 4) 0.</S>
			<S sid ="166" ssid = "106">13 4 (5.</S>
			<S sid ="167" ssid = "107">16 2) 1 b e s t A S R O u t p u t C E R ( % ) 7 . 4 2 5 . 5 1 3 . 1 1 4 . 9 B a s e l i n e L M : 0.</S>
			<S sid ="168" ssid = "108">1 2 9 (4 .1 5) 0.</S>
			<S sid ="169" ssid = "109">0 5 1 (2 .7 7) 0.</S>
			<S sid ="170" ssid = "110">0 8 6 (3 .5 0) 0.</S>
			<S sid ="171" ssid = "111">0 9 5 (3 .9 0) b L S A (b L SA A da pt ed L M ): 0.</S>
			<S sid ="172" ssid = "112">1 3 2 (4 .1 6) 0.</S>
			<S sid ="173" ssid = "113">0 5 0 (2 .7 9) 0.</S>
			<S sid ="174" ssid = "114">0 8 9 (3 .5 3) 0.</S>
			<S sid ="175" ssid = "115">0 9 6 (3 .9 1) Table 4: Translation performance of baseline and bLSA-Adapted ChineseEnglish SMT systems on manual transcriptions and 1-best ASR hypotheses 3.5 Performance of Baseline SMT System.</S>
			<S sid ="176" ssid = "116">First, the baseline system performance was evaluated by applying the system described above to the reference transcriptions and 1-best ASR hypotheses generated by our Mandarin speech recognition system.</S>
			<S sid ="177" ssid = "117">The translation accuracy in terms of BLEU and NIST for each individual show (“CCTV ”, “RFA”, and “NTDTV ”), and for the complete test-set, are shown in Table 4 (Baseline LM).</S>
			<S sid ="178" ssid = "118">When applied to the reference transcriptions an overall BLEU score of 0.132 was obtained.</S>
			<S sid ="179" ssid = "119">BLEU-scores ranged between 0.087 and 0.162 for the “RFA”, “NTDTV ” and “CCTV ” shows, respectively.</S>
			<S sid ="180" ssid = "120">As the “RFA” show contained a large segment of conversational speech, 0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 Baseline-LM bLSA Adapted LM CCTV RFA NTDTV All shows translation quality was considerably lower for this show due to genre mismatch with the training corpora of newspaper text.</S>
			<S sid ="181" ssid = "121">For the 1-best ASR hypotheses, an overall BLEU score of 0.095 was achieved.</S>
			<S sid ="182" ssid = "122">For the ASR case, the relative reduction in BLEU scores for the RFA and NTDTV shows is large, due to the significantly lower recognition accuracies for these shows.</S>
			<S sid ="183" ssid = "123">BLEU score is also degraded due to poor alignment of references during scoring.</S>
			<S sid ="184" ssid = "124">3.6 Incorporation of bLSA Adaptation.</S>
			<S sid ="185" ssid = "125">Next, the effectiveness of bLSA based LM adaptation was evaluated.</S>
			<S sid ="186" ssid = "126">For each show the target English LM was adapted using bLSA-adaptation, as described in Section 2.3.</S>
			<S sid ="187" ssid = "127">SMT was then applied using an identical setup to that used in the baseline experiments.</S>
			<S sid ="188" ssid = "128">The translation accuracy when bLSA adaptation was incorporated is shown in Table 4.</S>
			<S sid ="189" ssid = "129">When ap Figure 5: BLEU score for those 25% utterances which resulted in different translations after bLSA adaptation (manual transcriptions) plied to the manual transcriptions, bLSA adaptation improved the overall BLEU-score by 1.7% relative (from 0.132 to 0.134).</S>
			<S sid ="190" ssid = "130">For all three shows bLSA adaptation gained higher BLEU and NIST metrics.</S>
			<S sid ="191" ssid = "131">A similar trend was also observed when the proposed approach was applied to the 1-best ASR output.</S>
			<S sid ="192" ssid = "132">On the evaluation set a relative improvement in BLEU score of 1.0% was gained.</S>
			<S sid ="193" ssid = "133">The semantic interpretation of the majority of utterances in broadcast news are not affected by topic context.</S>
			<S sid ="194" ssid = "134">In the experimental evaluation it was observed that only 25% of utterances produced different translation output when bLSA adaptation was performed compared to the topic-independent baseline.</S>
			<S sid ="195" ssid = "135">Although the improvement in translation quality (BLEU) was small when evaluated over the entire test set, the improvement in BLEU score for these 25% utterances was significant.</S>
			<S sid ="196" ssid = "136">The translation quality for the baseline and bLSA-adaptive system when evaluated only on these utterances is shown in Figure 5 for the manual transcription case.</S>
			<S sid ="197" ssid = "137">On this subset of utterances an overall improvement in BLEU of 0.007 (5.7% relative) was gained, with a gain of 0.012 (10.6% relative) points for the “NTDTV ” show.</S>
			<S sid ="198" ssid = "138">A similar trend was observed when applied to the 1-best ASR output.</S>
			<S sid ="199" ssid = "139">In this case a relative improvement in BLEU of 12.6% was gained for “NTDTV ”, and for “All shows” 0.007 (3.7%) was gained.</S>
			<S sid ="200" ssid = "140">Current evaluation metrics for translation, such as “BLEU”, do not consider the relative importance of specific words or phrases during translation and thus are unable to highlight the true effectiveness of the proposed approach.</S>
			<S sid ="201" ssid = "141">In future work, we intend to investigate other evaluation metrics which consider the relative informational content of words.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "4">
			<S sid ="202" ssid = "1">We proposed a bilingual latent semantic model for crosslingual LM adaptation in spoken language translation.</S>
			<S sid ="203" ssid = "2">The bLSA model consists of a set of monolingual LSA models in which a one-to-one topic correspondence is enforced between the LSA models through the sharing of variational Dirichlet posteriors.</S>
			<S sid ="204" ssid = "3">Bootstrapping a LSA model for a new language can be performed rapidly with topic transfer from a well-trained LSA model of another language.</S>
			<S sid ="205" ssid = "4">We transfer the inferred topic distribution from the input source text to the target language effectively to obtain an in-domain target LSA marginals for LM adaptation.</S>
			<S sid ="206" ssid = "5">Results showed that our approach significantly reduces the word perplexity on the target language in both cases using ASR hypotheses and manual transcripts.</S>
			<S sid ="207" ssid = "6">Interestingly, the adaptation performance is not much affected when ASR hypotheses were used.</S>
			<S sid ="208" ssid = "7">We evaluated the adapted LM on SMT and found that the evaluation metrics are crucial to reflect the actual improvement in performance.</S>
			<S sid ="209" ssid = "8">Future directions include the exploration of story-dependent LM adaptation with automatic story segmentation instead of show-dependent adaptation due to the possibility of multiple stories within a show.</S>
			<S sid ="210" ssid = "9">We will investigate the incorporation of monolingual documents for po tentially better bilingual LSA modeling.</S>
	</SECTION>
	<SECTION title="Acknowledgment">
			<S sid ="211" ssid = "10">This work is partly supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.</S>
			<S sid ="212" ssid = "11">HR001106-20001.</S>
			<S sid ="213" ssid = "12">Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.</S>
	</SECTION>
</PAPER>
