Cache-based Document-level Statistical Machine Translation




Zhengxian Gong1	Min Zhang2	Guodong Zhou1*
1 School of Computer Science and Technology
Soochow University, Suzhou, China 215006
2 Institute for Infocomm Research, Singapore 138632
{zhxgong, gdzhou}@suda.edu.cn	mzhang@i2r.a-star.edu.sg






Abstract

Statistical machine translation systems are 
usually trained on a large amount of bilingual 
sentence pairs and translate one sentence at a 
time, ignoring document-level information. In 
this paper, we propose a cache-based approach 
to document-level translation. Since caches 
mainly depend on relevant data to supervise 
subsequent decisions, it is critical to fill the 
caches with highly-relevant data of a reasonable 
size. In this paper, we present three kinds of 
caches to store relevant document-level infor- 
mation: 1) a dynamic cache, which stores bilin- 
gual phrase pairs from the best translation 
hypotheses of previous sentences in the test 
document; 2) a static cache, which stores rele- 
vant bilingual phrase pairs extracted from simi- 
lar bilingual document pairs (i.e. source 
documents  similar  to  the  test  document  and 
their corresponding target documents) in the 
training parallel corpus; 3) a topic cache, which 
stores the target-side topic words related with 
the test document in the source-side. In particu- 
lar, three new features are designed to explore 
various kinds of document-level information in 
above three kinds of caches. Evaluation shows 
the effectiveness of our cache-based approach 
to document-level translation with the perfor- 
mance  improvement  of  0.81  in  BLUE  score 
over Moses. Especially, detailed analysis and 
discussion are presented to give new insights to 
document-level translation.

1	Introduction

During  last  decade,  tremendous  work  has  been 
done to improve the quality of statistical machine

* Corresponding author.


translation (SMT) systems. However, there is still 
a huge performance gap between the state-of-the- 
art SMT systems and human translators. Bond 
(2002) suggested nine ways to improve machine 
translation by imitating the best practices of human 
translators (Nida, 1964), with parsing the entire 
document before translation as the first priority. 
However, most SMT systems still treat parallel 
corpora as a list of independent sentence-pairs and 
ignore document-level information.
  Document-level information can and should be 
used to help document-level machine translation. 
At least, the topic of a document can help choose 
specific translation candidates,  since when taken 
out  of  the  context  from  their  document,  some 
words, phrases and even sentences may be rather 
ambiguous and thus difficult to understand. Anoth- 
er advantage of document-level machine transla- 
tion is its ability in keeping a consistent translation.
However, document-level translation has drawn
little attention from the SMT research community. 
The reasons are manifold. First of all, most of pa- 
rallel corpora lack the annotation of document 
boundaries (Tam, 2007). Secondly, although it is 
easy to incorporate a new feature into the classical 
log-linear model (Och, 2003), it is difficult to cap- 
ture document-level information and model it via 
some simple features. Thirdly, reference transla- 
tions of a test document written by human transla- 
tors tend to have flexible expressions in order to 
avoid producing monotonous texts. This makes the 
evaluation of document-level SMT systems ex- 
tremely difficult.
Tiedemann  (2010)  showed  that  the  repetition
and consistency are very important when modeling 
natural language and translation. He proposed to 
employ cache-based language and translation 
models in a phrase-based SMT system for domain




909


Proceedings of the 2011 Conference on Empirical  Methods in Natural  Language Processing, pages 909–919, 
Edinburgh, Scotland, UK, July 27–31, 2011. Qc 2011 Association for Computational Linguistics


adaptation. Especially, the cache in the translation 
model dynamically grows up by adding bilingual 
phrase pairs from the best translation hypotheses of 
previous sentences. One problem with the dynamic 
cache is that those initial sentences in a test docu- 
ment may not benefit from the dynamic cache. 
Another problem is that the dynamic cache may be 
prone to noise and cause error propagation. This 
explains why the dynamic cache fails to much im- 
prove the performance.
  This paper proposes a cache-based approach for 
document-level SMT using a static cache and a 
dynamic cache. While such a approach applies to 
both phrase-based and syntax-based SMT, this pa- 
per  focuses  on phrase-based SMT. In particular, 
the static cache is employed to store relevant bilin- 
gual phrase pairs extracted from similar bilingual 
document pairs (i.e. source documents similar to 
the test document and their target counterparts) in 
the  training  parallel  corpus  while  the  dynamic 
cache is employed to store bilingual phrase pairs 
from the best translation hypotheses of previous 
sentences in the test document. In this way, our 
cache-based approach can provide useful data at 
the  beginning  of  the  translation  process  via  the 
static cache. As the translation process continues, 
the dynamic cache grows and contributes more and 
more to the translation of subsequent sentences.
  Our motivation to employ similar bilingual doc- 
ument pairs in the training parallel corpus is simple: 
a human translator often collects similar bilingual 
document  pairs  to  help  translation.  If  there  are 
translation  pairs   of   sentences/phrases/words   in 
similar  bilingual document  pairs, this  makes  the 
translation much easier. Given a test document, our 
approach imitates this procedure by first retrieving 
similar bilingual document pairs from the training 
parallel corpus, which has often been applied in 
IR-based  adaptation  of  SMT  systems  (Zhao  et 
al.2004; Hildebrand et al.2005; Lu et al.2007) and 
then extracting bilingual phrase pairs from similar 
bilingual document pairs to store them in a static 
cache.
However, such a cache-based approach may in-
troduce many noisy/unnecessary bilingual phrase 
pairs in both the static and dynamic caches. In or- 
der to resolve this problem, this paper employs a 
topic model to weaken those noisy/unnecessary 
bilingual phrase pairs by recommending the de- 
coder to choose most likely phrase pairs according 
to the topic words extracted from the target-side


text of similar bilingual document pairs. Just like a 
human translator, even with a big bilingual dictio- 
nary, is often confused when he meets a source 
phrase which corresponds to several possible trans- 
lations. In this case, some topic words can help 
reduce the perplexity. In this paper, the topic words 
are stored in a topic cache. In some sense, it has 
the similar effect of employing an adaptive lan- 
guage model with the advantage of avoiding the 
interpolation of a global language model with a 
specific domain language model.
  The rest of this paper is organized as follows. 
Section 2 reviews the related work. Section 3 
presents our cache-based approach to document- 
level SMT. Section 4 presents the experimental 
results.  Session  5  gives  new  insights  on  cache- 
based document-level translation. Finally, we 
conclude this paper in Section 6.

2     Related work

There are only a few studies on document-level 
SMT. Representative work includes Zhao et al. 
(2006), Tam et al. (2007), Carpuat (2009).
  Zhao et al. (2006) assumed that the parallel sen- 
tence pairs within a document pair constitute a 
mixture of hidden topics and each word pair fol- 
lows a topic-specific bilingual translation model. It 
shows that the performance of word alignment can 
be improved with the help of document-level in- 
formation, which indirectly improves the quality of 
SMT.
Tam  et  al.  (2007)  proposed  a  bilingual-LSA
model on the basis of a parallel document corpus 
and built a topic-based language model for each 
language. By automatically building the corres- 
pondence between the source and target language 
models, this method can match the topic-based 
language model and improve the performance of 
SMT.
  Carpuat (2009) revisited the “one sense per dis- 
course” hypothesis of Gale et al. (1992) and gave a 
detailed comparison and analysis of the “one trans- 
lation per discourse” hypothesis. However, she 
failed to propose an effective way to integrate doc- 
ument-level information into a SMT system. For 
example, she simply recommended some transla- 
tion candidates to replace some target words in the 
post-process stage.
In principle, the cache-based approach can be
well suited for document-level translation. Basical-


ly, the cache is analogous to “cache memory” in 
hardware terminology, which tracks short-term 
fluctuation  (Iyer   et   al.,   1999).   As   the  cache 
changes with different documents, the document- 
level information should be capable of influencing 
SMT.
Previous cache-based approaches  mainly point
to cache-based language modeling (Kuhn and Mori,
1990), which uses a large global language model to 
mix with a small local model estimated from recent 
history data. However, applying such a language 
model in SMT is very difficult due to the risk of 
introducing extra noise (Raab, 2007).
  For  cache-based translation  modeling,  Nepveu 
et  al.  (2004) explored user-edited translations  in 
the context of interactive machine translation. Tie- 
demann (2010) proposed to fill the cache with bi-
lingual  phrase  pairs  from  the  best  translation


choosing noisy/unnecessary bilingual phrase pairs 
in both the static and dynamic caches is wakened 
with the help of the topic words in the topic cache. 
In particular, only the most similar document pair 
is used to construct the static cache and the topic 
cache unless specified.
In  this  section,  we  first  introduce  the  basic
phrase-based SMT system and then present our 
cache-based approach to achieve document-level 
SMT with focus on constructing the caches (static, 
dynamic and topic) and designing their corres- 
ponding features.
3.1    Basic phrase-based SMT system
It  is  well  known  that  the translation  process  of 
SMT can be modeled as obtaining the best transla- 
tion e of the source sentence f by maximizing fol- 
lowing posterior probability (Brown et al., 1993):


hypotheses of previous sentences in the test docu-


ebest


= arg max P(e | f ) = arg max P( f | e)Plm (e) (1)


e	e


ment. Both Nepveu et al. (2004) and Tiedemann
(2010) also explored traditional cache-based lan- 
guage models and found that a cache-based lan- 
guage model often contributes much more than a 
cache-based translation model.

3     Cache-based document-level SMT

Given a test document, our system works as fol- 
lows:
1)   clears  the  static,  topic  and  dynamic  caches 
when switching to a new test document dx;
2)  retrieves a set of most similar bilingual docu- 
ment pairs dds  for dx  from the training parallel 
corpus using the cosine similarity with tf-idf 
weighting;
3)   fills the static cache with bilingual phrase pairs


where P(e|f) is a translation model and Plm is a lan- 
guage model.
  Our  system adopted Moses (a  state-of-art 
phrase-based SMT system) as a baseline, which 
follows Koehn et al. (2003) and mainly adopts six
groups of popular features: 1) two phrase transla- 
tion probabilities (two directions): Pphr(e|f) and 
Pphr(f|e); 2) two word translation probabilities (two 
directions) : Pw(e|f) and Pw(f|e); 3) one language 
model (target language): LM(e); 4) one phrase pe- 
nalty (target language): PP(f); 5) one word penalty 
(target language):WP(e); 6) a lexicalized reorder- 
ing model. Besides, the log-linear model as de- 
scribed in (Och and Ney, 2003) is employed to 
linearly interpolate these features for obtaining the 
best translation according to the formula (2):
M


extracted from dds;


e 	= argmax{


l h (e, f )}



(2)


4)   fills the topic cache with topic words extracted
from the target-side documents of dds;
5)   for each sentence in the test document, trans-
lates it using cache-based SMT and conti- 
nuously expands the dynamic cache with 
bilingual phrase pairs obtained from the best 
translation hypothesis of the   previous sen- 
tences.
  In this way, our cache-based approach can pro- 
vide useful data at the beginning of the translation 
process via the static cache. As the translation 
process continues, the dynamic cache grows and 
contributes more and more to the translation of 
subsequent  sentences.  Besides,  the possibility  of


best 	Â m  m
m=1
where hm(e , f) is a feature function, and λm  is the 
weight of hm(e , f)  optimized by a discriminative 
training method on a held-out development data.
  In  principle,  a  phrase-based  SMT  system can 
provide the best  phrase segmentation  and  align-
ment that cover a bilingual sentence pair. Here, a 
segmentation of a sentence into K phrases is de- 
fined as:
(f~e)≈ ∑	(f ,e , ~)  (3)
where tuple (f ,e ) refers to a phrase pair, and ~
indicates corresponding alignment information.


3.2    Dynamic Cache
Our dynamic cache is mostly inspired by Tiede- 
mann (2010),  which adopts  a  dynamic cache to 
store relevant bilingual phrase pairs from the best 
translation hypotheses of previous sentences in the 
test document. In particular, a specific feature is
incorporated S           to  capture  useful  document-
level information in the dynamic cache:
K 	-∂i



a sub-phrase of ei and  e   for a sub-phrase of e   , to 
allow partial matching. Finally, F   measures the 
overall value of a target candidate f  by summing
over the scores of K phrase pairs.
Obviously, F   rewards  both full  matching and
partial matching. In order to avoid too much noise,
we put some constraints on the number of words in
the  target  phrase  of  <ec,fc>  or  <ei,fi>,  such  as
∥ e  ∥> 3 ,  where " ∥∥ "  measures  the  number  of



S cache



(ec



| f c


I
 
(
<
 
e
  
,
 
f
    
>
=
<
 
e
 
,
  
f
   
>
)
 
¥
 
e
 
	
) =
Â
K
   
I
 
(
 
f
   
=
 
f
 
)





(4)



non-
blank 
chara
cters 
in a 
phras
e. 
For 
exam
ple, 
if 
phras
e 
pair 
“, 
减少||
| and 
reduc
ed” 
occur
s in 
the



where


e-∂i is a 
decay 
factor 
to 
avoid 
the 
depen-


cache, 
phrase 
pair 
“,|||and
” is not 
reward
ed 
becaus
e



dence of the feature’s contribution on the cache 
size. Given <ec, fc> an existing phrase pair in the 
dynamic cache and <ei,fi> a phrase pair in a new
hypothesis, if ( ei=ec ∧ fi=fc ) is true (i.e. full match-
ing), function I(.) returns 1 , otherwise 0.
One problem with the dynamic cache in Tiede-
mann (2010) is that it continuously updates the 
weight of a phrase pair in the dynamic cache. This 
may cause noticeable computational burden with 
the increasing number of phrase pairs in the dy- 
namic cache. In addition, as a source phrase (fc) 
may occur many times in the dynamic cache, the 
weights for related phrase pairs may degrade se- 
verely and thus his decoder needs a decay factor, 
which is difficult to optimize. Finally, Tiedemann 
(2010) only allowed full matching. This largely 
lowers down the probability of hitting the dynamic 
cache and thus much affects its effectiveness.
To overcome above problems, we only employ
the bilingual phrase pairs in the dynamic cache to 
inform the decoder whether one bilingual phrase 
pair exists in the dynamic cache or not, which is 
slightly similar to (Nepveu et al, 2004) ,thus avoid- 
ing extra computational burden and the fine-tuning 
of the decay factor.  In particular, following new 
feature is incorporated to better explore the dynam- 
ic cache:
= ∑ 	dpairmatch(e ,f )    (5)
where dpairmatch(   ,   )
⎧ 1   (e  = e  ∧ f  = f )
=	∨   e  = e   ∧ f  = f   ∧∥ e   ∥> 3 
⎨   ∨   e  = e   ∧ f  = f   ∧∥ e   ∥> 3 
⎩0   other
Here, F    is  called  the  dynamic  cache  feature.
Assume (ec,fc  ) is  a  phrase pair in the dynamic
cache and (ei,fi) is a phrase pair candidate for a
new hypothesis. Besides full matching, we intro-
duce a symbol of “^” for sub-phrase, such as e  for


such shorter phrase pairs occur frequently and may
largely degrade the effect of the cache. In accor-
dance,  the  dynamic  cache  only  contains  phrase 
pairs whose target phrases contain 4 or more non- 
blank characters.
3.3    Static Cache
In Tiedemann (2010), initial sentences in a test 
document fail to benefit from the dynamic cache 
due to the lack of contents in the dynamic cache at 
the beginning of the translation process. To over- 
come this problem, a static cache is included to 
store relevant bilingual phrase pairs extracted from 
similar bilingual document pairs in the training 
parallel corpus. In particular, a static cache feature
F   is designed to capture useful information in the
static cache in the same way as the dynamic cache
feature, shown in Formula (5).
  For this purpose, all the document pairs in the 
training parallel corpus are aligned at the phrase 
level  using  2-fold  cross-validation.  That  is,  we 
adopt 50% of the training parallel corpus to train a 
model using Moses and apply the model to enforce 
phrase alignment  of the remaining training data, 
and vice versa. Here, the enforcement is done by 
guaranteeing the occurrence of the target phrase 
candidate of a source phrase in the sentence pair. 
Besides, all the words pairs trained on the whole 
training parallel corpus are included in both folds 
to ensure at least one possible translation. Finally, 
the phrase pairs in the best translation hypothesis 
of a sentence pair is retrieved from the decoder. In 
this way, we can extract a set of phrase pairs for 
each bilingual document pairs.
  Given a test document, we first find a set of sim- 
ilar source documents by computing the Cosine 
similarity using the TF-IDF weighting scheme and 
their corresponding target documents, from the 
training parallel corpus. Then, the phrase pairs ex-


tracted from these similar bilingual document pairs 
are collected into the static cache.  To avoid noise, 
we filter out those phrase pairs which occur less 
than two times in the training parallel corpus.


general language model and added additional two 
adaptive lexicon probabilities in his phrase table.
In principle,  LDA is  a  probabilistic  model  of
text data, which provides a generative analog of
PLSA (Blei et al., 2003), and is primarily meant to


出口 ||| exports
放慢 ||| slowdown 
股市 ||| stock market 
现行 ||| leading
出口 增幅 ||| export growth
多种 原因 ||| various reasons


汇率 ||| exchange
活力 ||| vitality
加快 ||| speed up the
经济学家 ||| economists


reveal hidden topics in 
text documents. Like 
most of the text mining 
techniques, LDA 
assumes that 
documents are made up 
of words and the 
ordering of the words 
within a document is 
unimportant (i.e. the 
“bag-of-words” 
assumption).


国家 著名 ||| a well-known  international 
议会 委员会 ||| congressional committee 
不 乐观 的 预期 ||| pessimistic predictions
保持 一定 的 增长 ||| maintain a certain growth
美元 汇率 下跌 ||| a drop in the dollar exchange rate
Table 1: Phrase pairs extracted from a document pair 
with an economic topic

  Similar to the dynamic cache, we only consider 
those phrase pairs whose target phrases contain 4 
or more non-blank characters to avoid noise. We 
do not deliberately remove long phrase pairs. It is 
possible to use these long phrase pairs if our test 
document is very similar to one training document 
pair.  Table 1 shows some bilingual phrase pairs 
extracted from a document pair, which reports a
piece of news about “impact on slowdown in US
economic growth”. Obviously, these phrase pairs 
are closely related to economics.

3.4    Topic Cache

Both the dynamic and static caches may still intro- 
duce noisy/unnecessary bilingual phrase pairs even 
with constraints on the length of phrases and their 
occurrence frequency in the training parallel cor- 
pus. In order to resolve this problem, this paper 
adopts a topic cache to store relevant topic words 
and employs a topic cache feature to weaken those 
noisy/unnecessary phrase pairs.
  Given w  is a topic word in the topic cache, the 
topic cache feature F  is designed as follows:
=	topicexist(e ,f )	(6)
where topicexist(e ,f )
= 	1   (w  ∈ e )
0    other
Here, the target phrase which contains a topic word
w  will be rewarded. w  is derived by a topic mod-
el, LDA (Latent Dirichlet Allocation). This is dif-
ferent from the previous work (Tam, 2007), which
mainly interpolated a topic language model with a


Figure 1 shows the principle of LDA, where α is
the parameter of the uniform Dirichlet prior on the 
per-document topic distributions, β is the parame- 
ter of the uniform Dirichlet prior on the per-topic 
word distribution, θi is the topic distribution for 
document i, zij is the topic for the jth word in doc- 
ument i, and wij is the specific word. Among all 
variables, wij  is the only observable variable with 
all the other variables latent. In particular, K de- 
notes the number of topics considered in the model 
and φ is a K*V (V is the dimension of the vocabu- 
lary) Markov matrix each line of which denotes the 
word distribution of a topic. The inner plate over z 
and w illustrates the repeated sampling of topics 
and words until N words have been generated for 
document d. The plate surrounding θ illustrates the 
sampling of a distribution over topics for each 
document d for a total of M documents. The plate 
surrounding φ illustrates the repeated sampling of 
word distributions for each topic z until K topics 
have been generated.









Figure 1: LDA
  We use a LDA tool1 to build a topic model using 
the target-side documents in the training parallel 
corpus. Using LDA, we can obtain the topic distri- 
bution of each word w, namely p(z|w) for topic z
ϵK. Moreover, using the obtained word topic dis-
tributions, we can infer the topic distribution of a
new document, namely p(z|d) for each topic z ϵK.
Given a test document, we first find the most
similar source document from the training data in


1 http://www.arbylon.net/projects/


the same way as done in the static cache. After that, 
we  retrieve  its  corresponding  target  document. 
Then, the topic of the target document is deter- 
mined by its major topic, with the maximum p(z|d). 
Finally, we load some topic words corresponding 
to this topic z into the topic cache. In particular, 
our LDA model deploy the setting of K=15, α=0.5 
and β=0.1. Besides, only top 1000 topic words are 
reserved for each topic. Table 2 shows top 10 topic 
words for five topics.
  

In this paper, we use FBIS as the training data, the 
2003 NIST MT evaluation test data as the de- 
velopment data, and the 2005 NIST MT test data 
as  the test  data. Table 3  shows  the statistics  of 
these data sets (with document boundaries anno- 
tated).


















Table 2: Topic words extracted from target-side doc- 
uments


4     Experimentation

We have systematically evaluated our cache-based 
approach to document-level SMT on the Chinese- 
English translation task.

4.1    Experimental Setting

Here,  we use SRI  language  modeling  toolkit  to 
train a trigram general language model on English 
newswire text, mostly from the Xinhua portion of 
the Gigaword corpus (2007) and performed word 
alignment on the training parallel corpus using 
GIZA++(Och and Ney,2000) in two directions. For 
evaluation,  the  NIST  BLEU  script  (version  13) 
with  the  default  setting  is  used  to  calculate the 
Bleu score (Papineni et al. 2002), which measures 
case-insensitive matching of n-grams with n up to
4. To see whether an improvement is statistically 
significant, we also conduct significance tests us- 
ing the paired bootstrap approach (Koehn, 2004)2.
In this paper, ‘***’, ‘**’, and ‘*’ denote
p-values less than or equal to 0.01, in-between 
(0.01, 0.05), and bigger than 0.05, which mean 
significantly better, moderately better and slightly 
better, respectively.

2 http://www.ark.cs.cmu.edu/MT


Table 3: Corpus statistics

  In particular, the sizes of the static, topic and 
dynamic caches are fine-tuned to 2000, 1000 and
5000 items, respectively. For the dynamic cache, 
we only keep those most recently-visited items, 
while for the static cache; we always keep the most
frequently-occurring items.

4.2    Experimental Results

Table 4 shows the contribution of various caches in 
our cache-based document-level SMT system. The 
column of “BLEU_W” means the BLEU score 
computed over the whole test set and “BLEU_D” 
corresponds to the average BLEU score over sepa- 
rated documents.

Sys
tem
BL
EU   
on
De
v(
%)
BLE
U on 
Test(
%)


BL
EU
_W
NI
ST
BL
EU
_D
Mo
ses
29.
87
25.
76
7.7
84
25.
08
Fd
29.
90
26.
03 
(*)
7.8
52
25.
39
Fd+
Fs
30.
29
26.
30 
(**)
7.8
84
25.
86
Fd+
Ft
30.
11
26.
24 
(**)
7.8
71
25.
74
Fd+
Fs+
Ft
30.
50
26.
42 
(**
*)
7.8
96
26.
11
Fd+
Fs+
Ft
wi
th 
me
rg
- 
in
g
-
26.
57 
(**
*)
7.9
01
26.
32

Table 4: Contribution of various caches in our cache- 
based document-level SMT system. Note that signific- 
ance tests are done against Moses.

Contribution of dynamical cache (Fd)

Table 4 shows that the dynamic cache slightly im- 
proves the performance by 0.27 (*) in BLEU_W. 
This is similar to Tiedemann (2010). However, 
detailed analysis indicates that the dynamic cache 
does have negative effect on about one third of 
documents, largely due to the instability of the dy- 
namic cache at the beginning of translating a doc- 
ument.  Figure  2  shows  the  distribution  of  the


BLEU_D difference of 100 test documents (sorted 
by BLEU_D). It shows that about 55% of test 
documents benefit from the dynamic cache.

8.00%


effectiveness of combining the dynamic and topic 
caches (sorted by BLEU_D).


10.00%

8.00%


6.00%
4.00%
2.00%
0.00%	 
	


fd-moses



6.00%

4.00%

2.00%


fd+ft-moses


-2.00% 1  8  15 22 29 36 43 50 57 64 71 78 85 92 99
-4.00%
-6.00%
-8.00%
Figure 2: Contribution of employing the dynamic 
cache on different test documents

Contribution of static cache (Fs)

Table 4 shows that the combination of the static 
cache with the dynamic cache further improves the 
performance by 0.27(*) in BLEU_W. This sug- 
gests the effectiveness of the static cache in elimi- 
nating the instability of the dynamic cache when 
translating first few sentences of a test document. 
Together, the dynamic and static caches much im- 
prove the performance by 0.54 (**) in BLEU_W 
over Moses.  Figure 3 shows the distribution of the 
BLEU_D difference of 100 test documents (sorted 
by BLEU_D), with more positive effect on those 
borderline documents, compared to Figure 2.


0.00%	 	
1  7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97
-2.00%

-4.00%

-6.00%

Figure 4: Contribution of combining the dynamic 
and topic caches

  However, detailed analysis shows that the topic 
cache and the static cache are quite complementary 
by contributing on different test documents, largely 
due to that while the static cache tends to keep 
translation consistent, the topic cache plays like a 
document-specific language model. This is justi- 
fied by Table 4 that the combination of the dynam- 
ic, static and topic caches significantly improve the 
performance by 0.66 (***) in BLEU_W, and by 
Figure 5 that about 75% of test documents benefit 
from the combination of the three caches (sorted 
by BLEU_D).

10.00%




6.00%

4.00%





fd+fs-
moses



8.00%

6.00%

4.00%


fd+fs+ft
-moses





2.00%


2.00%



0.00%

-2.00%




1  8  
15 22 
29 36 
43 50 
57 64 
71 78 
85 92 
99


0.00%

-2.00%

-4.00%



1 	9  
17  25  
33  41  
49  57  
65  73  
81  89  
97


-4.00%

-6.00%
Figure 3: Contribution of combining the dynamic and 
static cache on different test documents

Contribution of topic cache (Ft)

Table 4 shows that the topic cache has comparable 
effect on improving the performance as the static 
cache  when  combined  with  the  dynamic  cache 
(0.48 vs. 0.54 in BLEU_W). Figure 4 shows the



Figure 5: Contribution of combining the three caches

Contribution of merging phrase pairs of similar 
document pairs

Here, the number of similar documents we adopt is 
different from previous experiments. In the pre- 
vious experiments, we only cache bilingual phrase 
pairs extracted from the most similar document. 
Here, we merge phrase pairs for several most simi- 
lar documents (5 at most) which have the same 
topic.


  Table 4 shows that employing this trick can fur- 
ther improve the performance by 0.15 in BLEU_W. 
As a result, the cache-based approach significantly 
improve the performance by 0.81  (***)  in 
BLEU_W over Moses.

5     Discussion

In this section, we explore in more depth why the 
static cache can help the dynamic cache, some 
constrained factors which impact the effectiveness 
of our cache-based approach.

Effectiveness of the static cache

We investigate why the static cache affects the per- 
formance. Basically, it is difficult for the dynamic 
cache to capture such similar information in the 
static cache.
In principle, the static cache can both influence
the initial and subsequent sentences; however sub- 
sequent ones can be affected by multiple caches. In 
order to give an insight of the static cache, we eva- 
luate its effectiveness on the first sentence for each 
test document. Figure 6 shows the contribution of 
the static cache on translating these first sentences 
(y-axis shows BLEU value of the first sentence for 
each test document). It notes that the most BLEU 
scores of them are zeros because of the length limi- 
tation of first sentences.

0.9

0.8


quency of the dynamic cache is 504 on whole test 
sets, this figure increases to 685 with the static 
cache. This means that the static cache significant- 
ly enlarges the effectiveness of the dynamic cache 
by including more relevant phrase pairs to the dy- 
namic cache, largely due to the positive impact of 
the static cache on the initial sentences of each test 
document.

Size of topic cache

Table 5 shows the impact of the topic cache when 
the number of the retained topic words for each 
topic increases from 500 to 2000. It shows that too 
more topic words actually harm the performance, 
due  to  the  increase  of  noise.  1000  topic  words 
seem a lot largely due to that we didn’t do stem- 
ming for our topic modeling since we hope to in- 
troduce some tense information of them in the 
future.

Nu
mb
er 
of 
top
ic 
wo
rds
B
L
E
U
_
W
50
0
26.
27
70
0
26.
31
10
00
26.
42
15
00
26.
23
20
00
26.
19
Table 5: Impact of the topic cache size

Influenced translations

In order to explore how our cache-based system 
impacts  on  translation  results,  we  manually  in-



0.7

0.6

0.5
0.4

0.3

0.2
0.1

0


F
d
	
F
d
F
s











0 	20 	40 	60 	80 	100 	120


spe
cte
d 5 
doc
um
ent
s 
res
pec
tive
ly 
whi
ch 
is 
im- 
pro
ved 
or 
deg
rad
ed 
in 
tra
nsl
atio
n 
qua
lity 
co
mp
are
d to  
the  
bas
elin
e  
Mo
ses  
out
put.  
Th
ose  
doc
um
ent
s
h
a
v
e
 
1
0
7
 
s
e
n
t
e
n
c
e
s
 
i
n
 
s
u
m
.
  T
he 
goo
d 
effe
ctiv
enes
s  of 
eac
h 
kind 
of 
cac
he 
can 
be 
obs
erve
d by 
the 
exa
mpl
e 1 
and 
2 
sho
wed 
in 
Tab
le 6. 
Bot
h 
the 
exa
mpl
e 1 
and 
2 
com
e 
fro
m 
the 
sam
e 
doc
ume
nt 
who
se 
“BL
EU
_D” 
scor
e 
exc
eeds 
Mo
ses 
with 
8.4 
poin
t. 
The 
exa
mpl
e 1 
ben
efits 
fro
m 
the 
topi
c 
cac
he 
whi
ch 
cont
ains 
the 
ite
m 
of 
“act
ion”
.


Figure 6: Contribution of the static cache on the first 
sentence of each test document
(i.e. with empty dynamic cache)
  Furthermore, we count the hit (matching) fre- 
quency of the static cache for each test documents. 
Since we use 1 or 0 for the static cache feature, it is 
easy to retrieve its effect for each test document. 
Our statistics shows that the hit frequency on static 
cache fluctuates between 5 and 18 for each test 
document.  Without  the static  cache,  the hit  fre-


The example 2 benefits from the static cache which 
contains a phrase pair of  “承诺||| promised to” 
while Moses use “commitment” for “承诺” , which
may be the reason for  missing the part of “prime 
minister” in Moses output. Furthermore, due to the
phrase pair of “停火 协议||| the ceasefire agree-
ment”  existing  in  our  static  cache,  our  decoder 
keeps using “ceasefire” to translate “停火” in the
whole document while Moses randomly use “cea- 
sefire” or “cease-fire” for this translation.



1
官
员 
预
测 
“ 
准
备 
工
作 
将 
会 
进
行 
到 
七
月 
, 
然
后 
再 
展
开 
政
治 
动
作 
”

Mo
ses
: 
offi
cial 
for
eca
sts 
sai
d 
that 
pre
par
ato
ry 
wo
rk 
wil
l be 
car
rie
d 
out 
in 
jul
y 
and 
the
n 
lau
nch
ed 
a 
pol
itic
al 
ma
ne
uve
r .

Ou
rs:   
offi
cial 
for
eca
sts 
sai
d 
that 
pre
par
ato
ry 
wo
rk 
wil
l be 
car
rie
d 
out 
in 
jul
y , 
the
n 
beg
an 
a 
pol
itic
al 
act
ion 
.

Re
fer
enc
e: 
offi
cial
s 
exp
ect
ed 
that 
"pr
epa
rati
ons 
wo
uld 
tak
e 
pla
ce 
unt
il 
Jul
y, 
afte
r 
whi
ch 
pol
itic
al 
acti
on 
wil
l 
beg
in".
2
关
于 
这 
一 
点 
, 
中
东
新
闻
社 
说 
, 
以
色
列 
总
理 
夏
隆 
承
诺 
“ 
只 
要 
巴
勒
斯
坦 
当
局 
尊
重 
停
火 
协
议 
, 
控
制 
好 
它
们 
的 
地
方 
, 
以
色
列 
将 
会 
停
止 
对 
巴
勒
斯
坦 
人 
的 
军
事 
行
动 
” 
。

Mo
ses
: 
on 
this 
poi
nt , 
sai
d 
that 
isra
eli 
co
m
mit
me
nt 
to 
the 
pal
esti
nia
n 
aut
hor
itie
s 
to 
res
pec
t 
the 
cea
se-
fire 
agr
ee
me
nt ,
wh
ere 
the
y 
are 
wel
l 
und
er 
con
trol 
, 
isra
el 
wil
l 
sto
p 
its 
mil
itar
y 
acti
ons 
aga
inst 
pal
esti
nia
ns .

Ou
rs: 
on 
this 
poi
nt , 
sai
d 
that 
isra
eli 
pri
me 
mi
nis
ter 
pro
mi
sed 
to 
res
pec
t 
the 
cea
sefi
re 
agr
ee
me
nt , 
the 
pal
esti
nia
n 
au-
tho
riti
es 
to 
pro
per
ly 
con
trol 
thei
r 
are
as 
and 
isra
el 
wil
l 
sto
p 
its 
mil
itar
y 
acti
ons 
aga
inst 
pal
esti
nia
ns .

Re
fer
enc
e:F
or 
this 
poi
nt , 
ME
NA 
sai
d 
Isra
eli 
Pri
me 
Mi
nist
er 
Sha
ron 
has 
pro
mis
ed 
to " 
sto
p 
Isra
eli 
mil
itar
y 
ope
rati
ons
aga
inst 
the 
Pal
esti
nia
ns 
ins
ofa
r as 
the
y 
con
tin
ue 
to 
res
pec
t 
the 
cea
sefi
re 
dea
l 
and 
con
trol 
thei
r 
terr
itor
y . 
"
3
17 
日 
晚 
，
近 
300
0 
多 
名 
市
民 
在 
市
中
心 
的 
武
器 
广
场 
观
看 
了 
由 
市
政
府 
举
办 
的 
精
彩
纷
呈 
的 
歌
舞
晚
会 
，
五
颜
六
色 
的 
灯
光 
装
扮 
着 
广
场 
周
围 
的 
古
老 
建
筑 
，
著
名 
歌
舞 
艺
术
家 
们 
表
演 
了 
不
同 
地
区
的 
民
族 
歌
舞 
。

Mo
ses
: 
on 
the 
eve
nin
g , 
nea
rly 
3,0
00 
resi
den
ts 
in 
the 
do
wnt
ow
n 
squ
are 
of 
the 
we
apo
ns 
hel
d 
by 
the 
mu
nici
pal 
gov
ern
-
m
en
t , 
w
at
ch
ed 
a 
so
ng 
an
d 
da
nc
e 
so
ire
e , 
ha
vi
ng 
co
lo
rf
ul 
lig
hti
ng 
di
sg
ui
se 
of 
an
ci
en
t 
bu
ild
in
gs 
ar
ou
nd 
th
e 
sq
ua
re 
, 
si
ng
- 
in
g 
an
d 
da
nc
in
g 
fa
m
ou
s 
art
ist
s 
st
ag
ed 
di
ffe
re
nt 
re
gi
on
s 
of 
et
hn
ic 
so
ng 
an
d 
da
nc
e .

Ou
rs: 
late
r 
on, 
nea
rly 
3,0
00 
resi
den
ts 
in 
the 
do
wnt
ow
n 
squ
are 
to 
wat
ch 
the 
gov
ern
me
nt 
of 
hav
ing 
a 
son
g 
and 
dan
ce
pe
rf
or
m
an
ce
s 
w
er
e 
he
ld 
un
de
r 
th
e 
di
sg
ui
se 
of 
co
lor
ful 
lig
hti
ng 
ar
ou
nd 
th
e 
sq
ua
re 
, a 
fa
m
ou
s 
an
ci
en
t 
bu
ild
in
gs 
an
d 
lo
ca
l 
art
ist
s 
of 
dif
fer
en
t 
et
hn
ic 
so
ng 
an
d 
da
nc
e .

Re
fer
enc
e: 
On 
the 
nig
ht 
of 
the 
17t
h , 
nea
rly 
3,0
00 
resi
den
ts 
wat
che
d a 
wo
nde
rful 
gal
a 
of 
son
gs 
and 
dan
ces 
, 
org
ani
zed
by 
th
e 
m
un
ici
pa
l 
go
ve
rn
m
en
t , 
at 
Pl
az
a 
da 
Ar
m
as 
. 
C
ol
or
ful 
lig
ht
s 
lig
ht
ed 
up 
an
ci
en
t 
ar
ch
ite
ct
ur
e 
ar
ou
nd 
th
e 
pl
az
a . 
Fa
m
ou
s 
art
ist
s 
in
cl
ud
in
g 
si
ng
er
s 
an
d 
da
nc
er
s 
sta
ge
d 
pe
rf
or
m
an
ce
s 
of 
na
tio
na
l 
so
ng
s 
an
d 
da
nc
es 
of 
dif
fe
re
nt 
re
gi
on
s .
4
利
马 
的 
城
市 
面
积 
已 
从 
建
城 
之 
初 
的 
2.1
4 
平
方
公
里 
发
展 
到 
260
0 
多 
平
方
公
里 
， 
而 
人
口 
也 
增
加
到 
80
0 
万 
左
右 
， 
约 
占 
全
国 
总
人
口 
的 
31% 
。

Mo
ses
: at 
lim
a 's 
urb
an 
are
a 
fro
m 
the 
beg
inn
ing 
of 
260
0 
squ
are 
to 
2.1
4 
mil
lio
n 
squ
are 
kil
om
ete
rs , 
whi
le 
the 
pop
ula
-
tio
n 
has 
inc
rea
sed 
to 
8 
per
cen
t of 
the 
cou
ntr
y 's 
tota
l , 
abo
ut 
31
% .

Ou
rs: 
lim
a , 
the 
urb
an 
are
a 
fro
m 
the 
beg
inn
ing 
of 
260
0 
squ
are 
kil
om
ete
rs 
to 
2.1
4 
mil
lio
n 
squ
are 
kil
om
ete
rs , 
but 
als
o
inc
rea
sed 
to 
abo
ut 
8 
mil
lio
n 
pop
ulat
ion 
, 
the 
cou
ntr
y 's 
tota
l 
pop
ulat
ion 
of 
abo
ut 
31
% .

Re
fer
enc
e: 
Th
e 
are
a 
of 
Li
ma 
city 
has 
exp
and
ed 
to 
mo
re 
tha
n 
2,6
00 
squ
are 
kilo
met
ers 
fro
m 
the 
ori
gin
al 
2.1
4 
squ
are 
ki-
lo
met
ers 
wh
en 
the 
city 
wa
s 
fou
nde
d , 
whi
le 
the 
pop
ulat
ion 
has 
inc
rea
sed 
to 
aro
und 
8 
mil
lio
n , 
rou
ghl
y 
acc
oun
tin
g 
for
31
% 
of 
the 
nati
on's 
tota
l .
Table 6: Positive and negative examples


  The example 3 and 4 also come from the same 
document however whose performance degrades 
with 2.17 point.   We don’t think the translation 
quality for example 4 in our system is worse than 
Moses. However, the translation quality for exam- 
ple 3 in our system is very bad and especially 
showed on “re-ordering”. We found this sentence 
did not match any item in our static cache and top- 
ic cache. Although this phenomenon also happens 
in other documents, but this is the most typical 
negative example among these documents.

Document-specific characteristics

It seems that using the same weight for the whole 
test sets (all documents) is not very reasonable. 
Actually, if we can determine those negative doc- 
uments which are not suitable for the cache-based 
approach,  our  cache-based  approach  may  gain 
much  improvement.  Tiedemann  (2010)  explored 
the correlation to document length, baseline per- 
formance and source document repetition. Howev-


er, it seems that there are no obvious rules to filter 
out those negative documents. Besides, there may 
be two more document-specific factors: repetition 
of the reference text and document style.
  Tiedemann (2010) only considered the repetition 
of the test  text  in the source side.  Since BLEU 
score is computed against the reference text, the 
repetition in the reference text may greatly influ- 
ence the performance of our cache-based approach 
to document-level SMT. As for document style, it 
is quite possible that a document may contain sev- 
eral topics.  Therefore,  it  may be useful to track 
such change over topics and refresh various caches 
when there is a topic change. We will leave the 
above issues to the future work.

6     Conclusion

We have shown that our cache-based approach 
significantly improves the performance with the 
help of various caches, such as the dynamic, static 
and  topic  caches,  although  the  cache-based  ap-


proach  may  introduce  some  negative  impact  on
BLEU scores for certain documents.
In the future, we will further explore how to re-
flect document divergence during training and dy- 
namically adjust cache weights according to 
different documents.
There are many useful components in training
documents, such as named entity, event and co- 
reference. In this experiment, we only adopt the 
flat data in our cache. However, the structured data 
may improve the correctness of matching and thus 
effectively avoid noise. We will explore more ef- 
fective ways to pick up various kinds of useful in- 
formation from the training parallel corpus to 
expand our cache-based approach. Besides, we will 
resort to comparable corpora to enlarge our cache- 
based approach to document-level SMT.

Acknowledgments

This research was supported by Projects 90920004,
60873150, and 61003155 under the National Natu- 
ral    Science    Foundation    of    China,    Project
20093201110006 under the Specialized Research 
Fund for the Doctoral Program of Higher Educa- 
tion of China.

References

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of machine
Learning Research 3, pages 993–1022.

Francis Bond. 2002. Toward a Science of Machine 
Translation. Asian Association of Machine Transla- 
tion (AAMT) Journal, N0.22, Tokyo, Japan, pages
12-20.

PF Brown, SA Della Pietra, VJ Della Pietra, RL Merc- 
er.1992.The Mathematics of Statistical Machine 
Translation: Parameter Estimation. Computational 
Linguistics. 19(2):263-309.

Marine Carpuat. 2009. One Translation per Discourse.
In Proc. of the NAACL HLT workshop on Semantic
Evaluation, pages 19-26.

John  DeNero,  Alexandre  Buchard-Cˆot´e,  and  Dan 
Klein. 2008. Sampling Alignment Structure under a 
Bayesian  Translation  Model.  In  Proc.  of  EMNLP
2008, pages 314–323, Honolulu, October.

William A. Gale, Kenneth W. Church, and David Ya- 
rowsky. 1992. One Sense per Discourse. In Proceed- 
ings of the workshop on Speech and Natural 
Language, Harriman, NY.


Almut Silja Hildebrand, Matthias Eck, Stephan Vo- 
gel,and Alex Waibel. 2005. Adaptation of the Trans- 
lation  Model  for  Statistical  Machine  Translation 
based   on   Information   Retrieval.   Proceedings   of 
EAMT 2005:133-142.

Rukmini M. Iyer and Mari Ostendorf. 1999. Modeling 
Long Distance Dependence in Language:Topic 
Mixtures Versus Dynamic Cache Models. IEEE 
Transactions on speech and audio processing, 7(1).

Philipp Koehn, Franz Josef Och ,and DanielMarcu.2003.
Statistical Phrase-Based Translation. Proceedings of 
the 2003 Conference of the North American Chapter 
of the Association for Computational Linguistics on 
Human Language Technology, pages 48-54.

Philipp  Koehn.2004.Statistical  Significance  Tests  for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pages 388–395.

Roland  Kuhn  and  Renato  De  Mori.  1990.  A Cache- 
based Natural Language Model for Speech Recogni- 
tion. IEEE Transactions on Pattern Analysis and Ma- 
chine Intelligence,12(6):570-583.

Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving 
statistical machine translation performance by train- 
ing data selection and optimization. In Proc. of 
EMNLP 2007,pages 343–350, Prague, Czech Repub- 
lic, June.

Daniel Marcu and William Wong. 2002. A phrase-based 
Joint Probability Model for Statistical Machine 
Translation. In Proc. of  EMNLP 2002, July.

Laurent Nepveu, Guy Lapalme, Philippe Langlais and 
George Foster.2004. Adaptive Language and Trans- 
lation Models for Interactive Machine Translation. In 
Proc. of EMNLP 2004, pages 190-197.

Eugene A. Nida. 1964. Toward a Science of Translating.
Leiden, Netherlands:E.J.Brill.

Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of ACL, 
pages 160–167.

Franz Josef Och  and Hermann  Ney.  2000.  Improved
Statistical Alignment Models. In Proc. of ACL, pages
440–447.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei- 
jing Zhu. 2002. BLEU: A Method for Automatic 
Evaluation   of  Machine  Translation.  In  Proc.   of 
ACL02, pages 311–318.

Martin  Raab.  2007. Language Modeling for  Machine
Translation. VDM Verlag, Saarbrucken, Germany.


G. Salton and C. Buckley. 1988. Term-weighting Ap- 
proaches in Automatic Text Retrieval. Information 
Processing and management,24(5):513-523,1988.

Yik-Cheung Tam, Ian Lane and Tanja Schultz. 2007.
Bilingual ISA-based Adaptation for Statistical Ma- 
chine Translation. Machine Translation, 28:187-207.

Jorg Tiedemann. 2010. Context Adaptation in Statistical 
Machine Translation Using Models with Exponen- 
tially Decaying Cache. In Proc. of the 2010 work- 
shop on domain adaptation for Natural Language 
Processing, ACL 2010, pages 8-15.

Joern  Wuebker  and Arne Mauser  and Hermann Ney.
2010. Training Phrase Translation Models with Leav- 
ing-One-Out. In Proc. of ACL, pages 475-484.

Bing   Zhao,   Matthias   Eck,   and   Stephan   Vogel.
2004.Language Model Adaptation for Statistical Ma- 
chine Translation with Structured Query Models. In 
COLING 2004, Geneva, August.

Bing Zhao and Eric P. Xing .2006. BiTAM:Bilingual 
Topic Ad-Mixture Models for Word Alignment. In 
Proc. of ACL2006.

