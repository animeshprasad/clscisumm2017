Topic Models for Dynamic Translation Model Adaptation



Vladimir Eidelman 
Computer Science 
and UMIACS
University of Maryland 
College Park, MD 
vlad@umiacs.umd.edu


Jordan Boyd-Graber
iSchool and UMIACS 
University of Maryland 
College Park, MD
jbg@umiacs.umd.edu


Philip Resnik 
Linguistics and 
UMIACS
University of Maryland 
College Park, MD 
resnik@umd.edu


Abstract


We propose an approach that biases machine 
translation systems  toward relevant transla- 
tions based on topic-specific contexts, where 
topics are induced in an unsupervised way 
using topic models; this can be thought of 
as  inducing subcorpora for adaptation with- 
out any human annotation. We use these topic 
distributions to compute topic-dependent lex- 
ical weighting probabilities and directly in- 
corporate them into our translation model as 
features.  Conditioning lexical probabilities 
on the topic biases translations  toward topic- 
relevant output, resulting in significant im- 
provements of up to 1 BLEU  and 3 TER  on 
Chinese to English translation over a strong 
baseline.



1   Introduction

The performance of a statistical machine translation 
(SMT) system on a translation  task depends largely 
on the suitability of the available parallel training 
data. Domains (e.g., newswire vs. blogs) may vary 
widely in their lexical choices and stylistic prefer- 
ences, and what may be preferable in a general set- 
ting, or in one domain, is not necessarily preferable 
in another domain. Indeed, sometimes the domain 
can change the meaning of a phrase entirely.
In a food related context, the Chinese  sentence
“粉 丝 很 多 ” (“feˇns¯i heˇnduo¯ ”) would mean “They
have a lot of vermicelli”; however, in an informal In- 
ternet conversation,  this sentence would mean “They 
have a lot of fans”. Without the broader context, it 
is impossible to determine the correct translation in 
otherwise identical sentences.
  

This problem  has led to a substantial  amount  of 
recent work in trying to bias, or adapt, the transla- 
tion model (TM) toward particular domains of inter- 
est (Axelrod et al., 2011; Foster et al., 2010; Snover 
et al., 2008).1   The intuition behind TM  adapta- 
tion is to increase the likelihood of selecting rele- 
vant phrases for translation. Matsoukas et al. (2009) 
introduced assigning  a pair of binary features to 
each training sentence, indicating sentences’ genre 
and collection as a way to capture domains. They 
then learn a mapping from these features  to sen- 
tence weights,  use the sentence weights to bias the 
model probability estimates and subsequently learn 
the model weights. As sentence weights were found 
to be most beneficial for lexical weighting, Chiang 
et al. (2011) extends the same notion of condition- 
ing on provenance (i.e., the origin of the text) by re- 
moving  the separate mapping step, directly optimiz- 
ing the weight of the genre and collection features 
by computing  a separate word translation table for 
each feature,  estimated from only those sentences 
that comprise that genre or collection.
  The common thread throughout prior work is the 
concept of a domain. A domain is typically a hard 
constraint that is externally  imposed and hand la- 
beled,  such as genre or corpus collection. For ex- 
ample,  a sentence either comes from newswire, or 
weblog, but not both.   However, this poses  sev- 
eral problems. First, since a sentence contributes  its 
counts only to the translation table for the source it 
came from, many word pairs will be unobserved for 
a given table. This sparsity requires smoothing. Sec- 
ond, we may not know the (sub)corpora our training

  1 Language model adaptation is also prevalent but is not the 
focus of this work.




115


Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 115–119, 
Jeju, Republic of Korea, 8-14 July 2012. Qc 2012 Association for Computational Linguistics



data come from; and even if we do, “subcorpus” may 
not be the most useful notion of domain for better 
translations.
  We take a finer-grained, flexible, unsupervised ap- 
proach for lexical weighting by domain. We induce 
unsupervised domains from large corpora, and we 
incorporate soft, probabilistic domain membership 
into a translation  model. Unsupervised modeling of 
the training data produces naturally occurring sub- 
corpora, generalizing beyond corpus and genre. De- 
pending on the model used to select subcorpora, we 
can bias our translation toward any arbitrary distinc- 
tion. This reduces the problem to identifying what 
automatically defined subsets of the training corpus 
may be beneficial for translation.
  In this work, we consider the underlying latent 
topics of the documents (Blei et al., 2003). Topic 
modeling has received some use in SMT, for in- 
stance Bilingual LSA adaptation (Tam et al., 2007), 
and the BiTAM  model (Zhao and Xing,  2006), 
which uses  a  bilingual topic model for  learning 
alignment. In our case, by building a topic distri- 
bution for the source side of the training data, we 
abstract the notion of domain to include automati- 
cally derived subcorpora with probabilistic member- 
ship. This topic model infers the topic distribution 
of a test set and biases sentence translations  to ap- 
propriate topics. We accomplish this by introduc- 
ing topic dependent lexical probabilities directly as


c(f, e)/2.e c(f, e) . Phrase pair probabilities p(e|f )
are computed  from these as described  in Koehn et
al. (2003).
  Chiang et al. (2011) showed that is it  benefi- 
cial to condition the lexical weighting features on 
provenance by assigning  each sentence pair a set
of features, fs(e|f ), one for each domain s, which
compute  a new word translation table ps(e|f ) esti-
mated from only those sentences which belong to s:
cs(f, e)/2.e cs(f, e) , where cs(·) is the number of
occurrences of the word pair in s.

Topic Modeling for MT    We extend provenance 
to cover a set of automatically  generated topics zn. 
Given a parallel training corpus T composed of doc- 
uments di, we build a source side topic model over
T , which provides  a topic distribution p(zn|di) for
zn  = {1, . . . , K } over each document, using Latent
Dirichlet Allocation (LDA) (Blei et al., 2003). Then, 
we assign p(zn|di) to be the topic distribution for 
every sentence xj  ∈ di, thus enforcing topic sharing
across sentence pairs in the same document instead 
of treating them as unrelated.  Computing  the topic 
distribution over a document and assigning it to the 
sentences serves to tie the sentences together in the 
document context.
  To obtain the lexical probability conditioned on 
topic distribution, we first compute the expected 
count ezn (e, f ) of a word pair under topic zn:


features in the translation model, and interpolating
them log-linearly with our other features, thus allow-



ezn


(e, f ) =     p(zn|di)     cj (e, f ) 	(1)


ing us to discriminatively optimize their weights on


di ∈T


xj ∈di


an arbitrary objective function. Incorporating  these 
features into our hierarchical  phrase-based transla- 
tion system significantly improved translation per-


where cj (·) denotes the number of occurrences of
the word pair in sentence xj , and then compute:

ezn (e, f )


formance, by up to 1 BLEU and 3 TER over a strong
Chinese to English baseline.


pzn (e|f ) = 2.



e ezn (e, f )


(2)





2	Model Description
  

Thus, we will  
introduce 2·K  new word 
trans- lation tables, one 
for each pzn (e|f ) and 
pzn (f |e),


Lexical Weighting	Lexical weighting features es-
timate the quality of a phrase  pair by combining


and as many new corresponding features fzn


(e|f ),



the lexical translation probabilities of the words in 
the phrase2   (Koehn et al., 2003).  Lexical condi-
tional probabilities p(e|f ) are obtained with maxi-
mum likelihood estimates from relative frequencies

2 For hierarchical systems,  these correspond  to translation


fzn (f |e). The actual feature values we compute will
depend on the topic distribution of the document we

are translating.  For a test document  V , we infer 
topic assignments on V , p(zn|V ), keeping the topics
found from T fixed. The feature value then becomes
fzn (e|f )  = − log {pzn (e|f ) · p(zn|V )},  a combi-


topic distribution of the sentence from which we are 
extracting the phrase. To optimize the weights of 
these features we combine them in our linear model 
with the other features when computing the model 
score for each phrase pair3:



are learning how useful knowledge of the topic dis- 
tribution is, i.e., f1     :=  p(arg maxzn (p(zn |V ))(e|f ) · 
p(arg maxzn (p(zn|V ))|V ).
  Using F1, if we restrict our topics to have a one- 
to-one mapping with genre/collection4  we see that
our method fully recovers Chiang (2011).


λphp(e, f )
p


+ 	λzn fzn (e|f )
zn


(3)


F1   is appropriate  for  
cross-domain  
adaptation


  	    	  
unadapted features


  	    	  
adapted features


when we have advance 
knowledge that the 
distribu-
tion of the tuning data will 
match the test data, as in



 Combining the topic conditioned word translation 
table pzn (e|f ) computed from the training corpus 
with the topic distribution p(zn|V ) of the test sen-
tence being translated provides a probability on how 
relevant that translation table is to the sentence. This 
allows us to bias the translation toward the topic of 
the sentence. For example, if topic k is dominant in
T , pk (e|f ) may be quite large, but if p(k|V ) is very
small, then we should steer away from this phrase
pair and select a competing  phrase pair which may 
have a lower probability in T , but which is more rel- 
evant to the test sentence at hand.
  In many cases, document delineations may not be 
readily available for the training corpus. Further- 
more, a document  may be too broad, covering too 
many disparate topics, to effectively  bias the weights 
on a phrase level. For this case, we also propose a 
local LDA model (LTM), which treats each sentence 
as a separate document.
While  Chiang et al.  (2011) has to  explicitly
smooth the resulting ps(e|f ), since many word pairs
will be unseen for a given domain s, we are already
performing an implicit form of smoothing (when 
computing the expected counts),  since each docu- 
ment has a distribution over all topics, and therefore 
we have some probability of observing  each word 
pair in every topic.

Feature Representation   After obtaining the topic 
conditional features, there are two ways to present 
them to the model. They could answer the question 
F1: What is the probability under topic 1, topic 2, 
etc., or F2: What is the probability under the most 
probable topic, second most, etc.
  A model using F1 learns whether a specific topic 
is useful for translation, i.e., feature f1  would be
f1      :=  pz=1(e|f ) · p(z  = 1|V ).   With F2, we


Chiang (2011), where they tune and test on web. In 
general, we may not know what our data will be, so 
this will overfit the tuning set.
  F2, however, is intuitively what we want, since 
we do not want to bias our system toward a spe- 
cific distribution, but rather learn to utilize informa- 
tion from any topic distribution if it helps us cre- 
ate topic relevant translations. F2  is useful for dy- 
namic adaptation, where the adapted feature weight 
changes based on the source sentence.
  Thus, F2  is the approach we use in our work, 
which allows us to tune our system weights toward 
having topic information be useful, not toward a spe- 
cific distribution.
3   Experiments

Setup  To evaluate our approach, we performed ex- 
periments on Chinese to English MT in two set- 
tings. First, we use the FBIS corpus as our training 
bitext.  Since FBIS has document delineations,  we 
compare local topic modeling (LTM)  with model- 
ing at the document level (GTM). The second setting 
uses the non-UN and non-HK Hansards portions of 
the NIST training corpora with LTM only. Table 1 
summarizes the data statistics. For both settings, 
the data were lowercased, tokenized and aligned us- 
ing GIZA++ (Och and Ney, 2003) to obtain bidi- 
rectional alignments, which were symmetrized us- 
ing the grow-diag-final-and method (Koehn 
et al., 2003). The Chinese data were segmented us- 
ing the Stanford segmenter.  We trained a trigram 
LM on the English side of the corpus with an addi- 
tional 150M words randomly selected from the non- 
NYT and non-LAT portions of the Gigaword v4 cor- 
pus using modified Kneser-Ney smoothing (Chen 
and Goodman, 1996). We used cdec (Dyer et al.,

4



3 The unadapted lexical weight p(e|f )  is included in the


By having as many topics as genres/collections  and setting
p(zn |di ) to 1 for every sentence in the collection and 0 to ev-


Corpus	Sentences	Tokens


E
n
Z
h
F
B
I
S
N
I
S
T
2
6
9
K
1
.
6
M
1
0
.
3
M
4
4
.
4
M
7.
9
M
40
.4
M



Table 1: Corpus statistics

2010) as our decoder, and tuned the parameters of 
the system to optimize BLEU (Papineni et al., 2002) 
on the NIST MT06 tuning corpus using the Mar- 
gin Infused Relaxed Algorithm (MIRA) (Crammer 
et al., 2006; Eidelman, 2012). Topic modeling was 
performed with Mallet (Mccallum, 2002), a stan- 
dard implementation of LDA, using a Chinese  sto- 
plist and setting the per-document Dirichlet parame- 
ter α = 0.01. This setting of was chosen to encour- 
age sparse topic assignments, which make induced 
subdomains consistent within a document.

Results  Results for both settings are shown in Ta- 
ble 2. GTM models the latent topics at the document 
level, while LTM models each sentence as a separate 
document. To evaluate the effect topic granularity 
would have on translation, we varied the number of 
latent topics in each model to be 5, 10, and 20. On 
FBIS,  we can see that both models achieve moderate 
but consistent gains over the baseline on both BLEU 
and TER. The best model, LTM-10, achieves a gain 
of about 0.5 and 0.6 BLEU and 2 TER. Although the 
performance on BLEU for both the 20 topic models 
LTM-20 and GTM-20 is suboptimal, the TER im- 
provement is better. Interestingly, the difference in 
translation quality between capturing document co- 
herence in GTM and modeling purely on the sen- 
tence level is not substantial.5 In fact, the opposite 
is true, with the LTM models achieving better per- 
formance.6
  On the NIST corpus, LTM-10 again achieves the 
best gain of approximately 1 BLEU and up to 3 TER. 
LTM performs on par with or better than GTM, and 
provides significant gains even in the NIST data set- 
ting, showing that this method can be effectively  ap- 
plied directly on the sentence level to large training

  5 An avenue of future work would condition the sentence 
topic distribution on a document  distribution over topics (Teh 
et al., 2006).
6 As an empirical validation of our earlier intuition regarding
feature representation, presenting the features in the form of F1 
caused the performance to remain virtually unchanged from the 
baseline model.




















Table 2: Performance using FBIS training corpus 
(top) and NIST corpus (bottom). Improvements  are 
significant at the p <0.05 level, except where indicated 
(ns ).


corpora which have no document markings.  
De- pending on the diversity of training corpus,  a 
vary- ing number of underlying topics may be 
appropriate. However, in both settings, 10 topics 
performed best.

4   Discussion and 
Conclusion

Applying SMT to new domains requires 
techniques to inform our algorithms how best to 
adapt. This pa- per extended the usual notion of 
domains to finer- grained topic distributions 
induced in an unsuper- vised fashion.  We 
show that incorporating lexi- cal weighting 
features conditioned on soft domain membership 
directly into our model is an effective strategy for 
dynamically biasing SMT towards rele- vant 
translations, as evidenced by significant perfor- 
mance gains. This method presents several 
advan- tages over existing  approaches.  We can 
construct a topic model once on the training 
data, and use it infer topics on any test set to 
adapt the transla- tion model. We can also 
incorporate large quanti- ties of additional data 
(whether parallel or not) in the source language 
to infer better topics without re- lying on 
collection or genre annotations.  Multilin- gual 
topic models (Boyd-Graber and Resnik, 2010) 
would provide a technique to use data from 
multiple languages to ensure consistent topics.


Acknowledgments

Vladimir Eidelman is supported by a National De- 
fense Science  and Engineering Graduate  Fellow- 
ship.   This work was  also supported  in part by 
NSF grant #1018625, ARL  Cooperative  Agree- 
ment W911NF-09-2-0072, and by the BOLT and 
GALE programs of the Defense Advanced Research 
Projects Agency, Contracts HR0011-12-C-0015 and 
HR0011-06-2-001, respectively. Any opinions, find- 
ings, conclusions,  or recommendations expressed 
are the authors’ and do not necessarily reflect those 
of the sponsors.


References

Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec- 
tion. In Proceedings of Emperical Methods in Natural 
Language Processing.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty.    2003.   Latent Dirichlet Allocation.
  Journal of Machine Learning Research, 3:2003. 
Jordan Boyd-Graber  and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su- 
pervised latent Dirichlet allocation. In Proceedings of 
Emperical Methods in Natural Language Processing.
Stanley F. Chen and Joshua Goodman.  1996. An empir-
ical study of smoothing techniques for language mod- 
eling.  In Proceedings of the 34th Annual Meeting of 
the Association for Computational Linguistics, pages
310–318.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements  to lexical weighting. In Pro- 
ceedings of the Human Language Technology Confer- 
ence.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer.  2006.  Online passive- 
aggressive algorithms. Journal of Machine Learning 
Research, 7:551–585.
Chris Dyer, Adam Lopez, Juri Ganitkevitch,   Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, 
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A 
decoder, alignment, and learning framework for finite- 
state and context-free translation models. In Proceed- 
ings of ACL System Demonstrations.
Vladimir Eidelman. 2012.  Optimization strategies for
online large-margin learning in machine translation. 
In Proceedings of the Seventh Workshop on Statistical 
Machine Translation.
George Foster, Cyril Goutte, and Roland Kuhn.  2010.
Discriminative  instance weighting for domain adapta- 
tion in statistical machine translation. In Proceedings



of Emperical Methods in Natural Language Process- 
ing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.  Statistical phrase-based translation.  In Pro- 
ceedings of the 2003 Conference of the North Ameri- 
can Chapter of the Association for Computational Lin- 
guistics on Human Language Technology - Volume 1, 
NAACL ’03, Stroudsburg, PA, USA.
Spyros Matsoukas, Antti-Veikko  I.  Rosti,  and Bing 
Zhang. 2009. Discriminative  corpus weight estima- 
tion for machine translation. In Proceedings of Em- 
perical Methods in Natural Language Processing.
A. K. Mccallum. 2002. MALLET:  A Machine Learning 
for Language Toolkit.
Franz Och and Hermann Ney. 2003. A systematic com- 
parison of various statistical alignment models.  In 
Computational Linguistics, volume 29(21), pages 19–
51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei- 
Jing Zhu. 2002. BLEU: a method for automatic evalu- 
ation of machine translation. In Proceedings of the As- 
sociation for Computational Linguistics, pages 311–
318.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us- 
ing comparable corpora. In Proceedings of Emperical 
Methods in Natural Language Processing.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz.  2007.
Bilingual LSA-based adaptation for statistical machine 
translation. Machine Translation, 21(4):187–207.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and 
David M. Blei.   2006.   Hierarchical Dirichlet pro- 
cesses. Journal of the American Statistical Associa- 
tion, 101(476):1566–1581.
Bing Zhao and Eric P. Xing.  2006. BiTAM:  Bilingual 
topic admixture models for word alignment. In Pro- 
ceedings of the Association for Computational Lin- 
guistics.



