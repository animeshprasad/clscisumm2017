Proceedings of NAACL-HLT 2013, pages 897?906,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Same Referent, Different Words:
Unsupervised Mining of Opaque Coreferent Mentions
Marta Recasens*, Matthew Can?, and Dan Jurafsky*
*Linguistics Department, Stanford University, Stanford, CA 94305
?Computer Science Department, Stanford University, Stanford, CA 94305
recasens@google.com, {mattcan,jurafsky}@stanford.edu
Abstract
Coreference resolution systems rely heav-
ily on string overlap (e.g., Google Inc. and
Google), performing badly on mentions with
very different words (opaque mentions) like
Google and the search giant. Yet prior at-
tempts to resolve opaque pairs using ontolo-
gies or distributional semantics hurt precision
more than improved recall. We present a new
unsupervised method for mining opaque pairs.
Our intuition is to restrict distributional se-
mantics to articles about the same event, thus
promoting referential match. Using an En-
glish comparable corpus of tech news, we built
a dictionary of opaque coreferent mentions
(only 3% are in WordNet). Our dictionary can
be integrated into any coreference system (it
increases the performance of a state-of-the-art
system by 1% F1 on all measures) and is eas-
ily extendable by using news aggregators.
1 Introduction
Repetition is one of the most common coreferential
devices in written text, making string-match features
important to all coreference resolution systems. In
fact, the scores achieved by just head match and a
rudimentary form of pronominal resolution1 are not
far from that of state-of-the-art systems (Recasens
and Hovy, 2010). This suggests that opaque men-
tions (i.e., lexically different) such as iPad and the
Cupertino slate are a serious problem for modern
systems: they comprise 65% of the non-pronominal
1Closest NP with the same gender and number.
errors made by the Stanford system on the CoNLL-
2011 data. Solving this problem is critical for over-
coming the recall gap of state-of-the-art systems
(Haghighi and Klein, 2010; Stoyanov et al, 2009).
Previous systems have turned either to ontologies
(Ponzetto and Strube, 2006; Uryupina et al, 2011;
Rahman and Ng, 2011) or distributional semantics
(Yang and Su, 2007; Kobdani et al, 2011; Bansal
and Klein, 2012) to help solve these errors. But nei-
ther semantic similarity nor hypernymy are the same
as coreference: Microsoft and Google are distribu-
tionally similar but not coreferent; people is a hy-
pernym of both voters and scientists, but the peo-
ple can corefer with the voters, but is less likely
to corefer with the scientists. Thus ontologies lead
to precision problems, and to recall problems like
missing NE descriptions (e.g., Apple and the iPhone
maker) and metonymies (e.g., agreement and word-
ing), while distributional systems lead to precision
problems like coreferring Microsoft and the Moun-
tain View giant because of their similar vector rep-
resentation (release, software, update).
We increase precision by drawing on the intuition
that referents that are both similar and participate in
the same event are likely to corefer. We restrict dis-
tributional similarity to collections of articles that
discuss the same event. In the following two doc-
uments on the Nexus One from different sources,
we take the subjects of the identical verb release?
Google and the Mountain View giant?as coreferent.
Document 1: Google has released a software update.
Document 2: The Mountain View giant released an update.
Based on this idea, we introduce a new unsuper-
vised method that uses verbs in comparable corpora
897
as pivots for extracting the hard cases of corefer-
ence resolution, and build a dictionary of opaque
coreferent mentions (i.e., the dictionary entries are
pairs of mentions). This dictionary is then inte-
grated into the Stanford coreference system (Lee et
al., 2011), resulting in an average 1% improvement
in the F1 score of all the evaluation measures.
Our work points out the importance of context to
decide whether a specific mention pair is coreferent.
On the one hand, we need to know what semantic
relations are potentially coreferent (e.g., content and
video). On the other, we need to distinguish contexts
that are compatible for coreference?(1) and (2-a)?
from those that are not?(1) and (2-b).
(1) Elemental helps those big media entities process
content across a full slate of mobile devices.
(2) a. Elemental provides the picks and shovels to
make video work across multiple devices.
b. Elemental is powering the video for HBO Go.
Our dictionary of opaque coreferent pairs is our so-
lution to the first problem, and we report on some
preliminary work on context compatibility to ad-
dress the second problem.
2 Building a Dictionary for Coreference
To build a dictionary of semantic relations that are
appropriate for coreference we will use a cluster of
documents about the same news event, which we
call a story. Consider as an example the story Sprint
blocks out vacation days for employees. We deter-
mine using tf-idf the representative verbs for this
story, the main actions and events of the story (e.g.,
block out). Since these verbs are representative of
the story, different instances across documents in the
cluster are likely to refer to the same events (Sprint
blocks out. . . and the carrier blocks out. . . ). By the
same logic, the subjects and objects of the verbs are
also likely to be coreferent (Sprint and the carrier).
2.1 Comparable corpus
To build our dictionary, we require a monolingual
comparable corpus, containing clusters of docu-
ments from different sources that discuss the same
story. To ensure likely coreference, the story must
be the very same; documents that are merely clus-
tered by (general) topic do not suffice. The corpus
does not need to be parallel in the sense that docu-
ments in the same cluster do not need to be sentence
aligned.
We used Techmeme,2 a news aggregator for tech-
nology news, to construct a comparable corpus. Its
website lists the major tech stories, each with links
to several articles from different sources. We used
the Readability API3 to download and extract the ar-
ticle text for each document. We scraped two years
worth of data from Techmeme and only took stories
containing at least 5 documents. Our corpus con-
tains approximately 160 million words, 25k stories,
and 375k documents. Using a corpus from Tech-
meme means that our current coreference dictionary
is focused on the technological domain. Our method
can be easily extended to other domains, however,
since getting comparable corpora is relatively sim-
ple from the many similar news aggregator sites.
2.2 Extraction
After building our corpus, we used Stanford?s
CoreNLP tools4 to tokenize the text and annotate it
with POS tags and named entity types. We parsed
the text using the MaltParser 1.7, a linear time de-
pendency parser (Nivre et al, 2004).5
We then extracted the representative verbs of each
story by ranking the verbs in each story according
to their tf-idf scores. We took the top ten to be the
representative set. For each of these verbs, we clus-
tered together its subjects and objects (separately)
across instances of the verb in the document clus-
ter, excluding pronouns and NPs headed by the same
noun. For example, suppose that crawl is a represen-
tative verb and that in one document we have Google
crawls web pages and The search giant crawls sites
in another document. We will create the clusters
{Google, the search giant} and {web pages, sites}.
When detecting representative verbs, we kept
phrasal verbs as a unit (e.g., give up) and excluded
auxiliary and copular verbs,6 light verbs,7 and report
2http://www.techmeme.com
3http://www.readability.com/developers/api
4http://nlp.stanford.edu/software/corenlp.shtml
5http://www.maltparser.org
6Auxiliary and copular verbs include appear, be, become,
do, have, seem.
7Light verbs include do, get, give, go, have, keep, make, put,
set, take.
898
verbs,8 as they are rarely representative of a story
and tend to add noise to our dictionary. To increase
recall, we also considered the synonyms from Word-
Net and nominalizations from NomBank of the rep-
resentative verbs, thus clustering together the sub-
jects and objects of any synonym as well as the ar-
guments of nominalizations.9 We used syntactic re-
lations instead of semantic roles because the Malt-
Parser is faster than any SRL system, but we checked
for frequent syntactic structures in which the agent
and patient are inverted, such as passive and ergative
constructions.10
From each cluster of subject or object mentions,
we generated all pairs of mentions. This forms the
initial version of our dictionary. The next sections
describe how we filter and generalize these pairs.
2.3 Filtering
We manually analyzed 200 random pairs and clas-
sified them into coreference and spurious relations.
The spurious relations were caused by errors due to
the parser, the text extraction, and violations of our
algorithm assumption (i.e., the representative verb
does not refer to a unique event). We employed a fil-
tering strategy to improve the precision of the dictio-
nary. We used a total of thirteen simple rules, which
are shown in Table 1. For instance, we sometimes
get the same verb with non-coreferent arguments,
especially in tech news that compare companies or
products. In these cases, NEs are often used, and so
we can get rid of a large number of errors by auto-
matically removing pairs in which both mentions are
NEs (e.g., Google and Samsung).
Before filtering, 53% of all relations were good
coreference relations versus 47% spurious ones. Of
the relations that remained after filtering, 74% were
8Report verbs include argue, claim, say, suggest, tell, etc.
9As a general rule, we extract possessive phrases as subjects
(e.g. Samsung?s plan) and of -phrases as objects (e.g. develop-
ment of the new logo).
10We can easily detect passive subjects (i-b) as they have their
own dependency label, and ergative subjects (ii-b) using a list
of ergative verbs extracted from Levin (1993).
(i) a. Developers hacked the device.
b. The device was hacked.
(ii) a. Police scattered the crowds.
b. The crowds scattered.
Both mentions are NEs
Both mentions appear in the same document
Object of a negated verb
Enumeration or list environment
Sentence is ill-formed
Number NE
Temporal NE
Quantifying noun
Coordinated
Verb is preceded by a determiner or an adjective
Head is not nominal
Sentence length ? 100
Mention length ? 70% of sentence length
Table 1: Filters to improve the dictionary precision. Un-
less otherwise noted, the filter was applied if either men-
tion in the relation satisfied the condition.
coreferent and only 26% were spurious. In total,
about half of the dictionary relations were removed
in the filtering process, resulting in a total of 128,492
coreferent pairs.
2.4 Generalization
The final step of generating our dictionary is to pro-
cess the opaque mention pairs so that they gener-
alize better. We strip mentions of any determiners,
relative clauses, and -ing and -ed clauses. However,
we retain adjectives and prepositional modifiers be-
cause they are sometimes necessary for corefer-
ence to hold (e.g., online piracy and distribution
of pirated material). We also generalize NEs to
their types so that our dictionary entries can func-
tion as templates (e.g., Cook?s departure becomes
<person>?s departure), but we keep NE tokens that
are in the head position as these are pairs containing
world knowledge (e.g., iPad and slate). Finally, we
replace all tokens with their lemmas. Table 2 shows
a snapshot of the dictionary.
2.5 Semantics of coreference
From manually classifying a sample of 200 dictio-
nary pairs (e.g., Table 2), we find that our dictio-
nary includes many synonymy (e.g., IPO and offer-
ing) and hypernymy relations (e.g., phone and de-
vice), which are the relations that are typically ex-
tracted from ontologies for coreference resolution.
However, not all synonyms and hypernyms are valid
for coreference (recall the voters-people vs. scien-
tists-people example in the introduction), so our dic-
899
Mention 1 Mention 2
offering IPO
user consumer
phone device
Apple company
hardware key digital lock
iPad slate
content photo
bug issue
password login information
Google search giant
site company
filing complaint
company government
TouchPad tablet
medical record medical file
version handset
information credit card
government chairman
app software
Android platform
the leadership change <person>?s departure
change update
Table 2: Coreference relations in our dictionary.
tionary only includes the ones that are relevant for
coreference (e.g., update and change). Furthermore,
only 3% of our 128,492 opaque pairs are related in
WordNet, confirming that our method is introducing
a large number of new semantic relations.
We also discover other semantic relations that are
relevant for coreference, such as various metonymy
relations like mentioning the part for the whole.
Again though, we can use some part-whole rela-
tions coreferentially (e.g., car and engine) but not
others (e.g., car and window). Our dictionary in-
cludes part-whole relations that have been observed
as coreferent at least once (e.g., company and site).
We also extract world-knowledge descriptions for
NEs (e.g., Google and the Internet giant).
3 Integration into a Coreference System
We next integrated our dictionary into an existing
coreference resolution system to see if it improves
resolution.
3.1 Stanford coreference resolution system
Our baseline is the Stanford coreference resolution
system (Lee et al, 2011) which was the highest-
scoring system in the CoNLL-2011 Shared Task,
Sieve number Sieve name
1 Discourse processing
2 Exact string match
3 Relaxed string match
4 Precise constructs
5?7 Strict head match
8 Proper head noun match
9 Relaxed head match
10 Pronoun match
Table 3: Rules of the baseline system.
and was also part of the highest-scoring system in
the CoNLL-2012 Shared Task (Fernandes et al,
2012). It is a rule-based system that includes a to-
tal of ten rules (or ?sieves?) for entity coreference,
shown in Table 3. The sieves are applied from high-
est to lowest precision, each rule extending entities
(i.e., mention clusters) built by the previous tiers, but
never modifying links previously made. The major-
ity of the sieves rely on string overlap.11
The highly modular architecture made it easy for
us to integrate additional sieves using our dictionary
to increase recall.
3.2 Dictionary sieves
We propose four new sieves, each one using a differ-
ent granularity level from our dictionary, with each
consecutive sieve using higher precision relations
than the previous one. The Dict 1 sieve uses only
the heads of mentions in each relation (e.g., devices).
Dict 2 uses the heads and one premodifier, if it ex-
ists (e.g., iOS devices). Dict 3 uses the heads and up
to two premodifiers (e.g., new iOS devices). Dict 4
uses the full mentions, including any postmodifiers
(e.g., new iOS devices for businesses).
We take advantage of frequency counts to get rid
of low-precision coreference pairs and only keep
(i) pairs that have been seen more than 75 times
(Dict 1) or 15 times (Dict 2, Dict 3, Dict 4);
and (ii) pairs with a frequency count larger than 8
(Dict 1) or 2 (Dict 2, Dict 3, Dict 4) and a normal-
ized PMI score larger than 0.18. We use the nor-
malized PMI score (Bouma, 2009) as a measure of
association between the mentions mi and mj of a
11Exceptions: sieve 1 links first-person pronouns inside a
quotation with the speaker; sieve 4 links mention pairs that ap-
pear in an appositive, copular, acronym, etc., construction; sieve
10 implements generic pronominal coreference resolution.
900
dictionary pair, computed as
(ln p(mi,mj)p(mi)p(mj)) /? ln p(mi,mj)
These thresholds were set on the development set.
Since the different coreference rules in the Stan-
ford system are arranged in decreasing order of pre-
cision, we start by applying the sieve that uses the
highest-precision relations in the dictionary (Dict 4),
followed by Dict 3, Dict 2, and Dict 1. We add
these new sieves right before the last sieve, as the
pronominal sieve can perform better if opaque men-
tions have been successfully linked. The current
sieves only use the dictionary for linking singular
mentions, as the experiments on the dev showed that
plural mentions brought too much noise.
For any mention pair under analysis, each sieve
checks whether it is supported by the dictionary as
well as whether basic constraints are satisfied, such
as number, animacy and NE-type agreement, and
NE?common noun order (not the opposite).
4 Experiments
4.1 Data
Although our dictionary creation technology can ap-
ply across domains, our current coreference dictio-
nary is focused on the technical domain, so we cre-
ated a coreference labeled corpus in this domain for
evaluation. We extracted new data from Techmeme
(different from that used to extract the dictionary) to
create a development and a test set. It is important
to note that we do not need comparable data at this
stage. A massive comparable corpus is only needed
for mining the coreference dictionary (Section 2);
once it is built, it can be used for solving corefer-
ence within and across documents.
The annotation was performed by two experts, us-
ing the Callisto annotation tool. The development
and test sets were annotated with coreference rela-
tions following the OntoNotes guidelines (Pradhan
et al, 2007). We annotated full NPs (with all mod-
ifiers), excluding appositive phrases and predicate
nominals. Only premodifiers that were proper nouns
or possessive phrases were annotated. We extended
the OntoNotes guidelines by also annotating single-
tons. Table 4 shows the dataset statistics.
Dataset Stories Docs Tokens Entities Mentions
Dev 4 27 7837 1360 2279
Test 24 24 8547 1341 2452
Table 4: Dataset statistics: development (dev) and test.
4.2 Evaluation measures
We evaluated using six coreference measures, as
they sometimes provide different results and there is
no agreement on a standard. We used the scorer of
the CoNLL-2011 Shared Task (Pradhan et al, 2011).
? MUC (Vilain et al, 1995). Link-based metric
that measures how many links the true and sys-
tem partitions have in common.
? B3 (Bagga and Baldwin, 1998). Mention-based
metric that measures the proportion of mention
overlap between gold and predicted entities.
? CEAF-?3 (Luo, 2005). Mention-based metric
that, unlike B3, enforces a one-to-one align-
ment between gold and predicted entities.
? CEAF-?4 (Luo, 2005). The entity-based ver-
sion of the above metric.
? BLANC (Recasens and Hovy, 2011). Link-
based metric that considers both coreference
and non-coreference links.
? CoNLL (Denis and Baldridge, 2009). Average
of MUC, B3 and CEAF-?4. It was the official
metric of the CoNLL-2011 Shared Task.
4.3 Results
We always start from the baseline, which corre-
sponds to the Stanford system with the sieves listed
in Table 3. This is the set of sieves that won the
CoNLL-2011 Shared Task (Pradhan et al, 2011),
and they exclude WordNet.
Table 5 shows the incremental scores, on the de-
velopment set, for the four sieves that use the dictio-
nary, corresponding to the different granularity lev-
els, from the highest precision one (Dict 4) to the
lowest one (Dict 1). The largest improvement is
achieved by Dict 4 and Dict 3, as they improve re-
call (R) without hurting precision (P). R is equiva-
lent to P for CEAF-?4, and vice versa. The other
two sieves increase R further, especially Dict 1,
but also decrease P, although the trade-off for the
F-score (F1) is still positive. It is the best score, with
the exception of B3.
901
MUC B3 CEAF-?3 CEAF-?4 BLANC CoNLL
System R P F1 R P F1 R / P / F1 R P F1 R P B F1
Baseline 55.9 72.8 63.3 74.1 89.8 81.2 74.6 85.2 73.6 79.0 66.6 87.1 72.6 74.5
+Dict 4 57.0 72.8 63.9 75.1 89.4 81.6 75.3 85.2 74.3 79.4 68.2 87.3 74.2 75.0
+Dict 3 57.6 72.8 64.3 75.4 89.3 81.7 75.5 85.1 74.6 79.5 68.4 87.2 74.4 75.2
+Dict 2 57.6 72.5 64.2 75.4 89.1 81.7 75.4 85.0 74.6 79.5 68.4 87.0 74.3 75.1
+Dict 1 58.4 71.9 64.5 75.7 88.5 81.6 75.5 84.6 75.1 79.6 68.6 86.6 74.4 75.2
Table 5: Incremental results for the four sieves using our dictionary on the development set. Baseline is the Stanford
system without the WordNet sieves. Scores are on gold mentions.
MUC B3 CEAF-?3 CEAF-?4 BLANC CoNLL
System R P F1 R P F1 R / P / F1 R P F1 R P B F1
Baseline 62.4 78.2 69.4 73.7 89.5 80.8 75.1 86.2 73.8 79.5 71.4 88.6 77.3 76.6
w/ WN 63.5 75.3 68.9 74.2 87.5 80.3 74.1 83.7 74.1 78.6 71.8 87.3 77.3 75.9
w/ Dict 64.7* 77.6* 70.6* 75.7* 88.5* 81.6* 76.5* 85.3* 75.0* 79.9* 74.6* 88.6 79.9* 77.3*
w/ Dict +
Context
64.8* 77.8* 70.7* 75.7* 88.6* 81.7* 76.5* 85.5* 75.1* 80.0* 74.6* 88.7 79.9* 77.5*
Table 6: Performance on the test set. Scores are on gold mentions. Stars indicate a statistically significant difference
with respect to the baseline.
Table 6 reports the scores on the test set and com-
pares the scores obtained by adding the WordNet
sieves to the baseline (w/ WN) with those obtained
by adding the dictionary sieves (w/ Dict). Whereas
adding WordNet only brings a small improvement
in R that is much lower than the loss in P, the dic-
tionary sieves succeed in increasing R by a larger
amount and at a smaller cost to P, resulting in a sig-
nificant improvement in F1: 1.2 points according to
MUC, 0.8 points according to B3, 1.4 points accord-
ing to CEAF-?3, 0.4 points according to CEAF-?4,
2.6 points according to BLANC, and 0.7 points ac-
cording to CoNLL. Section 5.2 presents the last line
(w/ Dict + Context).
5 Discussion
5.1 Error analysis
Thanks to the dictionary, the coreference system im-
proves the baseline by establishing coreference links
between the bolded mentions in (3) and (4).
(3) With Groupon Inc.?s stock down by half from its IPO
price and the company heading into its first earnings
report since an accounting blowup [...] outlining op-
portunity ahead and the promise of new products for
the daily-deals company.
(4) Thompson revealed the diagnosis as evidence arose
that seemed to contradict his story about why he was
not responsible for a degree listed on his resume that
he does not have, the newspaper reports, citing anony-
mous sources familiar with the situation [...] a Yahoo
board committee appointed to investigate the matter.
The first case requires world knowledge and the sec-
ond case, semantic knowledge.
We manually analyzed 40 false positive errors
caused by the dictionary sieves. Only a small num-
ber of them were due to noise in the dictionary. The
majority of errors were due to the discourse context:
the two mentions could be coreferent, but not in the
given context. For example, Apple and company are
potentially coreferent?which is successfully cap-
tured by our dictionary?and while they are coref-
erent in (5), they are not in (6).12
(5) It will only get better as Apple will be updating it
with iOS6, an operating system that the company will
likely be showing off this summer.
(6) Since Apple reinvented the segment, Microsoft is the
latest entrant into the tablet market, banking on its
Windows 8 products to bridge the gap between PCs
and tablets. [...] The company showed off Windows 8
last September.
12Examples in this section show gold coreference relations in
bold and incorrectly predicted coreferent mentions in italics.
902
In these cases it does not suffice to check whether
the opaque mention pair is included in the corefer-
ence dictionary, but we need a method for taking the
surrounding context into account. In the next section
we present our preliminary work in this direction.
5.2 Context fit
To help the coreference system choose the right an-
tecedent in examples like (6), we exploit the fact
that the company is closely followed by Windows 8,
which is a clue for selecting Microsoft instead of Ap-
ple as the antecedent. We devise a contextual con-
straint that rules out a mention pair if the contexts are
incompatible. To check for context compatibility,
we borrow the idea of topic signatures from Lin and
Hovy (2000) and that Agirre et al (2001) used for
Word Sense Disambiguation. Instead of identifying
the keywords of a topic, we find the NEs that tend
to co-occur with another NE. For example, the sig-
nature for Apple should include terms like iPhone,
MacBook, iOS, Steve Jobs, etc. This is what we call
the NE signature for Apple.
To construct NE signatures, we first compute the
log-likelihood ratio (LLR) statistic between NEs in
our corpus (the same one used to build the dictio-
nary). Then, the signature for a NE, w, is the list of
k other NEs that have the highest LLR with w. The
LLR between two NEs, w1 and w2, is ?2 ln
L(H1)
L(H2)
,
where H1 is the hypothesis that
P (w1 ? sent|w2 ? sent) = P (w1 ? sent|w2 /? sent),
H2 is the hypothesis that
P (w1 ? sent|w2 ? sent) 6= P (w1 ? sent|w2 /? sent),
and L(?) is the likelihood. We assume a binomial
distribution for the likelihood.
Once we have NE signatures, we determine the
context fit as follows. When the system compares a
NE antecedent with a (non-NE) anaphor, we check
whether any NEs in the anaphor?s sentence are in
the antecedent?s signature. We also check whether
the antecedent is in the signature list of any NE?s in
the anaphor?s sentence. If neither of these is true,
we do not allow the system to link the antecedent
and the anaphor. In (6), Apple is not linked with the
company because it is not in Windows? signature,
and Windows is not in Apple?s signature either (but
Microsoft is in Windows? signature).
The last two lines in Table 6 compare the scores
without using this contextual feature (w/ Dict) with
those using context (w/ Dict + Context). Our feature
for context compatibility leads to a small but posi-
tive improvement, taking the final improvement of
the dictionary sieves to be about 1 percentage point
above the baseline according to all six evaluation
measures. We leave as future work to test this idea
on a larger test set and refine it further so as to ad-
dress more challenging cases where comparing NEs
is not enough, like in (7).
(7) Snapchat will notify users [...] The program is avail-
able for free in Apple?s App Store [...] While the com-
pany ?attempts to delete image data as soon as possi-
ble after the message is transmitted,? it cannot guaran-
tee messages will always be deleted.
To resolve (7), it would be helpful to know that
Snapchat is a picture messaging platform, as the
context mentions image data and messages.
6 Related Work
Existing ontologies are not optimal for solving
opaque coreferent mentions because of both a preci-
sion and a recall problem (Lee et al, 2011; Uryupina
et al, 2011). On the other hand, using data-driven
methods such as distributional semantics for coref-
erence resolution suffers especially from a precision
problem (Ng, 2007). Our work combines ideas from
distributional semantics and paraphrase acquisition
methods in order to efficiently use contextual infor-
mation to extract coreference relations.
The main idea that we borrow from paraphrase
acquisition is the use of monolingual (non-parallel)
comparable corpora, which have been exploited
to extract both sentence-level (Barzilay and McK-
eown, 2001) and sub-sentential-level paraphrases
(Shinyama and Sekine, 2003; Wang and Callison-
Burch, 2011). To ensure that the NPs are coreferent,
we limit the meaning of comparable corpora to col-
lections of documents that report on the very same
story, as opposed to collections of documents that
are about the same (general) topic. However, the
distinguishing factor is that while most paraphrasing
studies, including Lin and Pantel (2001), use NEs?
or nouns in general?as pivots to learn paraphrases
of their surrounding context, we use verbs as pivots
to learn coreference relations at the NP level.
There are many similarities between paraphrase
and coreference, and our work is most similar to
903
that by Wang and Callison-Burch (2011). However,
some paraphrases that might not be considered to
be valid (e.g., under $200 and around $200) can
be acceptable coreference relations. Unlike Wang
and Callison-Burch (2011), we do not work on doc-
ument pairs but on sets of at least five (comparable)
documents, and we do not require sentence align-
ment, but just verb alignment.
Another source of inspiration is the work by Bean
and Riloff (2004). They use contextual roles (i.e.,
the role that an NP plays in an event) for extract-
ing patterns that can be used in coreference reso-
lution, showing the relevance of verbs in deciding
on coreference between their arguments. However,
they use a very small corpus (two domains) and do
not aim to build a dictionary. The idea of creating
a repository of extracted concept-instance relations
appears in Fleischman et al (2003), but restricted
to person-role pairs, e.g. Yasser Arafat and leader.
Although it was originally designed for answering
who-is questions, Daume? III and Marcu (2005) suc-
cessfully used it for coreference resolution.
The coreference relations that we extract might
overlap but go beyond those detected by Bansal and
Klein (2012)?s Web-based features. First, they focus
on NP headwords, while we extract full NPs, includ-
ing multi-word mentions. Second, the fact that they
use the Google n-gram corpus means that the two
headwords must appear at most four words apart,
thus ruling out coreferent mentions that can only ap-
pear far from each other. Finally, while their extrac-
tion patterns focus on synonymy and hypernymy re-
lations, we discover other types of semantic relations
that are relevant for coreference (Section 2.5).
7 Conclusions
We have pointed out an important problem with cur-
rent coreference resolution systems: their heavy re-
liance on string overlap. Pronouns aside, opaque
mentions account for 65% of the errors made by
state-of-the-art systems. To improve coreference
scores beyond 60-70%, we therefore need to make
better use of semantic and world knowledge to deal
with non-identical-string coreference. But, as we
have also shown, coreference is not the same as se-
mantic similarity or hypernymy. Only certain se-
mantic relations in certain contexts are good cues for
coreference. We therefore need semantic resources
specifically targeted at coreference.
We proposed a new solution for detecting opaque
mention pairs: restricting distributional similarity to
a comparable corpus of articles about the very same
story, thus ensuring that similar mentions will also
likely be coreferent. We used this corpus to build a
dictionary focused on coreference, and successfully
extracted the specific semantic and world knowledge
relevant for coreference. The resulting dictionary
can be added on top of any coreference system to
increase recall at a minimum cost to precision. Inte-
grated into the Stanford coreference resolution sys-
tem, which won the CoNLL-2011 shared task, the
F-score increases about 1 percentage point accord-
ing to all of the six evaluation measures. The dictio-
nary and NE signatures are available on the Web.13
We showed that apart from the need for extracting
coreference-specific semantic and world knowledge,
we need to take into account the context surrounding
the mentions. The results from our preliminary work
for identifying incompatible contexts is promising.
Our unsupervised method for extracting opaque
coreference relations can be easily extended to other
domains by using online news aggregators, and
trained on more data to build a more comprehensive
dictionary that can increase recall even further. We
integrated the dictionary into a rule-based corefer-
ence system, but it remains for future work to in-
tegrate it into a learning-based architecture, where
the system can combine the dictionary features with
other features. This can also make it easier to in-
clude contextual features that take into account how
well a dictionary pair fits in a specific context.
Acknowledgments
We would like to thank the members of the Stanford
NLP Group, Valentin Spitkovsky, and Ed Hovy for
valuable comments at various stages of the project.
The first author was supported by a Beatriu de
Pino?s postdoctoral scholarship (2010 BP-A 00149)
from Generalitat de Catalunya. We also gratefully
acknowledge the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
13http://nlp.stanford.edu/pubs/coref-dictionary.zip
904
References
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching wordnet concepts with topic
signatures. In Proceedings of the NAACLWorkshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations, pages 23?28.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
Mohit Bansal and Dan Klein. 2012. Coreference seman-
tics from web features. In Proceedings of ACL, pages
389?398.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, pages 50?57.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. In Proceedings of NAACL-HTL.
Geolof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceedings
of the Biennial GSCL Conference, pages 31?40.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
HLT-EMNLP, pages 97?104.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87?96.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidiu?.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Pro-
ceedings of CoNLL - Shared Task, pages 41?48.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In Proceedings of ACL, pages 1?7.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL, pages 385?393.
Hamidreza Kobdani, Hinrich Schu?tze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of ACL, pages 783?792.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 Shared Task. In Proceedings
of CoNLL - Shared Task, pages 28?34.
Beth Levin. 1993. English Verb Class and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of COLING, pages 495?501.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discov-
ery of inference rules from text. In Proceedings of the
ACM SIGKDD, pages 323?328.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP, pages
25?32.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of IJCAI, pages 1689?
1694.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT-NAACL, pages 192?199.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
OntoNotes. In Proceedings of ICSC, pages 446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL - Shared Task, pages 1?27.
Altaf Rahman and Vincent Ng. 2011. Coreference reso-
lution with world knowledge. In Proceedings of ACL,
pages 814?824.
Marta Recasens and Eduard Hovy. 2010. Corefer-
ence resolution across corpora: Languages, coding
schemes, and preprocessing information. In Proceed-
ings of ACL, pages 1423?1432.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase
acquisition for information extraction. In Proceedings
of ACL, pages 65?71.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP, pages 656?664.
Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and
filtering methods in using web knowledge for coref-
erence resolution. In Proceedings of FLAIRS, pages
317?322.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
905
Rui Wang and Chris Callison-Burch. 2011. Para-
phrase fragment extraction from monolingual compa-
rable corpora. In Proceedings of the 4th ACL Work-
shop on Building and Using Comparable Corpora,
pages 52?60.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings of ACL,
pages 528?535.
906
