<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes an automatic, context-sensitive, word-error correction system based on statistical language modeling (SLM) as applied to optical character recognition (OCR) post­ processing.</S>
		<S sid ="2" ssid = "2">The system exploits information from multiple sources, including letter n-grams, character confusion probabilities, and word-bigram probabilities.</S>
		<S sid ="3" ssid = "3">Letter n-grams are used to index the words in the lexicon.</S>
		<S sid ="4" ssid = "4">Given a sentence to be corrected, the system decomposes each string in the sentence into letter n-grams and retrieves word candidates from the lexicon by comparing string n-grams with lexicon-entry n-grams.</S>
		<S sid ="5" ssid = "5">The retrieved candidates are ranked by the conditional probability of matches with the string, given character confusion probabilities.</S>
		<S sid ="6" ssid = "6">Finally, the word-bigram model and Viterbi algorithm are used to determine the best scoring word sequence for the sentence.</S>
		<S sid ="7" ssid = "7">The system can correct non-word errors as well as real-word errors and achieves a 60.2% error reduction rate for real OCR text.</S>
		<S sid ="8" ssid = "8">In addition, the system can learn the character confusion probabilities for a specific OCR environment and use them in self-calibration to achieve better performance.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">Word errors present problems for various text- or speech-based applications such as optical char­ acter recognition (OCR) and voice-input computer interfaces.</S>
			<S sid ="10" ssid = "10">In particular, though current OCR technology is quite refined and robust, sources such as old books, poor-quality (nth-generation) photocopies, and faxes can still be difficult to process and may cause many OCR errors.</S>
			<S sid ="11" ssid = "11">For OCR to be truly useful in a wide range of applications, such as office automation and information retrieval systems, OCR reliability must be improved.</S>
			<S sid ="12" ssid = "12">A method for the automatic correction of OCR errors would be clearly beneficial.</S>
			<S sid ="13" ssid = "13">Essentially, there are two types of word errors: non-word errors and real-word errors.</S>
			<S sid ="14" ssid = "14">A non­ word error occurs when a word in a source text is interpreted (under OCR) as a string that does not correspond to any valid word in a given word list or dictionary.</S>
			<S sid ="15" ssid = "15">A real-word error occurs when a source-text word is interpreted as a string that actually does occur in the dictionary, but is not identical with the source-text word.</S>
			<S sid ="16" ssid = "16">For example, if the source text &quot;John found the man&quot; is rendered as &quot;John fomd he man&quot; by an OCR device, then &quot;fomd&quot; is a non-word error and &quot;he&quot; is a real-word error.</S>
			<S sid ="17" ssid = "17">In general, non-word errors will never correspond to any dictionary entries and will include wildly incorrect strings (such as&quot;# &amp;&amp;&quot;) as well as misrecognized alphanumeric sequences (such as &quot;BN234&quot; for &quot;8N234&quot;).</S>
			<S sid ="18" ssid = "18">However, some non-word errors might become real­ word errors if the size of the word list or dictionary increases.</S>
			<S sid ="19" ssid = "19">(For example, the word &quot;ruel&quot;1 might count as a non-word error for the source-text word &quot;rut&quot; if a small dictionary is used for reference, but count as a real-word error if an unabridged dictionary is used.)</S>
			<S sid ="20" ssid = "20">While non-word errors might be corrected without considering the context in which the error occurs, a real-word error can only be corrected by taking context into account.</S>
			<S sid ="21" ssid = "21">The problems of word-error detection and correction have been studied for several decades.</S>
			<S sid ="22" ssid = "22">A good survey in this area can be found in [Kukich 1992].</S>
			<S sid ="23" ssid = "23">Most traditional word-correction techniques concentrate on non-word error correction and do not consider the context in which the error appears.</S>
			<S sid ="24" ssid = "24">Recently, statistical language models (SLMs) and feature-based methods have been used for context-sensitive spelling-error correction.</S>
			<S sid ="25" ssid = "25">For example, Atwell and Elliittm [1987] have used a part-of-speech (POS) tagging method to detect the real-word errors in text.</S>
			<S sid ="26" ssid = "26">Mays and colleagues [1991] have exploited word trigrams to detect and correct both the non-word and real-word errors that were artificially generated from 100 sentences.</S>
			<S sid ="27" ssid = "27">Church and Gale [1991] have used a Bayesian classifier method to improve the performance for non-word error correction.</S>
			<S sid ="28" ssid = "28">Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.</S>
			<S sid ="29" ssid = "29">The goal of the work described here is to investigate the effectiveness and efficiency of SLM­ based methods applied to the problem of OCR error correction.</S>
			<S sid ="30" ssid = "30">Since POS-based methods are not effective in distinguishing among candidates with the same POS tags and since methods based on word-trigram models involve extensive training data and require that huge word-trigram tables be available at run time, we used a word-bigram SLM as the first step in our investigation.</S>
			<S sid ="31" ssid = "31">In this paper, we describe a system that uses a word-bigram SLM technique to correct OCR errors.</S>
			<S sid ="32" ssid = "32">The system takes advantage of information from multiple sources, including letter n­ grams, character confusion probabilities, and word bigram probabilities, to effect context-based word error correction.</S>
			<S sid ="33" ssid = "33">It can correct non-word as well as real-word errors.</S>
			<S sid ="34" ssid = "34">In addition, the system can learn the character confusion probability table for a specific OCR environment and use it to achieve better performance.</S>
	</SECTION>
	<SECTION title="The Approach. " number = "2">
			<S sid ="35" ssid = "1">2.1 Problem Statement.</S>
			<S sid ="36" ssid = "2">The problem of context-based OCR word-error correction can be stated as follows: Let L = { w 1, w2 , ••• , wm} be the set of all the words in a given lexicon.</S>
			<S sid ="37" ssid = "3">For an input sentence, S = s 1 , •••,sn, produced as the output of an OCR device, where s 1 , •••,sn are character strings separated by spaces, find the best word sequence, vV = wb &apos;11&apos;2, ••• , Wn, for W; E L, that maximizes the probability pr(lVjS): (1) 1&quot;Ruel&quot; is an obscure French-derivative word meaning the space between a bed and the wall.</S>
			<S sid ="38" ssid = "4">Using Bayes&apos; formula, we can rewrite 1as: argmaxw(pr(WIS)) pr(W) * pr(SIW)) argmaxw ( pr(S) argmaxw(pr(W) * pr(SIW)) (2) The probability pr(W) is given by the language model and can be decomposed as: n pr(W) = IIpr(w;lw1,i-1) i=1 (3) wherepr( w;jw1,;_1) is the probability thattheword w; appears given thatw1, w2 , ••• , w;_1 appeared previously.</S>
			<S sid ="39" ssid = "5">In a word-bigram language model, we assume that the probability that a word w; will appear is affected only by the immediately preceding word.</S>
			<S sid ="40" ssid = "6">Thus, (4) and n pr ( W ) = IT pr ( w; jw; 1) i = 1 (5) The conditional probability, pr(SIW), reflects the channel (processing) characteristics of the OCR environment.</S>
			<S sid ="41" ssid = "7">If we assume that strings produced under OCR are independent of one another, we have the following formula: n pr(SIW) = ITpr(s;lw;) i=l So, w argmaxw(pr(W) * pr(SIW)) n argmaxw(ITpr( w;jw;-1) * pr(s;lw;)) i=l (6) (7) Thus, the problem of calculatingW is reduced to estimating the word-bigram probability, pr( w; lw;_1), and the word confusion probability, pr( s; Iw;).</S>
			<S sid ="42" ssid = "8">The word-bigram probability, pr( w; Iw;-1), can be estimated by a maximum likelihood estimator (MLE): prML ( I ) = c( will w;) W; Wi1 c(wi1) where c( w;_1) is the number of times that wi_1 occurs in the text and c( will wi) is the number of times that the word bignim (will w;) occurs in the text.</S>
			<S sid ="43" ssid = "9">However, the estimatation of unseen bigrams is a problem.</S>
			<S sid ="44" ssid = "10">We use a back-off model similar to that described in [Dagan &amp; Pereira 1994] to estimate the word-bigram probabilities in our system.</S>
			<S sid ="45" ssid = "11">If we already have estimates of the probabilities pr( w;lw;-1) and pr( s;jw;), the Viterbi algo­ rithm [Chamiak 1993] could be used to determine the best word sequence for the given sentence.</S>
			<S sid ="46" ssid = "12">Details of the back-off model and Viterbi algorithm can be found in [Dagan &amp; Pereira 1994] and [Charniak 1993].</S>
			<S sid ="47" ssid = "13">2.2 Estimate of Channel Probabilities and Learning of Character Confusion Table.</S>
			<S sid ="48" ssid = "14">The probability pr( slw )-the conditional probability that, given a word w, it is recognized by the OCR software as the string scan be estimated by the confusion probabilities of the characters in s if we assume that character recognition in OCR is an independent process.</S>
			<S sid ="49" ssid = "15">We assume that an OCR string is generated from the original word by one or more of the following operations: (a) delete a character; (b) insert a character; or (c) substitute one character for another.</S>
			<S sid ="50" ssid = "16">Under such circumstances, a dynamic programming method can be used to determine the operations that maximize the conditional probability when transforming the original word to the OCR string, given a character confusion probability table.</S>
			<S sid ="51" ssid = "17">Let t 1 ,t 2 ••• t; be the first i characters of the string that is produced by the OCR process for a source word s and let s1 , s2 ••• si be the first j actual characters of s. Define pr( ilj) to be the conditional probability that the substring s1,i is recognized as substring h,; by the OCR process, i.e., pr(t 1 ,;is 1 ,j)· The dynamic programming recurrence is given as follows: pr(i- llj) * pr(ins(t;)) pr(ilj) =max pr(ilj1) * pr(del(si)lsi) pr(i- llj- 1) * pr(t;lsi) (8) where pr( ins(y)) is the probability that letter y is inserted.</S>
			<S sid ="52" ssid = "18">pr(del(y)IY) is the probability that letter y is deleted.</S>
			<S sid ="53" ssid = "19">pr(xly) is the probability that letter y is replaced by letter x. For example, suppose that source word &quot;flag&quot; is recognized as &quot;flo&quot; by an OCR device.</S>
			<S sid ="54" ssid = "20">For­ mula 8 may determine that a sequence of four operations-(!}substitute &quot;f&quot; for &quot;f&quot;; (2) substitute &quot;1&quot; for &quot;1&quot;; (3) substitute &quot;a&quot; for &quot;o&quot;, and (4) delete &quot;g&quot;-maximizes the conditional probability pr(&quot;flo&quot;l&quot;flag&quot;).</S>
			<S sid ="55" ssid = "21">Then the probability of &quot;flag&quot; being rendered as &quot;flo&quot; can be estimated as: pr(&quot;flo&quot;l&quot;flag&quot;) = pr(&quot;f&quot;l&quot;f&quot;) * pr(&quot;l&quot;l&quot;l&quot;) * pr(&quot;o&quot;l&quot;a&quot;) * pr(del(&quot;g&quot;)l&quot;g&quot;) This method is similar to what was described in [Wagner 1974] where the minimum edit distance between two strings was computed.</S>
			<S sid ="56" ssid = "22">The minimum edit distance is the minimum number of oper­ ations that transform the source string to the target string.</S>
			<S sid ="57" ssid = "23">Note that to effect spelling correction, we could include character transposition probabilities.</S>
			<S sid ="58" ssid = "24">If we have no information about the character confusion probabilities, we can estimate them as: pr(ylx) { a ify=x 1-;t otherwise (9) pr( del( x )lx) = pr(ins(x)) = 1-a (10) where N is the total number of printable characters.</S>
			<S sid ="59" ssid = "25">The estimator a can be regarded as the probability that a given character is correctly recognized.</S>
			<S sid ="60" ssid = "26">Our experiments show that system performance is very sensitive to the value of a, especially for real-word error correction.</S>
			<S sid ="61" ssid = "27">For example, if a is very high, then the probability pr( sis) will be too high to be affected by subsequent processing and will not be changed.</S>
			<S sid ="62" ssid = "28">On the other hand if a is very low, some correct words may be detected as real-word errors and will be changed.</S>
			<S sid ="63" ssid = "29">If we have both the original text and the corresponding OCR output and if we assume that the errors made by a particular OCR system are not random (but semi-deterministic), we can count the cases of substitution, deletion, and insertion using a method similar to computing the minimum edit distance between strings [Wagner 1974] and we can estimate the probabilities using formulas similar to those in [Church &amp; Gale 1991]: pr(ylx) = num(sub(x, y))jnum(x) (11) pr( del (x) ) n u m ( d e l ( x ) ) j n u m ( x ) (12 ) pr( ins (y) ) n u m ( i n s ( y ) ) j n u m ( &lt; a l l l e t t e r s &gt; ) (13 ) Obviously, in practice, we typically do not have the original text to compare to the OCR text or to use for correction.</S>
			<S sid ="64" ssid = "30">Moreover, as noted in [Liu et al. 1991], the character confusion characteristics are heavily dependent on the OCR environment, encompassing everything from the performance biases of the specific OCR software to the size of characters in the source text, fonts used, individual character types, and print quality of the text being processed.</S>
			<S sid ="65" ssid = "31">It is not feasible to train on texts to acquire character confusion probabilities for each OCR environment.</S>
			<S sid ="66" ssid = "32">The current system employs an iterative learning1rom-correcting technique that treats the cor­ rected OCR text as an approximation of the original text.</S>
			<S sid ="67" ssid = "33">The system starts by assuming all characters are equally likely to be misrecognized (with some uniform, small probability) and learns the character confusion probabilities by comparing the OCR text to the corrected OCR text after each pass.</S>
			<S sid ="68" ssid = "34">Then the learned character confusion probabilities are used for the next pass processing (feedback processing).</S>
			<S sid ="69" ssid = "35">This method proves to be quite effective in improving system performance.</S>
			<S sid ="70" ssid = "36">2.3 Generation of Word Candidates for a Given String.</S>
			<S sid ="71" ssid = "37">Ideally, each word, w, in the lexicon should be compared to a given OCR string, s, to compute the conditional probability, pr( wls).</S>
			<S sid ="72" ssid = "38">However, this approach would be computationally too expensive.</S>
			<S sid ="73" ssid = "39">Instead, the system operates in two steps, first to generate the candidates and then to specify the maximal number of candidates, N, to be considered for the correction of an OCR string.</S>
			<S sid ="74" ssid = "40">In step 1, the system retrieves a large list of word candidates for a given string.</S>
			<S sid ="75" ssid = "41">To nominate candidates, we use a vector space information retrieval technique [Salton 1989]: all the words in the lexicon are indexed by letter n-grams and the (OCR) string, also parsed into letter n-grams, is treated as a query over the database of lexicon entries.</S>
			<S sid ="76" ssid = "42">In particular, all words (or OCR strings) are indexed by their letter trigrams, including the &apos;beginning&apos; and &apos;end&apos; spaces surrounding the string.</S>
			<S sid ="77" ssid = "43">Words of four or fewer characters are also indexed by their letter bigrams.</S>
			<S sid ="78" ssid = "44">For example: &quot;the&quot; &quot;example&quot; =&gt; {#th, the, he#, #t, th, he, e#} =&gt; {#ex, exa, xam, mpl, ple, le#} A given OCR string to be corrected is represented by a vector containing its letter n-grams.</S>
			<S sid ="79" ssid = "45">Using the vector as the query, the lexicon words that are similar to the word error are retrieved, giving a large list of candidate correct forms.</S>
			<S sid ="80" ssid = "46">Candidates must share at least some features with the input string (query).</S>
			<S sid ="81" ssid = "47">A ranked list can be generated by scoring matches using a simple term frequency (TF) count-the number of matches between the query vector and then-gram vector of a candidate word.</S>
			<S sid ="82" ssid = "48">For example, given the string: &quot;exanple&quot; =&gt; {#exanple#} =&gt; {#ex, exa, xan, anp, npl, ple, le#} the word &quot;example&quot; is a candidate: &quot;example&quot; :::} {#example#} :::} {#ex, exa, xam, amp, mpl, ple, le#} Since the two items share four letter n-grams-&quot;#ex&quot;, &quot;exa&quot;, &quot;ple&quot;, and &quot;le#&quot;-the TF score of the candidate word &quot;example&quot; for the input string &quot;exanple&quot; is four.</S>
			<S sid ="83" ssid = "49">Note also that the TF score can be used to establish a threshold or cutoff score to limit the number of candidates to consider.</S>
			<S sid ="84" ssid = "50">In step 2, the system re-ranks the words in the candidate list using channel probabilities as described above.</S>
			<S sid ="85" ssid = "51">On average, the system generates several hundred candidates for a given string.</S>
			<S sid ="86" ssid = "52">Only the first N candidates are retained for context-based word-error correction.</S>
			<S sid ="87" ssid = "53">2.4 The Word Correction System for OCR Post-Processing.</S>
			<S sid ="88" ssid = "54">The architecture of the word correction system for OCR post-processing is given in Figure 1.</S>
			<S sid ="89" ssid = "55">[ o_c_R T_e_x_t ] J Candidate Generation Lexicon I / Candidate Retrieval J Character Confusion Candidate Ranking I Table [ Word Bigram l ...</S>
			<S sid ="90" ssid = "56">Maximum Likelihood Table Word Sequence Finding Feedback Corrected OCR Text t Figure 1: System Architecture The lexicon is generated from the training text; it includes all the words in the training set with frequency greater than the preset threshold.</S>
			<S sid ="91" ssid = "57">The words in the lexicon are indexed by letter n-grams as described in the previous section.</S>
			<S sid ="92" ssid = "58">The overall process for correcting a sentence is as follows: 1.</S>
			<S sid ="93" ssid = "59">Read a sentence from the input OCR text..</S>
			<S sid ="94" ssid = "60">2.</S>
			<S sid ="95" ssid = "61">Retrieve up to lv1 candidates from the lexicon for each possible error.</S>
			<S sid ="96" ssid = "62">2 Rerank the lv1.</S>
			<S sid ="97" ssid = "63">candidates by their conditional probabilities to the error.</S>
			<S sid ="98" ssid = "64">Keep only the top N candidates for the next processing step.</S>
			<S sid ="99" ssid = "65">(In the current system, lv1 is 10,000 and N is 10.)</S>
			<S sid ="100" ssid = "66">3.</S>
			<S sid ="101" ssid = "67">Use the Viterbi algorithm to get the best word sequence for the strings in the sentence..</S>
			<S sid ="102" ssid = "68">Figure 2 illustrates the alternative choices and the optimal path found during the processing (correcting) of the sentence &quot;john fomd he man&quot;.</S>
			<S sid ="103" ssid = "69">Original Sentence: John found the man. Input Sentence: john fornd he man. Corrected Sentence: John found the man. john famed he Joh ------:::::&gt;:;:;.</S>
			<S sid ="104" ssid = "70">found ::::=:==---­ fond ford man man &gt; an may Sohn jobs joint job Johns for form forms food force He The de her she De can J a n m a n y m e n S a n Man Kahn sound Le van Best Word Sequence: John found the man. Figure 2: Process of Correcting a Sentence The system requires several passes to correct an OCR text.</S>
			<S sid ="105" ssid = "71">In the first pass, the system has no information on the character confusion probabilities, so it will assume a prior belief a as the probability that a character is correctly recognized.</S>
			<S sid ="106" ssid = "72">The system distributes the rest of the probability uniformly among other events.</S>
			<S sid ="107" ssid = "73">(Cf.</S>
			<S sid ="108" ssid = "74">Formula 9.)</S>
			<S sid ="109" ssid = "75">In each feedback step, the system first generates a character confusion probability table by comparing the OCR text to the corrected OCR text from the last pass.</S>
			<S sid ="110" ssid = "76">It uses the new confusion table for the next-pass correction of the OCR text.</S>
			<S sid ="111" ssid = "77">2 1n its non-word error mode of operation, the system treats every word that does not match a lexicon entry as a possible error.</S>
			<S sid ="112" ssid = "78">In its non-word and real-word error mode, the system treats every word as though it were a possible error.</S>
	</SECTION>
	<SECTION title="Experiments and Results. " number = "3">
			<S sid ="113" ssid = "1">To test our OCR-error-correction process, we used a set of electronic documents from the ZiffDavis (ZIFF) news wire.3 The documents in the corpus are business articles in the domain of computer science and computer engineering.</S>
			<S sid ="114" ssid = "2">We used 90% of the collection for training and the remaining 10% for testing.</S>
			<S sid ="115" ssid = "3">The system created a lexicon and collected word-bigram sequences and statistics from the training data.</S>
			<S sid ="116" ssid = "4">Words or word-bigrams with frequency less than three were discarded.</S>
			<S sid ="117" ssid = "5">The resulting lexicon contained about 100,000 words; these were indexed using 34,847letter n-grams.</S>
			<S sid ="118" ssid = "6">The resulting word-bigram table had about 1,000,000 entries.</S>
			<S sid ="119" ssid = "7">Seventy pages of ZIFF data in the test set were printed in 7-point Times font.</S>
			<S sid ="120" ssid = "8">We degraded the print quality of the documents by photocopying them on a &apos;light&apos; setting.</S>
			<S sid ="121" ssid = "9">The photocopies were then scanned by a Fujitsu 3097E scanner and the resulting images were processed by Xerox Textbridge OCR software.</S>
			<S sid ="122" ssid = "10">The set of documents contained 55,699 strings and the overall word error rate after OCR processing was 22.9% (12,760).</S>
			<S sid ="123" ssid = "11">For literal words in the source (only letter sequences, not alpha­ numeric ones), the error rate was lower, 14.7% (8,198).</S>
			<S sid ="124" ssid = "12">Table 1 gives the number of real-word and non-word errors for literal words in the OCR data.</S>
			<S sid ="125" ssid = "13">N on W or d Er ro rs Real W or d Er ro rs T ot al Er ro rs N u m be r 6 , 5 0 6 1 , 6 9 2 8 , 1 9 8 % 7 9 . 4 2 0 . 6 1 0 0 Table 1: OCR Errors Originating from Literal Words We conducted three experiments: 1.</S>
			<S sid ="126" ssid = "14">Isolated-Word Error Correction: The system used only channel probabilities without consid­.</S>
			<S sid ="127" ssid = "15">ering context information, i.e., it always selected the candidate with the highest rank in the candidates list to correct a given OCR string.</S>
			<S sid ="128" ssid = "16">2.</S>
			<S sid ="129" ssid = "17">Context-Dependent Non-Word Error Correction: The system used context to correct strings that.</S>
			<S sid ="130" ssid = "18">did match valid lexicon words.</S>
			<S sid ="131" ssid = "19">3.</S>
			<S sid ="132" ssid = "20">Context-Dependent Non- and Real-Word Error Correction: The system treated all input strings.</S>
			<S sid ="133" ssid = "21">as possible errors and tried to correct them by taking into account the contexts in which the strings appeared.</S>
			<S sid ="134" ssid = "22">In each experiment, the system conducted four correction passes: one initial pass with prior probability a= 0.99 and three feedback passes.</S>
			<S sid ="135" ssid = "23">Results are given in Tables 2, 3, and 4.</S>
			<S sid ="136" ssid = "24">In all cases, we considered only those strings whose correct forms are literal words (not alpha-numerics).</S>
			<S sid ="137" ssid = "25">Note that errors can be introduced by the system when it incorrectly changes a correct word in the OCR text into another word.</S>
			<S sid ="138" ssid = "26">In fact, we distinguish two types of errors introduced by the system: errors caused by changing correct 3The ZIFF collection is distributed as part of the data used in the Text Retrieval Conference (TREC) evaluations.</S>
			<S sid ="139" ssid = "27">The corpus contains about 33 million words.</S>
			<S sid ="140" ssid = "28">unknown words and errors caused by changing correct lexicon words.</S>
			<S sid ="141" ssid = "29">The error reduction rate was calculated by subtracting total errors from 8,198 and dividing by 8,198.</S>
			<S sid ="142" ssid = "30">The system, running unoptimized code on a 128MHz DECalpha processor, processed the test corpus at a rate of about 200 words (strings) per second for experiments 1 and 2; and 30 words (strings) per second for experiment 3.</S>
			<S sid ="143" ssid = "31">P a s sNon Word Error sReal Word Errors Int ro du ce d Err ors To tal Err ors E r r o r R ed uc tio n( % ) Re ma in Co rre cte d Re ma in Co rre cte d Un kn ow n W ds Le x W ds F i r s t 3, 0 4 9 3 , 4 5 7 1, 6 9 2 0 1 8 2 0 4, 92 3 3 9 . 9 Fe ed back 1 2, 8 1 6 3 , 6 9 0 1, 6 9 2 0 1 8 2 0 4, 69 0 4 2 . 8 Fe ed back 2 2, 7 9 1 3 , 7 1 5 1, 6 9 2 0 1 8 2 0 4, 66 5 4 3 . 1 Fe ed back 3 2, 7 8 4 3 , 7 2 2 1, 6 9 2 0 1 8 2 0 4, 65 8 4 3 . 2 Table 2: Results from Isolated-Word Error Correction P a s sNon Word Error sReal Word Errors Int ro du ce d Err ors T ot al Er ror s E r r o r R ed uc tio n( % ) Re ma in Co rre cte d Re ma in Co rre cte d Un kn ow n W ds Le x W ds F i r s t 2, 6 8 4 3 , 8 2 2 1, 6 9 2 0 1 8 2 0 4, 55 8 4 4 . 4 Fe ed back 1 1, 9 7 2 4 , 3 5 4 1, 6 9 2 0 1 8 2 0 3, 84 6 5 3 . 1 Fe ed back 2 1 , 9 4 3 4 , 5 6 3 1, 6 9 2 0 1 8 2 0 3, 81 7 5 3 . 4 Fe ed back 3 1, 9 4 8 4 , 5 5 8 1, 6 9 2 0 1 8 2 0 3, 82 2 5 3 . 4 Table 3: Results from Context-Dependent Non-Word Error Correction P a s sNon Word Error sReal Word Errors Int ro du ce d Err ors T ot al Err ors E r r o r R ed uc ti on ( % ) Re ma in Co rre cte d Re ma in Co rre cte d Un kn ow n W ds Le x W ds F i r s t 2, 5 2 9 3 , 9 7 7 1, 2 2 5 4 6 7 1 8 2 5 4 3, 99 0 5 1 . 3 Fe ed back 1 1, 9 7 8 4 , 5 2 8 1, 0 3 1 6 6 1 1 8 2 1 1 9 3, 31 0 5 9 . 6 Fe ed back 2 1, 9 3 5 4 , 5 7 1 1, 0 0 8 6 8 4 1 8 2 1 4 1 3, 26 6 6 0 . 2 Fe ed ba ck3 1 , 9 2 6 4 , 5 8 0 1 , 0 1 5 6 7 7 1 8 2 1 4 7 3, 27 0 6 0 . 1 Table 4: Results from Context-Dependent Real- and Non-Word Error Correction</S>
	</SECTION>
	<SECTION title="Analysis. " number = "4">
			<S sid ="144" ssid = "1">Based on the results, we can see that the predominant, positive effect in correction occurs in the first pass.</S>
			<S sid ="145" ssid = "2">Performance also improves significantly in the first feedback process, as the system learns the character confusion probabilities by correcting the OCR text.</S>
			<S sid ="146" ssid = "3">The second and third feedback steps have only slight effect on the error reduction rates.</S>
			<S sid ="147" ssid = "4">Indeed, in experiment 3, the result from the third feedback pass is actually worse than that from the second feedback pass.</S>
			<S sid ="148" ssid = "5">These results indicate that an initial pass followed by two feedback passes may optimize the method.</S>
			<S sid ="149" ssid = "6">In the following discussion, we compare the three experiments using the results obtained from the second feedback step (Feedback-2).</S>
			<S sid ="150" ssid = "7">As we might expect, the results from the context-based experiments are much better than those from the isolated-word experiment.</S>
			<S sid ="151" ssid = "8">The error reduction rates in experiments 2 and 3 are, respectively, 10.3% and 17.1% higher than the rate in experiment 1.</S>
			<S sid ="152" ssid = "9">This indicates that even a modest (e.g., bigram-based) representation of context is useful in selecting the best candidates for word-error correction.</S>
			<S sid ="153" ssid = "10">Inall three experiments, the system introduced 182 new errors due to false corrections of words that were not in the lexicon.</S>
			<S sid ="154" ssid = "11">(Recall that the system lexicon is based on the words derived from the training corpus; some words may be present in the test corpus that are not in the training corpus.)</S>
			<S sid ="155" ssid = "12">Whenever the system encounters an unknown word, it treats it as a non-word error and attempts to correct it.</S>
			<S sid ="156" ssid = "13">In such cases, the system replaces the presumed non-word error with a word from its lexicon.</S>
			<S sid ="157" ssid = "14">Thus, for example, if the system encounters the word &quot;MobileData&quot; (a correct name) in the OCR output, but does not have &quot;MobileData&quot; in its lexicon, it might change &quot;MobileData&quot; to &quot;MobileComm&quot; (a word that does exist in the training corpus lexicon).</S>
			<S sid ="158" ssid = "15">Of course, such problems in processing unknown words are not unique to OCR error correction; they represent a general problem for all natural-language processing tasks.</S>
			<S sid ="159" ssid = "16">As shown by experiment 3, when the system uses context-based non- and real-word error correction, it achieves a total error reduction rate of 60.2%.</S>
			<S sid ="160" ssid = "17">This is 6.8% higher than the rate achieved in the context-based non-word experiment.</S>
			<S sid ="161" ssid = "18">The improvement in performance is gained principally from the reduction of the real-word errors.</S>
			<S sid ="162" ssid = "19">Although the system introduces additional errors-since all the strings in the OCR text are treated as possible errors and subject to change-the number of corrected real-word errors far exceeds the number of real-word errors introduced.</S>
			<S sid ="163" ssid = "20">In the second feedback pass, for example, the system introduced 141 new errors by changing correct lexicon words into other lexicon words.</S>
			<S sid ="164" ssid = "21">On the other hand, the system properly corrected 684 real errors-32.1% of all the real errors.</S>
			<S sid ="165" ssid = "22">The corrected OCR text, therefore, has 543 fewer real-word errors than the original OCR text.</S>
			<S sid ="166" ssid = "23">Certain types of errors in the source or OCR-output text present systematic problems for our approach, highlighting the limitations of the system.</S>
			<S sid ="167" ssid = "24">In particular, because the process is based on the structural definition of a word (viz., a character sequence &apos;between white space&apos;)-not a morphological one-any errors that obscure word boundaries will defy correction.</S>
			<S sid ="168" ssid = "25">For example, run-on errors (e.g., &quot;of the&quot;/&quot;ofthe&quot;) and split-word errors (&quot;training&quot;/&quot;train ng&quot;) cannot be corrected.</S>
			<S sid ="169" ssid = "26">In addition, the use of a vector-space querying to find candidate lexical entries­ including our special approach to word decomposition and scoring an present problems when processing some OCR errors, especially short strings.</S>
			<S sid ="170" ssid = "27">For example, if &quot;both&quot; (in the source) is ·rendered as &quot;hotn&quot; (in the OCR text), it is not possible for the system to generate &quot;both&quot; as one of the high-ranked candidates-they share only one feature, the bigram &quot;ot&quot;-despite the fact that the conditional probability pr(&quot;hotn&quot;l&quot;both&quot;) might be high.</S>
			<S sid ="171" ssid = "28">Finally, the system suffers from the common limitation of word bigram or trigram models in that it cannot capture discourse properties of context, such as topic and tense, which are sometimes required to select the correct word.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="172" ssid = "1">The system we have created uses information from a variety of sources-letter n-grams, charac­ ter confusion probabilities, and word-bigram probabilities-to realize context-based, automatic, word-error correction.</S>
			<S sid ="173" ssid = "2">It can correct non-word errors as well as real-word errors.</S>
			<S sid ="174" ssid = "3">The system can also learn character confusion probability tables by correcting OCR text and use such information to achieve better performance.</S>
			<S sid ="175" ssid = "4">Overall, for complete (real- and non-word) error correction, it achieved a 60.2% rate of error reduction.</S>
			<S sid ="176" ssid = "5">The techniques we have used are subject to certain systematic problems.</S>
			<S sid ="177" ssid = "6">However, we believe they will prove to be useful not only in improving the quality of OCR processing, but also in enhancing a variety of information retrieval applications.</S>
			<S sid ="178" ssid = "7">In future work, we plan to explore different heuristics to deal with word boundary problems and to incorporate other models of context representation, including both SLM approaches, such as word trigram models, and simple discourse structures.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="179" ssid = "8">We thank Natasa MilicFrayling and an anonymous reviewer for their excellent comments on an earlier version of this paper.</S>
			<S sid ="180" ssid = "9">Naturally, the authors alone are responsible for any errors or omissions in the current version.</S>
	</SECTION>
</PAPER>
