<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Contextual spelling errors are defined as the use of an incorrect, though valid, word in a particular sentence or context.</S>
		<S sid ="2" ssid = "2">Tra­ ditional spelling checkers flag misspelled words, but they do not typically attempt to identify words that are used incorrectly in a sentence.</S>
		<S sid ="3" ssid = "3">We explore the use of Latent Se­ mantic Analysis for correcting these incor­ rectly used words and the results are com­ pared to earlier work based on a Bayesian classifier.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Spelling checkers are now available for all major word processing systems.</S>
			<S sid ="5" ssid = "5">However, these spelling checkers only catch errors that result in misspelled words.</S>
			<S sid ="6" ssid = "6">If an· error results in a different, but incor­ rect word, it will go undetected.</S>
			<S sid ="7" ssid = "7">For example, quite may easily be mistyped as quiet.</S>
			<S sid ="8" ssid = "8">Another type of er­ ror occurs when a writer simply doesn&apos;t know which word of a set of homophones 1 (or near homophones) is the proper one for a particular context.</S>
			<S sid ="9" ssid = "9">For ex­ ample, the usage of affect and effect is commonly confused.</S>
			<S sid ="10" ssid = "10">Though the cause is different for the two types of errors, we can treat them similarly by examining the contexts in which they appear.</S>
			<S sid ="11" ssid = "11">Consequently, no effort is made to distinguish between the two er­ ror types and both are called contextual spelling er­ rors.</S>
			<S sid ="12" ssid = "12">Kukich (1992a; 1992b) reports that 40% to 45% of observed spelling errors are contextual er­ rors.</S>
			<S sid ="13" ssid = "13">Sets of words which are frequently misused or mistyped for one another are identified as confusion sets.</S>
			<S sid ="14" ssid = "14">Thus, from our earlier examples, {quiet, quite} and {affect, effect} are two separate confusion sets.</S>
			<S sid ="15" ssid = "15">In this paper, we introduce Latent Semantic Anal­ ysis (LSA) as a method for correcting contextual spelling errors for a given collection of confusion sets.</S>
			<S sid ="16" ssid = "16">1Homophones are words that sound the same, but are spelled differently.</S>
			<S sid ="17" ssid = "17">LSA was originally developed as a model for infor­ mation retrieval (Dumais et al., 1988; Deerwester et al., 1990), but it has proven useful in other tasks too.</S>
			<S sid ="18" ssid = "18">Some examples include an expert Expert lo­ cator (Streeter and Lochbaum, 1988) and a confer­ ence proceedings indexer (Foltz, 1995) which per­ forms better than a simple keyword-based index.</S>
			<S sid ="19" ssid = "19">Recently, LSA has been proposed as a theory of se­ mantic learning (Landauer and Dumais, (In press)).</S>
			<S sid ="20" ssid = "20">Our motivation in using LSA was to test its effec­ tiveness at predicting words based on a given sen­ tence and to compare it to a Bayesian classifier.</S>
			<S sid ="21" ssid = "21">LSA makes predictions by building a high-dimensional, &quot;semantic&quot; space which is used to compare the sim­ ilarity of the words from a confusion set to a given context.</S>
			<S sid ="22" ssid = "22">The experimental results from LSA predic­ tion are then compared to both a baseline predic­ tor and a hybrid predictor based on trigrams and a Bayesian classifier.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="23" ssid = "1">Latent Semantic Analysis has been applied to the problem of spelling correction previously (Kukich, 1992b).</S>
			<S sid ="24" ssid = "2">However, this work focused on detect­ ing misspelled words, not contextual spelling errors.</S>
			<S sid ="25" ssid = "3">The approach taken used letter n-grams to build the semantic space.</S>
			<S sid ="26" ssid = "4">In this work, we use the words di­ rectly.</S>
			<S sid ="27" ssid = "5">Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of prob­ lems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration.</S>
			<S sid ="28" ssid = "6">This class of prob­ lems has been attacked by many others.</S>
			<S sid ="29" ssid = "7">A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).</S>
			<S sid ="30" ssid = "8">Recently, Golding and Schabes (1996) de­ scribed a system, Tribayes, that combines a trigram model of the words&apos; parts of speech with a Bayesian classifier.</S>
			<S sid ="31" ssid = "9">The trigram component of the system is used to make decisions for those confusion sets that 166 documents D&apos; 0 terms X r x r r x d t X d t x r Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, Sand D&apos;.</S>
			<S sid ="32" ssid = "10">contain words with different parts of speech.</S>
			<S sid ="33" ssid = "11">The Bayesian component is used to predict the correct word from among same part-of-speech words.</S>
			<S sid ="34" ssid = "12">Golding and Schabes selected 18 confusion sets from a list of commonly confused words plus a few that represent typographical errors.</S>
			<S sid ="35" ssid = "13">They trained their system using a random 80% of the Browri cor­ pus (Kucera and Francis, 1967).</S>
			<S sid ="36" ssid = "14">The remaining 20% of the corpus was used to test how well the system performed.</S>
			<S sid ="37" ssid = "15">We have chosen to use the same 18 con­ fusion sets and the Brown corpus in order to compare LSA to Tribayes.</S>
	</SECTION>
	<SECTION title="Latent Semantic Analysis. " number = "3">
			<S sid ="38" ssid = "1">Latent Semantic Analysis (LSA) was developed at Bellcore for use in information retrieval tasks (for which it is also known as LSI) (Dumais et al., 1988; Deerwester et al., 1990).</S>
			<S sid ="39" ssid = "2">The premise of the LSA model is that an author begins with some idea or information to be communicated.</S>
			<S sid ="40" ssid = "3">The selection of particular lexical items in a collection of texts is simply evidence for the underlying ideas or informa­ tion being presented.</S>
			<S sid ="41" ssid = "4">The goal of LSA, then, is to take the &quot;evidence&quot; (i.e., words) presented and un­ cover the underlying semantics of the text passage.</S>
			<S sid ="42" ssid = "5">Because many words are polysemous (have multi­ ple meanings) and synonymous (have meanings in common with other words), the evidence available in the text tends to be somewhat &quot;noisy.&quot;</S>
			<S sid ="43" ssid = "6">LSA at­ tempts to eliminate the noise from the data by first representing the texts in a high-dimensional space and then reducing the dimensionality of the space to only the most important dimensions.</S>
			<S sid ="44" ssid = "7">This pro­ cess is described in more detail in Dumais (1988) or Deerwester (1990), but a brief description is pro­ vided here.</S>
			<S sid ="45" ssid = "8">A collection of texts is represented in matrix for­ mat.</S>
			<S sid ="46" ssid = "9">The rows of the matrix correspond to terms and the columns represent documents.</S>
			<S sid ="47" ssid = "10">The indi­ vidual cell values are based on some function of the term&apos;s frequency in the corresponding document and its frequency in the whole collection.</S>
			<S sid ="48" ssid = "11">The func­ tion for selecting cell values will be discussed in sec­ tion 4.2.</S>
			<S sid ="49" ssid = "12">A singular value decomposition (SVD) is performed on this matrix.</S>
			<S sid ="50" ssid = "13">SVD factors the origi­ nal matrix into the product of three matrices.</S>
			<S sid ="51" ssid = "14">We&apos;ll identify these matrices as T, S, and D&apos; (see Figure 1).</S>
			<S sid ="52" ssid = "15">The T matrix is a representation of the original term vectors as vectors of derived orthogonal factor val­ ues.</S>
			<S sid ="53" ssid = "16">D&apos; is a similar representation for the original document vectors.</S>
			<S sid ="54" ssid = "17">S is a diagonal matrix2 of rank r. It is also called the singular value matrix.</S>
			<S sid ="55" ssid = "18">The sin­ gular values are sorted in decreasing order along the diagonal.</S>
			<S sid ="56" ssid = "19">They represent a scaling factor for each dimension in the T and D&apos; matrices.</S>
			<S sid ="57" ssid = "20">Multiplying T, S, and D&apos; together perfectly repro­ duces the original representation of the text collec­ tion.</S>
			<S sid ="58" ssid = "21">Recall, however, that the original representa­ tion is expected to be noisy.</S>
			<S sid ="59" ssid = "22">What we really want is an approximation of the original space that elim­ inates the majority of the noise and captures the most important ideas or semantics of the texts.</S>
			<S sid ="60" ssid = "23">An approximation of the original matrix is created by eliminating some number of the least important singular values in S. They correspond to the least important (and hopefully, most noisy) dimensions in the space.</S>
			<S sid ="61" ssid = "24">This step leaves a new matrix (So) of rank k.3 A similar reduction is made in T and D by retaining the first k columns of T and the first k rows of D&apos; as depicted in Figure 2.</S>
			<S sid ="62" ssid = "25">The product of the resulting T0 , S0 , and D &apos;0 matrices is a least squares best fit reconstruction of the original matrix (Eckart and Young, 1939).</S>
			<S sid ="63" ssid = "26">The reconstructed ma­ trix defines a space that represents or predicts the frequency with which each term in the space would appear in a given document or text segment given an infinite sample of semantically similar texts (Lan 2 A diagonal matrix is a square matrix that contains nonzero values only along the diagonal running from the upper left to the lower right.</S>
			<S sid ="64" ssid = "27">3 The number of factors k to be retained is generally selected empirically.</S>
			<S sid ="65" ssid = "28">documents D&apos; 0 A terms X 0 kxk k X d t X d t X k Figure 2: Results of reducing the T, S and D&apos; matrices produced by SVD to rank k. Recombining the reduced matrices gives X, a least squares best fit reconstruction of the original matrix.</S>
			<S sid ="66" ssid = "29">dauer and Dumais, {In press)).</S>
			<S sid ="67" ssid = "30">New text passages can be projected into the space by computing a weighted average of the term vectors which correspond to the words in the new text.</S>
			<S sid ="68" ssid = "31">In the contextual spelling correction task, we can gen­ erate a vector representation for each text passage in which a confusion word appears.</S>
			<S sid ="69" ssid = "32">The similarity between this text passage vector and the confusion word vectors can be used to predict the most likely word given the context or text in which it will ap­ pear.</S>
			<S sid ="70" ssid = "33">·</S>
	</SECTION>
	<SECTION title="Experimental Method. " number = "4">
			<S sid ="71" ssid = "1">4.1 Data.</S>
			<S sid ="72" ssid = "2">Separate corpora for training and testing LSA&apos;s abil­ ity to correct contextual word usage errors were cre­ ated from the Brown corpus (Kucera and Francis, 1967).</S>
			<S sid ="73" ssid = "3">The Brown corpus was parsed into individ­ ual sentences which are randomly assigned to either a training corpus or a test corpus.</S>
			<S sid ="74" ssid = "4">Roughly 80% of the original corpus was assigned as the training corpus and the other 20% was reserved as the test corpus.</S>
			<S sid ="75" ssid = "5">For each confusion set, only those sentences in the training corpus which contained words in the confusion set were extracted for construction of an LSA space.</S>
			<S sid ="76" ssid = "6">Similarly, the sentences used to test the LSA space&apos;s predictions were those extracted from the test corpus which contained words from the con­ fusion set being examined.</S>
			<S sid ="77" ssid = "7">The details of the space construction and testing method are described be­ low.</S>
			<S sid ="78" ssid = "8">4.2 Training.</S>
			<S sid ="79" ssid = "9">Training the system consists of processing the train­ ing sentences and constructing an LSA space from them.</S>
			<S sid ="80" ssid = "10">LSA requires the corpus to be segmented into documents.</S>
			<S sid ="81" ssid = "11">For a given confusion set, an LSA space is constructed by treating each training sentence as a document.</S>
			<S sid ="82" ssid = "12">In other words, each training sentence is used as a column in the LSA matrix.</S>
			<S sid ="83" ssid = "13">Before be ing processed by LSA, each sentence undergoes the following transformations: context reduction, stem­ ming, bigram creation, and term weighting.</S>
			<S sid ="84" ssid = "14">Context reduction is a step in which the sen­ tence is reduced in size to the confusion word plus the seven words on either side of the word or up to the sentence boundary.</S>
			<S sid ="85" ssid = "15">The average sentence length in the corpus is 28 words, so this step has the effect of reducing the size of the data to approximately half the original.</S>
			<S sid ="86" ssid = "16">Intuitively, the reduction ought to improve performance by disallowing the distantly lo­ cated words in long sentences to have any influence on the prediction of the confusion word because they usually have little or nothing to do with the selec­ tion of the proper word.</S>
			<S sid ="87" ssid = "17">In practice, however, the reduction we use had little effect on the predictions obtained from the LSA space.</S>
			<S sid ="88" ssid = "18">We ran some experiments in which we built LSA spaces using the whole sentence as well as other con­ text window sizes.</S>
			<S sid ="89" ssid = "19">Smaller context sizes didn&apos;t seem to contain enough information to produce good pre­ dictions.</S>
			<S sid ="90" ssid = "20">Larger context sizes (up to the size of the entire sentence) produced results which were not sig­ nificantly different from the results reported here.</S>
			<S sid ="91" ssid = "21">However, using a smaller context size reduces the total number of unique terms by an average of 13%.</S>
			<S sid ="92" ssid = "22">Correspondingly, using fewer terms in the initial ma­ trix reduces the average running time and storage space requirements by 17% and 10% respectively.</S>
			<S sid ="93" ssid = "23">Stemming is the process of reducing each word to its morphological root.</S>
			<S sid ="94" ssid = "24">The goal is to treat the dif­ ferent morphological variants of a word as the same entity.</S>
			<S sid ="95" ssid = "25">For example, the words smile, smiled, smiles, smiling, and smilingly (all from the corpus) are re­ duced to the root smile and treated equally.</S>
			<S sid ="96" ssid = "26">We tried different stemming algorithms and all improved the predictive performance of LSA.</S>
			<S sid ="97" ssid = "27">The results pre­ sented in this paper are based on Porter&apos;s (Porter, 1980) algorithm.</S>
			<S sid ="98" ssid = "28">Bigram creation is performed for the words that were not removed in the context reduction step.</S>
			<S sid ="99" ssid = "29">• 168 Bigrams are formed between all adjacent pairs of words.</S>
			<S sid ="100" ssid = "30">The bigrams are treated as additional terms during the LSA space construction process.</S>
			<S sid ="101" ssid = "31">In other words, the bigrams fill their own row in the LSA ma­ trix.</S>
			<S sid ="102" ssid = "32">Term weighting is an effort to increase the weight or importance of certain terms in the high dimensional space.</S>
			<S sid ="103" ssid = "33">A local and global weighting is given to each term in each sentence.</S>
			<S sid ="104" ssid = "34">The local weight is a combination of the raw count of the par­ ticular term in the sentence and the term&apos;s prox­ imity to the confusion word.</S>
			<S sid ="105" ssid = "35">Terms located nearer to the confusion word are given additional weight in a linearly decreasing manner.</S>
			<S sid ="106" ssid = "36">The local weight of each term is then flattened by taking its log2.</S>
			<S sid ="107" ssid = "37">The global weight given to each term is an attempt to measure its predictive power in the corpus as a whole.</S>
			<S sid ="108" ssid = "38">We found that entropy (see also (Lochbaum and Streeter, 1989)) performed best as a global mea­ sure.</S>
			<S sid ="109" ssid = "39">Furthermore, terms which did not appear in more than one sentence in the training corpus were removed.</S>
			<S sid ="110" ssid = "40">While LSA can be used to quickly obtain satisfac­ tory results, some tuning of the parameters involved can improve its performance.</S>
			<S sid ="111" ssid = "41">For example, we chose (somewhat arbitrarily) to retain 100 factors for each LSA space.</S>
			<S sid ="112" ssid = "42">We wanted to fix this variable for all confusion sets and this number gives a good average performance.</S>
			<S sid ="113" ssid = "43">However, tuning the number of factors to select the &quot;best&quot; number for each space shows an average of 2% improvement over all the results and up to 8% for some confusion sets.</S>
			<S sid ="114" ssid = "44">4.3 Testing.</S>
			<S sid ="115" ssid = "45">Once the LSA space for a confusion set has been cre­ ated, it can be used to predict the word (from the confusion set) most likely to appear in a given sen­ tence.</S>
			<S sid ="116" ssid = "46">We tested the predictive accuracy of the LSA space in the following manner.</S>
			<S sid ="117" ssid = "47">A sentence from the test corpus is selected and the location of the confu­ sion word in the sentence is treated as an unknown word which must be predicted.</S>
			<S sid ="118" ssid = "48">One at a time, the words· from the confusion set are inserted into the sentence at the location of the word to be predicted and the same transformations that the training sen­ tences undergo are applied to the test sentence.</S>
			<S sid ="119" ssid = "49">The inserted confusion word is then removed from the sentence (but not the bigrams of which it is a part) because its presence biases the comparison which oc­ curs later.</S>
			<S sid ="120" ssid = "50">A vector in LSA space is constructed from the resulting terms.</S>
			<S sid ="121" ssid = "51">The word predicted most likely to appear in a sen­ tence is determined by comparing the similarity of each test sentence vector to each confusion word vec­ tor from the LSA space.</S>
			<S sid ="122" ssid = "52">Vector similarity is evalu­ ated by computing the cosine between two vectors.</S>
			<S sid ="123" ssid = "53">The pair of sentence and confusion word vectors with the largest cosine is identified and the corresponding confusion word is chosen as the most likely word for the test sentence.</S>
			<S sid ="124" ssid = "54">The predicted word is compared to the correct word and a tally of correct predictions is kept.</S>
	</SECTION>
	<SECTION title="Results. " number = "5">
			<S sid ="125" ssid = "1">The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).</S>
			<S sid ="126" ssid = "2">Seven of the 18 confusion sets contain words that are all the same part of speech and the remaining 11 con­ tain words with different parts of speech.</S>
			<S sid ="127" ssid = "3">Golding and Schabes (1996) have already shown that using a trigram model to predict words from a confusion set based on the expected part of speech is very effec­ tive.</S>
			<S sid ="128" ssid = "4">Consequently, we will focus most of our atten­ tion on the seven confusion sets containing words of the same part of speech.</S>
			<S sid ="129" ssid = "5">These seven sets are listed first in all of our tables and figures.</S>
			<S sid ="130" ssid = "6">We also show the results for the remaining 11 confusion sets for comparison purposes, but as expected, these aren&apos;t as good.</S>
			<S sid ="131" ssid = "7">We, therefore, consider our system com­ plementary to one (such as Tribayes) that predicts based on part of speech when possible.</S>
			<S sid ="132" ssid = "8">5.1 Baseline Prediction System.</S>
			<S sid ="133" ssid = "9">We describe our results in terms of a baseline predic­ tion system that ignores the context contained in the test sentence and always predicts the confusion word that occurred most frequently in the training corpus.</S>
			<S sid ="134" ssid = "10">Table 1 shows the performance of this baseline pre­ dictor.</S>
			<S sid ="135" ssid = "11">The left half of the table lists the various con­ fusion sets.</S>
			<S sid ="136" ssid = "12">The next two columns show the training and testing corpus sentence counts for each confu­ sion set.</S>
			<S sid ="137" ssid = "13">Because the sentences in the Brown corpus are not tagged with a markup language, we identi­ fied individual sentences automatically based on a small set of heuristics.</S>
			<S sid ="138" ssid = "14">Consequently, our sentence counts for the various confusion sets differ slightly from the counts reported in (Golding and Schabes, 1996).</S>
			<S sid ="139" ssid = "15">The right half of Table 1 shows the most frequent word in the training corpus from each confusion set.</S>
			<S sid ="140" ssid = "16">Following the most frequent word is the baseline performance data.</S>
			<S sid ="141" ssid = "17">Baseline performance is the per­ centage of correct predictions made by choosing the given (most frequent) word.</S>
			<S sid ="142" ssid = "18">The percentage of cor­ rect predictions also represents the frequency of sen­ tences in the test corpus that contain the given word.</S>
			<S sid ="143" ssid = "19">The final column lists the training corpus frequency of the given word.</S>
			<S sid ="144" ssid = "20">The difference between the base­ line performance column and the training corpus frequency column gives some indication about how evenly distributed the words are between the two corpora.</S>
			<S sid ="145" ssid = "21">For example, there are 158 training sentences for the confusion set {principal, principle} and 34 test sentences.</S>
			<S sid ="146" ssid = "22">Since the word principle is listed in the right half of the table, it must have appeared more frequently in the training set.</S>
			<S sid ="147" ssid = "23">From the final column, C on fu si on Se t Train Test M ost Fr eq. Base (Train Freq.)</S>
			<S sid ="148" ssid = "24">p ri n ci p al p ri n ci pl e 1 5 8 3 4 ra is e n se 1 1 7 3 6 af fe ct ef fe ct 1 9 3 5 3 pe ac e pi ec e 2 5 7 6 2 c o u nt ry co u nt y 3 8 9 9 1 a m o u nt n u m b er 4 8 0 1 2 2 a m o n g be t w ee n 8 5 3 2 0 3 p r i n c i p l e 4 1 . 2 5 7 . 6 ) n s e 7 2 . 2 ( 6 5 . 0 ) e f f e c t 8 8 . 7 ( 8 5 . 0 ) p e a c e 5 8 . 1 ( 5 9 . 5 ) c o u n t r y 5 9 . 3 ( 7 1 . 0 ) n u m b e r 7 5 . 4 ( 7 3 . 8 ) b e t w e e n 6 2 . 1 ( 6 6 . 7 ) a c c e pt e x c e pt 1 8 9 6 2 be gi n be in g 6 2 3 1 6 1 le ad le d 1 9 7 6 3 pa ss ed pa st 3 5 3 8 1 q ui et q ui te 2 8 0 7 6 w e at h er w h et h er 2 6 7 6 7 ci te si g ht sit e 1 2 8 3 2 it&apos; s its 1 5 7 7 3 9 1 th a n th en 2 4 9 7 5 7 8 y o u&apos; re y o ur 7 3 4 2 2 0 th ei r th er e th e y&apos; re 4 1 7 6 9 7 8 e x c e p t 6 7 . 7 ( 7 3 . 5 } b e i n g 8 8 . 8 ( 8 9 . 4 ) l e d 5 0 . 8 ( 5 2 . 3 ) p a s t 6 4 . 2 ( 6 3 . 2 ) q u i t e 8 8 . 2 ( 7 6 . 1 ) w h e t h e r 7 3 . 1 ( 7 9 . 0 ) s i g h t 6 2 . 5 ( 5 4 . 7 ) i t s 8 4 . 7 ( 8 4 . 9 ) t h a n 5 8 . 8 ( 5 5 . 3 ) y o u r 8 6 . 8 ( 8 4 . 5 ) t h e r e 5 3 . 4 ( 5 3 . 1 ) Table 1: Baseline performance for 18 confusion sets.</S>
			<S sid ="149" ssid = "25">The table is divided into confusion sets containing words of the same part of speech and those which have different parts of speech.</S>
			<S sid ="150" ssid = "26">we can see that it occurred in almost 58% of the training sentences.</S>
			<S sid ="151" ssid = "27">However, it occurs in only 41% of the test sentences and thus the baseline predictor scores only 41% for this confusion set.</S>
			<S sid ="152" ssid = "28">5.2 Latent Semantic Analysis.</S>
			<S sid ="153" ssid = "29">Table 2 shows the performance of LSA on the con­ textual spelling correction task.</S>
			<S sid ="154" ssid = "30">The table provides the baseline performance information for compari­ son to LSA.</S>
			<S sid ="155" ssid = "31">In all but the case of {amount, number}, LSA improves upon the baseline performance.</S>
			<S sid ="156" ssid = "32">The improvement provided by LSA averaged over all con­ fusion sets is about 14% and for the sets with the same part of speech, the average improvement is 16%.</S>
			<S sid ="157" ssid = "33">Table 2 also gives the results obtained by Tribayes as reported in (Golding and Schabes, 1996).</S>
			<S sid ="158" ssid = "34">The baseline performance given in connection with Trib­ ayes corresponds to the partitioning of the Brown corpus used to test Tribayes.</S>
			<S sid ="159" ssid = "35">It should be noted that we did not implement Tribayes nor did we use the same partitioning of the Brown corpus as Tribayes.</S>
			<S sid ="160" ssid = "36">Thus, the comparison between LSA and Tribayes is an indirect one.</S>
			<S sid ="161" ssid = "37">The differences in the baseline predictor for each system are a result of different partitions of the Brown corpus.</S>
			<S sid ="162" ssid = "38">Both systems randomly split the data such that roughly 80% is allocated to the train­ ing corpus and the remaining 20% is reserved for the test corpus.</S>
			<S sid ="163" ssid = "39">Due to the random nature of this process, however, the corpora must differ between the two systems.</S>
			<S sid ="164" ssid = "40">The baseline predictor presented in this paper and in (Golding and Schabes, 1996) are based on the same method so the correspond ing columns in Table 2 can be compared to get an idea of the distribution of sentences that contain the most frequent word for each confusion set.</S>
			<S sid ="165" ssid = "41">Examination of Table 2 reveals that it is difficult to make a direct comparison between the results of LSA and Tribayes due to the differences in the par­ titioning of the Brown corpus.</S>
			<S sid ="166" ssid = "42">Each system should perform well on the most frequent confusion word in the training data.</S>
			<S sid ="167" ssid = "43">Thus, the distribution of the most frequent word between the the training and the test corpus will affect the performance of the system.</S>
			<S sid ="168" ssid = "44">Because the baseline score captures infor­ mation about the percentage of the test corpus that should be easily predicted (i.e., the portion that con­ tains the most frequent word), we propose a com­ parison of the results by examination of the respec­ tive systems&apos; improvement over the baseline score reported for each.</S>
			<S sid ="169" ssid = "45">The results of this comparison are charted in Figure 3.</S>
			<S sid ="170" ssid = "46">The horizontal axis in the figure represents the baseline predictor performance for each system (even though it varies between the two systems).</S>
			<S sid ="171" ssid = "47">The vertical bar thus represents the performance above (or below) the baseline predictor for each system on each confusion set.</S>
			<S sid ="172" ssid = "48">LSA performs slightly better, on average, than Tribayes for those confusion sets which contain words of the same part of speech.</S>
			<S sid ="173" ssid = "49">Tribayes clearly outperforms LSA for those words of a different part of speech.</S>
			<S sid ="174" ssid = "50">Thus, LSA is doing better than the Bayesian component of Tribayes, but it doesn&apos;t in­ clude part of speech information and is therefore not capable of performing as well as the part of speech trigram component of Tribayes.</S>
			<S sid ="175" ssid = "51">Consequently, we believe that LSA is a competitive alternative to Co nf usi on Se t L S A T r i b a y e s tla sel me LSA tla sel me Tnbayes pri nc ip al pri nc ipl e r a 1 s e n s e a f f e c t e f f e c t p e a c e p i e c e c o u n t r y c o u n t y a m o u n t n u m b e r a m o n g b e t w e e n 4 1 . 2 91.2 7 2 . 2 80.6 8 8 . 7 94.3 5 8 . 1 83.9 5 9 . 3 81.3 7 5 . 4 56.6 6 2 . 1 80.8 5 8 . 8 88.2 6 4 . 1 76.9 9 1 . 8 95.9 4 4 . 0 90.0 9 1 . 9 85.5 7 1 . 5 82.9 7 1 . 5 75.3 ac ce pt ex ce pt b e g i n b e i n g l e a d l e d p a s s e d p a s t q u i e t q u i t e we ath er w he th er c i t e s i g h t s i t e i t &apos; s i t s t h a n t h e n y o u &apos; r e y o u r the ir the re th ey &apos;re 6 7 . 7 82.3 8 8 . 8 93.2 5 0 . 8 73.0 6 4 . 2 80.3 8 8 . 2 90.8 7 3 . 1 85.1 6 2 . 5 78.1 8 4 . 7 92.8 5 8 . 8 90.5 8 6 . 8 91.4 5 3 . 4 73.9 7 0 . 0 82.0 9 3 . 2 97.3 4 6 . 9 83.7 6 8 . 9 95.9 8 3 . 3 95.5 8 6 . 9 93.4 6 4 . 7 70.6 9 1 . 3 98.1 6 3 . 4 94.9 8 9 . 3 98.9 5 6 . 8 97.6 Table 2: LSA performance for 18 confusion sets.</S>
			<S sid ="176" ssid = "52">The results of Tribayes (Golding and Schabes, 1996) are also given.</S>
			<S sid ="177" ssid = "53">Tribayes and LSA performance compared to baseline predictor ....</S>
			<S sid ="178" ssid = "54">Q) ..c ..c Figure 3: Comparison of Tribayes vs. LSA performance above the baseline metric.</S>
			<S sid ="179" ssid = "55">a Bayesian classifier for making predictions among words of the same part of speech.</S>
			<S sid ="180" ssid = "56">5.3 Performance Tuning.</S>
			<S sid ="181" ssid = "57">The results that have been presented here are based on uniform treatment for each confusion set.</S>
			<S sid ="182" ssid = "58">That is, the initial data processing steps and LSA space con­ struction parameters have all been the same.</S>
			<S sid ="183" ssid = "59">How­ ever, the model does not require equivalent treat­ ment of all confusion sets.</S>
			<S sid ="184" ssid = "60">In theory, we should be able to increase the performance for each confusion set by tuning the various parameters for each confu­ sion set.</S>
			<S sid ="185" ssid = "61">In order to explore this idea further, we selected the confusion set {amount, number} as a testbed for performance tuning to a particular confusion set.</S>
			<S sid ="186" ssid = "62">As previously mentioned, we can tune the number of factors to a particular confusion set.</S>
			<S sid ="187" ssid = "63">In the case of this confusion set, using 120 factors increases the performance by 6%.</S>
			<S sid ="188" ssid = "64">However, tuning this param­ eter alone still leaves the performance short of the baseline predictor.</S>
			<S sid ="189" ssid = "65">A quick examination of the context in which both words appear reveals that a significant percentage (82%) of all training instances contain either the hi­ gram of the confusion word preceded by the, fol­ lowed by of, or in some cases, both.</S>
			<S sid ="190" ssid = "66">For exam­ ple, there are many instances of the collocation the+number+ofin the training data.</S>
			<S sid ="191" ssid = "67">However, there are only one third as many training instances for amount (the less frequent word) as there are for number.</S>
			<S sid ="192" ssid = "68">This situation leads LSA to believe that the bigrams the+amount and amount+ofhave more dis­ crimination power than the corresponding bigrams which contain number.</S>
			<S sid ="193" ssid = "69">As a result, LSA gives them a higher weight and LSA almost always predicts amount when the confusion word in the test sen­ tence appears in this context.</S>
			<S sid ="194" ssid = "70">This local context is a poor predictor of the confusion word and its pres­ ence tends to dominate the decision made by LSA.</S>
			<S sid ="195" ssid = "71">By eliminating the words the and offrom the train­ ing and testing process, we permit the remaining context to be used for prediction.</S>
			<S sid ="196" ssid = "72">The elimination of the poor local context combined with the larger number of factors increases the performance of LSA to 13% above the baseline predictor (compared to 11% for Tribayes).</S>
			<S sid ="197" ssid = "73">This is a net increase in perfor­ mance of 32%!</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="198" ssid = "1">We&apos;ve shown that LSA can be used to attack the problem of identifying contextual misuses of words, particularly when those words are the same part of speech.</S>
			<S sid ="199" ssid = "2">It has proven to be an effective alternative to Bayesian classifiers.</S>
			<S sid ="200" ssid = "3">Confusions sets whose words are different parts of speech are more effectively han­ dled using a method which incorporates the word&apos;s part of speech as a feature.</S>
			<S sid ="201" ssid = "4">We are exploring tech niques for introducing part of speech information into the LSA space so that the system can make better predictions for those sets on which it doesn&apos;t yet measure up to Tribayes.</S>
			<S sid ="202" ssid = "5">We&apos;ve also shown that for the cost of experimentation with different param­ eter combinations, LSA&apos;s performance can be tuned for individual confusion sets.</S>
			<S sid ="203" ssid = "6">While the results of this experiment look very nice, they still don&apos;t tell us anything about how useful the technique is when applied to unedited text.</S>
			<S sid ="204" ssid = "7">The testing procedure assumes that a confusion word must be predicted as if the author of the text hadn&apos;t supplied a word or that writers misuse the confusion words nearly 50% of the time.</S>
			<S sid ="205" ssid = "8">For example, consider the case of the confusion set {principal, principle}.</S>
			<S sid ="206" ssid = "9">The LSA prediction accuracy for this set is 91%.</S>
			<S sid ="207" ssid = "10">However, it might be the case that, in practice, peo­ ple tend to use the correct word 95% of the time.</S>
			<S sid ="208" ssid = "11">LSA has thus introduced a 4% error into the writing process.</S>
			<S sid ="209" ssid = "12">Our continuing work is to explore the error rate that occurs in unedited text as a means of as­ sessing the &quot;true&quot; performance of contextual spelling correction systems.</S>
	</SECTION>
	<SECTION title="Acknowledgments. " number = "7">
			<S sid ="210" ssid = "1">The first author is supported under DARPA con­ tract SOL BAA9510.</S>
			<S sid ="211" ssid = "2">We gratefully acknowledge the comments and suggestions of Thomas Landauer and the anonymous reviewers.</S>
	</SECTION>
</PAPER>
