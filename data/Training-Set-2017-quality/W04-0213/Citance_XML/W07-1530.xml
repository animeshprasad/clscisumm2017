<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">None</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="2" ssid = "2">The classical “success story” of corpus annotation are the various syntax treebanks that provide structural analyses of sentences and have enabled researchers to develop a range of new and highly successful data-oriented approaches to sentence parsing.</S>
			<S sid ="3" ssid = "3">In recent years, however, a number of corpora have been constructed that provide annotations on the discourse level, i.e. information that reaches beyond the sentence boundaries.</S>
			<S sid ="4" ssid = "4">Phenomena that have been annotated include coreference links, the scope of connectives, and coherence relations.</S>
			<S sid ="5" ssid = "5">Many of these are phenomena on whose handling there is not a general agreement in the research community, and therefore the question of “recycling” corpora by other people and for other purposes is often difficult.</S>
			<S sid ="6" ssid = "6">(To some extent, this is due to the fact that discourse annotation deals “only” with surface reflections of underlying, abstract objects.)</S>
			<S sid ="7" ssid = "7">At the same time, the efforts needed for building high-quality discourse corpora are considerable, and thus one should be careful in deciding how to invest those efforts.</S>
			<S sid ="8" ssid = "8">One aspect of providing added-value with annotation projects is that of shared corpora: If a variety of annotation efforts is executed on the same primary data, the series of annotation levels can yield insights that the creators of the individual levels had not explicitly planned for.</S>
			<S sid ="9" ssid = "9">A clear case is the relationship between coherence relations and connective use: When both levels are marked individually and with independent annotation guidelines, then afterwards the correlations between coherence relations, cue usage (and possibly other factors, if annotated) can be studied systematically.</S>
			<S sid ="10" ssid = "10">This conception of multi-level annotation presupposes, of course, that the technical problems of setting annotation levels in correspondence to one another be resolved.</S>
			<S sid ="11" ssid = "11">The panel on discourse annotation is organized by Manfred Stede and Janyce Wiebe.</S>
			<S sid ="12" ssid = "12">It aims at surveying the scene of discourse corpora, exploring chances for synergy, and identifying desiderata for future corpus creation projects.</S>
			<S sid ="13" ssid = "13">In preparation for the panel, the participants have provided the following short descriptions of the various copora in whose construction they have been involved.</S>
	</SECTION>
	<SECTION title="Prague Dependency Treebank. " number = "2">
			<S sid ="14" ssid = "1">(Eva Hajicˇova´ , Prague) One of the maxims of the work on the Prague Dependency Treebank is that one should not overlook, disregard and thus lose what the sentence structure offers when one attempts to analyze the structure of discourse, thus moving from “the trees” to “the forest”.</S>
			<S sid ="15" ssid = "2">Therefore, we emphasize that discourse annotation should make use of every possible detail the annotation of the component parts of the discourse, namely the sentences, puts at our disposal.</S>
			<S sid ="16" ssid = "3">This is, of course, not only true for the surface shape of the sentence (i.e., the surface means of expression), but (and most importantly) for the underlying representation of sentences.</S>
			<S sid ="17" ssid = "4">The panel contribution will introduce the (multilayered) annotation scenario of the Prague Dependency Treebank and illustrate the point using some of the particular features of the underlying structure of sentences that can be made use of in planning the scenario of discourse ‘treebanks’.</S>
			<S sid ="18" ssid = "5">191 Proceedings of the Linguistic Annotation Workshop, pages 191–196, Prague, June 2007.</S>
			<S sid ="19" ssid = "6">Qc 2007 Association for Computational Linguistics</S>
	</SECTION>
	<SECTION title="SDRT in Newspaper Text. " number = "3">
			<S sid ="20" ssid = "1">(Brian Reese, Austin) We are currently working under the auspices of an NSF grant to build and train a discourse parser and codependent anaphora resolution program to test discourse theories empirically.</S>
			<S sid ="21" ssid = "2">The training requires the construction of a corpus annotated with discourse structure and coreference information.</S>
			<S sid ="22" ssid = "3">So far, we have annotated the MUC61 corpus for discourse structure and are in the process of annotating the ACE22 corpus; both corpora are already annotated for coreference.</S>
			<S sid ="23" ssid = "4">One of the goals of the project is to investigate whether using the right frontier constraint improves the system’s performance in resolving anaphors.</S>
			<S sid ="24" ssid = "5">Here we detail some experiences we have had with the discourse annotation process.</S>
			<S sid ="25" ssid = "6">An implementation of the extant SD RT (Asher and genre-specific corpus of German newspaper commentaries, taken from the daily papers Ma¨ rkische Allgemeine Zeitung and Tagesspiegel.</S>
			<S sid ="26" ssid = "7">One central aim is to provide a tool for studying mechanisms of argumentation and how they are reflected on the linguistic surface.</S>
			<S sid ="27" ssid = "8">The corpus on the one hand is a collection of “raw” data, which is used for genre- oriented statistical explorations.</S>
			<S sid ="28" ssid = "9">On the other hand, we have identified two sub-corpora that are subject to a rich multi-level annotation (MLA).</S>
			<S sid ="29" ssid = "10">The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.</S>
			<S sid ="30" ssid = "11">It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.</S>
			<S sid ="31" ssid = "12">The sentences have been PoS-tagged automatically (and manually checked); sentence syntax was annotated semi-automatically using the TIGER scheme 3 Lascarides, 2003) glue logic for building discourse (Brants et al., 2002) and Annotate tool.</S>
			<S sid ="32" ssid = "13">In addition, structures is insufficient to deal with open domain text, and we cannot envision an extended version at the present time able to deal with the problem.</S>
			<S sid ="33" ssid = "14">Thus, we have opted for a machine learning based approach to discourse parsing based on superficial features, like BNL.</S>
			<S sid ="34" ssid = "15">To build an implementation to test these ideas, we have had to devise a corpus of texts annotated for discourse structure in SD RT.</S>
			<S sid ="35" ssid = "16">Each of the 60 texts in the MUC6 corpus, and now 18 of the news stories in ACE2, were annotated by two people familiar with SD RT.</S>
			<S sid ="36" ssid = "17">The annotators then conferred and agreed upon a gold standard.</S>
			<S sid ="37" ssid = "18">Our annotation effort took the hierarchical structure of SD RT seriously and built graphs in which the nodes are discourse units and the arcs represent discourse relations between the units.</S>
			<S sid ="38" ssid = "19">The units could either be simple (elementary discourse units: ED Us) or they could be complex.</S>
			<S sid ="39" ssid = "20">We assumed that in principle the units were recursively generated and could have an arbitrary though finite degree of complexity.</S>
	</SECTION>
	<SECTION title="Potsdam Commentary Corpus. " number = "4">
			<S sid ="40" ssid = "1">(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.</S>
			<S sid ="41" ssid = "2">It is a 1 The Message Understanding Conference, wwwnlpir.</S>
			<S sid ="42" ssid = "3">nist.gov/related projects/muc/.</S>
			<S sid ="43" ssid = "4">2 The Automated Content Extraction program,.</S>
			<S sid ="44" ssid = "5">www.nist.gov/speech/tests/ace/.</S>
			<S sid ="45" ssid = "6">we annotated coreference (PoCos (Krasavina and Chiarcos, 2007)) and rhetorical structure according to RST (Mann and Thompson, 1988).</S>
			<S sid ="46" ssid = "7">Our annotation software architecture consists of a variety of standard, external tools that can be used effectively for the different annotation types.</S>
			<S sid ="47" ssid = "8">Their XML output is then automatically converted to a generic format (PAULA, (Dipper, 2005)), which is read into the linguistic database ANNIS (Dipper et al., 2004), where the annotations are aligned, so that the data can be viewed and queried across annotation levels.</S>
			<S sid ="48" ssid = "9">The PCC10 is a sub-corpus of 10 commentaries that serves as “testbed” for further developing the annotation levels.</S>
			<S sid ="49" ssid = "10">On the one hand, we are applying recent guidelines on annotation of information structure (Go¨ tze et al., 2007).</S>
			<S sid ="50" ssid = "11">On the other hand, based on experiences with the RST annotation, we are replacing the rhetorical trees with a set of distinct, simpler annotation layers: thematic structure, conjunctive relations (Martin, 1992), and argumentation structure (Freeman, 1991); these are complemented by the other levels mentioned above for the PCC176.</S>
			<S sid ="51" ssid = "12">The primary motivation for this step is the high degree of arbitrariness that annotators reported when producing the RST trees (see (Stede, 2007)).</S>
			<S sid ="52" ssid = "13">By separating the thematic from the intentional information, and accounting for the surface-oriented 3 www.coli.unisaarland.de/projects/ sfb378/negra-corpus/annotate.html conjunctive relations (which are similar to what is annotated in the PDTB, see Section 6), we hope to • make annotation easier: handling several “simple” levels individually should be more effective than a single, very complex annotation step; • end up with less ambiguity in the annotations, since the reasons for specific decisions can be made explicit (by annotations on “simpler” levels); • be more explicit than a single tree can be: if a discourse fulfills, for example, a function both for thematic development and for the writer’s intention, they can both be accounted for; • provide the central information that a “traditional” rhetorical tree conveys, without loosing essential information.</S>
	</SECTION>
	<SECTION title="AZ Corpus. " number = "5">
			<S sid ="53" ssid = "1">(Simone Teufel, Cambridge) The Argumentative Zoning (AZ) annotation scheme (Teufel, 2000; Teufel and Moens, 2002) is concerned with marking argumentation steps in scientific articles.</S>
			<S sid ="54" ssid = "2">One example for an argumentation step is the description of the research goal, another an overt comparison of the authors’ work with rival approaches.</S>
			<S sid ="55" ssid = "3">In our scheme, these argumentation steps have to be associated with text spans (sentences or sequences of sentences).</S>
			<S sid ="56" ssid = "4">AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)).</S>
			<S sid ="57" ssid = "5">The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper.</S>
			<S sid ="58" ssid = "6">(Teufel et al., 1999) reports on interannotator agreement studies with this scheme.</S>
			<S sid ="59" ssid = "7">There is a strong interrelationship between the argumentation in a paper, and the citations writers use to support their argument.</S>
			<S sid ="60" ssid = "8">Therefore, a part of the computational linguistics corpus has a second layer of annotation, called CFC (Teufel et al., 2006) or Citation Function Classification.</S>
			<S sid ="61" ssid = "9">CFC– annotation records for each citation which rhetorical function it plays in the argument.</S>
			<S sid ="62" ssid = "10">This is following the spirit of research in citation content analysis (e.g., (Moravcsik and Murugesan, 1975)).</S>
			<S sid ="63" ssid = "11">An example for a ci tation function would be “motivate that the method used is sound”.</S>
			<S sid ="64" ssid = "12">The annotation scheme contains 12 functions, clustered into “superiority”, “neutral comparison/contrast”, “praise or usage” and “neutral”.</S>
			<S sid ="65" ssid = "13">One type of research we hope to do in the future is to study the relationship between these rhetorical phonemena with more traditional discourse phenomena, e.g. anaphoric expressions.</S>
			<S sid ="66" ssid = "14">The CmpLg/ACL Anthology corpora consist of 320/9000 papers in computational linguistics.</S>
			<S sid ="67" ssid = "15">They are partially annotated with AZ and CFC markup.</S>
			<S sid ="68" ssid = "16">A subcorpus of 80 parallelly annotated papers (AZ and CFF) can be obtained from us for research (12000 sentences, 1756 citations).</S>
			<S sid ="69" ssid = "17">We are currently porting both schemes to chemistry in the framework of the EPSRC-sponsored project SciBorg.</S>
			<S sid ="70" ssid = "18">In the course of this work a larger, more general AZ annotation scheme was developed.</S>
			<S sid ="71" ssid = "19">The SciBorg effort will result in an AZ/CFC–annotated chemistry corpus available to the community in 2009.</S>
			<S sid ="72" ssid = "20">In terms of challenges, the most time-consuming aspects of creating this annotated corpus were format conversions on the corpora, and cyclic adaptations of scheme and guidelines.</S>
			<S sid ="73" ssid = "21">Another problem is the simplification of annotating only full sentences; sometimes, annotators would rather mark a clause or sometimes even just an NP.</S>
			<S sid ="74" ssid = "22">However, we found these cases to be relatively rare.</S>
	</SECTION>
	<SECTION title="Penn Discourse Treebank. " number = "6">
			<S sid ="75" ssid = "1">(Bonnie Webber, Edinburgh) The Penn Discourse TreeBank (Miltsakaki et al., 2004; Prasad et al., 2004; Webber, 2005) annotates discourse relations over the Wall Street Journal corpus (Marcus et al., 1993), in terms of discourse connectives and their arguments.</S>
			<S sid ="76" ssid = "2">Following the approach towards discourse structure in (Webber et al., 2003), the PDTB takes a lexicalized approach, treating discourse connectives as the anchors of the relations and thus as discourse-level predicates taking two Abstract Objects as their arguments.</S>
			<S sid ="77" ssid = "3">Annotated are the text spans that give rise to these arguments.</S>
			<S sid ="78" ssid = "4">There are primarily two types of connectives in the PDTB: explicit and implicit, the latter being inserted between adjacent paragraph-internal sentence pairs not related by an explicit connective.</S>
			<S sid ="79" ssid = "5">Also annotated in the PDTB is the attribution of each discourse relation and of its arguments (Dinesh et al., 2005; Prasad et al., 2007).</S>
			<S sid ="80" ssid = "6">(Attribution itself is not considered a discourse relation.)</S>
			<S sid ="81" ssid = "7">A preliminary version of the PDTB was released in April 2006 (PDTB-Group, 2006), and is available for download at http://www.seas.upenn.edu/˜pdtb.</S>
			<S sid ="82" ssid = "8">This release only has implicit connectives annotated in three sections of the corpus.</S>
			<S sid ="83" ssid = "9">The annotation of all implicit connectives, along with a hierarchical semantic classification of all connectives (Miltsakaki et al., 2005), will appear in the final release of the PDTB in August 2007.</S>
			<S sid ="84" ssid = "10">Here I want to mention three of the challenges we have faced in developing the PDTB: (I) Words and phrases that can function as connectives can also serve other roles.</S>
			<S sid ="85" ssid = "11">(Eg, when can be a relative pronoun, as well as a subordinating conjunction.)</S>
			<S sid ="86" ssid = "12">It has been difficult to identify all and only those cases where a token functions as a discourse connective, and in many cases, the syntactic analysis in the Penn TreeBank (Marcus et al., 1993) provides no help.</S>
			<S sid ="87" ssid = "13">For example, is as though always a subordinating conjunction (and hence a connective) or do some tokens simply head a manner adverbial (eg, seems as though . . .</S>
			<S sid ="88" ssid = "14">versus seems more rushed as though . . .</S>
			<S sid ="89" ssid = "15">Is also sometimes a discourse connective relating two abstract objects and other times, an adverb that presupposes that a particular property holds of some other entity?</S>
			<S sid ="90" ssid = "16">If so, when one and when the other?</S>
			<S sid ="91" ssid = "17">In the PDTB, annotation has erred on the side of false positives.</S>
			<S sid ="92" ssid = "18">(II) In annotating implicit connectives, we discovered systematic non-lexical indicators of discourse relations.</S>
			<S sid ="93" ssid = "19">In English, these include cases of marked syntax (eg, Had I known the Queen would be here, I would have dressed better.)</S>
			<S sid ="94" ssid = "20">and cases of sentence- initial PPs and adjuncts with anaphoric or deictic NPs such as at the other end of the spectrum, adding to that speculation.</S>
			<S sid ="95" ssid = "21">These cases labelled A LTLEX, for “alternative lexicalisation” have not been annotated as connectives in the PDTB because they are fully productive (ie, not members of a more easily annotated closed set of tokens).</S>
			<S sid ="96" ssid = "22">They comprise about 1% of the cases the annotators have considered.</S>
			<S sid ="97" ssid = "23">Future discourse annotation will benefit from further specifying the types of these cases.(III) The way in which spans are annotated as ar guments to connectives also raises a challenge.</S>
			<S sid ="98" ssid = "24">First, because the PDTB annotates both structural and anaphoric connectives (Webber et al., 2003), a span can serve as argument to &gt;1 connective.</S>
			<S sid ="99" ssid = "25">Secondly, unlike in the RST corpus (Carlson et al., 2003) or the Discourse GraphBank (Wolf and Gibson, 2005), discourse segments are not separately annotated, with annotators then identifying what discourse relations hold between them.</S>
			<S sid ="100" ssid = "26">Instead, in annotating arguments, PDTB annotators have selected the minimal clausal text span needed to interpret the relation.</S>
			<S sid ="101" ssid = "27">This could comprise an embedded, subordinate or coordinate clause, an entire sentence, or a (possibly disjoint) sequence of sentences.</S>
			<S sid ="102" ssid = "28">As a result, there are fairly complex patterns of spans within and across sentences that serve as arguments to different connectives, and there are parts of sentences that don’t appear within the span of any connective, explicit or implicit.</S>
			<S sid ="103" ssid = "29">The result is that the PDTB provides only a partial but complexly-patterned cover of the corpus.</S>
			<S sid ="104" ssid = "30">Understanding what’s going on and what it implies for discourse structure (and possibly syntactic structure as well) is a challenge we’re currently trying to address (Lee et al., 2006).</S>
	</SECTION>
	<SECTION title="MPQA Opinion Corpus. " number = "7">
			<S sid ="105" ssid = "1">(Theresa Wilson, Pittsburgh) Our opinion annotation scheme (Wiebe et al., 2005) is centered on the notion of private state, a general term that covers opinions, beliefs, thoughts, sentiments, emotions, intentions and evaluations.</S>
			<S sid ="106" ssid = "2">As Quirk et al.</S>
			<S sid ="107" ssid = "3">(1985) define it, a private state is a state that is not open to objective observation or verification.</S>
			<S sid ="108" ssid = "4">We can further view private states in terms of their functional components — as states of experiencers holding attitudes, optionally toward targets.</S>
			<S sid ="109" ssid = "5">For example, for the private state expressed in the sentence John hates Mary, the experiencer is John, the attitude is hate, and the target is Mary.</S>
			<S sid ="110" ssid = "6">We create private state frames for three main types of private state expressions (subjective expressions) in text: • explicit mentions of private states, such as “fears” in ”The U.S. fears a spill-over” • speech events expressing private states, such as “said” in “The report is full of absurdities,” XiraoNima said.</S>
			<S sid ="111" ssid = "7">• expressive subjective elements, such as “full of absurdities” in the sentence just above.</S>
			<S sid ="112" ssid = "8">Frames include the source (experiencer) of the private state, the target, and various properties such as polarity (positive, negative, or neutral) and intensity (high, medium, or low).</S>
			<S sid ="113" ssid = "9">Sources are nested.</S>
			<S sid ="114" ssid = "10">For example, for the sentence “China criticized the U.S. report’s criticism of China’s human rights record”, the source is (writer, China, U.S. report), reflecting the facts that the writer wrote the sentence and the U.S. report’s criticism is the target of China’s criticism.</S>
			<S sid ="115" ssid = "11">It is common for multiple frames to be created for a single clause, reflecting various levels of nesting and the type of subjective expression.</S>
			<S sid ="116" ssid = "12">The annotation scheme has been applied to a corpus, called the “Multi-Perspective Question Answering (MPQA) Corpus,” reflecting its origins in the 2002 NRRC Workshop on Multi-Perspective Question Answering (MPQA) (Wiebe et al., 2003) sponsored by ARDA AQUAINT (it is also called “OpinionBank”).</S>
			<S sid ="117" ssid = "13">It contains 535 documents and a total of 11,114 sentences.</S>
			<S sid ="118" ssid = "14">The articles in the corpus are from 187 different foreign and U.S. news sources, dating from June 2001 to May 2002.</S>
			<S sid ="119" ssid = "15">Please see (Wiebe et al., 2005) and Theresa Wilson’s forthcoming PhD dissertation for further information, including the results of inter-coder agreement studies.</S>
	</SECTION>
</PAPER>
