<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Thompson (1988), the web site of RST1 and the In this article we present the RST Spanish Treebank, the first corpus annotated with rhetorical relations for this language.</S>
		<S sid ="2" ssid = "2">We describe the characteristics of the corpus, the annotation criteria, the annotation procedure, the inter-annotator agreement, and other related aspects.</S>
		<S sid ="3" ssid = "3">Moreover, we show the interface that we have developed to carry out searches over the corpus’ annotated texts.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">The Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is a language independent theory based on the idea that a text can be segmented into Elementary Discourse Units (EDUs) linked by means of nucleus-satellite or multinuclear rhetorical relations.</S>
			<S sid ="5" ssid = "5">In the first case, the satellite gives additional information about the other one, the nucleus, on which it depends (ex.</S>
			<S sid ="6" ssid = "6">Result, Condition, Elaboration or Concession).</S>
			<S sid ="7" ssid = "7">In the second case, several elements, all nuclei, are connected at the same level, that is, there are no elements dependent on others and they all have the same importance with regard to the intentions of the author of the text (ex.</S>
			<S sid ="8" ssid = "8">Contrast, List, Joint or Sequence).</S>
			<S sid ="9" ssid = "9">The rhetorical analysis of a text by means of RST includes 3 phases: segmentation, detection of relations and building of hierarchical rhetorical trees.</S>
			<S sid ="10" ssid = "10">For more information about RST we recommend the original article of Mann and RST review by Taboada and Mann (2006a).</S>
			<S sid ="11" ssid = "11">RST has been used to develop several applications, like automatic summarization, information extraction (IE), text generation, question-answering, automatic translation, etc.</S>
			<S sid ="12" ssid = "12">(Taboada and Mann, 2006b).</S>
			<S sid ="13" ssid = "13">Nevertheless, most of these works have been developed for English, German or Portuguese.</S>
			<S sid ="14" ssid = "14">This is due to the fact that at present corpora annotated with RST relations are available only for these languages (for English: Carlson et al., 2002, Taboada and Renkema, 2008; for German: Stede, 2004; for Portuguese: Pardo et al., 2008) and there are automatic RST parsers for two of them (for English: Marcu, 2000; for Portuguese: Pardo et al., 2008) or automatic RST segmenters (for English: Tofiloski et al., 2009).</S>
			<S sid ="15" ssid = "15">Scientific community working on RST applied to Spanish is very small.</S>
			<S sid ="16" ssid = "16">For example, BouayadAgha et al.</S>
			<S sid ="17" ssid = "17">(2006) apply RST to text generation in several languages, Spanish among them.</S>
			<S sid ="18" ssid = "18">Da Cunha et al.</S>
			<S sid ="19" ssid = "19">(2007) develop a summarization system for medical texts in Spanish based on RST.</S>
			<S sid ="20" ssid = "20">Da Cunha and Iruskieta (2010) perform a contrastive analysis of Spanish and Basque texts.</S>
			<S sid ="21" ssid = "21">Romera (2004) analyzes coherence relations by means of RST in spoken Spanish.</S>
			<S sid ="22" ssid = "22">Taboada (2004) applies RST to analyze the resources used by speakers to elaborate conversations in English and Spanish.</S>
			<S sid ="23" ssid = "23">We consider that it is necessary to build a Spanish corpus annotated by means of RST.</S>
			<S sid ="24" ssid = "24">This corpus should be useful for the development of a rhetorical parser for this language and several other applications related to computational linguistics, like those developed for other languages 1 http://www.sfu.ca/rst/index.html 1 Proceedings of the Fifth Law Workshop (LAW V), pages 1–10, Portland, Oregon, 2324 June 2011.</S>
			<S sid ="25" ssid = "25">Qc 2011 Association for Computational Linguistics (automatic translation, automatic summarization, IE, etc.).</S>
			<S sid ="26" ssid = "26">And that is what we pretend to achieve with our work.</S>
			<S sid ="27" ssid = "27">We present the development of the RST Spanish Treebank, the first Spanish corpus annotated by means of RST.</S>
			<S sid ="28" ssid = "28">In Section 2, we present the state of the art about RST annotated corpora.</S>
			<S sid ="29" ssid = "29">In Section 3, we explain the characteristics of the RST Spanish Treebank.</S>
			<S sid ="30" ssid = "30">In Section 4, we show the search interface we have developed.</S>
			<S sid ="31" ssid = "31">In Section 5, we establish some conclusions and future work.</S>
	</SECTION>
	<SECTION title="State of the Art. " number = "2">
			<S sid ="32" ssid = "1">The most known RST corpus is the RST Discourse Treebank, for English (Carlson et al., 2002a, 2002b).</S>
			<S sid ="33" ssid = "2">It includes 385 texts of the journalistic domain, extracted from the Penn Treebank (Marcus et al., 1993), such as cultural reviews, editorials, economy articles, etc. 347 texts are used as a learning corpus and 38 texts are used as a test corpus.</S>
			<S sid ="34" ssid = "3">It contains 176,389 words and 21,789 EDUs.</S>
			<S sid ="35" ssid = "4">13.8% of the texts (that is, 53) were annotated by two people with a list of 78 relations.</S>
			<S sid ="36" ssid = "5">For annotation, the annotation tool RSTtool 2 (O&apos;Donnell, 2000) was used, with some adaptations.</S>
			<S sid ="37" ssid = "6">The principal advantages of this corpus stand on the high number of annotated texts (for the moment it is the biggest RST corpus) and the clarity of the annotation method (specified in the annotation manual by Carlson and Marcu, 2001).</S>
			<S sid ="38" ssid = "7">However, some drawbacks remain.</S>
			<S sid ="39" ssid = "8">The corpus is not free, it is not online and it only includes texts of one domain (journalistic).</S>
			<S sid ="40" ssid = "9">For English there is also the Discourse Relations Reference Corpus (Taboada and Renkema, 2008).</S>
			<S sid ="41" ssid = "10">This corpus includes 65 texts specified and it does not include texts annotated by several people.</S>
			<S sid ="42" ssid = "11">Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).</S>
			<S sid ="43" ssid = "12">This corpus includes 173 texts on politics from the online newspaper Märkische Allgemeine Zeitung.</S>
			<S sid ="44" ssid = "13">It contains 32,962 words and 2,195 sentences.</S>
			<S sid ="45" ssid = "14">It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.</S>
			<S sid ="46" ssid = "15">Nevertheless, only a part of this corpus (10 texts), which the authors name &quot;core corpus&quot;, is annotated with all this information.</S>
			<S sid ="47" ssid = "16">The texts were annotated with the RSTtool.</S>
			<S sid ="48" ssid = "17">This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).</S>
			<S sid ="49" ssid = "18">The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.</S>
			<S sid ="50" ssid = "19">For Portuguese, there are 2 corpora, built in order to develop a rhetorical parser (Pardo et al., 2008).</S>
			<S sid ="51" ssid = "20">The first one, the CorpusTCC (Pardo et al., 2008), was used as learning corpus for detection of linguistic patterns indicating rhetorical relations.</S>
			<S sid ="52" ssid = "21">It contains 100 introduction sections of computer science theses (53,000 words and 1,350 sentences).</S>
			<S sid ="53" ssid = "22">To annotate the corpus a list of 32 rhetorical relations was used.</S>
			<S sid ="54" ssid = "23">The annotation manual by Carlson and Marcu (2001) was adapted to Portuguese.</S>
			<S sid ="55" ssid = "24">The annotation tool was the ISI RST 3 (each one tagged by one annotator) of several types Annotation Tool , an extension of the RSTtool.</S>
			<S sid ="56" ssid = "25">and from several sources: 21 articles from the Wall Street Journal extracted from the RST Discourse Treebank, 30 movies and books’ reviews extracted from the epinions.com website, and 14 diverse texts, including letters, webs, magazine articles, newspaper editorials, etc. The tool used for annotation was also the RSTtool.</S>
			<S sid ="57" ssid = "26">The advantages of this corpus are that it is free and online, and it includes texts of several types and domains.</S>
			<S sid ="58" ssid = "27">The disadvantages are that the amount of texts is not very high, the annotation methodology is not The advantages of this corpus are: it is free, it contains an acceptable number of texts and words and it follows a specific annotation methodology.</S>
			<S sid ="59" ssid = "28">The disadvantage is: it only includes texts of one genre and domain, only annotated by one person.</S>
			<S sid ="60" ssid = "29">The second one, Rhetalho (Pardo and Seno, 2005), was used as reference corpus for the parser evaluation.</S>
			<S sid ="61" ssid = "30">It contains 50 texts: 20 introduction sections and 10 conclusion sections from computer science scientific articles, and 20 texts from the online newspaper Folha de São Paulo (7 from the Daily section, 7 from the World section and 6 from 2 http://www.wagsoft.com/RSTTool/ 3 http://www.isi.edu/~marcu/discourse/ the Science section).</S>
			<S sid ="62" ssid = "31">It includes approximately 5,000 words.</S>
			<S sid ="63" ssid = "32">The relations and the annotation tool are the same as those used in the CorpusTCC.</S>
			<S sid ="64" ssid = "33">The advantages of this corpus are that it is free, it was annotated by 2 people (they both were RST experts and followed an annotation manual) and it contains texts of several genres and domains.</S>
			<S sid ="65" ssid = "34">The main disadvantage is the scarce amount of texts.</S>
			<S sid ="66" ssid = "35">The Penn Discourse Treebank (Rashmi et al., 2008)f for English includes texts annotated with information related to discourse structure and semantics (without a specific theoretical approach).</S>
			<S sid ="67" ssid = "36">Its advantages are: its big size (it contains 40,600 annotated discourse relations) allows to apply machine learning, and the discourse annotations are aligned with the syntactic constituency annotations of the Penn Treebank.</S>
			<S sid ="68" ssid = "37">Its limitations are: dependencies across relations are not marked, it only includes texts of the journalistic domain, and it is not free.</S>
			<S sid ="69" ssid = "38">Although there are several corpora annotated with discourse relations, there is not a corpus of this type for Spanish.</S>
	</SECTION>
	<SECTION title="The RST Spanish Treebank. " number = "3">
			<S sid ="70" ssid = "1">As Sierra (2008) states, a corpus consists of a compilation of a set of written and/or spoken texts sharing some characteristics, created for certain investigation purposes.</S>
			<S sid ="71" ssid = "2">According to Hovy (2010), we use 7 core questions in corpus design, detailed in the next subsections.</S>
			<S sid ="72" ssid = "3">3.1 Selecting a Corpus.</S>
			<S sid ="73" ssid = "4">For the RST Spanish Treebank, we wanted to include short texts (finally, the average is 197 words by text; the longest containing 1,051 words and the shortest, 25) in order to get a best online visualization of the RST trees.</S>
			<S sid ="74" ssid = "5">Moreover, in the first stage of the project, we preferred to select specialized texts of very different areas, although in the future we plan to include also non- specialized texts (ex.</S>
			<S sid ="75" ssid = "6">blogs, news, websites) in order to guarantee the representativity of the corpus.</S>
			<S sid ="76" ssid = "7">We did not find a preexisting Spanish corpus with these characteristics, so we decided to build our own corpus.</S>
			<S sid ="77" ssid = "8">Following Cabré (1999), we consider that a text is specialized if it is written by a professional in a given domain.</S>
			<S sid ="78" ssid = "9">According to this work, specialized texts can be divided in three levels: high (both the author and the potential reader of the text are specialists), average (the author of the text is a specialist, and the potential reader of that text is a student or someone interested in or possessing some prior knowledge about the subject) and low (the author of the text is a specialist, and the potential reader is the general public).</S>
			<S sid ="79" ssid = "10">The RST Spanish Treebank includes specialized texts of the three mentioned levels: high (scientific articles, conference proceedings, doctoral theses, etc.), average (textbooks) and low (articles and reports from popular magazines, associations’ websites, etc.).</S>
			<S sid ="80" ssid = "11">The texts have been divided in 9 domains (some of them including subdivisions): Astrophysics, Earthquake Engineering, Economy, Law, Linguistics (Applied Linguistics, Language Acquisition, PLN, Terminology), Mathematics (Primary Education, Secondary Education, Scientific Articles), Medicine (Administration of Health Services, Oncology, Orthopedy), Psychology and Sexuality (Clinical Perspective, Psychological Perspective).</S>
			<S sid ="81" ssid = "12">The size of a corpus is also a polemic question.</S>
			<S sid ="82" ssid = "13">If the corpus is developed for machine learning, its size will be enough when the application we want to develop obtains acceptable percentages of precision and recall (in the context of that application).</S>
			<S sid ="83" ssid = "14">Nevertheless, if the corpus is built with descriptive purposes, it is difficult to determine the corpus size.</S>
			<S sid ="84" ssid = "15">In the case of a corpus annotated with rhetorical relations, it is even more difficult, because there are various factors involved: EDUs, SPANs (that is, a group of related EDUs), nuclearity and relations.</S>
			<S sid ="85" ssid = "16">In addition, relations are multiple (we use 28).</S>
			<S sid ="86" ssid = "17">As Hovy (2010: 13) mentions, one of the most difficult phenomena to annotate is the discourse structure.</S>
			<S sid ="87" ssid = "18">Our corpus contains 52,746 words and 267 texts.</S>
			<S sid ="88" ssid = "19">Table 1 includes RST Spanish Treebank statistics in terms of texts, words, sentences and EDUs.</S>
			<S sid ="89" ssid = "20">Te xts Wo rds Sen tenc es ED Us Lear ning corp us 1 8 3 41, 55 5 1, 7 5 9 2,6 55 Test corp us 8 4 11, 19 1 4 9 7 6 9 4 Tota l corp us 2 6 7 52, 74 6 2, 2 5 6 3,3 49 Table 1: RST Spanish Treebank statistics To increase the linear performance of a statistical method, it is necessary that the training corpus size grows exponentially (Zhao et al., 2010).</S>
			<S sid ="90" ssid = "21">However, the RST Spanish Treebank is not designed only to use statistical methods; we think it will be useful to employ symbolic or hybrid algorithms (combining symbolic and statistical methods).</S>
			<S sid ="91" ssid = "22">Moreover, this corpus will be dynamic, so we expect to have a bigger corpus in the future, useful to apply machine learning methods.</S>
			<S sid ="92" ssid = "23">If we measure the corpus size in terms of words or texts, we can take as a reference the other RST corpora.</S>
			<S sid ="93" ssid = "24">Nevertheless, as Sierra states (2008), it is “absurd” to try to build an exhaustive corpus covering all the aspects of a language.</S>
			<S sid ="94" ssid = "25">On the contrary, the linguist looks for the representativeness of the texts, that is, tries to create a sample of the studied language, selecting examples which represent the linguistic reality, in order to analyze them in a pertinent way.</S>
			<S sid ="95" ssid = "26">In this sense and in the frame of this work, we consider that the size will be adequate if the rhetorical trees of the corpus include a representative number of examples of rhetorical relations, at least 20 examples of each one (taking into account that the corpus contains 3115 relations, we consider that this quantity is acceptable; however, we expect to have even more examples when the corpus grows).</S>
			<S sid ="96" ssid = "27">Table 2 shows the number of examples of each relation currently included into the RST Spanish Treebank (N-S: nucleus-satellite relation; N-N: multinuclear relation).</S>
			<S sid ="97" ssid = "28">As it can be observed, it contains more than 20 examples of most of the relations.</S>
			<S sid ="98" ssid = "29">The exceptions are the nucleus-satellite relations of Enablement, Evaluation, Summary, Otherwise and Unless, and the multinuclear relations of Conjunction and Disjunction, because it is not so usual to find these rhetorical relations in the language, in comparison with others.</S>
			<S sid ="99" ssid = "30">Hovy (2010: 128) states that, given the lack of examples in the corpus, there are 2 possible strategies: a) to leave the corpus as it is, with few or no examples of some cases (but the problem will be the lack of training examples for machine learning systems), or b) to add low-frequency examples artificially to “enrich” the corpus (but the problem will be the distortion of the native frequency distribution and perhaps the confusion of machine learning systems).</S>
			<S sid ="100" ssid = "31">In the current state of our project, we have chosen the first option.</S>
			<S sid ="101" ssid = "32">We think that, including specialized texts in a second stage, we will get more examples of these less common relations.</S>
			<S sid ="102" ssid = "33">If we carry out a more granulated segmentation maybe we could obtain more examples; however, we wanted to employ the segmentation criteria used to develop the Spanish RST discourse segmenter (da Cunha et al., 2011).</S>
			<S sid ="103" ssid = "34">R e l a t i o n Ty pe Q u a n t i t y N º % Elab orati onN S 7 6 5 2 4 . 5 6 Prep arati onN S 4 7 5 1 5 . 2 5 Back grou ndN S 2 0 4 6 . 5 5 Resu ltN S 1 9 3 6 . 2 0 Mea nsN S 1 7 5 5 . 6 2 ListN N 1 7 2 5 . 5 2 JointN N 1 6 0 5 . 1 4 Circ umst anceN S 1 4 0 4 . 4 9 Purp oseN S 1 2 2 3 . 9 2 Inter preta tionN S 8 8 2 . 8 3 Antit hesisN S 8 0 2 . 5 7 Caus eN S 7 7 2 . 4 7 Sequ encyN N 7 4 2 . 3 8 Evid enceN S 5 9 1 . 8 9 Cont rastN N 5 8 1 . 8 6 Con ditio nN S 5 3 1 . 7 0 Con cessi onN S 5 0 1 . 6 1 Justi ficati onN S 3 9 1 . 2 5 Solut ionN S 3 2 1 . 0 3 Moti vatio nN S 2 8 0 . 9 0 Refo rmul ationN S 2 2 0 . 7 1 Othe rwis eN S 3 0 . 1 0 Conj uncti onN N 1 1 0 . 3 5 Eval uatio nN S 1 1 0 . 3 5 Disj uncti onN N 9 0 . 2 9 Sum maryN S 8 0 . 2 6 Enab leme ntN S 5 0 . 1 6 Unle ssN S 2 0 . 0 6 Table 2: Rhetorical relations in RST Spanish Treebank 3.2 Instantiating the Theory.</S>
			<S sid ="104" ssid = "35">Our segmentation and annotation criteria are very similar to the original ones used by Mann and Thompson (1988) for English, and by da Cunha and Iruskieta (2010) for Spanish.</S>
			<S sid ="105" ssid = "36">We also explore the annotation manual for English by Carlon and Marcu (2001).</S>
			<S sid ="106" ssid = "37">Though we use some of their postulates, we think that their analysis is too meticulous in some aspects.</S>
			<S sid ="107" ssid = "38">Because of this, we consider that it is not adjusted to our interest, which is the finding of the simplest and most objective annotation method, orientated to the future development of a rhetorical parser for Spanish.</S>
			<S sid ="108" ssid = "39">To sum up, our segmentation criteria are: a) All the sentences of the text are segmented as EDUs (we consider that a sentence is a textual passage between a period and another period, a semicolon, a question mark or an exclamation point; texts’ titles are also segmented).</S>
			<S sid ="109" ssid = "40">Exs.4 [Éstas son las razones fundamentales que motivaron este trabajo.]</S>
			<S sid ="110" ssid = "41">[These are the fundamental reasons which motivated this work.]</S>
			<S sid ="111" ssid = "42">[Estudio de caso único sobre violencia conyugal] [Study of a case on conjugal violence] b) Intra-sentence EDUs are segmented, using the following criteria: b1) An intra-sentence EDU has to include a finite verb, an infinitive or a gerund.</S>
			<S sid ="112" ssid = "43">Ex.</S>
			<S sid ="113" ssid = "44">[Siendo una variante de la eliminación Gaussiana,] [posee características didácticas ventajosas.]</S>
			<S sid ="114" ssid = "45">[Being a variant of Gaussian elimination,] [it possesses didactic profitable characteristics.]</S>
			<S sid ="115" ssid = "46">b2) Subject/object subordinate clauses or substantive sentences are not segmented.</S>
			<S sid ="116" ssid = "47">Ex.</S>
			<S sid ="117" ssid = "48">[Se muestra que el modelo discreto en diferencias finitas es convergenteyquesurealizaciónsereducearesolver una sucesión de sistemas lineales tridiagonales.]</S>
			<S sid ="118" ssid = "49">[It appears that the discreet model in finite differences is convergent and that its accomplishment is to solve a succession of tridiagonal linear systems.]</S>
			<S sid ="119" ssid = "50">b3) Subordinate relative clauses are not segmented.</S>
			<S sid ="120" ssid = "51">Ex.</S>
			<S sid ="121" ssid = "52">[Durante el proceso, que utiliza solo aritmética entera, se obtiene el determinante de la matriz de coeficientes del sistema, sin necesidad de cálculos adicionales.]</S>
			<S sid ="122" ssid = "53">[During the process, which only uses entire arithmetic, the determinant of the system coefficient matrix is obtained, without additional calculations.]</S>
			<S sid ="123" ssid = "54">b4) Elements in parentheses are only segmented if they follow the criterion b1.</S>
			<S sid ="124" ssid = "55">Ex.</S>
			<S sid ="125" ssid = "56">[Este año se cumple el bicentenario del nacimiento de Niels (Nicolás, en nuestro idioma) Henrik Abel.]</S>
			<S sid ="126" ssid = "57">[This year is the bicentenary of Niels&apos;s birth (Nicolás, in our language) Henrik Abel.]</S>
			<S sid ="127" ssid = "58">b5) Embedded units are segmented by means of the non-relation Same-Unit proposed by Carlon and Marcu (2001).</S>
			<S sid ="128" ssid = "59">Figure 1 shows this structure.</S>
			<S sid ="129" ssid = "60">[En décadas precedentes se ha puesto de manifiesto,] [y así lo han atestiguado muchos investigadores de la</S>
	</SECTION>
	<SECTION title="Spanish  examples  were extracted from the corpus. English. " number = "4">
			<S sid ="130" ssid = "1">translations are ours.</S>
			<S sid ="131" ssid = "2">terminología científica serbia,] [una tendencia a importar préstamos del inglés.] [In previous decades it has been shown,] [and it has been testified by many researchers of the scientific Serbian terminology,] [a trend to import loanwords from English.]</S>
			<S sid ="132" ssid = "3">Figure 1: Example of the non-relation Same-Unit 3.3 Designing the Interface.</S>
			<S sid ="133" ssid = "4">The annotation tool used in this work is the RSTtool, since it is free and easy to use.</S>
			<S sid ="134" ssid = "5">Therefore, we preferred to use it instead of designing a new one.</S>
			<S sid ="135" ssid = "6">Nevertheless, we have designed an online interface to include the corpus and to carry out searches over it (see Section 4).</S>
			<S sid ="136" ssid = "7">3.4 Selecting and Training the Annotators.</S>
			<S sid ="137" ssid = "8">With regard to the corpus annotators, we have a team of 10 people (last year Bachelor’s degree students, Master’s degree students and PhDs) 5 . Before the annotation, they took a RST course of 6 months (100 hours), where the segmentation and annotation methodology used for the development of the RST Spanish Treebank was explained.6 We called this period &quot;training phase&quot;.</S>
			<S sid ="138" ssid = "9">The course had a theoretical and a practical part.</S>
			<S sid ="139" ssid = "10">In the theoretical part, some criteria with regard to the 3 phases of rhetorical analysis (segmentation, detection of relations, and rhetorical trees building) were given to annotators.</S>
			<S sid ="140" ssid = "11">In the practical part, firstly, it was explained how to use the RSTtool.</S>
			<S sid ="141" ssid = "12">Secondly, annotators extracted several texts from the web, following their personal interests, as for example, music, video games, cookery or art webs.</S>
			<S sid ="142" ssid = "13">They segmented those texts, using the established segmentation criteria.</S>
			<S sid ="143" ssid = "14">Once segmented, all the doubts and problematic examples were discussed, and they tried to get an agreement on the most complicated cases.</S>
			<S sid ="144" ssid = "15">Thirdly, the relations were</S>
	</SECTION>
	<SECTION title="We  thank  annotators  (Adriana  Valerio,  Brenda  Castro,. " number = "5">
			<S sid ="145" ssid = "1">Daniel Rodríguez, Ita Cruz, Jessica Méndez, Josué Careaga, Luis Cabrera, Marina Fomicheva and Paulina De La Vega) and interface developers (Luis Cabrera and Juan Rolland).</S>
			<S sid ="146" ssid = "2">in the Spanish Linguistics Degree at UNAM (Mexico City).</S>
			<S sid ="147" ssid = "3">analyzed (using a given relations list) and, once again, annotators discussed the difficult cases.</S>
			<S sid ="148" ssid = "4">After the discussion, texts were re-annotated to verify if the difficulties were solved.</S>
			<S sid ="149" ssid = "5">This process was doubly interesting, since it helped to create common criteria for the annotation of the final corpus and to define the annotation criteria more clearly and consensually, in order to include them in the RST Spanish Treebank annotation manual.</S>
			<S sid ="150" ssid = "6">Once annotators agreed on the most difficult cases, we consider that the training phase finished.</S>
			<S sid ="151" ssid = "7">3.5 Designing and Managing the Annotation.</S>
			<S sid ="152" ssid = "8">Procedure We start from the following annotation definition: Annotation (‘tagging’) is the process of adding new information into source material by humans (annotators) or suitably trained machines.</S>
			<S sid ="153" ssid = "9">The addition process usually requires some sort of mental decision that depends both on the source material and on some theory or knowledge that the annotator has internalized earlier.</S>
			<S sid ="154" ssid = "10">(Hovy, 2010: 6) Exactly, after our annotators internalized the theory and annotation criteria during the training phase, the &quot;annotation phase&quot; of the final texts included in the RST Spanish Treebank started.</S>
			<S sid ="155" ssid = "11">In this phase, the annotation tasks were assigned to annotators (the number of texts assigned to each annotator was different, depending on their availability).</S>
			<S sid ="156" ssid = "12">They were asked to carry out the annotation individually and without questions among them.</S>
			<S sid ="157" ssid = "13">We calculated that the average time to carry out the annotation of one text was between 15 minutes and 1 hour.</S>
			<S sid ="158" ssid = "14">This time difference is due to the fact that the corpus includes both short and long texts.</S>
			<S sid ="159" ssid = "15">The annotation process is the following: once a text is segmented, rhetorical relations between EDUs are annotated.</S>
			<S sid ="160" ssid = "16">First, EDUs inside the same sentence are annotated in a binary way.</S>
			<S sid ="161" ssid = "17">Second, sentences inside the same paragraph are linked.</S>
			<S sid ="162" ssid = "18">Finally, paragraphs are linked.</S>
			<S sid ="163" ssid = "19">Hovy (2010) states that it is difficult to determine if, for the same money (we add “for the same time”), it is better to double-annotate less, or to single-annotate more.</S>
			<S sid ="164" ssid = "20">As he explains, Dligach et al.</S>
			<S sid ="165" ssid = "21">(2010) made an experiment with OntoNotes (Pradhan et al., 2007) verb sense annotation.</S>
			<S sid ="166" ssid = "22">The result was that, assuming the annotation is stable (that is, inter-annotator agreement is high), it is better to annotate more, even with only one annotator.</S>
			<S sid ="167" ssid = "23">The problem with RST annotation is that there are so many categories to annotate, that is very difficult to obtain a stable annotation.</S>
			<S sid ="168" ssid = "24">Therefore, we consider it is necessary to have at least some texts double-annotated (or even triple- annotated), in order to have an adequate discourse corpus.</S>
			<S sid ="169" ssid = "25">This is the reason why, following the RST Discourse Treebank methodology, we use some texts as learning corpus and some others (from the Mathematics, Psychology and Sexuality domains) as test corpus: 69% (183 texts) and 31% (84 texts), respectively.</S>
			<S sid ="170" ssid = "26">The texts of the learning corpus were annotated by 1 person, whereas the texts of the test corpus were annotated by 2 people.</S>
			<S sid ="171" ssid = "27">3.6 Validating Results.</S>
			<S sid ="172" ssid = "28">Da Cunha and Iruskieta (2010) measure inter- annotator agreement by using the RST trees comparison methodology by Marcu (2000).</S>
			<S sid ="173" ssid = "29">This methodology evaluates the agreement on 4 elements (EDUs, SPANs, Nuclearity and Relations), by means of precision and recall measures (an annotation with regard to the other one).</S>
			<S sid ="174" ssid = "30">Following this methodology, we have measured inter-annotator agreement over the test corpus.</S>
			<S sid ="175" ssid = "31">We employ an online automatic tool for RST trees comparison, RSTeval (Mazeiro and Pardo, 2009), where Marcu’s methodology has been implemented (for 4 languages: English, Portuguese, Spanish and Basque).</S>
			<S sid ="176" ssid = "32">We know that there are some other ways to measure agreement, such as Cohen&apos;s kappa (Cohen, 1960) or Fleiss&apos;s kappa (Fleiss, 1971), for example.</S>
			<S sid ="177" ssid = "33">Nevertheless, we consider that Marcu&apos;s methodology (2000) is suitable to compare adequately 2 annotations of the same original text, because it has been designed specifically for this task.</S>
			<S sid ="178" ssid = "34">For each trees pair from the test corpus, precision and recall were measured separately.</S>
			<S sid ="179" ssid = "35">Afterwards, all those individual results were put together to obtain general results.</S>
			<S sid ="180" ssid = "36">Table 3 shows global results for the 4 categories.</S>
			<S sid ="181" ssid = "37">The category with more agreement was EDUs (recall: 91.04% / precision: 87.20%), that is, segmentation.</S>
			<S sid ="182" ssid = "38">This result was expected, since the segmentation criteria given to the annotators were quite precise and the possibility of mistake was low.</S>
			<S sid ="183" ssid = "39">The lowest agreement was obtained for the category Relations (recall: 78.48% / precision: 76.81%).</S>
			<S sid ="184" ssid = "40">This result is lower than the other, but we think it is acceptable.</S>
			<S sid ="185" ssid = "41">In the RST Discourse Treebank the trend was similar to the one detected in our corpus: the highest agreement is obtained at the segmentation level and the lowest at the relations level.</S>
			<S sid ="186" ssid = "42">C a t e g o r y P r e c i s i o n R e c a l l ED Us 8 7 . 2 0 % 9 1 . 0 4 % SPA Ns 8 6 % 8 7 . 3 1 % Nucl earit y 8 2 . 4 6 % 8 4 . 6 6 % Rela tions 7 6 . 8 1 % 7 8 . 4 8 % Table 3: Inter-annotator agreement Precision and recall have not been calculated with respect to a gold standard because it does not exist for Spanish.</S>
			<S sid ="187" ssid = "43">Our future aim is to reach a consensus on the annotation of the test corpus (using an external &quot;judge&quot;), in order to establish a set of texts considered as a preliminary gold standard for this language.</S>
			<S sid ="188" ssid = "44">We consider that the annotations have quality at present, because inter- annotator agreement is quite high; however, this consensus could solve the typical annotation mistakes we have detected or some ambiguities.</S>
			<S sid ="189" ssid = "45">We have analyzed the main discrepancy reasons between annotators.</S>
			<S sid ="190" ssid = "46">With regard to the segmentation, the main one was human mistake; ex.</S>
			<S sid ="191" ssid = "47">segmenting EDUs without a verb (one annotator segmented the following passage into 2 EDUs because she detected a Means relation, but the second EDU does not include any verb): [Además estudiamos el desarrollo de criterios para determinar si un semigrupo dado tiene dicha propiedad ] [mediante el estudio de desigualdades de curvaturadimensión.</S>
			<S sid ="192" ssid = "48">] [We also study the development of tests in order to determine if a given semi group has this property] [by means of curvature-dimension inequalities.]</S>
			<S sid ="193" ssid = "49">The second reason was that in the manual some aspects were not explained in detail.</S>
			<S sid ="194" ssid = "50">For example, if a substantive sentence or a direct/object clause (which must not be segmented, according to the point b2) includes two coordinated clauses, these must not be segmented either.</S>
			<S sid ="195" ssid = "51">Thus, we found some erroneous segmentations.</S>
			<S sid ="196" ssid = "52">For example: [Los hombres adultos tienen miedo de fracasar] [y no cumplir con el rol masculino de ser proveedores del hogar y de proteger a su familia.]</S>
			<S sid ="197" ssid = "53">[Adult men are scared to fail] [and not to fulfill the masculine role of being the suppliers of the home and to protect their family.]</S>
			<S sid ="198" ssid = "54">This kind of mistakes allowed us to refine our segmentation manual a posteriori.</S>
			<S sid ="199" ssid = "55">In the future, we will ask the test corpus annotators to make a new annotation of the texts, using the refined manual, in order to check if the agreement increases, in the same way as the RST Discourse Treebank.</S>
			<S sid ="200" ssid = "56">With regard to rhetorical annotations, we detected 2 main reasons of inter-annotator disagreement.</S>
			<S sid ="201" ssid = "57">The first one was the ambiguity of some relations and their corresponding connectors; for example, Justification-Reason, Antithesis- Concession or Circumstance-Means relations, like in the following passage (in Spanish, “al” may indicate time or manner): [Los niños aprenden matemáticas] [al resolver problemas.]</S>
			<S sid ="202" ssid = "58">[Children learn mathematics] [when solving problems.]</S>
			<S sid ="203" ssid = "59">The second one is due to differences between annotators when determining nuclearity.</S>
			<S sid ="204" ssid = "60">For example, in the following passage, one annotator marked Background and the other one Elaboration: [Quedó un hueco en la pared de 60 x 1.20cm.]S_Background [Norma y Andrés quieren colocar en el hueco una pecera.</S>
			<S sid ="205" ssid = "61">]N_Background [Quedó un hueco en la pared de 60 x 1.20cm.]N_Elaboration [Norma y Andrés quieren colocar en el hueco una pecera.</S>
			<S sid ="206" ssid = "62">]S_Elaboration [A hole of 60 x 1.20 cm remained in the wall.]</S>
			<S sid ="207" ssid = "63">[Norma and Andrés want to place a fish tank in the hole.]</S>
			<S sid ="208" ssid = "64">It is easier to solve segmentation disagreement than relations disagreement, since in this case annotator subjectivity is more evident; we must consider how to refine our manual in this sense.</S>
			<S sid ="209" ssid = "65">3.7 Delivering and Maintaining the Product.</S>
			<S sid ="210" ssid = "66">Hovy (2010) mentions some technical issues regarding these points: licensing, distribution, maintenance and updates.</S>
			<S sid ="211" ssid = "67">With regard to licensing and distribution, the RST Spanish Treebank will be free for research purposes.</S>
			<S sid ="212" ssid = "68">We have a data manager responsible for maintenance and updates.</S>
			<S sid ="213" ssid = "69">The description of the annotated corpus is also a very important issue (Ide and Pustejovsky, 2010).</S>
			<S sid ="214" ssid = "70">It is important to provide a high level description of the corpus, including the theoretical framework, the methodology (annotators, annotation manual and tool, agreement, etc.), the means for resource maintenance, the technical aspects, the project leader, the contact, the team, etc. The RST Spanish Treebank includes all this detailed information.</S>
			<S sid ="215" ssid = "71">XML (with a DTD) has been used, in order the corpus can be reused for several aplications.</S>
			<S sid ="216" ssid = "72">In the future, we plan to use the standard XCES.</S>
			<S sid ="217" ssid = "73">To know more about resources development, linguistic annotation or inter-annotator agreement, we recommend: Palmer et al.</S>
			<S sid ="218" ssid = "74">(online), Palmer and Xue (2010), and Artstein and Poesio (2008).</S>
			<S sid ="219" ssid = "75">4 The Search Interface of the RST.</S>
			<S sid ="220" ssid = "76">Spanish Treebank The RST Spanish Treebank interface is freely available online7.</S>
			<S sid ="221" ssid = "77">It allows the visualization and downloading of all the texts in txt format, with their corresponding annotated trees in RSTtool format (rs3), as well as in image format (jpg).</S>
			<S sid ="222" ssid = "78">Each text includes its title, its reference, its web link (if it is an online text) and its number of words.</S>
			<S sid ="223" ssid = "79">The interface shows texts by areas and allows the user to select a subcorpus (including individual files or folders containing several files).</S>
			<S sid ="224" ssid = "80">The selected subcorpus can be saved on local disk (generating a xml file) for future analyses.</S>
			<S sid ="225" ssid = "81">The interface includes a statistical tool which allows obtaining statistics of rhetorical relations in a subcorpus selected by the user.</S>
			<S sid ="226" ssid = "82">The RSTtool also offers this option but it can be only used for one text.</S>
			<S sid ="227" ssid = "83">We consider that it is more useful for the user to obtain statistics from various texts, in order to get significant statistical results.</S>
			<S sid ="228" ssid = "84">As the RSTtool, our tool allows to count the multinuclear relations in two ways: a) one unit for each detected multinuclear relation, and b) one unit for each detected nucleus.</S>
			<S sid ="229" ssid = "85">If we use b), the statistics of the multinuclear relations of Table 2 are higher: List (864), Joint (537), Sequence (289), Contrast (153), Conjunction (28) and Disjunction (24).</S>
			<S sid ="230" ssid = "86">We are developing another tool, aimed to extract information from the annotated texts, which we will soon include into the interface.</S>
			<S sid ="231" ssid = "87">This tool will allow to the user to select a subcorpus and to extract from it the EDUs corresponding to the rhetorical relations selected, like a multidocument specialized summarizer guided by user&apos;s interests.</S>
			<S sid ="232" ssid = "88">The RST Spanish Treebank interface also includes a screen which permits the users to send their own annotated texts.</S>
			<S sid ="233" ssid = "89">Our aim is for the RST Spanish Treebank to become a dynamic corpus, in constant evolution, being increased with texts annotated by users.</S>
			<S sid ="234" ssid = "90">This has a double advantage since, on the one hand, the corpus will grow and, on the other hand, users will profit from the interface&apos;s applications, using their own subcorpora.</S>
			<S sid ="235" ssid = "91">The only requirement is to use the relations and the segmentation and annotation criteria of our project.</S>
			<S sid ="236" ssid = "92">Once the texts are sent, the RST Spanish Treebank data manager will verify if the annotation corresponds to these criteria.</S>
			<S sid ="237" ssid = "93">5 Conclusions and Future Work.</S>
			<S sid ="238" ssid = "94">We think that this work means an important step for the RST research in Spanish, and that the RST Spanish Treebank will be useful to carry out diverse researches about RST in this language, from a descriptive point of view (ex.</S>
			<S sid ="239" ssid = "95">analysis of texts from different domains or genres) and an applied point of view (development of discourse parsers and NLP applications, like automatic summarization, automatic translation, IE, etc.).</S>
			<S sid ="240" ssid = "96">For the moment the corpus&apos; size is acceptable and, though the percentage of double-annotated texts is not very high, we think that having 10 annotators (using the same annotation manual) avoids the bias of only one annotator.</S>
			<S sid ="241" ssid = "97">In addition, the corpus includes texts of diverse domains and genres, which provides us with a heterogeneous Spanish corpus.</S>
			<S sid ="242" ssid = "98">Moreover, the corpus interface that we have designed allows the user to select a subcorpus and to analyze it statistically.</S>
			<S sid ="243" ssid = "99">In addition, we think that it is essential to release a free corpus, online and dynamic, that is, in continuous growth.</S>
			<S sid ="244" ssid = "100">Nevertheless, we are conscious that our work still has certain limitations, which we will try to solve in the future.</S>
			<S sid ="245" ssid = "101">In the short term, we have 5 aims: a) To add one more annotator for the test corpus and to measure inter-annotator agreement.</S>
			<S sid ="246" ssid = "102">b) To use more agreement measures, like kappa.</S>
			<S sid ="247" ssid = "103">c) To reach a consensus on the annotation of the test corpus, in order to establish a set of texts considered as a preliminary gold standard.</S>
			<S sid ="248" ssid = "104">d) To finish and to evaluate the IE tool.</S>
			<S sid ="249" ssid = "105">e) To analyze the corpus to extract linguistic patterns for the automatic relations detection.</S>
			<S sid ="250" ssid = "106">In the long term, we consider other aims: f) To increase the corpus, by adding non- specialized texts, and new domains and genres.</S>
			<S sid ="251" ssid = "107">g) To annotate all the texts by 3 people, to get a representative gold-standard for Spanish (this aim will depend on the funding of the project).</S>
			<S sid ="252" ssid = "108">7 http://www.corpus.unam.mx/rst/</S>
	</SECTION>
</PAPER>
