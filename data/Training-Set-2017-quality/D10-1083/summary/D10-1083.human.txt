In this paper, the authors are of the opinion that the sequence models-based approaches usually treat token-level tag assignment as the primary latent variable. However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity. In this work, they take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. Their work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. There are clustering approaches that assign a single POS tag to each word type. These clusters are computed using an SVD variant without relying on transitional structure. The departure from the traditional token-based tagging approach allow them to explicitly capture type-level distributional properties of valid POS tag assignments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive. Their empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints. This assumption, however, is not inherent to type-based tagging models.