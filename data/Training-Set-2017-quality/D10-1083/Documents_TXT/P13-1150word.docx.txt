Unsupervised Consonant-Vowel Prediction over Hundreds of Languages



Young-Bum Kim and Benjamin Snyder
University of Wisconsin-Madison
{ybkim,bsnyder}@cs.wisc.edu







Abstract

In this paper, we present a solution to one 
aspect of the decipherment task: the pre- 
diction of consonants and vowels for an 
unknown language and alphabet. Adopt- 
ing a classical Bayesian perspective, we 
performs posterior inference over hun- 
dreds of languages, leveraging knowledge 
of known languages and alphabets to un- 
cover general linguistic patterns of typo- 
logically coherent language clusters. We 
achieve average accuracy in the unsuper- 
vised consonant/vowel prediction task of
99% across 503 languages. We further 
show that our methodology can be used 
to predict more fine-grained phonetic dis- 
tinctions.   On a  three-way classification 
task between vowels, nasals, and non- 
nasal consonants, our model yields unsu- 
pervised accuracy of 89% across the same 
set of languages.

1   Introduction

Over the past centuries, dozens of lost languages 
have  been deciphered  through the painstaking 
work  of  scholars, often after decades  of  slow 
progress and dead ends. However,  several impor- 
tant writing systems and languages remain unde- 
ciphered to this day.
  In this paper, we present a successful  solution 
to one aspect of the decipherment puzzle: auto- 
matically identifying basic phonetic properties of 
letters in an unknown alphabetic writing system. 
Our key idea is to use knowledge  of the phonetic 
regularities  encoded in known language vocabu- 
laries to automatically build a universal probabilis- 
tic model to successfully decode new languages.
  Our approach adopts a classical  Bayesian per- 
spective.   We  assume that each language has 
an unobserved  set of parameters explaining  its


observed vocabulary.   We  further assume  that 
each language-specific  set of parameters was itself 
drawn from an unobserved common prior, shared 
across a cluster of typologically related languages. 
In turn, each cluster derives its parameters from 
a universal  prior common to all language groups. 
This approach allows us to mix together data from 
languages with various levels of observations and 
perform joint posterior inference over unobserved 
variables of interest.
  At   the  bottom  layer  (see Figure  1),   our 
model assumes a language-specific data generat- 
ing HMM over words in the language vocabulary. 
Each word is modeled  as an emitted sequence of 
characters, depending on a corresponding  Markov 
sequence of phonetic tags. Since individual letters 
are highly constrained in their range of phonetic 
values, we make the assumption of one-tag-per- 
observation-type (e.g. a single letter is constrained 
to be always a consonant or always a vowel across 
all words in a language).
  Going one layer up, we posit that the language- 
specific HMM  parameters are themselves drawn 
from  informative,  non-symmetric distributions 
representing   a  typologically  coherent language 
grouping. By applying the model to a mix of lan- 
guages with observed and unobserved phonetic se- 
quences, the cluster-level distributions can be in- 
ferred and help guide prediction for unknown lan- 
guages and alphabets.
  We apply this approach to two small decipher- 
ment tasks:

1. predicting whether individual characters in 
an unknown  alphabet and language represent 
vowels or consonants, and

2. predicting whether individual characters in 
an unknown  alphabet and language represent 
vowels, nasals, or non-nasal consonants.

For both tasks, our approach yields considerable




1527

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1527–1536, 
Sofia, Bulgaria, August 4-9 2013. Qc 2013 Association for Computational Linguistics


success. We experiment  with a data set consist- 
ing of vocabularies of 503 languages from around 
the world, written in a mix of Latin, Cyrillic, and 
Greek alphabets. In turn for each language,  we 
consider it and its alphabet “unobserved”  — we 
hide the graphic and phonetic properties of the 
symbols — while treating the vocabularies of the 
remaining  languages as fully observed with pho- 
netic tags on each of the letters.
  On average, over these 503 leave-one-language- 
out scenarios, our model predicts consonant/vowel 
distinctions with 99% accuracy. In the more chal- 
lenging task of vowel/nasal/non-nasal prediction, 
our model achieves average accuracy over 89%.

2   Related Work

The most direct precedent to the present work is 
a section in Knight et al. (2006) on universal pho- 
netic decipherment. They build a trigram HMM 
with three hidden states, corresponding  to conso- 
nants, vowels,  and spaces. As in our model, indi- 
vidual characters are treated as the observed emis- 
sions of the hidden states. In contrast to the present 
work, they allow letters to be emitted by multiple 
states.
  Their experiments show that the HMM trained 
with EM successfully clusters Spanish letters into 
consonants and vowels.  They further design a 
more sophisticated finite-state model, based  on 
linguistic universals  regarding syllable structure 
and sonority. Experiments with the second model 
indicate that it can distinguish sonorous conso- 
nants (such as n, m, l, r) from non-sonorous con- 
sonants in Spanish. An advantage of the linguis- 
tically structured model is that its predictions do 
not require an additional mapping step from unin- 
terpreted hidden states to linguistic categories,  as 
they do with the HMM.
  Our model and experiments can be viewed as 
complementary to the work of Knight et al., while 
also extending it to hundreds of languages. We 
use the simple HMM with EM as our baseline. In 
lieu of a linguistically designed model structure, 
we choose an empirical approach, allowing poste- 
rior inference over hundreds of known languages 
to guide the model’s decisions for the unknown 
script and language.
  In this sense, our model bears some similarity 
to the decipherment model of Snyder et al. (2010), 
which used knowledge  of a related language (He-


cipher the ancient language of Ugaritic. While the 
aim of the present work is more modest (discover- 
ing very basic phonetic properties of letters) it is 
also more widely applicable,  as we don’t required 
detailed analysis of a known related language.
  Other  recent work  has employed a   simi- 
lar  perspective  for  tying  learning across lan- 
guages. Naseem et al. (2009) use a non-parametric 
Bayesian model over parallel text to jointly learn 
part-of-speech  taggers across 8 languages, while 
Cohen and Smith (2009) develop  a shared logis- 
tic normal prior to couple multilingual learning 
even  in the absence  of parallel text.   In simi- 
lar veins, Berg-Kirkpatrick and Klein (2010) de- 
velop hierarchically tied grammar priors over lan- 
guages  within  the same family,  and Bouchard- 
Côté et al. (2013) develop a probabilistic  model of 
sound change using data from 637 Austronesian 
languages.
  In our own previous work, we have developed 
the idea that supervised knowledge of some num- 
ber of languages can help guide the unsupervised 
induction of linguistic structure,  even in the ab- 
sence of parallel text (Kim et al., 2011; Kim and
Snyder, 2012)1. In the latter work we also tack-
led the problem of unsupervised phonemic predic- 
tion for unknown languages by using textual reg- 
ularities of known languages. However, we as- 
sumed that the target language was written in a 
known (Latin) alphabet, greatly reducing the dif- 
ficulty of the prediction task. In our present case, 
we assume no knowledge of any relationship be- 
tween the writing system of the target language 
and known languages, other than that they are all 
alphabetic in nature.
  Finally, we note some similarities of our model 
to some ideas  proposed in other contexts.  We 
make the assumption that each observation  type 
(letter) occurs with only one hidden state (con- 
sonant or vowel).  Similar constraints have been 
developed for part-of-speech tagging (Lee et al.,
2010; Christodoulopoulos et al., 2011), and the 
power of type-based sampling has been demon- 
strated, even in the absence of explicit model con- 
straints (Liang et al., 2010).

3   Model

  Our generative  Bayesian  model over the ob- 
served vocabularies  of hundreds of languages is

1 We note that similar ideas were simultaneously proposed





For example, the cluster Poisson parameter over 
vowel observation types might be λ  = 9 (indi- 
cating 9 vowel letters on average for the cluster), 
while the parameter over consonant observation 
types might be λ  = 20 (indicating 20 consonant 
letters on average).  These priors will  be distinct
  

In contrast, the transition Dirichlet parameters 
may be asymmetric, and thus very specific and 
informative. For example, one cluster may have 
the property that CCC consonant clusters are ex- 
ceedingly rare across all its languages. This prop- 
erty would be expressed by a  very small mean


for each language cluster and serve to characterize


′
CCC


≪ 1 but large precision α0. Later we shall


its general linguistic and typological properties.

  We  pause at this point to review the Dirich- 
let distribution in more detail. A k−dimensional

Dirichlet with parameters α1 ... αk  defines a distri- 
bution over the k − 1 simplex with the following
density:
f (θ1 ... θk |α1 ... αk ) ∝ n θαi −1
i
where αi  > 0, θi  > 0, and 'L-i θi  = 1. The Dirich-
let serves as the conjugate prior for the Multino- 
mial, meaning that the posterior θ1 ...θk |X1 ...Xn is
again distributed  as a Dirichlet (with updated pa- 
rameters). It is instructive to reparameterize the 
Dirichlet with k + 1 parameters:


see examples  of learned transition Dirichlet pa-
rameters.

3.3   Cluster Generation
The generation of the cluster parameters (Algo- 
rithm 1) defines the highest layer of priors for our 
model.  As Dirichlets lack a standard  conjugate 
prior, we simply use uniform priors over the in- 
terval [0, 500]. For the cluster Poisson parameters, 
we use conjugate Gamma distributions with vague 
priors.3

4   Inference

In  this section we detail the inference proce- 
dure we followed to make predictions under our


f (θ1 ... θk |α0, α′ ... α′ ) ∝ n θ


α0 αi −1


model.	We  run the procedure  over 
data from


1	k 	i
i



503 languages,  assuming  that all 
languages but


where α0   = 'L-i αi,  and α′



= αi/α0.   In this


one have observed character 
and tag sequences:


parameterization, we have E[θi]  = α′ .  In other 
words, the parameters α′ give the mean of the dis- 
tribution, and α0  gives the precision of the dis-
tribution.  For large α0   ≫ k, the distribution is
highly peaked around the mean (conversely, when
α0 ≪ k, the mean lies in a valley).
Thus, the Dirichlet parameters of a language
cluster characterize both the average HMMs of in- 
dividual languages within the cluster, as well as 
how much we expect  the HMMs  to vary from 
the mean.   In  the case of  emission distribu- 
tions, we assume symmetric Dirichlet priors — i.e. 
one-parameter Dirichlets with densities given by
f (θ1 ...θk |β) ∝ n θ(β−1). This assumption is nec-
essary,  as we have no way to identify characters
across  languages  in the decipherment  scenario, 
and even the number of consonants and vowels 
(and thus multinomial/Dirichlet dimensions) can 
vary across the languages of a cluster. Thus, the 
mean of these Dirichlets will always be a uniform 
emission distribution. The single Dirichlet emis- 
sion parameter per cluster will  specify whether


w1, w2, . . . , t1, t2, . . . Since each character type w
is assumed to have  a single tag category, this is 
equivalent to observing the character token se- 
quence along with a character-type-to-tag   map- 
ping tw . For the target language, we observe only 
character token sequence w1, w2, . . .
We  assume fixed and known parameter  val-
ues only at the cluster generation level.  Unob- 
served variables include (i) the cluster parameters 
α, β, λ, (ii) the cluster assignments z, (iii) the per- 
language HMM parameters θ, φ for all languages, 
and (iv)  for the target language,  the tag tokens 
t1, t2, . . . — or equivalently the character-type-to- 
tag mappings tw  — along with the observation 
type-counts Nt.

4.1   Monte Carlo Approximation

Our goal in inference is to predict the most likely 
tag tw,ℓ for each character type w in our target lan- 
guage ℓ according to the posterior:

f (tw,ℓ | w, t−ℓ)


this mean is on a peak (large β) or in a valley
(small β).  In other words, it will  control the ex-


= ˆ f (tℓ, z, α, β | w, t


) d Θ   (1)


pected sparsity of the resulting per-language emis- 
sion multinomials.



  3 (1,19) for consonants, (1,10) for vowels, (0.2, 15) for 
nasals, and (1,16) for non-nasal consonants.


where Θ = (t−w,ℓ, z, α, β), w are the observed 
character sequences for all languages, t−ℓ are the 
character-to-tag  mappings for the observed  lan- 
guages, z are the language-to-cluster assignments, 
and α and β are all the cluster-level transition and 
emission Dirichlet parameters.


re-write Equation 4 as a product  of three terms:

f (Nℓ|Nk−ℓ)                                      (5) 
f (t1, t2, . . . |αk )                                 (6) 
f (w1, w2, . . . |Nℓ, t1, t2, . . . , βk )        (7)


Sampling values (tℓ, z, α, β)N


from the inte-



The first term is the posterior 
predictive distribu-


grand in Equation 1 allows us to perform the stan-
dard Monte Carlo approximation:

f (tw,ℓ = t | w, t−ℓ)
N


tion for the Poisson-Gamma compound distribu- 
tion and is easy to derive. The second term is the 
tag transition predictive distribution given Dirich- 
let hyperparameters, yielding a familiar Polya urn 
scheme form. Removing terms that don’t depend


≈ N −1 ) I (tw,ℓ = t in sample n)   (2)
n=1


on the tag assignment t



ℓ,w


gives us:

[n′ (t,t′ )]


To maximize the Monte Carlo posterior, we sim-


n	 αk,t,t′  + n(t, t′) 


ply take the most commonly sampled tag value
for character type w in language ℓ.	Note that 
we leave out the language-level HMM  parame-


n  'L-
t



t′ αk,t,t′  + n(t)


 [n′ (t)]


ters (θ, φ) as well as the cluster-level Poisson pa- 
rameters λ from Equation 1 (and thus our sample 
space),  as we can analytically integrate them out 
in our sampling equations.

4.2   Gibbs Sampling
To sample values (tℓ, z, α, β) from their poste- 
rior (the integrand of Equation 1), we use Gibbs 
sampling, a Monte Carlo technique that constructs 
a Markov chain over a high-dimensional  sample 
space by iteratively sampling each variable condi- 
tioned on the currently drawn sample values for 
the others, starting from a random  initialization. 
The Markov chain converges to an equilibrium 
distribution which is in fact the desired joint den- 
sity (Geman and Geman, 1984). We now sketch 
the sampling equations for each of our sampled 
variables.

Sampling tw,ℓ
To sample the tag assignment to character w in 
language ℓ, we need to compute:
f (tw,ℓ | w, t−w,ℓ, t−ℓ, z, α, β) 	(3)
∝ f (wℓ, tℓ, Nℓ | αk , βk , Nk−ℓ) 	(4)

where Nℓ are the types-per-tag counts implied by 
the mapping tℓ, k is the current cluster assignment 
for the target language (zℓ  = k), αk  and βk are the 
cluster parameters, and Nk−ℓ are the types-per-tag 
counts for all languages currently assigned to the 
cluster, other than language ℓ.
  Applying the chain rule along with our model’s 
conditional independence structure, we can further


where n(t) and n(t, t′) are, respectively,  unigram
and bigram tag counts excluding those containing 
character w.  Conversely, n′(t) and n′(t, t′) are, 
respectively, unigram and bigram tag counts only 
including those containing character w.  The no- 
tation a[n]  denotes the ascending factorial: a(a +
1) · · · (a + n −1). Finally, we tackle the third term,
Equation 7, corresponding to the predictive dis-
tribution of emission observations given Dirichlet 
hyperparameters. Again, removing constant terms 
gives us:
[n(w)]
k,t
n	[n(t′ )]
t′ 	ℓ,t′    k,t′

where n(w) is the unigram count of character w, 
and n(t′) is the unigram count of tag t, over all 
characters tokens (including w).

Sampling αk,t,t′
To sample the Dirichlet hyperparameter for cluster
k and transition t → t′, we need to compute:

f (αk,t,t′ |t, z)
∝ f (t, z|αz,t,t′ )
= f (tk |αz,t,t′ )

where tk are the tag sequences for all languages 
currently assigned to cluster k. This term is a pre- 
dictive distribution of the multinomial-Dirichlet 
compound when the observations are grouped into 
multiple multinomials all  with  the same prior. 
Rather than inefficiently computing a product of 
Polya urn schemes (with many repeated ascending


factorials with the same base), we group common 
terms together and calculate:
nj=1(αk,t,t′  + k)n(j,k,t,t )
n	'L-


The three terms correspond to (1) a standard pre- 
dictive distributions for the Poisson-gamma com- 
pound and (2) the standard predictive  distribu- 
tions for the transition and emission multinomial-


j=1(


t′′  αk,t,t′′  + 
k)n(j,k,t)


Dirichlet 
compounds.


where n(j, k, t) and n(j, k, t, t′) are the numbers 
of languages currently  assigned to cluster k which 
have more than j occurrences of unigram (t) and 
bigram (t, t′), respectively.
  This gives us an efficient way to compute un- 
normalized posterior densities for α. However, we 
need to sample from these distributions, not just 
compute them.  To do so, we turn to slice sam- 
pling (Neal, 2003), a simple yet effective auxiliary 
variable scheme for sampling values from unnor- 
malized but otherwise computable densities.
  The key  idea is to supplement the variable 
x, distributed according to unnormalized density 
p˜(x), with a second variable u with joint density



5   Experiments

To test our model, we apply it to a corpus of 503 
languages for two decipherment tasks. In both 
cases, we will  assume no knowledge of our tar- 
get language or its writing system, other than that 
it is alphabetic in nature. At the same time, we 
will assume basic phonetic knowledge  of the writ- 
ing systems of the other 502 languages.  For our 
first task, we will  predict whether each character 
type is a consonant  or a vowel. In the second task, 
we further subdivide the consonants into two ma- 
jor categories: the nasal consonants, and the non- 
nasal consonants. Nasal consonants are known to


defined as p(x, u)  ∝  I(u <


p˜(x)).   It is easy


be perceptually  very salient 
and are unique in be-


to see that p˜(x)  ∝ ´ p(x, u)du.  We then itera-
tively sample u|x and x|u, both of which are dis-
tributed uniformly across appropriately  bounded
intervals. Our implementation follows the pseudo- 
code given in Mackay (2003).

Sampling βk,t
To sample the Dirichlet hyperparameter for cluster
k and tag t we need to compute:
f (βk,t|t, w, z, N)
∝ f (w|t, z, βk,t, N)
∝ f (wk |tk , βk,t, Nk )
where, as  before, tk  are the tag sequences  for 
languages assigned to cluster k, Nk are the tag 
observation type-counts for  languages  assigned 
to the cluster, and likewise  wk   are the char- 
acter sequences  of all languages in the cluster. 
Again,  we have  the predictive  distribution of 
the multinomial-Dirichlet compound with multi- 
ple grouped observations. We can apply the same 
trick as above to group terms in the ascending fac- 
torials for efficient computation. As before, we 
use slice sampling for obtaining samples.

Sampling zℓ
Finally, we consider sampling the cluster assign- 
ment zℓ for each language ℓ. We calculate:
f (zℓ = k|w, t, N, z−ℓ, α, β)
∝ f (wℓ, tℓ, Nℓ|αk , βk , Nk−ℓ)
= f (Nℓ|Nk−ℓ)f (tℓ|αk )f (wℓ|tℓ, Nℓ, βk )


ing high frequency consonants in all known lan- 
guages.

5.1	Data

Our data is drawn from online electronic transla- 
tions of the Bible (http://www.bible.is, 
http://www.crosswire.org/index. 
jsp,	and	http://www.biblegateway. 
com).   We  have identified translations covering
503  distinct  languages employing  alphabetic 
writing systems. Most of these languages (476) 
use variants of the Latin alphabet, a  few (26) 
use Cyrillic,  and one uses  the Greek alphabet. 
As Table 1 indicates, the languages cover a very 
diverse set of families and geographic regions, 
with  Niger-Congo languages  being the largest
represented family.4   Of these languages,  30 are
either language isolates, or sole members of their 
language family in our data set.
  For our experiments, we extracted unique word 
types occurring at least 5 times from the down- 
loaded Bible texts. We manually identified vowel, 
nasal, and non-nasal character types. Since the let- 
ter “y”  can frequently represent both a consonant 
and vowel, we exclude it from our evaluation. On 
average, the resulting vocabularies contain 2,388 
unique words, with 19 consonant characters, two
2 nasal characters, and 9 vowels. We include the 
data as part of the paper.

  4 In fact, the Niger-Congo grouping is often considered 
the largest language family in the world in terms of distinct 
member languages.




M
od
el
Co
ns 
vs 
Vo
w
el
C 
vs 
V 
vs 
N
A
l
l
E
M
S
Y
M
M
 
M
E
R
G
E 
C
L
U
S
T
9
3
.
3
7
9
5
.
9
9
9
7
.
1
4
9
8
.
8
5
7
4
.
5
9
8
0
.
7
2
8
6
.
1
3
8
9
.
3
7
I
s
o
l
a
t
e
s
E
M
S
Y
M
M
 
M
E
R
G
E 
C
L
U
S
T
9
4
.
5
0
9
6
.
1
8
9
7
.
6
6
9
8
.
5
5
7
4
.
5
3
7
8
.
1
3
8
6
.
4
7
8
9
.
0
7
No
n-
Lat
in
E
M
S
Y
M
M
 
M
E
R
G
E 
C
L
U
S
T
9
2
.
9
3
9
5
.
9
0
9
6
.
0
6
9
7
.
0
3
7
8
.
2
6
7
9
.
0
4
8
3
.
7
8
8
5
.
7
9

Table 2:   Average accuracy for EM baseline and 
model variants across 503 languages. First panel: 
results on all languages. Second panel: results for
30 isolate and singleton languages. Third panel: 
results for 27 non-Latin alphabet languages (Cyril- 
lic and Greek). Standard Deviations across lan- 
guages are about 2%.

















Table 1: Language families in our data set. The
Other category includes 9 language isolates and
21 language family singletons.


5.2   Baselines and Model Variants

As our baseline, we consider the trigram HMM 
model of Knight et al. (2006), trained with EM. In 
all experiments, we run 10 random restarts of EM, 
and pick the prediction with highest likelihood. 
We map the induced tags to the gold-standard tag 
categories (1-1 mapping) in the way that maxi- 
mizes accuracy.
  We then consider three variants of our model. 
The simplest version, SYMM,  disregards  all in- 
formation from  other languages, using simple 
symmetric hyperparameters on the transition and 
emission Dirichlet priors (all hyperparameters set 
to 1). This allows us to assess the performance of


our Gibbs sampling inference method for the type- 
based HMM, even in the absence of multilingual 
priors.
  We  next consider a  variant of  our  model, 
MERGE, that assumes that all languages reside in 
a single cluster. This allows knowledge from the 
other languages to affect our tag posteriors in a 
generic, language-neutral way.
  Finally,  we consider the full  version of  our 
model, CLUST, with 20 language clusters. By al- 
lowing for the division of languages into smaller 
groupings, we hope to learn more specific param- 
eters tailored for typologically coherent clusters of 
languages.

6   Results

The results of our experiments are shown in Ta- 
ble 2. In all cases, we report token-level accuracy 
(i.e.  frequent characters count more than infre- 
quent characters), and results are macro-averaged 
over the 503 languages. Variance across languages 
is quite low:  the standard deviations are about 2 
percentage points.
  For the consonant vs.  vowel prediction task, 
all tested models perform well. Our baseline, the 
EM-based HMM, achieves 93.4% accuracy. Sim- 
ply using our Gibbs sampler with symmetric priors 
boosts the performance up to 96%. Performance





 

Figure 4: Inferred Dirichlet transition hyperparameters for bigram CLUST on three-way classification 
task with four latent clusters. Row gives starting state, column gives target state. Size of red blobs are 
proportional to magnitude of corresponding hyperparameters.




La
ng
ua
ge 
Fa
mi
ly
P
ort
io
n
#l
an
gs
E
nt.

In
do
-
E
ur
op
ea
n
0
.
3
8
0
.
2
4
0
.
2
1
2
6
4
1
3
8
2.
26
3.
19
3.
77
Q
ue
ch
ua
n
M
ay
an
O
t
o
-
M
a
n
g
u
e
a
n
 
M
a
i
p
u
r
e
a
n
 
T
u
c
a
n
o
a
n
Ut
o-
Az
te
ca
n
Alt
ai
c
0
.
8
9
0
.
6
4
0
.
5
5
0
.
2
5
0
.
2
0
.
4
0
.
4
4
1
8
3
3
3
1
8
4
5
2
5
2
7
0.
61
1.
70
1.
99
2.
75
3.
98
2.
85
2.
76




Ni
ge
r-
C
on
go
1
0
.
7
8
0
.
7
4
0
.
6
8
0
.
6
7
0
.
5
0
.
2
4
2
2
3
2
7
2
2
3
3
1
8
2
5
0.
00
1.
26
1.
05
1.
22
1.
62
2.
21
3.
27

A
us
tro
ne
si
an
0
.
9
1
0
.
7
1
0
.
2
4
2
2
2
1
1
7
0.
53
1.
51
3.
06

Table 3:  Plurality language  families across  20 
clusters.  The columns indicate portion of lan- 
guages  in the plurality  family,  number of lan- 
guages, and entropy over families.


with  a  bigram HMM  with  four language  clus- 
ters.  Examining just the first row, we see  that 
the languages are partially grouped by their pref- 
erence for  the initial  tag of  words.   All  clus- 
ters favor  languages which prefer initial  conso- 
nants, though this preference is most weakly ex- 
pressed in cluster 3.   In contrast, both clusters
2 and 4 have very dominant tendencies towards 
consonant-initial  languages, but differ in the rel- 
ative weight given to languages preferring either 
vowels or nasals initially.
  

Finally, we examine the relationship between 
the induced clusters and language families in Ta- 
ble 3, for the trigram consonant vs. vowel CLUST 
model with 20 clusters.  We  see that for about 
half the clusters, there is a majority language fam- 
ily,  most often Niger-Congo.  We also observe 
distinctive  clusters devoted to Austronesian and 
Quechuan languages. The largest two clusters are 
rather indistinct, without any single language fam- 
ily achieving more than 24% of the total.

8   Conclusion

In this paper, we presented a successful  solution 
to one aspect of the decipherment task: the predic- 
tion of consonants and vowels for an unknown lan- 
guage and alphabet. Adopting a classical Bayesian 
perspective,  we develop a  model that performs 
posterior inference over hundreds of languages, 
leveraging knowledge of known languages to un- 
cover general linguistic patterns of typologically 
coherent language clusters. Using this model, we 
automatically  distinguish between consonant and 
vowel characters with nearly 99% accuracy across
503 languages. We further experimented  on a 
three-way classification task involving nasal char- 
acters, achieving nearly 90% accuracy.
  Future work will  take us in several new direc- 
tions: first, we would like to move beyond the as- 
sumption of an alphabetic writing system so that 
we can apply our method to undeciphered syllabic 
scripts such as Linear A. We would also like to 
extend our methods to achieve finer-grained reso- 
lution of phonetic properties beyond nasals, con- 
sonants, and vowels.

Acknowledgments
The authors thank the reviewers and acknowledge support by 
the NSF (grant IIS-1116676) and a research gift from Google. 
Any opinions, findings, or conclusions  are those of the au- 
thors, and do not necessarily reflect the views of the NSF.


References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic 
grammar induction.  In Proceedings of the ACL, pages
1288–1297. Association for Computational Linguistics.

Alexandre Bouchard-Côté, David Hall, Thomas L Griffiths, 
and Dan Klein.   2013.   Automated  reconstruction of 
ancient  languages using probabilistic models of sound 
change. Proceedings of the National Academy of Sci- 
ences, 110(11):4224–4229.

Christos Christodoulopoulos,  Sharon Goldwater,  and Mark 
Steedman.   2011. A Bayesian mixture model for part- 
of-speech induction using multiple features.  In Proceed- 
ings of EMNLP,  pages 638–647. Association for Compu- 
tational Linguistics.

Shay B Cohen and Noah A Smith.  2009. Shared logistic  nor- 
mal distributions for soft parameter tying in unsupervised 
grammar induction. In Proceedings of NAACL,  pages 74–
82. Association for Computational Linguistics.

Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011. Un- 
supervised structure prediction with non-parallel multilin- 
gual guidance.  In Proceedings of EMNLP,  pages 50–61. 
Association for Computational Linguistics.

Stuart Geman and Donald Geman.  1984. Stochastic relax- 
ation, Gibbs distributions, and the Bayesian restoration of 
images. Pattern Analysis and Machine Intelligence, IEEE 
Transactions on, (6):721–741.

Young-Bum Kim and Benjamin Snyder.  2012.  Univer- 
sal grapheme-to-phoneme prediction over latin alphabets. 
In Proceedings of EMNLP,  pages 332–343, Jeju Island, 
South Korea, July. Association for Computational Lin- 
guistics.

Young-Bum Kim,  João V  Graça, and Benjamin Snyder.
2011. Universal morphological analysis using structured 
nearest neighbor prediction. In Proceedings of EMNLP, 
pages 322–332. Association  for Computational Linguis- 
tics.

Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada.
2006. Unsupervised analysis for decipherment problems. 
In Proceedings of COLING/ACL, pages 499–506. Associ- 
ation for Computational Linguistics.

Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010.
Simple type-level unsupervised POS tagging. In Proceed- 
ings of EMNLP,  pages 853–861. Association for Compu- 
tational Linguistics.

Percy Liang, Michael I Jordan, and Dan Klein. 2010. Type- 
based MCMC. In Proceedings  of NAACL,  pages 573–581. 
Association for Computational Linguistics.

David JC MacKay. 2003. Information Theory, Inference and
Learning Algorithms. Cambridge University Press.

Tahira Naseem,  Benjamin Snyder, Jacob Eisenstein,  and 
Regina Barzilay. 2009. Multilingual part-of-speech tag- 
ging: Two unsupervised approaches. Journal of Artificial 
Intelligence  Research, 36(1):341–385.

Radford M Neal. 2003. Slice sampling. Annals of statistics,
31:705–741.

Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010.
A statistical model for lost language decipherment. In 
Proceedings of the ACL, pages 1048–1057. Association 
for Computational Linguistics.



