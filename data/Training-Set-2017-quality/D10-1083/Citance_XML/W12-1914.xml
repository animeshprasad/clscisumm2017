<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We propose an unsupervised approach to POS tagging where first we associate each word type with a probability distribution over word classes using Latent Dirichlet Allocation.</S>
		<S sid ="2" ssid = "2">Then we create a hierarchical clustering of the word types: we use an agglomerative clustering algorithm where the distance between clusters is defined as the JensenShannon divergence between the probability distributions over classes associated with each word-type.</S>
		<S sid ="3" ssid = "3">When assigning POS tags, we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag.</S>
		<S sid ="4" ssid = "4">This simple labeler outperforms a baseline based on Brown clusters on 9 out of 10 datasets.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Unsupervised induction of word categories has been approached from three broad perspectives.</S>
			<S sid ="6" ssid = "6">First, it is of interest to cognitive scientists who model syntactic category acquisition by children (Redington et al. 1998, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010), where the primary concern is matching human performance patterns and satisfying cog- nitively motivated constraints such as incremental learning.</S>
			<S sid ="7" ssid = "7">Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.</S>
			<S sid ="8" ssid = "8">(2010), Lamar et al.</S>
			<S sid ="9" ssid = "9">(2010), Christodoulopoulos et al.</S>
			<S sid ="10" ssid = "10">(2011)), and primarily motivated as useful for tagging under-resourced languages.</S>
			<S sid ="11" ssid = "11">Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, Ta¨ckstro¨ m et al. 2012).</S>
			<S sid ="12" ssid = "12">The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold part-of-speech tags.</S>
			<S sid ="13" ssid = "13">Some researchers have used both approaches to evaluation.</S>
			<S sid ="14" ssid = "14">This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations.</S>
			<S sid ="15" ssid = "15">For part-of-speech tagging what is needed is a mapping from word tokens to a small set of discrete, atomic labels.</S>
			<S sid ="16" ssid = "16">For feature learning, there are is no such limitation, and other types of representations have been used, such as low-dimensional continuous vectors learned by neural network language models as in Bengio et al.</S>
			<S sid ="17" ssid = "17">(2006), Mnih and Hinton (2009), or distributions over word classes learned using Latent Dirichlet Allocation as in Chrupala (2011).</S>
			<S sid ="18" ssid = "18">In this paper we propose a simple method of mapping distributions over word classes to a set of discrete labels by hierarchically clustering word class distributions using JensenShannon divergence as a distance metric.</S>
			<S sid ="19" ssid = "19">This allows us to effectively use the algorithm of Chrupala (2011) and similar ones in settings where using distributions directly is not possible or desirable.</S>
			<S sid ="20" ssid = "20">Equivalently, our approach can be seen as a generic method to convert a soft clustering to hard clustering while conserving much of the information encoded in the original soft cluster assignments.</S>
			<S sid ="21" ssid = "21">We evaluate this method on the unsupervised part-of-speech tagging task on ten datasets NAACLHLT Workshop on the Induction of Linguistic Structure, pages 100–104, Montre´al, Canada, June 38, 2012.</S>
			<S sid ="22" ssid = "22">Qc 2012 Association for Computational Linguistics in nine languages as part of the shared task at the NAACLHLT 2012 Workshop on Inducing Linguistic Structure.</S>
	</SECTION>
	<SECTION title="Architecture. " number = "2">
			<S sid ="23" ssid = "1">Our system consists of the following components (i) a soft word-class induction model (ii) a hierarchical clustering algorithm which builds a tree of word class distributions (iii) a labeler which for each word type finds the leaf in the tree with the most similar word-class distribution and outputs a prefix of the path leading to that leaf.</S>
			<S sid ="24" ssid = "2">2.1 Soft word-class model.</S>
			<S sid ="25" ssid = "3">We use the probabilistic soft word-class model proposed by Chrupala (2011), which is based on Latent Dirichlet Allocation (LDA).</S>
			<S sid ="26" ssid = "4">LDA was introduced by Blei et al.</S>
			<S sid ="27" ssid = "5">(2003) and applied to modeling the topic structure in document collections.</S>
			<S sid ="28" ssid = "6">It is a generative, probabilistic hierarchical Bayesian model which induces a set of latent variables, which correspond to the topics.</S>
			<S sid ="29" ssid = "7">The topics themselves are multinomial distributions over words.</S>
			<S sid ="30" ssid = "8">The generative structure of the LDA model is the following: φk ∼ Dirichlet(β), k ∈ [1, K ] on θd which we use to represent a word type d as a distribution over word classes.</S>
			<S sid ="31" ssid = "9">Soft word classes are more expressive than hard categories.</S>
			<S sid ="32" ssid = "10">They make it easy and efficient to express shared ambiguities: Chrupala (2011) gives an example of words used as either first names or surnames, where this shared ambiguity is reflected in the similarity of their word class distributions.</S>
			<S sid ="33" ssid = "11">Another important property of soft word classes is that they make it easy to express graded similarity between words types.</S>
			<S sid ="34" ssid = "12">With hard classes, a pair of words either belong to the same class or to different classes, i.e. similarity is a binary indicator.</S>
			<S sid ="35" ssid = "13">With soft word classes, we can use standard measures of similarity between probability distributions to judge how similar words are to each other.</S>
			<S sid ="36" ssid = "14">We take advantage of this feature to build a hierarchical clustering of word types.</S>
			<S sid ="37" ssid = "15">2.2 Hierarchical clustering of word types.</S>
			<S sid ="38" ssid = "16">In some settings, e.g. in the unsupervised part-of- speech tagging scenario, words should be labeled with a small set of discrete labels.</S>
			<S sid ="39" ssid = "17">The question then arises how to map a probability distribution over word classes corresponding to each word type in the soft word class setting to a discrete label.</S>
			<S sid ="40" ssid = "18">The most obvious method would be to simply output the highest scoring word class, but this has the disadvantage θd ∼ Dirichlet(α), d ∈ [1, D] znd ∼ Categorical(θd), nd ∈ [1, Nd] wnd ∼ Categorical(φznd ), nd ∈ [1, Nd] (1) of discarding much of the information present in the soft labeling.</S>
			<S sid ="41" ssid = "19">What we do instead is to create a hierarchical clustering of word types using the JensenShannon Chrupala (2011) interprets the LDA model in terms of word classes as follows: K is the number of classes, D is the number of unique word types, Nd is the number of context features (such as right or left neighbor) associated with word type d, znd is the class of word type d in the nth context, and (JS) divergence between the word-class distributions as a distance function.</S>
			<S sid ="42" ssid = "20">JS divergence is an information-theoretic measure of dissimilarity between two probability distributions (Lin 1991).</S>
			<S sid ="43" ssid = "21">It is defined as follows: wnd is the nd context feature of word type d. Hyperparameters α and β control the sparseness of the 1 JS (P, Q) = 2 (DKL (P, M ) + DKL (Q, M )) (2) vectors θd and φk .Inference in LDA in general can be performed us ing either variational EM or Gibbs sampling.</S>
			<S sid ="44" ssid = "22">Here where M is the mean distribution P +Q and DKL is the KullbackLeibler (KL) divergence: P (i) we use a collapsed Gibbs sampler to estimate two sets of parameters: the θd parameters correspond DKL(P, Q) = ) P (i) log2 i Q(i) (3) to word class probability distributions given a word type while the φk correspond to feature distributions given a word class.</S>
			<S sid ="45" ssid = "23">In the current paper we focus Unlike KL divergence, JS divergence is symmetric and is defined for any pair of discrete probability distributions over the same domain.</S>
			<S sid ="46" ssid = "24">We use a simple agglomerative clustering algorithm to build a tree hierarchy over the word class distributions corresponding to word types (see Algorithm 1).</S>
			<S sid ="47" ssid = "25">We start with a set of leaf nodes, one for each of D word types, containing the unnormalized word-class probabilities for the corresponding word type: i.e. the co-occurrence counts of word-type and word-class, n(z, d), output by the Gibbs sampler.</S>
			<S sid ="48" ssid = "26">We then merge that pair of nodes (P, Q) whose JS divergence is the smallest, remove these two nodes from the set, and add the new merged node with two branches.</S>
			<S sid ="49" ssid = "27">We proceed in this fashion until we obtain a single root node.</S>
			<S sid ="50" ssid = "28">When merging two nodes we sum their co- occurrence count tables: thus the nodes always contain unnormalized probabilities which are normalized only when computing JS scores.</S>
			<S sid ="51" ssid = "29">those these can that could Fraser it goin(g) Paul going Mommy ’ll Daddy Algorithm 1 Bottom-up clustering of word types S = {n(·, d) | d ∈ [1, D]} while |S| &gt; 1 do (P, Q) = argmin(P,Q)∈S×S JS (P, Q) S ← S \ {P, Q} ∪ {merge(P, Q)} The algorithm is simple but not very efficient: if implemented carefully it can be at best quadratic in the number of word types.</S>
			<S sid ="52" ssid = "30">However, in practice it is unnecessary to run it on more than a few hundred word types which can be done very quickly.</S>
			<S sid ="53" ssid = "31">In the experiments reported on below we build the tree based only on the 1000 most frequent words.</S>
			<S sid ="54" ssid = "32">Figure 1 shows two small fragments of a hierarchy built from 200 most frequent words of the English CHILDES dataset using 10 LDA word classes.</S>
			<S sid ="55" ssid = "33">2.3 Tree paths as labels.</S>
			<S sid ="56" ssid = "34">Once the tree is built, it can be used to assign a label to any word which has an associated word class distribution.</S>
			<S sid ="57" ssid = "35">In principle, it could be used to perform either type-level or token-level tagging: token-level distributions could be composed from the distributions associated with current word type (θ) and the distributions associated with the current context features (φ).</S>
			<S sid ="58" ssid = "36">Since preliminary experiments with token- level tagging were not successful, here we focus exclusively on type-level tagging.</S>
			<S sid ="59" ssid = "37">Given the tree and a word-type paired with a class distribution, we generate a path to a leaf in the tree Figure 1: Two fragments of a hierarchy over word class distrib utions as follows.</S>
			<S sid ="60" ssid = "38">If the word is one of the ones used to construct the tree, we simply record the path from the root to the leaf containing this word.</S>
			<S sid ="61" ssid = "39">If the word is not at any of the leaves (i.e. it is not one of the 1000 most frequent words), we traverse the tree, at each node comparing the JS divergence between the word and the left and right branches, and then descend along the branch for which JS is smaller.</S>
			<S sid ="62" ssid = "40">We record the path until we reach a leaf node.</S>
			<S sid ="63" ssid = "41">We can control the granularity of the labeling by varying the length of the prefix of the path from the root to the leaf.</S>
			<S sid ="64" ssid = "42">3 Experiment s We evaluate our method on the unsupervised part- of-speech tagging task on ten dataset in nine languages as part of the shared task.</S>
			<S sid ="65" ssid = "43">For each dataset we run LDA word class induction1 on the union of the unlabeled sentences in the train, development and test sets, setting the num ber of classes K ∈ {10, 20, 40, 80}, and build a hierarchy on top of the learned word-class proba bility distributions as explained above.</S>
			<S sid ="66" ssid = "44">We then label the development set using path prefixes of length L ∈ {8, 9, . . .</S>
			<S sid ="67" ssid = "45">, 20} for each of the trees, and record 1 We ran 200 Gibbs sampling passes, and set the LDA hyper- parameters to α = 10 and β = 0.1.</S>
			<S sid ="68" ssid = "46">● ar● nl ● eu English PTB 40 8 61.6 60.2 Portuguese 80 10 51.7 52.4 ● da ● en−ch ● sv ● sl ● pt ● cz Swedish 20 17 51.8 56.1 Table 1: Evaluation of coarse-grained POS tagging on test data ● en−ptb 0 20 40 60 80 100 120 Vocabulary size in thousands Table 2: Evaluation of coarse-grained POS tagging on test data the V-measure (Rosenberg and Hirschberg 2007) against gold part-of-speech tags.</S>
			<S sid ="69" ssid = "47">We choose the best-performing pair of K and L and use this setting to label the test set.</S>
			<S sid ="70" ssid = "48">We tune separately for coarse- grained and fine-grained POS tags.</S>
			<S sid ="71" ssid = "49">Other than using the development set labels to tune these two parameters our system is unsupervised and uses no data other than the sentences in the provided data files.</S>
			<S sid ="72" ssid = "50">Table 1 and Table 2 show the best settings for the coarse- and fine-grained POS tagging for all the datasets, and the V-measure scores on the test set achieved by our labeler (HCD for Hierarchy over Class Distributions).</S>
			<S sid ="73" ssid = "51">Also included are the scores of the official baseline, i.e. labeling with Brown clusters (Brown et al. 1992), with the number of clusters set to match the number of POS tags in each dataset.</S>
			<S sid ="74" ssid = "52">The best K stays the same when increasing the granularity in the majority of cases (7 out of 10).</S>
			<S sid ="75" ssid = "53">On the CHILDES dataset of child-directed speech, Figure 2: Error reduction as a function of vocabulary size which has the smallest vocabulary of all, the optimal number of LDA classes is also the smallest (10).</S>
			<S sid ="76" ssid = "54">As expected, the best path prefix length L is typically larger for the fine-grained labeling.</S>
			<S sid ="77" ssid = "55">Our labels outperform the baseline on 9 out of 10 datasets, for both levels of granularity.</S>
			<S sid ="78" ssid = "56">The only exception is the English Penn Treebank dataset, where the HCD V-measure scores are slightly lower than Brown cluster scores.</S>
			<S sid ="79" ssid = "57">This may be taken as an illustration of the danger arising if NLP systems are exclusively evaluated on a single dataset: such a dataset may well prove to not be very representative.</S>
			<S sid ="80" ssid = "58">Part of the story seems to be that our method tends to outperform the baseline by larger margins on datasets with smaller vocabularies2.</S>
			<S sid ="81" ssid = "59">The scatter- plot in Figure 2 illustrates this tendency for coarse grained POS tagging: Pearson’s correlation is −0.6.</S>
			<S sid ="82" ssid = "60">4 Conclusion We have proposed a simple method of converting a set of soft class assignments to a set of discrete labels by building a hierarchical clustering over word-class distributions associated with word types.</S>
			<S sid ="83" ssid = "61">This allows to use the efficient and effective LDA- based word-class induction method in cases where a hard clustering is required.</S>
			<S sid ="84" ssid = "62">We have evaluated this 2 We suspect performance on datasets with large vocabularies could be improved by increasing the number of frequent words used to build the word-type hierarchy; due to time constraints we had to postpone verifying it.</S>
			<S sid ="85" ssid = "63">method on the POS tagging task on which our approach outperforms a baseline based on Brown clusters in 9 out of 10 cases, often by a substantial margin.</S>
			<S sid ="86" ssid = "64">In future it would be interesting to investigate whether the hierarchy over word-class distributions would also be useful as a source of features in a semi-supervised learning scenario, instead, or in addition to using word-class probabilities as features directly.</S>
			<S sid ="87" ssid = "65">We would also like to revisit and further investigate the challenging problem of token-level labeling.</S>
	</SECTION>
</PAPER>
