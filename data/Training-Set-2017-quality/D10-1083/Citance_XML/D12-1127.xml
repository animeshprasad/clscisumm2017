<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Despite significant recent work, purely unsupervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks.</S>
		<S sid ="2" ssid = "2">Use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that significantly improves accuracy.</S>
		<S sid ="3" ssid = "3">However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps.</S>
		<S sid ="4" ssid = "4">In this paper we show that we can build POStaggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource, the Wiktionary.</S>
		<S sid ="5" ssid = "5">Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods.</S>
		<S sid ="6" ssid = "6">We achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised Penn Treebank.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Part-of-speech categories are elementary building blocks that play an important role in many natural language processing tasks, from machine translation to information extraction.</S>
			<S sid ="8" ssid = "8">Supervised learning of taggers from POS-annotated training text is a well-studied task, with several methods achieving near-human tagging accuracy (Ratnaparkhi, 1996; Toutanova et al., 2003; Shen et al., 2007).</S>
			<S sid ="9" ssid = "9">However, while English and a handful of other languages are fortunate enough to have comprehensive POS- annotated corpora such as the Penn Treebank (Marcus et al., 1993), most of the world’s languages have no labeled corpora.</S>
			<S sid ="10" ssid = "10">The annotated corpora that do exist were costly to build (Abeille´, 2003), and are often not freely available or restricted to research- only use.</S>
			<S sid ="11" ssid = "11">Furthermore, much of the annotated text is of limited genre, normally focusing on newswire or literary text.</S>
			<S sid ="12" ssid = "12">Performance of treebank-trained systems degrades significantly when applied to new domains (Blitzer et al., 2006).</S>
			<S sid ="13" ssid = "13">Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac¸a et al., 2011; Lee et al., 2010).</S>
			<S sid ="14" ssid = "14">Using additional information, in the form of tag dictionaries or parallel text, seems unavoidable at present.</S>
			<S sid ="15" ssid = "15">Early work on using tag dictionaries used a labeled corpus to extract all allowed word-tag pairs (Merialdo, 1994), which is quite an unrealistic scenario.</S>
			<S sid ="16" ssid = "16">More recent work has used a subset of the observed word-tag pairs and focused on generalizing dictionary entries (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007).</S>
			<S sid ="17" ssid = "17">Using corpus- based dictionaries greatly biases the test results, and gives little information about the capacity to generalize to different domains.</S>
			<S sid ="18" ssid = "18">Recent work by Das and Petrov (2011) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text.</S>
			<S sid ="19" ssid = "19">1389 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1389–1398, Jeju Island, Korea, 12–14 July 2012.</S>
			<S sid ="20" ssid = "20">Qc 2012 Association for Computational Linguistics The main idea is to rely on existing dictionaries for some languages (e.g. English) and use parallel data to build a dictionary in the desired language and extend the dictionary coverage using label propagation.</S>
			<S sid ="21" ssid = "21">However, parallel text does not exist for many pairs of languages and the proposed bilingual projection algorithms are fairly complex.</S>
			<S sid ="22" ssid = "22">In this work we use the Wiktionary, a freely available, high coverage and constantly growing dictionary for a large number of languages.</S>
			<S sid ="23" ssid = "23">We experiment with a very simple second-order Hidden Markov Model with feature-based emissions (Berg- Kirkpatrick et al., 2010; Grac¸a et al., 2011).</S>
			<S sid ="24" ssid = "24">We outperform best current results using parallel text supervision across 8 different languages, even when the word type coverage is as low as 20%.</S>
			<S sid ="25" ssid = "25">Furthermore, using the Brown corpus as out-of-domain data we show that using the Wiktionary produces better taggers than using the Penn Treebank dictionary (88.5% vs 85.9%).</S>
			<S sid ="26" ssid = "26">Our empirical analysis and the natural growth rate of the Wiktionary suggest that free, high-quality and multi-domain POStaggers for a large number of languages can be obtained by standard and efficient models.</S>
			<S sid ="27" ssid = "27">The source code, the dictionary mappings and the trained models described in this work are available at http://code.google.com/p/ wikily-supervised-postagger/.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="28" ssid = "1">The scarcity of labeled corpora for resource poor languages and the challenges of domain adaptation have led to several efforts to build systems for unsupervised POStagging.</S>
			<S sid ="29" ssid = "2">Several lines of research have addressed the fully unsupervised POS-tagging task: mutual information clustering (Brown et al., 1992; Clark, 2003) has been used to group words according to their distributional context.</S>
			<S sid ="30" ssid = "3">Using dimensionality reduction on word contexts followed by clustering has led to accuracy gains (Schu¨ tze, 1995; Lamar et al., 2010).</S>
			<S sid ="31" ssid = "4">Sequence models, HMMs in particular, have been used to represent the probabilistic dependencies between consecutive tags.</S>
			<S sid ="32" ssid = "5">In these approaches, each observation corresponds to a particular word and each hidden state corresponds to a cluster.</S>
			<S sid ="33" ssid = "6">However, using maximum likelihood training for such models does not achieve good results (Clark, 2003): maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse word-tag distribution.</S>
			<S sid ="34" ssid = "7">Several approaches have been proposed to mitigate this problem, including Bayesian approaches using an improper Dirichlet prior to favor sparse model parameters (Johnson, 2007; Gao and Johnson, 2008; Goldwater and Griffiths, 2007), or using the Posterior Regularization to penalize ambiguous posteriors distributions of tags given tokens (Grac¸a et al., 2009).</S>
			<S sid ="35" ssid = "8">Berg-Kirkpatrick et al.</S>
			<S sid ="36" ssid = "9">(2010) and Grac¸a et al.</S>
			<S sid ="37" ssid = "10">(2011) proposed replacing the multinomial emission distributions of standard HMMs by maximum entropy (ME) feature-based distributions.</S>
			<S sid ="38" ssid = "11">This allows the use of features to capture morphological information, and achieves very promising results.</S>
			<S sid ="39" ssid = "12">Despite these improvements, fully unsupervised systems require an oracle to map clusters to true tags and the performance still fails to be of practical use.</S>
			<S sid ="40" ssid = "13">In this paper we follow a different line of work where we rely on a prior tag dictionary indicating for each word type what POS tags it can take on (Merialdo, 1994).</S>
			<S sid ="41" ssid = "14">The task is then, for each word token in the corpus, to disambiguate between the possible POS tags.</S>
			<S sid ="42" ssid = "15">Even when using a tag dictionary, disambiguating from all possible tags is still a hard problem and the accuracy of these methods is still fall far behind their supervised counterparts.</S>
			<S sid ="43" ssid = "16">The scarcity of large, manually-constructed tag dictionaries led to the development of methods that try to generalize from a small dictionary with only a handful of entries (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007), however most previous works build the dictionary from the labeled corpus they learn on, which does not represent a realistic dictionary.</S>
			<S sid ="44" ssid = "17">In this paper, we argue that the Wiktionary can serve as an effective and much less biased tag dictionary.</S>
			<S sid ="45" ssid = "18">We note that most of the previous dictionary based approaches can be applied using the Wiktionary and would likely lead to similar accuracy increases that we show in this paper.</S>
			<S sid ="46" ssid = "19">For example, the work if Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via a integer program, hence discarding unlikely transitions that would confuse the model.</S>
			<S sid ="47" ssid = "20">Models can also be trained jointly using parallel corpora in sev eral languages, exploiting the fact that different languages present different ambiguities (Snyder et al., 2008).</S>
			<S sid ="48" ssid = "21">The Wiktionary has been used extensively for other tasks such as domain specific information retrieval (Mu¨ ller and Gurevych, 2009), ontology matching (Krizhanovsky and Lin, 2009), synonymy detection (Navarro et al., 2009), sentiment classification (Chesley et al., 2006).</S>
			<S sid ="49" ssid = "22">Recently, Ding (2011) used the Wiktionary to initialize an HMM for Chinese POS tagging combined with label propagation.</S>
			<S sid ="50" ssid = "23">*&quot;&quot;&quot;&quot;&quot;&quot;% )*&quot;&quot;&quot;&quot;&quot;% )&quot;&quot;&quot;&quot;&quot;&quot;% (*&quot;&quot;&quot;&quot;&quot;% (&quot;&quot;&quot;&quot;&quot;&quot;% &apos;*&quot;&quot;&quot;&quot;&quot;% &apos;&quot;&quot;&quot;&quot;&quot;&quot;% &amp;*&quot;&quot;&quot;&quot;&quot;% &amp;&quot;&quot;&quot;&quot;&quot;&quot;% *&quot;&quot;&quot;&quot;&quot;% &quot;% 288% -E/79F% 2==60/=4% -&quot;#&quot;&quot;$% !-#&quot;&quot;$% !!#&quot;&quot;$% !,#&quot;&quot;$% !+#&quot;&quot;$% !*#&quot;&quot;$% !)#&quot;&quot;$% !(#&quot;&quot;$% !&apos;#&quot;&quot;$% !&amp;#&quot;&quot;$% !&quot;#&quot;&quot;$%</S>
	</SECTION>
	<SECTION title="The Wiktionary and tagged corpora. " number = "3">
			<S sid ="51" ssid = "1">The Wiktionary1 is a collaborative project that aims to produce a free, large-scale multilingual dictionary.</S>
			<S sid ="52" ssid = "2">Its goal is to describe all words from all languages (currently more than 400) using definitions and descriptions in English.</S>
			<S sid ="53" ssid = "3">The coverage of the Wiktionary varies greatly between languages: currently there are around 75 languages for which there exists more than 1000 word types, and 27 for which there exists more than 10,000 word types.</S>
			<S sid ="54" ssid = "4">Nevertheless, the Wiktionary has been growing at a considerable rate (see Figure 1), and the number of available words has almost doubled in the last three years.</S>
			<S sid ="55" ssid = "5">As more people use the Wiktionary, it is likely to grow.</S>
			<S sid ="56" ssid = "6">Unlike tagged corpora, the Wiktionary provides natural incentives for users to contribute missing entries and expand this communal resource akin to Wikipedia.</S>
			<S sid ="57" ssid = "7">As with Wikipedia, the questions of accuracy, bias, consistency across languages, and selective coverage are paramount.</S>
			<S sid ="58" ssid = "8">In this section, we explore these concerns by comparing Wiktionary to dictionaries derived from tagged corpora.</S>
			<S sid ="59" ssid = "9">3.1 Labeled corpora and Universal tags.</S>
			<S sid ="60" ssid = "10">We collected part-of-speech tagged corpora for 9 languages, from CoNLL-X and CoNLL2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007).</S>
			<S sid ="61" ssid = "11">In this work we use the Universal POS tag set (Petrov et al., 2011) that defines 12 universal categories with a relatively stable functional definition across languages.</S>
			<S sid ="62" ssid = "12">These categories include NOUN, VERB, ADJ = adjective, ADV = adverb, NUM = number, ADP = adposition, CONJ = conjunction, DET = determiner, PRON = 1 http://www.wiktionary.org/ Figure 1: Growth of the Wiktionary over the last three years, showing total number of entries for all languages and for the 9 languages we consider (left axis).</S>
			<S sid ="63" ssid = "13">We also show the corresponding increase in average accuracy (right axis) achieved by our model across the 9 languages (see details below).</S>
			<S sid ="64" ssid = "14">pronoun, PUNC = punctuation, PRT = particle, and X = residual (a category for language-specific categories which defy cross-linguistic classification).</S>
			<S sid ="65" ssid = "15">We found several small problems with the mapping2 which we corrected as follows.</S>
			<S sid ="66" ssid = "16">In Spanish, the fine- level tag for date (“w”) is mapped to universal tag NUM, while it should be mapped to NOUN.</S>
			<S sid ="67" ssid = "17">In Danish there were no PRT, NUM, PUNC, or DET tags in the mapping.</S>
			<S sid ="68" ssid = "18">After examining the corpus guidelines and the mapping more closely, we found that the tag AC (Cardinal numeral) and AO (Ordinal numeral) are mapped to ADJ.</S>
			<S sid ="69" ssid = "19">Although the corpus guidelines indicate the category SsCatgram ‘adjective’ that encompasses both ‘normal’ adjectives (AN) as well as cardinal numeral (AC) and ordinal numerals (AO), we decided to tag AC and AO as NUM, since this assignment better fits the existing mapping.</S>
			<S sid ="70" ssid = "20">We also reassigned all punctuation marks, which were erroneously mapped to X, to PUNC and the tag U which is used for words at, de and som, to PRT.</S>
			<S sid ="71" ssid = "21">3.2 Wiktionary to Universal tags.</S>
			<S sid ="72" ssid = "22">There are a total of 330 distinct POS-type tags in Wiktionary across all languages which we have mapped to the Universal tagset.</S>
			<S sid ="73" ssid = "23">Most of the mapping was straightforward since the tags used in the Wiktionary are in fact close to the Universal tag set.</S>
			<S sid ="74" ssid = "24">Some exceptions like “Initialism”, “Suffix” 2 http://code.google.com/p/ universal-pos-tags/ were discarded.</S>
			<S sid ="75" ssid = "25">We also mapped relatively rare tags such as “Interjection”, “Symbol” to the “X” tag.</S>
			<S sid ="76" ssid = "26">A example of POS tags for several words in the Wiktionary is shown in Table 1.</S>
			<S sid ="77" ssid = "27">All the mappings are available at http://code.google.com/ p/wikily-supervised-postagger/.</S>
			<S sid ="78" ssid = "28">3.3 Wiktionary coverage.</S>
			<S sid ="79" ssid = "29">There are two kinds of coverage of interest: type coverage and token coverage.</S>
			<S sid ="80" ssid = "30">We define type coverage as the proportion of word types in the corpus that simply appear in the Wiktionary (accuracy of the tag sets are considered in the next subsection).</S>
			<S sid ="81" ssid = "31">Token coverage is defined similarly as the portion of all word tokens in the corpus that appear in the Wiktionary.</S>
			<S sid ="82" ssid = "32">These statistics reflect two aspects of the usefulness of a dictionary that affect learning in different ways: token coverage increases the density of supervised signal while type coverage increases the diversity of word shape supervision.</S>
			<S sid ="83" ssid = "33">At one extreme, with 100% word and token coverage, we recover the POS tag disambiguation scenario and, on the other extreme of 0% coverage, we recover the unsupervised POS induction scenario.</S>
			<S sid ="84" ssid = "34">The type and token coverage of Wiktionary for each of the languages we are using for evaluation is shown in Figure 2.</S>
			<S sid ="85" ssid = "35">We plot the coverage bar for three different versions of Wiktionary (v20100326, v20110321, v20120320), arranged chronologically.</S>
			<S sid ="86" ssid = "36">We chose these three versions of the Wiktionary simply by date, not any other factors like coverage, quality or tagging accuracy.</S>
			<S sid ="87" ssid = "37">As expected, the newer versions of the Wiktionary generally have larger coverage both on type level and token level.</S>
			<S sid ="88" ssid = "38">Nevertheless, even for languages whose type coverage is relatively low, such as Greek (el), the token level coverage is still quite good (more than half of the tokens are covered).</S>
			<S sid ="89" ssid = "39">The reason for this is likely the bias of the contributors towards more frequent words.</S>
			<S sid ="90" ssid = "40">This trend is even more evident when we break up the coverage by frequency of the words.</S>
			<S sid ="91" ssid = "41">Since the number of words varies from corpus to corpus, we normalize the word counts by the count of the most frequent word(s) in its corpus and group the normalized frequency into three categories labeled as “low”, “medium” and “high” and for each category, we calculate the word type coverage, shown in Figure 3.</S>
			<S sid ="92" ssid = "42">Figure 2: Type-level (top) and token-level (bottom) coverage for the nine languages in three versions of the Wiktionary.</S>
			<S sid ="93" ssid = "43">We also compared the coverage provided by the Wiktionary versus the Penn Treebank (PTB) extracted dictionary on the Brown corpus.</S>
			<S sid ="94" ssid = "44">Figure 4 shows that the Wiktionary provides a greater coverage for all sections of the Brown corpus, hence being a better dictionary for tagging English text in general.</S>
			<S sid ="95" ssid = "45">This is also reflected in the gain in accuracy on Brown over the taggers learned from the PTB dictionary in our experiments.</S>
			<S sid ="96" ssid = "46">3.4 Wiktionary.</S>
			<S sid ="97" ssid = "47">accuracy A more refined notion of quality is the accuracy of the tag sets for covered words, as measured against dictionaries extracted from labeled tree bank corpora.</S>
			<S sid ="98" ssid = "48">We consider word types that are in both the Wiktionary (W) and the tree bank dictionaries (T).</S>
			<S sid ="99" ssid = "49">For each word type, we compare the two tag sets and distinguish five different possibilities: 1.</S>
			<S sid ="100" ssid = "50">Identical: W = T. 2.</S>
			<S sid ="101" ssid = "51">Superset: W ⊃ T. 3.</S>
			<S sid ="102" ssid = "52">Subset: W ⊂ T. W i k t i o n a r y E n t r i e s Un ive rs al P O S S et L a n g u a g e W o r d P O S D e f i n i t i o n E n g l i s h E n g l i s h E n g l i s h t o d a y t o d a y t o d a y A d v e r b A d v e r b N o u n # I n t h e c u r r e n t [ [ e r a ] ] ; n o w a d a y s . # O n th e cu rr e nt [[ d a y] ] or [[ d at e] ].</S>
			<S sid ="103" ssid = "53"># A c u r r e n t d a y o r d a t e . { A D V , N O U N } G e r m a n ac ht zi g N u m e r a l # [ [ e i g h t y ] ] { N U M } S w e d i s h S C B A c r o n y m # [ [ s t a t i s t i s k a ] ] . . .</S>
			<S sid ="104" ssid = "54">{ N O U N } Po rtu gu es e n e ss a C on tra cti on # { { c o n t r a c t i o n . . .</S>
			<S sid ="105" ssid = "55">d i s c a r d e n t r y Table 1: Examples of constructing Universal POS tag sets from the Wiktionary.</S>
			<S sid ="106" ssid = "56">)!!&quot;!!#$ (!&quot;!!#$ &apos;!&quot;!!#$ &amp;!&quot;!!#$ -45$ 6 ,*076$ 8 098$ ) ! !</S>
			<S sid ="107" ssid = "57"># $ &apos;!&quot;!!#$ &amp;!&quot;!!#$ %!&quot;!!#$ %!&quot;!!#$ !&quot;!!#$ *+$ *,$ ,-$ ,.$ ,/$ 01$ .-$ 21$ /3$ !&quot;!!#$ *$ +$ ,$ -$ .$ /$ 0$ 1$ 2$ 3$ 4$ 5$ 6$ 7$ 8$ 9 + $ : ; &lt; = &gt; ? @ A B $ Figure 3: Word type coverage by normalized frequency: words are grouped by word count / highest word count ratio: low [0, 0.01), medium [0.01, 0.1), high [0.1, 1].</S>
			<S sid ="108" ssid = "58">5.</S>
			<S sid ="109" ssid = "59">Disjoint: W ∩ T = ∅..</S>
			<S sid ="110" ssid = "60">In Figure 5, the word types are grouped into the categories described above.</S>
			<S sid ="111" ssid = "61">Most of the tag sets (around 90%) in the Wiktionary are identical to or supersets of the tree bank tag sets for our nine languages, which is surprisingly accurate.</S>
			<S sid ="112" ssid = "62">About 10% Figure 4: PTB vs. Wiktionary type coverage across sections of the Brown corpus.</S>
			<S sid ="113" ssid = "63">with two conventional start tags y0 = start, y−1 =start, allowing us to write as p(y1|y0, y−1) the ini tial state probability of the SHMM.</S>
			<S sid ="114" ssid = "64">The probability of a sentence x along with a particular hidden state sequence y in the SHMM is given by: length(x) of the Wiktionary tag sets are subsets of, partially overlapping with, or disjoint from the tree bank tag sets.</S>
			<S sid ="115" ssid = "65">Our learning methods, which assume the given p(x, y) = n i=1 pt(yi | yi−1, yi−2)po(xi | yi), (1) tag sets are correct, may be somewhat hurt by these word types, as we discuss in Section 5.6.</S>
	</SECTION>
	<SECTION title="Models. " number = "4">
			<S sid ="116" ssid = "1">Our basic models are first and second order Hidden Markov Models (HMM and SHMM).</S>
			<S sid ="117" ssid = "2">We also used feature-based maxent emission models with both (HMM-ME and SHMM-ME).</S>
			<S sid ="118" ssid = "3">Below, we denote the sequence of words in a sentence as boldface x andwhere po(xi | yi) is the probability of observ ing word xi in state yi (emission probability), and pt(yi | yi−1, yi−2) is the probability of being in state yi, given two previous states yi−1, yi−2 (transition probability).</S>
			<S sid ="119" ssid = "4">In this work, we compare multinomial and maximum entropy (log-linear) emission models.</S>
			<S sid ="120" ssid = "5">Specifically, the maxent emission model is: exp(θ · f (x, y)) the sequence of hidden states which correspond to part-of-speech tags as boldface y. To simplify nota po(x|y) = xl exp(θ · f (x , y)) (2) tion, we assume that every tag sequence is prefixed where f (x, y) is a feature function, x ranges over all 3-/178.0# 295/:2/4# 29;2/4# &lt;6/:0.5# -32=&lt;314# $!!&quot;# ,!&quot;# +!&quot;# *!&quot;# )!&quot;# (!&quot;# &apos;!&quot;# &amp;!&quot;# %!&quot;# $!&quot;# !&quot;# -.# -/# /0# /1# /2# 34# 10# 54# 26# Figure 5: The Wiktionary vs. tree bank tag sets.</S>
			<S sid ="121" ssid = "6">Around 90% of the Wiktionary tag sets are identical or subsume tree bank tag sets.</S>
			<S sid ="122" ssid = "7">See text for details.</S>
			<S sid ="123" ssid = "8">word types, and θ are the model parameters.</S>
			<S sid ="124" ssid = "9">We use the following feature templates: • Word identity - lowercased word form if the word appears more than 10 times in the corpus.</S>
			<S sid ="125" ssid = "10">• Hyphen - word contains a hyphen • Capital - word is uppercased • Suffix - last 2 and 3 letters of a word if they appear in more than 20 different word types.</S>
			<S sid ="126" ssid = "11">• Number - word contains a digit The idea of replacing the multinomial models of an HMM by maximum entropy models has been applied before in different domains (Chen, 2003), as well as in POS induction (Berg-Kirkpatrick et al., 2010; Grac¸a et al., 2011).</S>
			<S sid ="127" ssid = "12">We use the EM algorithm to learn the models, restricting the tags of each word to those specified by the dictionary.</S>
			<S sid ="128" ssid = "13">For each tag y, the observations probabilities po(x | y) were initialized randomly for every word type that allows tag y accord ing to the Wiktionary and zero otherwise.</S>
			<S sid ="129" ssid = "14">For the M-step in maxent models, there is no closed form solution so we need to solve an unconstrained optimization problem.</S>
			<S sid ="130" ssid = "15">We use L-BFGS with Wolfe’s rule line search (Nocedal and Wright, 1999).</S>
			<S sid ="131" ssid = "16">We found that EM achieved higher accuracy across languages compared to direct gradient approach (Berg- Kirkpatrick et al., 2010).</S>
	</SECTION>
	<SECTION title="Results. " number = "5">
			<S sid ="132" ssid = "1">We evaluate the accuracy of taggers trained using the Wiktionary using the 4 different models: A first order Hidden Markov Model (HMM), a second order Hidden Markov Model (SHMM), a first order Hidden Markov Model with Maximum Entropy emission models (HMM-ME) and a second order Hidden Markov Model with Maximum Entropy emission models (SHMM-ME).</S>
			<S sid ="133" ssid = "2">For each model we ran EM for 50 iterations, which was sufficient for convergence of the likelihood.</S>
			<S sid ="134" ssid = "3">Following previous work (Grac¸a et al., 2011), we used a Gaussian prior with variance of 10 for the maxent model parameters.</S>
			<S sid ="135" ssid = "4">We obtain hard assignments using posterior decoding, where for each position we pick the label with highest posterior probability: this produces small but consistent improvements over Viterbi decoding.</S>
			<S sid ="136" ssid = "5">5.1 Upper and lower bounds.</S>
			<S sid ="137" ssid = "6">We situate our results against several upper bounds that use more supervision.</S>
			<S sid ="138" ssid = "7">We trained the SHMM- ME model with a dictionary built from the training and test tree bank (ALL TBD) and also with tree bank dictionary intersected with the Wiktionary (Covered TBD).</S>
			<S sid ="139" ssid = "8">The Covered TBD dictionary is more supervised than the Wiktionary in the sense that some of the tag set mismatches of the Wiktionary are cleaned using the true corpus tags.</S>
			<S sid ="140" ssid = "9">We also report results from training the SHMM-ME in the standard supervised fashion, using 50 (50 Sent.), 100 (100 Sent.)</S>
			<S sid ="141" ssid = "10">and all sentences (All Sent.).</S>
			<S sid ="142" ssid = "11">As a lower bound we include the results for unsupervised systems: a regular HMM model trained with EM (Johnson, 2007) and an HMM model using a ME emission model trained using direct gradient (Berg-Kirkpatrick et al., 2010)3.</S>
			<S sid ="143" ssid = "12">5.2 Bilingual baselines.</S>
			<S sid ="144" ssid = "13">Finally, we also compare our system against a strong set of baselines that use bilingual data.</S>
			<S sid ="145" ssid = "14">These approaches build a dictionary by transferring labeled data from a resource rich language (English) to a resource poor language (Das and Petrov, 2011).</S>
			<S sid ="146" ssid = "15">We compare against two such methods.</S>
			<S sid ="147" ssid = "16">The first, projection, builds a dictionary by transferring the pos 3 Values for these systems where taken from the D&amp;P paper..</S>
			<S sid ="148" ssid = "17">tags from English to the new language using word alignments.</S>
			<S sid ="149" ssid = "18">The second method, D&amp;P, is the current state-of-the-art system, and runs label propagation on the dictionary resulting from the projected method.</S>
			<S sid ="150" ssid = "19">We note that both of these approaches are orthogonal to ours and could be used simultaneously with the Wiktionary.</S>
			<S sid ="151" ssid = "20">5.3 Analysis.</S>
			<S sid ="152" ssid = "21">Table 2 shows results for the different models across languages.</S>
			<S sid ="153" ssid = "22">We note that the results are not directly comparable since both the Unsupervised and the Bilingual results use a different setup, using the number of fine grained tags for each language as hidden states instead of 12 (as we do).</S>
			<S sid ="154" ssid = "23">This greatly increases the degrees of freedom of the model allowing it to capture more fine grained distinctions.</S>
			<S sid ="155" ssid = "24">The first two observations are that using the ME entropy emission model always improves over the standard multinomial model, and using a second order model always performs better.</S>
			<S sid ="156" ssid = "25">Comparing with the work of D&amp;P, we see that our model achieves better accuracy on average and on 5 out of 8 languages.</S>
			<S sid ="157" ssid = "26">The most common errors are due to tag set idiosyncrasies.</S>
			<S sid ="158" ssid = "27">For instance, for English the symbol % is tagged as NUM by our system while in the Penn treebank it is tagged as Noun.</S>
			<S sid ="159" ssid = "28">Other common mistakes for English include tagging to as an adposition (preposition) instead of particle and tagging which as a pronoun instead of determiner.</S>
			<S sid ="160" ssid = "29">In the next subsections we analyze the errors in more detail.</S>
			<S sid ="161" ssid = "30">Finally, for English we also trained the SHMM- ME model using the Celex2 dictionary available from LDC4.</S>
			<S sid ="162" ssid = "31">Celex2 coverage for the PTB corpus is much smaller than the coverage provided by the Wiktionary (43.8% type coverage versus 80.0%).</S>
			<S sid ="163" ssid = "32">Correspondingly, the accuracy of the model trained using Celex2 is 75.5% compared 87.1% when trained using the Wiktionary.</S>
			<S sid ="164" ssid = "33">5.4 Performance vs. Wiktionary ambiguity.</S>
			<S sid ="165" ssid = "34">While many words overwhelmingly appear with one tag in a given genre, in the Wiktionary a large proportion of words are annotated with several tags, even when those are extremely rare events.</S>
			<S sid ="166" ssid = "35">Around 4 http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC96L14 35% of word types in English have more than one tag according to the Wiktionary.</S>
			<S sid ="167" ssid = "36">This increases the difficulty of predicting the correct tag as compared to having a corpus-based dictionary, where words have a smaller level of ambiguity.</S>
			<S sid ="168" ssid = "37">For example, in English, for words with one tag, the accuracy is 95% (the reason it is not 100% is due to a discrepancy between the Wiktionary and the tree bank.)</S>
			<S sid ="169" ssid = "38">For words with two possible tags, accuracy is 81% and for three tags, it drops to 63%.</S>
			<S sid ="170" ssid = "39">5.5 Generalization to unknown words.</S>
			<S sid ="171" ssid = "40">Comparing the performance of the proposed model for words in the Wiktionary against words not in the Wiktionary, we see an average drop from 89% to 63% for out-of-vocabulary words across nine languages.</S>
			<S sid ="172" ssid = "41">Table 2 shows that the average loss of accuracy between All TBD and Covered TBD of 4.5% (which is due purely to decrease in coverage) is larger than the loss between Covered TBD and the best Wiktionary model, of 3.2% (which is due to tag set inconsistency).</S>
			<S sid ="173" ssid = "42">One advantage of the Wiktionary is that it is a general purpose dictionary and not tailored for a particular domain.</S>
			<S sid ="174" ssid = "43">To illustrate this we compared several models on the Brown corpus: the SHMM-ME model using the Wiktionary (Wik), against using a model trained using a dictionary extracted from the PTB corpus (PTBD), or trained fully supervised using the PTB corpus (PTB).</S>
			<S sid ="175" ssid = "44">We tested all these models on the 15 different sections of the Brown corpus.</S>
			<S sid ="176" ssid = "45">We also compare against a state-of-the-art POStagger tagger (ST)5.</S>
			<S sid ="177" ssid = "46">Figure 6 shows the accuracy results for each model on the different sections.</S>
			<S sid ="178" ssid = "47">The fully supervised SHMM-ME model did not perform as well as the the Stanford tagger (about 3% behind on average), most likely because of generative vs. discriminate training of the two models and feature differences.</S>
			<S sid ="179" ssid = "48">However, quite surprisingly, the Wiktionary- tag-set-trained model performs much better not only than the PTB-tag-set-trained model but also the supervised model on the Brown corpus.</S>
			<S sid ="180" ssid = "49">5 Available at http://nlp.stanford.edu/.</S>
			<S sid ="181" ssid = "50">software/tagger.shtml Supervised Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models.</S>
			<S sid ="182" ssid = "51">Avg.</S>
			<S sid ="183" ssid = "52">is the average of all languages except English.</S>
			<S sid ="184" ssid = "53">Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.</S>
			<S sid ="185" ssid = "54">Bilingual systems are trained using a dictionary transferred from English into the target language using word alignments.</S>
			<S sid ="186" ssid = "55">The Projection model uses a dictionary build directly from the part-of-speech projection.</S>
			<S sid ="187" ssid = "56">The D&amp;P model extends the Projection model dictionary by using Label Propagation.</S>
			<S sid ="188" ssid = "57">Supervised models are trained using tree bank information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary and All TBD uses tree bank tag sets for all words.</S>
			<S sid ="189" ssid = "58">50, 100 and All Sent.</S>
			<S sid ="190" ssid = "59">models are trained in a supervised manner using increasing numbers of training sentences.</S>
			<S sid ="191" ssid = "60">joint, which together comprise another 50% of the errors.</S>
			<S sid ="192" ssid = "61">As Wiktionary grows, these types of errors will likely diminish.</S>
			<S sid ="193" ssid = "62">Figure 6: Model accuracy across the Brown corpus sections.</S>
			<S sid ="194" ssid = "63">ST: Stanford tagger, Wik: Wiktionary- tag-set-trained SHMM-ME, PTBD: PTB-tag-set-trained SHMM-ME, PTB: Supervised SHMM-ME. Wik outperforms PTB and PTBD overall.</S>
			<S sid ="195" ssid = "64">5.6 Error breakdown.</S>
			<S sid ="196" ssid = "65">In Section 3.4 we discussed the accuracy of the Wiktionary tag sets and as Table 2 shows, a dictionary with better tag set quality generally (except for Greek) improves the POS tagging accuracy.</S>
			<S sid ="197" ssid = "66">In Figure 7, we group actual errors by the word type classified into the five cases discussed above: identical, superset, subset, overlap, disjoint.</S>
			<S sid ="198" ssid = "67">We also add oov – out-of-vocabulary word types.</S>
			<S sid ="199" ssid = "68">The largest source of error across languages are out-of-vocabulary (oov) word types at around 45% of the errors, followed by tag set mismatch types: subset, overlap, dis Figure 7: Tag errors broken down by the word type classified into the six classes: oov, identical, superset, subset, overlap, disjoint (see text for detail).</S>
			<S sid ="200" ssid = "69">The largest source of error across languages are out-of-vocabulary (oov) word types, followed by tag set mismatch types: subset, overlap, disjoint.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="201" ssid = "1">We have shown that the Wiktionary can be used to train a very simple model to achieve state-of- art weakly-supervised and out-of-domain POS tag- gers.</S>
			<S sid ="202" ssid = "2">The methods outlined in the paper are standard and easy to replicate, yet highly accurate and should serve as baselines for more complex propos als.</S>
			<S sid ="203" ssid = "3">These encouraging results show that using free, collaborative NLP resources can in fact produce results of the same level or better than using expensive annotations for many languages.</S>
			<S sid ="204" ssid = "4">Furthermore, the Wiktionary contains other possibly useful information, such as glosses and translations.</S>
			<S sid ="205" ssid = "5">It would be very interesting and perhaps necessary to incorporate this additional data in order to tackle challenges that arise across a larger number of language types, specifically non-European languages.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="206" ssid = "6">We would like to thank Slav Petrov, Kuzman Ganchev and Andre´ Martins for their helpful feedback in early versions of the manuscript.</S>
			<S sid ="207" ssid = "7">We would also like to thank to our anonymous reviewers for their comments and suggestions.</S>
			<S sid ="208" ssid = "8">Ben Taskar was partially supported by a Sloan Fellowship, ONR 2010 Young Investigator Award and NSF Grant.</S>
			<S sid ="209" ssid = "9">1116676.</S>
	</SECTION>
</PAPER>
