<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags.</S>
		<S sid ="2" ssid = "2">This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers.</S>
		<S sid ="3" ssid = "3">Until now, however, such conversion schemes have been created manually.</S>
		<S sid ="4" ssid = "4">Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages.</S>
		<S sid ="5" ssid = "5">We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping.</S>
		<S sid ="6" ssid = "6">Given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings.</S>
		<S sid ="7" ssid = "7">Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing.1</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">In this paper, we explore an automatic method for mapping language-specific part-of-speech tags to a universal tagset.</S>
			<S sid ="9" ssid = "9">In multilingual parsing, this unified input representation is required for cross-lingual syntactic transfer.</S>
			<S sid ="10" ssid = "10">Specifically, the universal tagset annotations enable an unlexicalized parser to capitalize on annotations from one language when learning a model for another.</S>
			<S sid ="11" ssid = "11">1 The source code and data for the work presented in this paper is available at http://groups.csail.mit.edu/ rbg/code/unitag/emnlp2012 While the notion of a universal POS tagset is widely accepted, in practice it is hardly ever used for annotation of monolingual resources.</S>
			<S sid ="12" ssid = "12">In fact, available POS annotations are designed to capture language-specific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset.</S>
			<S sid ="13" ssid = "13">To reconcile these cross-lingual annotation differences, a number of mapping schemes have been proposed in the parsing community (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010).</S>
			<S sid ="14" ssid = "14">In all of these cases, the conversion is performed manually and has to be repeated for each language and annotation scheme anew.</S>
			<S sid ="15" ssid = "15">Despite the apparent simplicity, deriving a mapping is by no means easy, even for humans.</S>
			<S sid ="16" ssid = "16">In fact, the universal tagsets manually induced by Petrov et al.</S>
			<S sid ="17" ssid = "17">(2011) and by Naseem et al.</S>
			<S sid ="18" ssid = "18">(2010) disagree on 10% of the tags.</S>
			<S sid ="19" ssid = "19">An example of such discrepancy is the mapping of the Japanese tag “PVfin” to the universal tag “particle” according to one scheme, and to “verb” according to another.</S>
			<S sid ="20" ssid = "20">Moreover, the quality of this conversion has a direct implication on the parsing performance.</S>
			<S sid ="21" ssid = "21">In the Japanese example above, this difference in mapping yields a 6.7% difference in parsing accuracy.</S>
			<S sid ="22" ssid = "22">The goal of our work is to induce the mapping for a new language, utilizing existing manually- constructed mappings as training data.</S>
			<S sid ="23" ssid = "23">The existing mappings developed in the parsing community rely on gold POS tags for the target language.</S>
			<S sid ="24" ssid = "24">A more realistic scenario is to employ the mapping technique to resource-poor languages where gold POS annotations are lacking.</S>
			<S sid ="25" ssid = "25">In such cases, a mapping algorithm has to operate over automatically in 1368 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1368–1378, Jeju Island, Korea, 12–14 July 2012.</S>
			<S sid ="26" ssid = "26">Qc 2012 Association for Computational Linguistics duced clusters on the target language (e.g., using the Brown algorithm) and convert them to universal tags.</S>
			<S sid ="27" ssid = "27">We are interested in a mapping approach that can effectively handle both gold tags and induced clusters.</S>
			<S sid ="28" ssid = "28">Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across languages.</S>
			<S sid ="29" ssid = "29">Since universal tags play the same linguistic role in source and target languages, we expect similarity in their global distributional statistics.</S>
			<S sid ="30" ssid = "30">Figure 1a shows statistics for two close languages, English and German.</S>
			<S sid ="31" ssid = "31">We can see that their unigram frequencies on the five most common tags are very close.</S>
			<S sid ="32" ssid = "32">Other properties concern POS tag per sentence statistics – e.g., every sentence has to have at least one verb.</S>
			<S sid ="33" ssid = "33">Finally, the mappings can be further constrained by typological properties of the target language that specify likely tag sequences.</S>
			<S sid ="34" ssid = "34">This information is readily available even for resource poor language (Haspel- math et al., 2005).</S>
			<S sid ="35" ssid = "35">For instance, since English and German are prepositional languages, we expect to observe adposition-noun sequences but not the reverse (see Figure 1b for sample sentences).</S>
			<S sid ="36" ssid = "36">We encode these heterogeneous properties into an objective function that guides the search for the optimal mapping.</S>
			<S sid ="37" ssid = "37">Having defined a quality measure for mappings, our goal is to find the optimal mapping.</S>
			<S sid ="38" ssid = "38">However, such partition optimization problems2 are NP hard (Garey and Johnson, 1979).</S>
			<S sid ="39" ssid = "39">A naive approach to the problem is to greedily improve the map, but it turns out that this approach yields poor quality mappings.</S>
			<S sid ="40" ssid = "40">We therefore develop a method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings.</S>
			<S sid ="41" ssid = "41">We construct the objective in a way that facilitates simple monotonically improving updates corresponding to solving convex optimization problems.</S>
			<S sid ="42" ssid = "42">We evaluate our mapping approach on 19 languages that include representatives of IndoEuropean, Semitic, Basque, Japonic and Turkic families.</S>
			<S sid ="43" ssid = "43">We measure mapping quality based on the target language parsing accuracy.</S>
			<S sid ="44" ssid = "44">In addition to considering gold POS tags for the target language, 2 Instances of related hard problems are 3-partition and subset-sum.</S>
			<S sid ="45" ssid = "45">we also evaluate the mapping algorithm on automatically induced POS tags.</S>
			<S sid ="46" ssid = "46">In all evaluation scenarios, our model consistently rivals the quality of manually induced mappings.</S>
			<S sid ="47" ssid = "47">We also demonstrate that the proposed inference procedure outperforms greedy methods by a large margin, highlighting the importance of good optimization techniques.</S>
			<S sid ="48" ssid = "48">We further show that while all characteristics of the mapping contribute to the objective, our largest gain comes from distributional features that capture global statistics.</S>
			<S sid ="49" ssid = "49">Finally, we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="50" ssid = "1">Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer.</S>
			<S sid ="51" ssid = "2">In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008).</S>
			<S sid ="52" ssid = "3">However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010).</S>
			<S sid ="53" ssid = "4">These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language.</S>
			<S sid ="54" ssid = "5">Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags.</S>
			<S sid ="55" ssid = "6">In previous work, their mapping to universal tags was performed manually.</S>
			<S sid ="56" ssid = "7">Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz and Marsi, 2006; Nivre et al., 2007), they are not identical and yield different parsing performance (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010).</S>
			<S sid ="57" ssid = "8">The goal of our work is to automate this process and construct mappings that are optimized for performance on downstream tasks (here we focus on parsing).</S>
			<S sid ="58" ssid = "9">As our results show, we achieve this goal 0.35 0.3 0.25 0.2 0.15 0.1 0.05 Inves tors [are appe aling ] to the Secu rities and Exch ange Com missi on not to [limit ] their acces s to infor mati on [abo ut stock purc hase s] and sales [by corp orate insid ers] -Einer der sich [für den Millia rdär] [auss pricht ] [ist] Steve Jobs dem Perot [für den aufba u] der Com puter ﬁrma Next 20 Millio nen Dollar [berei tstellt e] 0 Noun Verb Det. Prep.</S>
			<S sid ="59" ssid = "10">Adj.</S>
			<S sid ="60" ssid = "11">(a) (b) Figure 1: Illustration of similarities in POS tag statistics across languages.</S>
			<S sid ="61" ssid = "12">(a) The unigram frequency statistics on five tags for two close languages, English and German.</S>
			<S sid ="62" ssid = "13">(b) Sample sentences in English and German.</S>
			<S sid ="63" ssid = "14">Verbs are shown in blue, prepositions in red and noun phrases in green.</S>
			<S sid ="64" ssid = "15">It can be seen that noun phrases follow prepositions.</S>
			<S sid ="65" ssid = "16">on a broad range of languages and evaluation scenarios.</S>
			<S sid ="66" ssid = "17">Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007).</S>
			<S sid ="67" ssid = "18">Our work differs from these approaches in two ways.</S>
			<S sid ="68" ssid = "19">First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing.</S>
			<S sid ="69" ssid = "20">Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing.</S>
			<S sid ="70" ssid = "21">In contrast, we are not given trees in the target language.</S>
			<S sid ="71" ssid = "22">Instead, our model is informed by mappings derived for other languages.</S>
	</SECTION>
	<SECTION title="Task Formulation. " number = "3">
			<S sid ="72" ssid = "1">The input to our task consists of a target corpus written in a language T , and a set of non-parallel source corpora written in languages {S1, . . .</S>
			<S sid ="73" ssid = "2">, Sn}.</S>
			<S sid ="74" ssid = "3">In the source corpora, each word is annotated with both a language-specific POS tag and a universal POS tag (Petrov et al., 2011).</S>
			<S sid ="75" ssid = "4">In the target corpus each word is annotated only with a language-specific POS tag, either gold or automatically induced.</S>
			<S sid ="76" ssid = "5">Our goal is to find a map from the set of LT target language tags to the set of K universal tags.</S>
			<S sid ="77" ssid = "6">We as sume that each language-specific tag is only mapped to one universal tag, which means we never split a language-specific tag and LT ≥ K holds for every language.</S>
			<S sid ="78" ssid = "7">We represent the map by a matrix A of size K × LT where A(c|f ) = 1 if the target lan guage tag f is mapped to the universal tag c, and A(c|f ) = 0 otherwise.3 Note that each column of A should contain a single value of 1.</S>
			<S sid ="79" ssid = "8">We will later relax the requirement that A(c|f ) ∈ {0, 1}.</S>
			<S sid ="80" ssid = "9">A candi date mapping A can be applied to the target language to produce sentences labeled with universal tags.</S>
	</SECTION>
	<SECTION title="Model. " number = "4">
			<S sid ="81" ssid = "1">In this section we describe an objective that reflects the quality of an automatic mapping.</S>
			<S sid ="82" ssid = "2">Our key insight is that for a good mapping, the statistics over the universal tags should be similar for source and target languages because these tags play the same role cross-linguistically.</S>
			<S sid ="83" ssid = "3">For example, we should expect the frequency of a particular universal tag to be similar in the source and target languages.</S>
			<S sid ="84" ssid = "4">One choice to make when constructing an objective is the source languages to which we want to be similar.</S>
			<S sid ="85" ssid = "5">It is clear that choosing all languages is not a good idea, since they are not all expected to have distributional properties similar to the target language.</S>
			<S sid ="86" ssid = "6">There is strong evidence that projecting from single languages can lead to good parsing performance 3 We use c and f to reflect the fact that universal tags are a coarse version (hence c) of the language specific fine tags (hence f ).</S>
			<S sid ="87" ssid = "7">(McDonald et al., 2011).</S>
			<S sid ="88" ssid = "8">Therefore, our strategy is to choose a single source language for comparison.</S>
			<S sid ="89" ssid = "9">The choice of the source language is based on similarity between typological properties; we describe this in detail in Section 5.</S>
			<S sid ="90" ssid = "10">We must also determine which statistical properties we expect to be preserved across languages.</S>
			<S sid ="91" ssid = "11">Our model utilizes three linguistic phenomena which are consistent across languages: POS tag global distributional statistics, POS tag per sentence statistics, and typology-based ordering statistics.</S>
			<S sid ="92" ssid = "12">We define each of these below.</S>
			<S sid ="93" ssid = "13">4.1 Mapping Characterization.</S>
			<S sid ="94" ssid = "14">We focus on three categories of mapping properties.</S>
			<S sid ="95" ssid = "15">For each of the relevant statistics we define a function Fi(A) that has low values if the source and target statistics are similar.</S>
			<S sid ="96" ssid = "16">Global distributional statistics: The unigram and bigram statistics of the universal tags are expected to be similar across languages with close typological profiles.</S>
			<S sid ="97" ssid = "17">We use pS (c1, c2) to denote the bigram distribution over universal tags in the source language, and pT (f1, f2) to denote the bigram distribution over language specific tags in the target language.</S>
			<S sid ="98" ssid = "18">The bigram distribution over universal tags in the target language depends on A and pT (f1, f2) and is given by: pT (c1, c2; A) = A(c1|f1)A(c2|f2)pT (f1, f2) f1 ,f2 (1) To enforce similarity between source and target distributions, we wish to minimize the KL divergence between the two: 4 Fbi(A) = DK L[pS (c1, c2)|pT (c1, c2; A)] (2) We similarly define Funi(A) as the distance be To express this constraint, we use nv (s, A) to denote the number of verbs (i.e., the universal tags corresponding to verbs according to A) in sentence s. This is a linear function of A. We also use E[nv (s, A)] to denote the average number of verbs per sentence, and V [nv (s, A)] to denote the variance.</S>
			<S sid ="99" ssid = "19">We estimate these two statistics from the source language and denote them by ESv , VSv . Good mappings are expected to follow these patterns by having a variance upper bounded by VSv and an average lower bounded by ESv .5 This corresponds to minimizing the following objectives: FEv (A) = max [0, ESv − E[nv (s, A)]] FV v (A) = max [0, V [nv (s, A)] − VSv ] Note that the above objectives are convex in A, which will make optimization simpler.</S>
			<S sid ="100" ssid = "20">We refer to the two terms jointly as Fverb(A).</S>
			<S sid ="101" ssid = "21">Typology-based ordering statistics: Typological features can be useful for determining the relative order of different tags.</S>
			<S sid ="102" ssid = "22">If we know that the target language has a particular typological feature, we expect its universal tags to obey the given relative ordering.</S>
			<S sid ="103" ssid = "23">Specifically, we expect it to agree with ordering statistics for source languages with a similar typology.</S>
			<S sid ="104" ssid = "24">We consider two such features here.</S>
			<S sid ="105" ssid = "25">First, in preposition languages the preposition is followed by the noun phrase.</S>
			<S sid ="106" ssid = "26">Thus, if T is such a language, we expect the probability of a noun phrase following the adposition to be high, i.e., cross some threshold.</S>
			<S sid ="107" ssid = "27">Formally, we define C1 = {noun, adj, num, pron, det} and consider the set of bigram distributions Spre that satisfy the following constraint: pT (adp,c) ≥ apre (3) c∈C1 tween unigram distributions.</S>
			<S sid ="108" ssid = "28">where apre = c∈C1 pS (adp,c) is calculated from Per sentence statistics: Another defining property of POS tags is their average count per sentence.</S>
			<S sid ="109" ssid = "29">Specifically, we focus on the verb count per sentence, which we expect be similar across languages.</S>
			<S sid ="110" ssid = "30">4 We use the KL divergence because it assigns low weights to infrequent universal tags.</S>
			<S sid ="111" ssid = "31">Furthermore, this choice results in a simple, EM-like parameter estimation algorithm as discussed in Section 5.the source language.</S>
			<S sid ="112" ssid = "32">This constraint set is non convex in A due to the bilinearity of the bi- gram term.</S>
			<S sid ="113" ssid = "33">To simplify optimization6 we take an 5 The rationale is that we want to put a lower bound on the number of verbs per sentence, and induce it from the source language.</S>
			<S sid ="114" ssid = "34">Furthermore, we expect the number of verbs to be well concentrated, and we induce its maximal variance from the source language.</S>
			<S sid ="115" ssid = "35">6 In Section 5 we shall see that this makes optimization eas-.</S>
			<S sid ="116" ssid = "36">ier.</S>
			<S sid ="117" ssid = "37">approach inspired by the posterior regularization method (Ganchev et al., 2010) and use the objective: The overall objective is then: F (A) = Fα(A) + λ · H [A], where λ is the weight of the entropy term.7 The resulting optimization problem is: Fc(A) = min r(c1 ,c2 )∈Spre DK L[r(c1, c2)|pT (c1, c2; A)] (4) min F (A) (7) A∈∆ The above objective will attain lower values for A such that pT (c1, c2; A) is close to the constraint set.</S>
			<S sid ="118" ssid = "38">Specifically, it will have a value of zero when the where ∆ is the set of non-negative matrices whose columns sum to one: bigram distribution induced by A has the property specified in Spre.</S>
			<S sid ="119" ssid = "39">We similarly define a set Spost for post-positional languages.</S>
			<S sid ="120" ssid = "40">∆ = A : A(c|f ) ≥ 0 ∀c, f c=1 A(c|f ) = 1 ∀f (8) As a second typological feature, we consider the Demonstrative-Noun ordering.</S>
			<S sid ="121" ssid = "41">In DN languages we want the probability of a determiner to come be fore C2 = {noun, adj, num}, (i.e., frequent universalnoun-phrase tags), to cross a threshold.</S>
			<S sid ="122" ssid = "42">This con straint translates to: pT (det, c) ≥ adet (5) c∈C2 where adet = c∈C2 pS (det, c) is a threshold determined from the source language.</S>
			<S sid ="123" ssid = "43">We denote the set of distributions that have this property by SDN, and add them to the constraint in (4).</S>
			<S sid ="124" ssid = "44">The overall constraint set is denoted by S. 4.2 The Overall Objective.</S>
			<S sid ="125" ssid = "45">We have defined a set of functions Fi(A) that are expected to have low values for good mappings.</S>
			<S sid ="126" ssid = "46">To combine those, we use a weighted sum: Fα(A) = i αi · Fi(A).</S>
			<S sid ="127" ssid = "47">(The weights in this equation are learned; we discussed the procedure in Section 5) Optimizing over the set of mappings is difficult since each mapping is a discrete set whose size is exponential size in LT . Technically, the difficulty comes from the requirement that elements of A are integral and its columns sum to one.</S>
			<S sid ="128" ssid = "48">To relax this</S>
	</SECTION>
	<SECTION title="Parameter Estimation. " number = "5">
			<S sid ="129" ssid = "1">In this section we describe the parameter estimation process for our model.</S>
			<S sid ="130" ssid = "2">We start by describing how to optimize A. Next, we discuss the weight selection algorithm, and finally the method for choosing source languages.</S>
			<S sid ="131" ssid = "3">5.1 Optimizing the Mapping A. Recall that our goal is to solve the optimization problem in Eq.</S>
			<S sid ="132" ssid = "4">(7).</S>
			<S sid ="133" ssid = "5">This objective is non convex since the function H [A] is concave, and the objective F (A) involves bilinear terms in A and logarithms of their sums (see Equations (1) and (2)).</S>
			<S sid ="134" ssid = "6">While we do not attempt to solve the problem globally, we do have a simple update scheme that monotonically decreases the objective.</S>
			<S sid ="135" ssid = "7">The update can be derived in a similar manner to expectation maximization (EM) (Neal and Hinton, 1999) and convex concave procedures (Yuille and Rangarajan, 2003).</S>
			<S sid ="136" ssid = "8">Figure 2 describes our optimization algorithm.</S>
			<S sid ="137" ssid = "9">The key ideas in deriving it are using posterior distributions as in EM, and using a variational formulation of entropy.</S>
			<S sid ="138" ssid = "10">The term Fc(A) is handled in a similar way to the posterior regularization algorithm derivation.</S>
			<S sid ="139" ssid = "11">A detailed derivation is provided 8restriction, we will allow A(c|f ) ∈ [0, 1] and en courage A to correspond to a mapping by adding an entropy regularization term: in the supplementary file.</S>
			<S sid ="140" ssid = "12">The kth iteration of the algorithm involves several steps: H [A] = − f c A(c|f ) log A(c|f ) (6) • In step 1, we calculate the current estimate of the bigram distribution over tags, pT (c1, c2; Ak ).This term receives its minimal value when the con ditional probability of the universal tags given a language-specific tag is 1 for one universal tag and zero for the others.</S>
			<S sid ="141" ssid = "13">7 Note that as λ → ∞, only valid maps will be selected by the objective.</S>
			<S sid ="142" ssid = "14">8 The supplementary file is available at http://groups.</S>
			<S sid ="143" ssid = "15">csail.mit.edu/rbg/code/unitag/emnlp2012.</S>
			<S sid ="144" ssid = "16">• In step 2, we find the bigram distribution in the constraint set S that is closest in K L di vergence to pT (c1, c2; Ak ), and denote it byrk (c1, c2).</S>
			<S sid ="145" ssid = "17">This optimization problem is con vex in r(c1, c2).</S>
			<S sid ="146" ssid = "18">Initialize A0 . Repeat Step 1 (calculate current bigram estimate): pT (c1 , c2 ; Ak ) = ) Ak (c1 |f1 )Ak (c2 |f2 )pT (f1 , f2 ) f ,f 1 2 • In step 3, we calculate the bigram posteriorover language specific tags given a pair of uni versal tags.</S>
			<S sid ="147" ssid = "19">This is analogous to the standard E-step in EM.</S>
			<S sid ="148" ssid = "20">• In step 4, we use the posterior in step 3 and the bigram distributions pS (c1, c2) and rk (c1, c2) to obtain joint counts over language specific and universal bigrams.</S>
			<S sid ="149" ssid = "21">• In step 5, we use the joint counts from step 4 to obtain counts over pairs of language specific Step 2 (incorporate constraints): rk (c1 , c2 ) = arg min DKL [r(c1 , c2 )|pT (c1 , c2 ; Ak )] r∈S Step 3 (calculate model posterior): p(f1 , f2 |c1 , c2 ; Ak ) ∝ Ak (c1 |f1 )Ak (c2 |f2 )pT (f1 , f2 ) Step 4: (complete joint counts): N k (c1 , c2 , f1 , f2 ) = p(f1 , f2 |c1 , c2 ; Ak ) rk (c1 , c2 ) + pS (c1 , c2 )\ Step 5 (obtain pairwise): M k (c, f ) = N k k and universal tags.</S>
			<S sid ="150" ssid = "22">where N k (c, f ) = 2:, c2 ,f2 1 (c, f ) + N2 (c, f ) N k (c, c2 , f, f2 ) and similarly for • In step 6, analogous to the M-step in EM, weoptimize over the mapping matrix A. The ob N k (c, f ).</S>
			<S sid ="151" ssid = "23">Step 6 (M step with entropy linearization): Set Ak+1 to be the solution of jective is similar to the Q function in EM, and also includes the Fverb(A) term, and a linear upper bound on the entropy term.</S>
			<S sid ="152" ssid = "24">The objec min − A∈∆ c,f M k (c, f ) log A(c|f ) + A(c|f ) log Ak (c|f )1 + Fverb (A) tive can be seen to be convex in A. As mentioned above, each of the optimization problems in steps 2 and 6 is convex, and can therefore be solved using standard convex optimization solvers.</S>
			<S sid ="153" ssid = "25">Here, we use the CVX package (Grant and Boyd, 2008; Grant and Boyd, 2011).</S>
			<S sid ="154" ssid = "26">It can be shown that the algorithm improves F (A) at every iteration and converges to a local optimum.</S>
			<S sid ="155" ssid = "27">The above algorithm generates a mapping A that may contain fractional entries.</S>
			<S sid ="156" ssid = "28">To turn it into a hard mapping we round A by mapping each f to the cthat maximizes A(c|f ) and then perform greedy im provement steps (one f at a time) to further improve the objective.</S>
			<S sid ="157" ssid = "29">The regularization constant λ is tuned to minimize the Fα(A) value of the rounded A. 5.2 Learning the Objective Weights.</S>
			<S sid ="158" ssid = "30">Our Fα(A) objective is a weighted sum of the individual Fi(A) functions.</S>
			<S sid ="159" ssid = "31">In the following, we describe how to learn the αi weights for every target language.</S>
			<S sid ="160" ssid = "32">We would like Fα(A) to have low values when A is a good map.</S>
			<S sid ="161" ssid = "33">Since our performance goal is parsing accuracy, we consider a map to be good Until Convergence of Ak Figure 2: An iterative algorithm for minimizing our objective in Eq.</S>
			<S sid ="162" ssid = "34">(7).</S>
			<S sid ="163" ssid = "35">For simplicity we assume that all the weights αi and λ are equal to one.</S>
			<S sid ="164" ssid = "36">It can be shown that the objective monotonically decreases in every iteration.</S>
			<S sid ="165" ssid = "37">if it results in high parsing accuracy, as measured when projecting a parser from to S to T . Since we do not have annotated parses in T , we use the other source languages S = {S1, . . .</S>
			<S sid ="166" ssid = "38">, Sn} to learn the weight.</S>
			<S sid ="167" ssid = "39">For each Si as the target, we first train a parser for each language in S \ {Si} as if it was the source, using the map of Petrov et al.</S>
			<S sid ="168" ssid = "40">(2011), and choose S∗ ∈ S \ {Si} which gives the highest parsing accuracy on Si.</S>
			<S sid ="169" ssid = "41">Next we generate 7000 candidate mappings for Si by randomly perturbing the map of (Petrov et al., 2011).</S>
			<S sid ="170" ssid = "42">We evaluate the quality of each candidate A by projecting theparser of S∗ to Si, and recording the parsing accu racy.</S>
			<S sid ="171" ssid = "43">Among all the candidates we choose the highest accuracy one and denote it by A∗(Si).</S>
			<S sid ="172" ssid = "44">We now want the score F (A∗(Si)) to be lower than that of all other candidates.</S>
			<S sid ="173" ssid = "45">To achieve this, we train a rankingSVM whose inputs are pairs of maps A∗(Si) and an other worse A(Si).</S>
			<S sid ="174" ssid = "46">These map pairs are taken from many different traget languages, i.e. many different Si.</S>
			<S sid ="175" ssid = "47">The features given to the SVM are the terms of the score Fi(A).</S>
			<S sid ="176" ssid = "48">The goal of the SVM is to weight these terms such that the better map A∗(Si) has a lower score.</S>
			<S sid ="177" ssid = "49">The weights assigned by the SVM are taken as αi. 5.3 Source Language Selection.</S>
			<S sid ="178" ssid = "50">As noted in Section 4 we construct F (A) by choosing a single source language S. Here we describe the method for choosing S. Our goal is to choose S that is closest to T in terms of typology.</S>
			<S sid ="179" ssid = "51">Assume that languages are described by binary typological vectors vL.</S>
			<S sid ="180" ssid = "52">We would like to learn a diagonal matrix D such that d(S, T ; D) = (vS − vT )T D(vS − vT ) reflects the similarity between the languages.</S>
			<S sid ="181" ssid = "53">In our context, a good measure of similarity is the performance of a parser trained on S and projected on T (using the optimal map A).</S>
			<S sid ="182" ssid = "54">We thus seek a matrix D such that d(S, T ; D) is ranked according to the parsing accuracy.</S>
			<S sid ="183" ssid = "55">The matrix D is trained using an SVM ranking algorithm that tries to follow the ranking of parsing accuracy.</S>
			<S sid ="184" ssid = "56">Similar to the technique for learning the objective weights, we train across many pairs of source languages.9 The typological features we use are a subset of the features described in “The World Atlas of Languages Structure” (WALS, (Haspelmath et al., 2005)), and are shown in Table 1.</S>
	</SECTION>
	<SECTION title="Evaluation Set-Up. " number = "6">
			<S sid ="185" ssid = "1">Datasets We test our model on 19 languages: Arabic, Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian, Italian, Japanese, Portuguese, Slovene, Spanish, Swedish, and Turkish.</S>
			<S sid ="186" ssid = "2">Our data is taken from the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007).</S>
			<S sid ="187" ssid = "3">The CoNLL datasets consist of manually created dependency trees and language-specific POS tags.</S>
			<S sid ="188" ssid = "4">Following Petrov et al.</S>
			<S sid ="189" ssid = "5">(2011), our model maps these language-specific tags to a set of 12 universal tags: noun, verb, adjective, adverb, pronoun, determiner, adposition, numeral, conjunction, particle, punctuation mark and X (a general tag).</S>
			<S sid ="190" ssid = "6">9 Ties are broken using the F (A) objective..</S>
			<S sid ="191" ssid = "7">Evaluation Procedure We perform a separate experiment for each of the 19 languages as the target and a source language chosen from the rest (using the method from Section 5.3).</S>
			<S sid ="192" ssid = "8">For the selected source language, we assume access to the mapping of Petrov et al.</S>
			<S sid ="193" ssid = "9">(2011).</S>
			<S sid ="194" ssid = "10">Evaluation Measures We evaluate the quality of the derived mapping in the context of the target language parsing accuracy.</S>
			<S sid ="195" ssid = "11">In both the training and test data, the language-specific tags are replaced with universal tags: Petrov’s tags for the source languages and learned tags for the target language.</S>
			<S sid ="196" ssid = "12">We train two non-lexicalized parsers using source annotations and apply them to the target language.</S>
			<S sid ="197" ssid = "13">The first parser is a non-lexicalized version of the MST parser (McDonald et al., 2005) successfully used in the multilingual context (McDonald et al., 2011).</S>
			<S sid ="198" ssid = "14">In the second parser, parameters of the target language are estimated as a weighted mixture of parameters learned from supervised source languages (Cohen et al., 2011).</S>
			<S sid ="199" ssid = "15">For the parser of Cohen et al.</S>
			<S sid ="200" ssid = "16">(2011), we trained the model on the four languages used in the original paper — English, German, Czech and Italian.</S>
			<S sid ="201" ssid = "17">When measuring the performance on each of these four languages, we selected another set of four languages with a similar level of diversity.10 Following the standard evaluation practice in parsing, we use directed dependency accuracy as our measure of performance.</S>
			<S sid ="202" ssid = "18">Baselines We compare mappings induced by our model against three baselines: the manually constructed mapping of Petrov et al.</S>
			<S sid ="203" ssid = "19">(2011), a randomly constructed mapping and a greedy mapping.</S>
			<S sid ="204" ssid = "20">The greedy mapping uses the same objective as our full model, but optimizes it using a greedy method.</S>
			<S sid ="205" ssid = "21">In each iteration, this method makes |LT | passes over the language-specific tags, selecting a substitution that contributes the most to the objective.</S>
			<S sid ="206" ssid = "22">Initialization To reduce the dimension of our algorithm’s search space and speed up our method, we start by clustering the language-specific POS tags ofthe target into |K | = 12 clusters using an unsuper 10 We also experimented with a version of the Cohen et al.</S>
			<S sid ="207" ssid = "23">(2011) model trained on all the source languages.</S>
			<S sid ="208" ssid = "24">This setup resulted in decreased performance.</S>
			<S sid ="209" ssid = "25">For this reason, we chose to train the model on the four languages.</S>
			<S sid ="210" ssid = "26">ID Fea ture De scri pti on Val ue s 81 A 85 A 86 A 87 A 88 A Ord er of Sub ject , Obj ect and Ver b O r d e r o f A d p o s i t i o n a n d N o u n O r d e r o f G e n i t i v e a n d N o u n O r d e r o f A d j e c t i v e a n d N o u n Ord er of De mo nstr ativ e and No un SV O, SO V, VS O, VO S, OV S, OS V P o s t p o s i t i o n s , P r e p o s i t i o n s , I n p o s i t i o n s G e n i t i v e N o u n , N o u n G e n i t i v e A d j e c t i v e N o u n , N o u n A d j e c t i v e De mo nstr ative No un, Noun De mo nstr ativ e, bef ore and aft er Table 1: The set of typological features that we use for source language selection.</S>
			<S sid ="211" ssid = "27">The first column gives the ID of the feature as listed in WALS.</S>
			<S sid ="212" ssid = "28">The second column describes the feature and the last column enumerates the allowable values for each feature; besides these values each feature can also have a value of ‘No dominant order’.</S>
			<S sid ="213" ssid = "29">vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags.</S>
			<S sid ="214" ssid = "30">For initialization, we perform multiple random restarts and select the one with the lowest final objective score.</S>
	</SECTION>
	<SECTION title="Results. " number = "7">
			<S sid ="215" ssid = "1">We first present the results of our model using the gold POS tags for the target language.</S>
			<S sid ="216" ssid = "2">Table 2 summarizes the performance of our model and the baselines.</S>
			<S sid ="217" ssid = "3">Comparison against Baselines On average, the mapping produced by our model yields parsers with higher accuracy than all of the baselines.</S>
			<S sid ="218" ssid = "4">These results are consistent for both parsers (McDonald et al., 2011; Cohen et al., 2011).</S>
			<S sid ="219" ssid = "5">As expected, random mappings yield abysmal results — 20.2% and 12.7% for the two parsers.</S>
			<S sid ="220" ssid = "6">The low accuracy of parsers that rely on the Greedy mapping — 29.9% and 25.4% — show that a greedy approach is a poor strategy for mapping optimization.</S>
			<S sid ="221" ssid = "7">Surprisingly, our model slightly outperforms the mapping of (Petrov et al., 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al., 2011).</S>
			<S sid ="222" ssid = "8">Similar results are observed for the mixture weights parser (Cohen et al., 2011).</S>
			<S sid ="223" ssid = "9">The main reason for these differences comes from mistakes introduced in the manual mapping.</S>
			<S sid ="224" ssid = "10">For example, in Czech tag “R” is labeled as “pronoun”, while actually it should be mapped to “adposition”.</S>
			<S sid ="225" ssid = "11">By correcting this mistake, we gain 5% in parsing accuracy for the direct transfer parser.</S>
			<S sid ="226" ssid = "12">11 This pre-clustering results in about 3% improvement, presumably since it uses contextual information beyond what our algorithm does.</S>
			<S sid ="227" ssid = "13">Overall, the manually constructed mapping and our model’s output disagree on 21% of the assignments (measured on the token level).</S>
			<S sid ="228" ssid = "14">However, the extent of disagreement is not necessarily predictive of the difference in parsing performance.</S>
			<S sid ="229" ssid = "15">For instance, the manual and automatic mappings for Catalan disagree on 8% of the tags and their parsing accuracy differs by 5%.</S>
			<S sid ="230" ssid = "16">For Greek on the other hand, the disagreement between mappings is much higher — 17%, yet the parsing accuracy is very close.</S>
			<S sid ="231" ssid = "17">This phenomenon shows that not all mistakes have equal weight.</S>
			<S sid ="232" ssid = "18">For instance, a confusion between “pronoun” and “noun” is less severe in the parsing context than a confusion between “pronoun” and “adverb”.</S>
			<S sid ="233" ssid = "19">Impact of Language Selection To assess the quality of our language selection method, we compare the model against an oracle that selects the best source for a given target language.</S>
			<S sid ="234" ssid = "20">As Table 2 shows our method is very close to the oracle performance, with only 0.7% gap between the two.</S>
			<S sid ="235" ssid = "21">In fact, for 10 languages our method correctly predicts the best pairing.</S>
			<S sid ="236" ssid = "22">This result is encouraging in other contexts as well.</S>
			<S sid ="237" ssid = "23">Specifically, McDonald et al.</S>
			<S sid ="238" ssid = "24">(2011) have demonstrated that projecting from a single oracle- chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle.</S>
			<S sid ="239" ssid = "25">Relations between Objective Values and Optimization Performance The suboptimal performance of the Greedy method shows that choosing a good optimization strategy plays a critical role in finding the desired mapping.</S>
			<S sid ="240" ssid = "26">A natural question to ask is whether the objective value is predictive of the end goal parsing performance.</S>
			<S sid ="241" ssid = "27">Figure 3 shows the objective values for the mappings computed by our method and the baselines for four languages.</S>
			<S sid ="242" ssid = "28">Over D i r e c t T r a n s f e r P a r s e r ( A c c u r a c y ) M i x t u r e W e i g h t P a r s e r ( A c c u r a c y ) Ta g Dif f. Ra nd om Greedy Petrov Model Best Pair Ra nd om Greedy Petrov Model.</S>
			<S sid ="243" ssid = "29">C at al a n It al ia n P or tu g u es e S p a ni s h 1 5 . 9 32.5 74.8 79.3 79.3 1 2 . 6 24.6 65.6 73.9 8 . 8 1 6 . 4 41.0 68.7 68.3 71.4 1 1 . 7 33.5 64.2 61.9 6 . 7 1 5 . 1 0 . 7 14.1 70.4 72.6 1 2 . 2 1 1 . 5 27.4 72.1 68.9 68.9 6 . 4 26.5 58.8 62.8 7 . 5 Da nis h D u t c h E n g l i s h G e r m a n S w e d i s h 3 5 . 5 23.7 46.6 46.5 49.2 4 . 2 23.7 51.4 51.7 5 . 0 1 8 . 0 22.1 58.2 56.8 57.3 7 . 1 15.3 54.9 53.2 4 . 9 1 4 . 7 19.0 51.6 49.0 49.0 1 3 . 3 15.1 47.5 41.8 1 7 . 7 1 5 . 2 0 . 9 18.7 52.4 51.8 1 5 . 0 1 5 . 1 26.3 63.1 63.1 63.1 9 . 1 36.5 55.7 55.9 8 . 2 Bu lga ria n Cz ec h Slo ven e 1 7 . 4 28.0 51.6 63.4 63.4 2 2 . 6 39.9 64.6 60.4 3 5 . 7 1 9 . 0 34.4 47.7 57.3 57.3 1 2 . 7 26.2 48.3 55.7 2 8 . 5 1 5 . 6 21.8 43.5 51.4 52.8 1 1 . 3 20.7 42.2 53.0 3 8 . 8 Gr ee k 1 7 . 3 19.5 62.3 59.7 59.8 2 2 . 0 15.2 56.2 57.0 1 7 . 0 Hu ng ari an 2 8 . 4 44.1 53.8 52.3 52.3 4 . 0 43.8 46.4 51.7 1 8 . 1 Ar abi c 2 2 . 1 45.4 51.5 51.2 52.9 3 . 9 40.9 48.3 51.1 1 5 . 7 Ba sq ue 1 8 . 0 19.2 27.9 33.1 35.1 6 . 3 8.3 32.3 30.6 4 3 . 8 Ch ine se 2 2 . 4 34.1 46.0 47.6 49.5 1 7 . 7 34.9 44.0 40.4 3 8 . 1 Jap ane se 3 6 . 5 46.2 51.4 53.6 53.6 1 5 . 4 18.0 25.7 28.7 7 3 . 8 Tu rki sh 2 8 . 1 9 . 7 20.3 27.7 27.5 9 . 9 Av er ag e 2 0 . 2 29.9 55.4 56.7 57.4 1 2 . 7 25.4 50.8 51.7 2 1 . 3 Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language.</S>
			<S sid ="244" ssid = "30">The first section of the table is for the direct transfer of the MST parser (McDonald et al., 2011).</S>
			<S sid ="245" ssid = "31">The second section is for the weighted mixture parsing model (Cohen et al., 2011).</S>
			<S sid ="246" ssid = "32">The first two columns (Random and Greedy) of each section present the parsing performance with a random or a greedy mapping.</S>
			<S sid ="247" ssid = "33">The third column (Petrov) shows the results when the mapping of Petrov et al.</S>
			<S sid ="248" ssid = "34">(2011) is used.</S>
			<S sid ="249" ssid = "35">The fourth column (Model) shows the results when our mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best source language is selected for every target language.</S>
			<S sid ="250" ssid = "36">The last column (Tag Diff.)</S>
			<S sid ="251" ssid = "37">presents the difference between our mapping and the mapping of Petrov et al.</S>
			<S sid ="252" ssid = "38">(2011) by showing the percentage of target language tokens for which the two mappings select a different universal tag.</S>
			<S sid ="253" ssid = "39">all, our method and the manual mappings reach similar values, both considerably better than other baselines.</S>
			<S sid ="254" ssid = "40">While the parsing performance correlates with the objective, the correlation is not perfect.</S>
			<S sid ="255" ssid = "41">For instance, on Greek our mapping has a better objective value, but lower parsing performance.</S>
			<S sid ="256" ssid = "42">Ablation Analysis We next analyze the contribution of each component of our objective to the resulting performance.12 The strongest factor in our objective is the distributional features capturing global statistics.</S>
			<S sid ="257" ssid = "43">Using these features alone achieves an average accuracy of 51.1%, only 5.6% less than the full model score.</S>
			<S sid ="258" ssid = "44">Adding just the verb-related constraints to the distributional similarity objectives improves the average model performance by 2.1%.</S>
			<S sid ="259" ssid = "45">Adding just the typological constraints yields a very modest performance gain of 0.5%.</S>
			<S sid ="260" ssid = "46">This is not surprising — the source language is selected to be typo- logically similar to the target language, and thus its distributional properties are consistent with typological features.</S>
			<S sid ="261" ssid = "47">However, adding both the verb-related constraints and the typological constraints results in a synergistic performance gain of 5.6% over the dis- tributional similarity objective, a gain which is much better than the sum of the two individual gains.</S>
			<S sid ="262" ssid = "48">Application to Automatically Induced POS Tags A potential benefit of the proposed method is to relate automatically induced clusters in the target language to universal tags.</S>
			<S sid ="263" ssid = "49">In our experiments, we induce such clusters using Brown clustering,13 which 12 The results are consistent for both parsers, here we report the accuracy for the direct transfer method (McDonald et al., 13 In our experiments, we employ Liang’s implementation http://cs.stanford.edu/∼pliang/software/.</S>
			<S sid ="264" ssid = "50">The number of clus 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 M o d e l P e t r o v G r e e d y R a n d o m Catalan German Greek Arabic b a s e l i n e b y a b o u t 5 % . 8 C o n c l u s i o n s We prese nt an autom atic metho d for mappi ng language specific part-of speech tags to a set of universal tags.</S>
			<S sid ="265" ssid = "51">Our work capitali zes on manua lly designed conver sion schem es to autom atically create mappi ngs for new langua ges.</S>
			<S sid ="266" ssid = "52">Our experi mental results demon strate that autom atically induce d mappings rival the qualit y of their hand crafted counterpart s. We also establi sh that the mappi ng qualit y Figure 3: Objective values for the different mappings used in our experiments for four languages.</S>
			<S sid ="267" ssid = "53">Note that the goal of the optimization procedure is to minimize the objective value.</S>
			<S sid ="268" ssid = "54">has been successfully used for similar purposes in parsing research (Koo et al., 2008).</S>
			<S sid ="269" ssid = "55">We then map these clusters to the universal tags using our algorithm.</S>
			<S sid ="270" ssid = "56">The average parsing accuracy on the 19 languages is 45.5%.</S>
			<S sid ="271" ssid = "57">Not surprisingly, automatically induced tags negatively impact parsing performance, yielding a decrease of 11% when compared to mappings obtained using manual POS annotations (see Table 2).</S>
			<S sid ="272" ssid = "58">To further investigate the impact of inaccurate tags on the mapping performance, we compare our model against the oracle mapping model that maps each cluster to the most common universal tag of its members.</S>
			<S sid ="273" ssid = "59">Parsing accuracy obtained using this method is 45.1%, closely matching the performance of our mapping algorithm.</S>
			<S sid ="274" ssid = "60">An alternative approach to mapping words into universal tags is to directly partition words into K clusters (without passing through language specific tags).</S>
			<S sid ="275" ssid = "61">In order for these clusters to be meaningful as universal tags, we can provide several prototypes for each cluster (e.g., “walk” is a verb etc.).</S>
			<S sid ="276" ssid = "62">To test this approach we used the prototype driven tagger of Haghighi and Klein (2006) with 15 prototypes per universal tag.14 The resulting universal tags yield an average parsing accuracy of 40.5%.</S>
			<S sid ="277" ssid = "63">Our method (using Brown clustering as above) outperforms this 14 Oracle prototypes were obtained by taking the 15 most frequent words for each universal tag.</S>
			<S sid ="278" ssid = "64">This yields almost the same total number of prototypes as those in the experiment of (Haghighi and Klein, 2006).</S>
			<S sid ="279" ssid = "65">has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic.</S>
			<S sid ="280" ssid = "66">Finally, our experiments show that the choice of mapping optimization scheme plays a crucial role in the quality of the derived mapping, highlighting the importance of optimization for the mapping task.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="281" ssid = "67">The authors acknowledge the support of the NSF (IIS0835445), the MURI program (W911NF10-1 0533) and the DARPA BOLT program.</S>
			<S sid ="282" ssid = "68">We thank Tommi Jaakkola, the members of the MIT NLP group and the ACL reviewers for their suggestions and comments.</S>
			<S sid ="283" ssid = "69">Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.</S>
	</SECTION>
</PAPER>
