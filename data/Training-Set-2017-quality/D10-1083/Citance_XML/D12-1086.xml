<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition.</S>
		<S sid ="2" ssid = "2">Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words.</S>
		<S sid ="3" ssid = "3">We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy.</S>
		<S sid ="4" ssid = "4">Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Grammar rules apply not to individual words (e.g. dog, eat) but to syntactic categories of words (e.g. noun, verb).</S>
			<S sid ="6" ssid = "6">Thus constructing syntactic categories (also known as lexical or part-of-speech categories) is one of the fundamental problems in language acquisition.</S>
			<S sid ="7" ssid = "7">Syntactic categories represent groups of words that can be substituted for one another without altering the grammaticality of a sentence.</S>
			<S sid ="8" ssid = "8">Linguists identify syntactic categories based on semantic, syntactic, and morphological properties of words.</S>
			<S sid ="9" ssid = "9">There is also evidence that children use prosodic and phonological features to bootstrap syntactic category acquisition (Ambridge and Lieven, 2011).</S>
			<S sid ="10" ssid = "10">However there is as yet no satisfactory computational model that can match human performance.</S>
			<S sid ="11" ssid = "11">Thus identify ing the best set of features and best learning algorithms for syntactic category acquisition is still an open problem.</S>
			<S sid ="12" ssid = "12">Relationships between linguistic units can be classified into two types: syntagmatic (concerning positioning), and paradigmatic (concerning substitution).</S>
			<S sid ="13" ssid = "13">Syntagmatic relations determine which units can combine to create larger groups and paradigmatic relations determine which units can be substituted for one another.</S>
			<S sid ="14" ssid = "14">Figure 1 illustrates the paradigmatic vs syntagmatic axes for words in a simple sentence and their possible substitutes.</S>
			<S sid ="15" ssid = "15">In this study, we represent the paradigmatic axis directly by building substitute vectors for each word position in the text.</S>
			<S sid ="16" ssid = "16">The dimensions of a substitute vector represent words in the vocabulary, and the magnitudes represent the probability of occurrence in the given position.</S>
			<S sid ="17" ssid = "17">Note that the substitute vector for a word position (e.g. the second word in Fig.</S>
			<S sid ="18" ssid = "18">1) is a function of the context only (i.e. “the cried”), and does not depend on the word that does actually appear there (i.e. “man”).</S>
			<S sid ="19" ssid = "19">Thus substi Figure 1: Syntagmatic vs. paradigmatic axes for words in a simple sentence (Chandler, 2007).</S>
			<S sid ="20" ssid = "20">940 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 940–951, Jeju Island, Korea, 12–14 July 2012.</S>
			<S sid ="21" ssid = "21">Qc 2012 Association for Computational Linguistics tute vectors represent individual word contexts, not word types.</S>
			<S sid ="22" ssid = "22">We refer to the use of features based on substitute vectors as paradigmatic representations of word context.</S>
			<S sid ="23" ssid = "23">Our preliminary experiments indicated that using context information alone without the identity or the features of the target word (e.g. using dimension- ality reduction and clustering on substitute vectors) has limited success and modeling the co-occurrence of word and context types is essential for inducing syntactic categories.</S>
			<S sid ="24" ssid = "24">In the models presented in this paper, we combine paradigmatic representations of word context with features of co-occurring words within the co-occurrence data embedding (CODE) framework (Globerson et al., 2007; Maron et al., 2010).</S>
			<S sid ="25" ssid = "25">The resulting embeddings for word types are split into 45 clusters using k-means and the clusters are compared to the 45 gold tags in the 1M word Penn Treebank Wall Street Journal corpus (Marcus et al., 1999).</S>
			<S sid ="26" ssid = "26">We obtain many-to-one accuracies up to .7680 using only distributional information (the identity of the word and a representation of its context) and .8023 using morphological and orthographic features of words improving the state-of- the-art in unsupervised part-of-speech tagging performance.</S>
			<S sid ="27" ssid = "27">The high probability substitutes reflect both semantic and syntactic properties of the context as seen in the example below (the numbers in parentheses give substitute probabilities): “Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.” the: its (.9011), the (.0981), a (.0006), . . .</S>
			<S sid ="28" ssid = "28">board: board (.4288), company (.2584), firm (.2024), bank (.0731), . . .</S>
			<S sid ="29" ssid = "29">Top substitutes for the word “the” consist of words that can act as determiners.</S>
			<S sid ="30" ssid = "30">Top substitutes for “board” are not only nouns, but specifically nouns compatible with the semantic context.</S>
			<S sid ="31" ssid = "31">This example illustrates two concerns inherent in all distributional methods: (i) words that are generally substitutable like “the” and “its” are placed in separate categories (DT and PRP$) by the gold standard, (ii) words that are generally not substitutable like “do” and “put” are placed in the same category (VB).</S>
			<S sid ="32" ssid = "32">Freudenthal et al.</S>
			<S sid ="33" ssid = "33">(2005) point out that categories with unsubstitutable words fail the standard linguistic definition of a syntactic category and children do not seem to make errors of substituting such words in utterances (e.g. “What do you want?” vs. *“What put you want?”).</S>
			<S sid ="34" ssid = "34">Whether gold standard part-of-speech tags or distributional categories are better suited to applications like parsing or machine translation can be best decided using extrinsic evaluation.</S>
			<S sid ="35" ssid = "35">However in this study we follow previous work and evaluate our results by comparing them to gold standard part-of-speech tags.</S>
			<S sid ="36" ssid = "36">Section 2 gives a detailed review of related work.</S>
			<S sid ="37" ssid = "37">Section 3 describes the dataset and the construction of the substitute vectors.</S>
			<S sid ="38" ssid = "38">Section 4 describes co- occurrence data embedding, the learning algorithm used in our experiments.</S>
			<S sid ="39" ssid = "39">Section 5 describes our experiments and compares our results with previous work.</S>
			<S sid ="40" ssid = "40">Section 6 gives a brief error analysis and Section 7 summarizes our contributions.</S>
			<S sid ="41" ssid = "41">All the data and the code to replicate the results given in this paper is available from the authors’ website at http://goo.gl/RoqEh.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="42" ssid = "1">There are several good reviews of algorithms for unsupervised part-of-speech induction (Christodoulopoulos et al., 2010; Gao and Johnson, 2008) and models of syntactic category acquisition (Ambridge and Lieven, 2011).</S>
			<S sid ="43" ssid = "2">This work is to be distinguished from supervised part-of-speech disambiguation systems, which use labeled training data (Church, 1988), unsupervised disambiguation systems, which use a dictionary of possible tags for each word (Merialdo, 1994), or prototype driven systems which use a small set of prototypes for each class (Haghighi and Klein, 2006).</S>
			<S sid ="44" ssid = "3">The problem of induction is important for studying under-resourced languages that lack labeled corpora and high quality dictionaries.</S>
			<S sid ="45" ssid = "4">It is also essential in modeling child language acquisition because every child manages to induce syntactic categories without access to labeled sentences, labeled prototypes, or dictionary constraints.</S>
			<S sid ="46" ssid = "5">Models of unsupervised part-of-speech induction fall into two broad groups based on the information they utilize.</S>
			<S sid ="47" ssid = "6">Distributional models only use word types and their context statistics.</S>
			<S sid ="48" ssid = "7">Word-feature models incorporate additional morphological and orthographic features.</S>
			<S sid ="49" ssid = "8">on a 45-tag 1M word corpus.</S>
			<S sid ="50" ssid = "9">HMMs: The prototypical bitag HMM model maximizes the likelihood of the corpus w1 . . .</S>
			<S sid ="51" ssid = "10">wn 2.1 Distributional models.</S>
			<S sid ="52" ssid = "11">expressed as P (w1|c1) Tin i i i i 1 Distributional models can be further categorized into three subgroups based on the learning algorithm.</S>
			<S sid ="53" ssid = "12">The first subgroup represents each word type with its context vector and clusters these vectors accordingly (Schu¨ tze, 1995).</S>
			<S sid ="54" ssid = "13">Work in modeling child syntactic category acquisition has generally followed this clustering approach (Redington et al., 1998; Mintz, 2003).</S>
			<S sid ="55" ssid = "14">The second subgroup consists of probabilistic models based on the Hidden Markov Model (HMM) framework (Brown et al., 1992).</S>
			<S sid ="56" ssid = "15">A third group of algorithms constructs a low dimensional representation of the data that represents the empirical co-occurrence statistics of word types (Glober- son et al., 2007), which is covered in more detail in Section 4.</S>
			<S sid ="57" ssid = "16">Clustering: Clustering based methods represent context using neighboring words, typically a single word on the left and a single word on the right called a “frame” (e.g., the dog is; the cat is).</S>
			<S sid ="58" ssid = "17">They cluster word types rather than word tokens based on the frames they occupy thus employing one-tag-per- word assumption from the beginning (with the exception of some methods in (Schu¨ tze, 1995)).</S>
			<S sid ="59" ssid = "18">They may suffer from data sparsity caused by infrequent words and infrequent contexts.</S>
			<S sid ="60" ssid = "19">The solutions suggested either restrict the set of words and set of contexts to be clustered to the most frequently observed, or use dimensionality reduction.</S>
			<S sid ="61" ssid = "20">Redington et al.</S>
			<S sid ="62" ssid = "21">(1998) define context similarity based on the number of common frames bypassing the data sparsity problem but achieve mediocre results.</S>
			<S sid ="63" ssid = "22">Mintz (2003) only uses the most frequent 45 frames and Biemann (2006) clusters the most frequent 10,000 words using contexts formed from the most frequent 150200 words.</S>
			<S sid ="64" ssid = "23">Schu¨ tze (1995) and Lamar et al.</S>
			<S sid ="65" ssid = "24">(2010b) employ SVD to enhance similarity between less frequently observed words and contexts.</S>
			<S sid ="66" ssid = "25">Lamar et al.</S>
			<S sid ="67" ssid = "26">(2010a) represent each context by the currently assigned left and right tag (which eliminates data sparsity) and cluster word types using a soft k-means style iterative algorithm.</S>
			<S sid ="68" ssid = "27">They report the best clustering result to date of .708 many-to-one accuracy i=2 P (w |c )P (c |c − )where wi are the word tokens and ci are their (hid den) tags.</S>
			<S sid ="69" ssid = "28">One problem with such a model is its tendency to distribute probabilities equally and the resulting inability to model highly skewed word-tag distributions observed in hand-labeled data (Johnson, 2007).</S>
			<S sid ="70" ssid = "29">To favor sparse word-tag distributions one can enforce a strict one-tag-per-word solution (Brown et al., 1992; Clark, 2003), use sparse priors in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007), or use posterior regularization (Ganchev et al., 2010).</S>
			<S sid ="71" ssid = "30">Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many- to-one accuracy.</S>
			<S sid ="72" ssid = "31">However Christodoulopoulos et al.</S>
			<S sid ="73" ssid = "32">(2010) show that the older one-tag-per-word models such as (Brown et al., 1992) outperform the more sophisticated sparse prior and posterior regularization methods both in speed and accuracy (the Brown model gets .68 many-to-one accuracy with a 1M word corpus).</S>
			<S sid ="74" ssid = "33">Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010), this is probably not surprising; one-tag-per-word is a fairly good first approximation for induction.</S>
			<S sid ="75" ssid = "34">2.2 Word-feature models.</S>
			<S sid ="76" ssid = "35">One problem with the algorithms in the previous section is the poverty of their input features.</S>
			<S sid ="77" ssid = "36">Of the syntactic, semantic, and morphological information linguists claim underlie syntactic categories, context vectors or bitag HMMs only represent limited syntactic information in their input.</S>
			<S sid ="78" ssid = "37">Experiments incorporating morphological and orthographic features into HMM based models demonstrate significant improvements.</S>
			<S sid ="79" ssid = "38">(Clark, 2003; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2011) incorporate similar orthographic features and report improvements of 3, 7, and 10% respectively over the baseline Brown model.</S>
			<S sid ="80" ssid = "39">Christodoulopoulos et al.</S>
			<S sid ="81" ssid = "40">(2010) use prototype based features as described in (Haghighi and Klein, 2006) with automatically in duced prototypes and report an 8% improvement over the baseline Brown model.</S>
			<S sid ="82" ssid = "41">Christodoulopoulos et al.</S>
			<S sid ="83" ssid = "42">(2011) define a type-based Bayesian multinomial mixture model in which each word instance is generated from the corresponding word type mixture component and word contexts are represented as features.</S>
			<S sid ="84" ssid = "43">They achieve a .728 MTO score by extending their model with additional morphological and alignment features gathered from parallel corpora.</S>
			<S sid ="85" ssid = "44">To our knowledge, nobody has yet tried to incorporate phonological or prosodic features in a computational model for syntactic category acquisition.</S>
			<S sid ="86" ssid = "45">2.3 Paradigmatic representations.</S>
			<S sid ="87" ssid = "46">Sahlgren (2006) gives a detailed analysis of paradigmatic and syntagmatic relations in the context of word-space models used to represent word meaning.</S>
			<S sid ="88" ssid = "47">Sahlgren’s paradigmatic model represents word types using co-occurrence counts of their frequent neighbors, in contrast to his syntagmatic model that represents word types using counts of contexts (documents, sentences) they occur in.</S>
			<S sid ="89" ssid = "48">Our substitute vectors do not represent word types at all, but contexts of word tokens using probabilities of likely substitutes.</S>
			<S sid ="90" ssid = "49">Sahlgren finds that in word-spaces built by frequent neighbor vectors, more nearest neighbors share the same part-of-speech compared to word- spaces built by context vectors.</S>
			<S sid ="91" ssid = "50">We find that representing the paradigmatic axis more directly using substitute vectors rather than frequent neighbors improve part-of-speech induction.</S>
			<S sid ="92" ssid = "51">Our paradigmatic representation is also related to the second order co-occurrences used in (Schu¨ tze, 1995).</S>
			<S sid ="93" ssid = "52">Schu¨ tze concatenates the left and right context vectors for the target word type with the left context vector of the right neighbor and the right context vector of the left neighbor.</S>
			<S sid ="94" ssid = "53">The vectors from the neighbors include potential substitutes.</S>
			<S sid ="95" ssid = "54">Our method improves on his foundation by using a 4-gram language model rather than bigram statistics, using the whole 78,498 word vocabulary rather than the most frequent 250 words.</S>
			<S sid ="96" ssid = "55">More importantly, rather than simply concatenating vectors that represent the target word with vectors that represent the context we use S-CODE to model their co-occurrence statistics.</S>
			<S sid ="97" ssid = "56">2.4 Evaluation.</S>
			<S sid ="98" ssid = "57">We report many-to-one and V-measure scores for our experiments as suggested in (Christodoulopoulos et al., 2010).</S>
			<S sid ="99" ssid = "58">The many-to-one (MTO) evaluation maps each cluster to its most frequent gold tag and reports the percentage of correctly tagged instances.</S>
			<S sid ="100" ssid = "59">The MTO score naturally gets higher with increasing number of clusters but it is an intuitive metric when comparing results with the same number of clusters.</S>
			<S sid ="101" ssid = "60">The V-measure (VM) (Rosenberg and Hirschberg, 2007) is an information theoretic metric that reports the harmonic mean of homogeneity (each cluster should contain only instances of a single class) and completeness (all instances of a class should be members of the same cluster).</S>
			<S sid ="102" ssid = "61">In Section 6 we argue that homogeneity is perhaps more important in part-of-speech induction and suggest MTO with a fixed number of clusters as a more intuitive metric.</S>
	</SECTION>
	<SECTION title="Substitute Vectors. " number = "3">
			<S sid ="103" ssid = "1">In this study, we predict the part of speech of a word in a given context based on its substitute vector.</S>
			<S sid ="104" ssid = "2">The dimensions of the substitute vector represent words in the vocabulary, and the entries in the substitute vector represent the probability of those words being used in the given context.</S>
			<S sid ="105" ssid = "3">Note that the substitute vector is a function of the context only and is indifferent to the target word.</S>
			<S sid ="106" ssid = "4">This section details the choice of the data set, the vocabulary and the estimation of substitute vector probabilities.</S>
			<S sid ="107" ssid = "5">The Wall Street Journal Section of the Penn Tree- bank (Marcus et al., 1999) was used as the test corpus (1,173,766 tokens, 49,206 types).</S>
			<S sid ="108" ssid = "6">The tree- bank uses 45 part-of-speech tags which is the set we used as the gold standard for comparison in our experiments.</S>
			<S sid ="109" ssid = "7">To compute substitute probabilities we trained a language model using approximately 126 million tokens of Wall Street Journal data (1987 1994) extracted from CSRIII Text (Graff et al., 1995) (we excluded the test corpus).</S>
			<S sid ="110" ssid = "8">We used SRILM (Stolcke, 2002) to build a 4-gram language model with KneserNey discounting.</S>
			<S sid ="111" ssid = "9">Words that were observed less than 20 times in the language model training data were replaced by UNK tags, which gave us a vocabulary size of 78,498.</S>
			<S sid ="112" ssid = "10">The perplexity of the 4-gram language model on the test cor pus is 96.</S>
			<S sid ="113" ssid = "11">It is best to use both left and right context when estimating the probabilities for potential lexical substitutes.</S>
			<S sid ="114" ssid = "12">For example, in “He lived in San Francisco suburbs.”, the token San would be difficult to guess from the left context but it is almost certain look ing at the right context.</S>
			<S sid ="115" ssid = "13">We define cw as the 2n − 1word window centered around the target word posi tion: w−n+1 . . .</S>
			<S sid ="116" ssid = "14">w0 . . .</S>
			<S sid ="117" ssid = "15">wn−1 (n = 4 is the n-gram order).</S>
			<S sid ="118" ssid = "16">The probability of a substitute word w in a given context cw can be estimated as: P (w0 = w|cw ) ∝ P (w−n+1 . . .</S>
			<S sid ="119" ssid = "17">w0 . . .</S>
			<S sid ="120" ssid = "18">wn−1)(1) = P (w−n+1)P (w−n+2|w−n+1) . . .</S>
			<S sid ="121" ssid = "19">P (wn 1|wn−2 ) (2)</S>
	</SECTION>
	<SECTION title="Co-occurrence Data Embedding. " number = "4">
			<S sid ="122" ssid = "1">The general strategy we follow for unsupervised syntactic category acquisition is to combine features of the context with the identity and features of the target word.</S>
			<S sid ="123" ssid = "2">Our preliminary experiments indicated that using the context information alone (e.g. clustering substitute vectors) without the target word identity and features had limited success.2 It is the co-occurrence of a target word with a particular type of context that best predicts the syntactic category.</S>
			<S sid ="124" ssid = "3">In this section we review the unsupervised methods we used to model co-occurrence statistics: the Co-occurrence Data Embedding (CODE) method (Globerson et al., 2007) and its spherical extension (S-CODE) introduced by (Maron et al., 2010).</S>
			<S sid ="125" ssid = "4">≈ P (w0|w−1 )P (w1|w0 ) n−2 Let X and Y be two categorical variables with fi where wj . . .</S>
			<S sid ="126" ssid = "5">P (wn −1| w0 ) (3) represents the sequence of words nite cardinalitie s |X | and |Y |.</S>
			<S sid ="127" ssid = "6">We observe a set of pairs {xi, yi n drawn IID from the joint distribu tion of X and Y . The basic idea behind CODE andwiwi+1 . . .</S>
			<S sid ="128" ssid = "7">wj . In Equation 1, P (w|cw ) is pro portional to P (w−n+1 . . .</S>
			<S sid ="129" ssid = "8">w0 . . .</S>
			<S sid ="130" ssid = "9">wn+1) because the words of the context are fixed.</S>
			<S sid ="131" ssid = "10">Terms without w0 are identical for each substitute in Equation 2 therefore they have been dropped in Equation 3.</S>
			<S sid ="132" ssid = "11">Finally, because of the Markov property of n-gram language model, only the closest n − 1 words are used in the experiments.</S>
			<S sid ="133" ssid = "12">Near the sentence boundaries the appropriate terms were truncated in Equation 3.</S>
			<S sid ="134" ssid = "13">Specifically, at related methods is to represent (embed) each value of X and each value of Y as points in a common low dimensional Euclidean space Rd such that values that frequently co-occur lie close to each other.</S>
			<S sid ="135" ssid = "14">There are several ways to formalize the relationship between the distances and co-occurrence statistics, in this paper we use the following: 1 2 p(x, y) = p¯(x)p¯(y)e−dx,y (4) the beginning of the sentence shorter n-gram con- Z texts were used and at the end of the sentence terms beyond the end-of-sentence token were dropped.</S>
			<S sid ="136" ssid = "15">2 For computational efficiency only the top 100 substitutes and their unnormalized probabilities were computed for each of the 1,173,766 positions in the test set1.</S>
			<S sid ="137" ssid = "16">The probability vectors for each position were normalized to add up to 1.0 giving us the final substitute vectors used in the rest of this study.where dx,y is the squared distance between the em beddings of x and y, p¯(x) and p¯(y) are empirical probabilities, and Z = x,y p¯(x)p¯(y)e−dx,y is a normalization term.</S>
			<S sid ="138" ssid = "17">If we use the notation φx for the point corresponding to x and ψy for the point corresponding to y then d2 2 x,y = φx − ψy . The log-likelihood of a given embedding £(φ, ψ) can be 1 The substitutes with unnormalized log probabilities can be.</S>
			<S sid ="139" ssid = "18">downloaded from http://goo.gl/jzKH0.</S>
			<S sid ="140" ssid = "19">For a description of the FASTSUBS algorithm used to generate the substitutes please see http://arxiv.org/abs/1205.5407v1.</S>
			<S sid ="141" ssid = "20">FASTSUBS accomplishes this task in about 5 hours, a naive algorithm that looks at the whole vocabulary would take more than 6 days on a typical 2012 workstation.</S>
			<S sid ="142" ssid = "21">2 A 10-nearest-neighbor supervised baseline using cosine distance between substitute vectors gives .7213 accuracy.</S>
			<S sid ="143" ssid = "22">Clustering substitute vectors using various distance metrics and dimensionality reduction methods give results inferior to this upper bound.</S>
			<S sid ="144" ssid = "23">expressed as: £(φ, ψ) = p¯(x, y) log p(x, y) (5) x,y = p¯(x, y)(− log Z + log p¯(x)p¯(y) − d2 ) x,y = − log Z + const − p¯(x, y)d2 x,y The likelihood is not convex in φ and ψ.</S>
			<S sid ="145" ssid = "24">We use gradient ascent to find an approximate solution for a set of φx, ψy that maximize the likelihood.</S>
			<S sid ="146" ssid = "25">The partitions the high dimensional space of substitute vectors into small neighborhoods and uses the partition id as a discrete context representation.</S>
			<S sid ="147" ssid = "26">Section 5.3 presents an even simpler model which pairs each word with a random substitute.</S>
			<S sid ="148" ssid = "27">When the left- word – right-word pairs used in the bigram model are replaced with word – partition-id or word – substitute pairs we see significant gains in accuracy.</S>
			<S sid ="149" ssid = "28">These results support our running hypothesis that paradigmatic features, i.e. potential substitutes of a word, are better determiners of syntactic category compared to left and right neighbors.</S>
			<S sid ="150" ssid = "29">Section 5.4 gradient of the d2 term pulls neighbors closer in explores morphologic and orthographic features as proportion to the empirical joint probability: additional sources of information and its results im ∂ ∂φx x,y −p¯(x, y)d2 = 2p¯(x, y)(ψy − φx) y (6) prove the stateof-the art in the field of unsup ervise d syntac tic categ ory acqui sition . Each expe rime nt was repe ated 10 time s with dif The gradient of the Z term pushes neighbors apart in proportion to the estimated joint probability: ferent random seeds and the results are reported with standard errors in parentheses or error bars in graphs.</S>
			<S sid ="151" ssid = "30">Table 1 summarizes all the results reported ∂ ∂φx (− log Z ) = 2p(x, y)(φx y − ψy ) (7) in this paper and the ones we cite from the literature . 5.1 Big ram model Thus the net effect is to pull pairs together if their estimated probability is less than the empirical probability and to push them apart otherwise.</S>
			<S sid ="152" ssid = "31">The gradients with respect to ψy are similar.S-CODE (Maron et al., 2010) additionally re stricts all φx and ψy to lie on the unit sphere.</S>
			<S sid ="153" ssid = "32">With this restriction, Z stays around a fixed value during gradient ascent.</S>
			<S sid ="154" ssid = "33">This allows S-CODE to substitute an approximate constant Z˜ in gradient calculations for the real Z for computational efficiency.</S>
			<S sid ="155" ssid = "34">In our experiments, we used S-CODE with its sampling based stochastic gradient ascent algorithm and smoothly decreasing learning rate.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="156" ssid = "1">In this section we present experiments that evaluate substitute vectors as representations of word context within the S-CODE framework.</S>
			<S sid ="157" ssid = "2">Section 5.1 replicates the bigram based S-CODE results from (Maron et al., 2010) as a baseline.</S>
			<S sid ="158" ssid = "3">The S-CODE algorithm works with discrete inputs.</S>
			<S sid ="159" ssid = "4">The substitute vectors as described in Section 3 are high dimensional and continuous.</S>
			<S sid ="160" ssid = "5">We experimented with two approaches to use substitute vectors in a discrete setting.</S>
			<S sid ="161" ssid = "6">Section 5.2 presents an algorithm that In (Maron et al., 2010) adjacent word pairs (bi- grams) in the corpus are fed into the S-CODE algorithm as X, Y samples.</S>
			<S sid ="162" ssid = "7">The algorithm uses stochastic gradient ascent to find the φx, ψy embeddings for left and right words in these bigrams on a single 25- dimensional sphere.</S>
			<S sid ="163" ssid = "8">At the end each word w in the vocabulary ends up with two points on the sphere, a φw point representing the behavior of w as the left word of a bigram and a ψw point representing it as the right word.</S>
			<S sid ="164" ssid = "9">The two vectors for w are concatenated to create a 50-dimensional representation at the end.</S>
			<S sid ="165" ssid = "10">These 50-dimensional vectors are clustered using an instance weighted k-means algorithm and the resulting groups are compared to the correct part-of-speech tags.</S>
			<S sid ="166" ssid = "11">Maron et al.</S>
			<S sid ="167" ssid = "12">(2010) report many-to-one scores of .6880 (.0016) for 45 clusters and .7150 (.0060) for 50 clusters (on the full PTB45 tag-set).</S>
			<S sid ="168" ssid = "13">If only φw vectors are clustered without concatenation we found the performance drops significantly to about .62.</S>
			<S sid ="169" ssid = "14">To make a meaningful comparison we reran the bigram experiments using our default settings and obtained a many-to-one score of .7314 (.0096) and the V-measure is .6558 (.0052) for 45 clusters.</S>
			<S sid ="170" ssid = "15">The following default settings were used: (i) each word Dis trib uti on al Mo del s MT O V M Mo del s wit h Ad diti on al Fe at ur es MT O V M (La ma r et al., 20 10 a) (B ro w n et al. , 1 9 9 2) * ( G ol d w at er et al. , 2 0 0 7) ( G a nc h ev et al. , 2 0 1 0) * ( M ar o n et al. , 2 0 1 0) Bi gr a m s (S ec . 5.</S>
			<S sid ="171" ssid = "16">1) P ar titi o ns (S ec . 5.</S>
			<S sid ="172" ssid = "17">2) S u bs tit ut es (S ec . 5.</S>
			<S sid ="173" ssid = "18">3) .70 8 .67 8 .63 2 .62 5 .68 8 (.0 01 6) .73 14 (.0 09 6) .75 54 (.0 05 5) .76 80 (.0 03 8) .63 0 .56 2 .54 8 .65 58 (.0 05 2) .67 03 (.0 03 7) .68 22 (.0 02 9) (Cl ark , 20 03 )* ( C hr is to d o ul o p o ul o s et al ., 2 0 1 1 ) ( B erg Ki rk p at ri c k a n d K le in , 2 0 1 0) ( C hr is to d o ul o p o ul o s et al ., 2 0 1 0 ) ( Bl u n s o m a n d C o h n, 2 0 1 1 ) S u b sti tu te s a n d F e at ur e s (S e c. 5.</S>
			<S sid ="174" ssid = "19">4 ) .71 2 .72 8 .75 5 .76 1 .77 5 .80 23 (.0 07 0) .65 5 .66 1 .68 8 .69 7 .72 07 (.0 04 1) Table 1: Summary of results in terms of the MTO and VM scores.</S>
			<S sid ="175" ssid = "20">Standard errors are given in parentheses when available.</S>
			<S sid ="176" ssid = "21">Starred entries have been reported in the review paper (Christodoulopoulos et al., 2010).</S>
			<S sid ="177" ssid = "22">Distributional models use only the identity of the target word and its context.</S>
			<S sid ="178" ssid = "23">The models on the right incorporate orthographic and morphological features.</S>
			<S sid ="179" ssid = "24">was kept with its original capitalization, (ii) the learning rate parameters were adjusted to ϕ0 =50, η0 = 0.2 for faster convergence in log likeli hood, (iii) the number of s-code iterations were increased from 12 to 50 million, (iv) k-means initialization was improved using (Arthur and Vassilvitskii, 2007), and (v) the number of k-means restarts were increased to 128 to improve clustering and reduce variance.</S>
			<S sid ="180" ssid = "25">5.2 Random partitions.</S>
			<S sid ="181" ssid = "26">Instead of using left-word – right-word pairs as in 0.8 0.79 0.78 0.77 0.76 0.75 0.74 0.73 0.72 0.71 0.7 m2o 10000 100000 number of random partitions puts to S-CODE we wanted to pair each word with a paradigmatic representation of its context to get a direct comparison of the two context representations.</S>
			<S sid ="182" ssid = "27">To obtain a discrete representation of the context, the random–partitions algorithm first designates a random subset of substitute vectors as centroids to partition the space, and then associates each context with the partition defined by the closest centroid in cosine distance.</S>
			<S sid ="183" ssid = "28">Each partition thus defined gets a unique id, and word (X ) – partition-id (Y ) pairs are given to S-CODE as input.</S>
			<S sid ="184" ssid = "29">The algorithm cycles through the data until we get approximately 50 million updates.</S>
			<S sid ="185" ssid = "30">The resulting φx vectors are clustered using the k-means algorithm (no vector concatenation is necessary).</S>
			<S sid ="186" ssid = "31">Using default settings (64K random partitions, 25 s-code dimensions, Z = 0.166) the many-to-one accuracy is .7554 (.0055) and the V-measure is .6703 (.0037).</S>
			<S sid ="187" ssid = "32">To analyze the sensitivity of this result to our specific parameter settings we ran a number of experiments where each parameter was varied over a range of values.</S>
			<S sid ="188" ssid = "33">Figure 2 gives results where the number of initial Figure 2: MTO is not sensitive to the number of partitions used to discretize the substitute vector space within our experimental range.</S>
			<S sid ="189" ssid = "34">random partitions is varied over a large range and shows the results to be fairly stable across two orders of magnitude.</S>
			<S sid ="190" ssid = "35">Figure 3 shows that at least 10 embedding dimensions are necessary to get within 1% of the best result, but there is no significant gain from using more than 25 dimensions.</S>
			<S sid ="191" ssid = "36">Figure 4 shows that the constant Z˜ approximation can be varied within two orders of magnitude without a significant performance drop in the many-to- one score.</S>
			<S sid ="192" ssid = "37">For uniformly distributed points on a 25 dimensional sphere, the expected Z ≈ 0.146.</S>
			<S sid ="193" ssid = "38">In the experiments where we tested we found the real Z al ways to be in the 0.1400.170 range.</S>
			<S sid ="194" ssid = "39">When the constant Z˜ estimate is too small the attraction in Eq. 6 dominates the repulsion in Eq. 7 and all points tend to converge to the same location.</S>
			<S sid ="195" ssid = "40">When Z˜ is too high, it prevents meaningful clusters from coalesc 0.8 0.75 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 m2o 1 10 100 number of s-code dimensions . 6 8 2 2 ( . 0 0 2 9 ) . This result is close to the previous result by the random partition algorithm , .7554 (.0055), demonstrating that two very different discrete representations of context based on paradigm atic features give consisten t results.</S>
			<S sid ="196" ssid = "41">Both results are significa ntly above the bigram baseline, .7314 (.0096).</S>
			<S sid ="197" ssid = "42">Figure 5 illustrates that the random substitute result is fairly robust as long as the training algorithm can observe more than a few random substitute s per word.</S>
			<S sid ="198" ssid = "43">0 . 8 Figure 3: MTO falls sharply for less than 10 S-CODE dimensions, but more than 25 do not help.</S>
			<S sid ="199" ssid = "44">0.8 0.79 0.78 0.77 0.76 m2o 0.79 0.78 0.77 0.76 0.75 0.74 0.73 0.72 0.71 m2o 0.75 0.74 0.73 0.72 0.71 0.7 1 10 100 number of random substitutes per word 0.7 0.01 0.1 1 s-code Z approximation Figure 5: MTO is not sensitive to the number of random substitutes sampled per word token.</S>
			<S sid ="200" ssid = "45">Figure 4: MTO is fairly stable as long as the Z˜ constant is within an order of magnitude of the real Z value.</S>
			<S sid ="201" ssid = "46">ing.</S>
			<S sid ="202" ssid = "47">We find the random partition algorithm to be fairly robust to different parameter settings and the resulting many-to-one score significantly better than the bigram baseline.</S>
			<S sid ="203" ssid = "48">5.3 Random substitutes.</S>
			<S sid ="204" ssid = "49">Another way to use substitute vectors in a discrete setting is simply to sample individual substitute words from them.</S>
			<S sid ="205" ssid = "50">The random-substitutes algorithm cycles through the test data and pairs each word with a random substitute picked from the pre- computed substitute vectors (see Section 3).</S>
			<S sid ="206" ssid = "51">We ran the random-substitutes algorithm to generate 14 million word (X ) – random-substitute (Y ) pairs (12 substitutes for each token) as input to S-CODE.</S>
			<S sid ="207" ssid = "52">Clustering the resulting φx vectors yields a many- to-one score of .7680 (.0038) and a V-measure of 5.4 Morphological and orthographic features.</S>
			<S sid ="208" ssid = "53">Clark (2003) demonstrates that using morphological and orthographic features significantly improves part-of-speech induction with an HMM based model.</S>
			<S sid ="209" ssid = "54">Section 2 describes a number other approaches that show similar improvements.</S>
			<S sid ="210" ssid = "55">This section describes one way to integrate additional features to the random-substitute model.</S>
			<S sid ="211" ssid = "56">The orthographic features we used are similar to the ones in (Berg-Kirkpatrick et al., 2010) with small modifications: • Initial-Capital: this feature is generated for capitalized words with the exception of sentence initial words.</S>
			<S sid ="212" ssid = "57">• Number: this feature is generated when the token starts with a digit.</S>
			<S sid ="213" ssid = "58">• Contains-Hyphen: this feature is generated for lowercase words with an internal hyphen.</S>
			<S sid ="214" ssid = "59">• Initial-Apostrophe: this feature is generated for tokens that start with an apostrophe.</S>
			<S sid ="215" ssid = "60">We generated morphological features using the unsupervised algorithm Morfessor (Creutz and La- gus, 2005).</S>
			<S sid ="216" ssid = "61">Morfessor was trained on the WSJ section of the Penn Treebank using default settings, and a perplexity threshold of 300.</S>
			<S sid ="217" ssid = "62">The program induced 5 suffix types that are present in a total of 10,484 word types.</S>
			<S sid ="218" ssid = "63">These suffixes were input to S-CODE as morphological features whenever the associated word types were sampled.</S>
			<S sid ="219" ssid = "64">In order to incorporate morphological and orthographic features into S-CODE we modified its input.</S>
			<S sid ="220" ssid = "65">For each word – random-substitute pair generated as in the previous section, we added word – feature pairs to the input for each morphological and orthographic feature of the word.</S>
			<S sid ="221" ssid = "66">Words on average have 0.25 features associated with them.</S>
			<S sid ="222" ssid = "67">This increased the number of pairs input to S-CODE from 14.1 million (12 substitutes per word) to 17.7 million (additional 0.25 features on average for each of the 14.1 million words).</S>
			<S sid ="223" ssid = "68">Using similar training settings as the previous section, the addition of morphological and orthographic features increased the many-to-one score of the random-substitute model to .8023 (.0070) and V-measure to .7207 (.0041).</S>
			<S sid ="224" ssid = "69">Both these results improve the state-of-the-art in part-of-speech induction significantly as seen in Table 1.</S>
	</SECTION>
	<SECTION title="Error Analysis. " number = "6">
			<S sid ="225" ssid = "1">Figure 6 is the Hinton diagram showing the relationship between the most frequent tags and clusters from the experiment in Section 5.4.</S>
			<S sid ="226" ssid = "2">In general the errors seem to be the lack of completeness (multiple large entries in a row), rather than lack of homogeneity (multiple large entries in a column).</S>
			<S sid ="227" ssid = "3">The algorithm tends to split large word classes into several clusters.</S>
			<S sid ="228" ssid = "4">Some examples are: • Titles like Mr., Mrs., and Dr. are split from the rest of the proper nouns in cluster (39).</S>
			<S sid ="229" ssid = "5">• Auxiliary verbs (10) and the verb “say” (22) have been split from the general verb clusters (12) and (7).• Determiners “the” (40), “a” (15), and capital ized “The”, “A” (6) have been split into their own clusters.</S>
			<S sid ="230" ssid = "6">• Prepositions “of” (19), and “by”, “at” (17) have been split from the general preposition cluster (8).</S>
			<S sid ="231" ssid = "7">Nevertheless there are some homogeneity errors as well: • The adjective cluster (5) also has some noun members probably due to the difficulty of sep arating noun-noun compounds from adjective modification.</S>
			<S sid ="232" ssid = "8">• Cluster (6) contains capitalized words that span a number of categories.</S>
			<S sid ="233" ssid = "9">Most closed-class items are cleanly separated into their own clusters as seen in the lower right hand corner of the diagram.</S>
			<S sid ="234" ssid = "10">The completeness errors are not surprising given that the words that have been split are not generally substitutable with the other members of their Penn Treebank category.</S>
			<S sid ="235" ssid = "11">Thus it can be argued that metrics that emphasize homogeneity such as MTO are more appropriate in this context than metrics that average homogeneity and completeness such as VM as long as the number of clusters is controlled.</S>
	</SECTION>
	<SECTION title="Contributions. " number = "7">
			<S sid ="236" ssid = "1">Our main contributions can be summarized as follows: • We introduced substitute vectors as paradigmatic representations of word context and demonstrated their use in syntactic category acquisition.</S>
			<S sid ="237" ssid = "2">• We demonstrated that using paradigmatic representations of word context and modeling co- occurrences of word and context types with the S-CODE learning framework give superior results when compared to a baseline bigram model.</S>
			<S sid ="238" ssid = "3">• We extended the S-CODE framework to incorporate morphological and orthographic features and improved the state-of-the-art in unsupervised part-of-speech induction to 80% many-to-one accuracy.</S>
			<S sid ="239" ssid = "4">Figure 6: Hinton diagram comparing most frequent tags and clusters.• All our code and data, including the sub stitute vectors for the one million word Penn Treebank Wall Street Journal dataset, is available at the authors’ website at http://goo.gl/RoqEh.</S>
	</SECTION>
</PAPER>
