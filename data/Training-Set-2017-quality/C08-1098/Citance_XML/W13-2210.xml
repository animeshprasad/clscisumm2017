<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes the phrase-based SMT systems developed for our participation in the WMT13 Shared Translation Task.</S>
		<S sid ="2" ssid = "2">Translations for English↔German and English↔French were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-of- speech (POS) and automatic cluster language models and discriminative word lexica (DWL).</S>
		<S sid ="3" ssid = "3">In addition, we combined reordering models on different sentence abstraction levels.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">In this paper, we describe our systems for the ACL 2013 Eighth Workshop on Statistical Machine Translation.</S>
			<S sid ="5" ssid = "5">We participated in the Shared Translation Task and submitted translations for English↔German and English↔French using a phrase-based decoder with lattice input.</S>
			<S sid ="6" ssid = "6">The paper is organized as follows: the next section gives a detailed description of our systems including all the models.</S>
			<S sid ="7" ssid = "7">The translation results for all directions are presented afterwards and we close with a conclusion.</S>
	</SECTION>
	<SECTION title="System Description. " number = "2">
			<S sid ="8" ssid = "1">The phrase table is based on a GIZA++ word alignment for the French↔English systems.</S>
			<S sid ="9" ssid = "2">For the German↔English systems we use a Discriminative Word Alignment (DWA) as described in Niehues and Vogel (2008).</S>
			<S sid ="10" ssid = "3">For every source phrase only the top 10 translation options are considered during decoding.</S>
			<S sid ="11" ssid = "4">The SRILM Toolkit (Stolcke, 2002) is used for training SRI language models using KneserNey smoothing.</S>
			<S sid ="12" ssid = "5">For the word reordering between languages, we used POS-based reordering models as described in Section 4.</S>
			<S sid ="13" ssid = "6">In addition to it, tree-based reordering model and lexicalized reordering were added for German↔English systems.</S>
			<S sid ="14" ssid = "7">An in-house phrase-based decoder (Vogel, 2003) is used to perform translation.</S>
			<S sid ="15" ssid = "8">The translation was optimized using Minimum Error Rate Training (MERT) as described in Venugopal et al.</S>
			<S sid ="16" ssid = "9">(2005) towards better BLEU (Papineni et al., 2002) scores.</S>
			<S sid ="17" ssid = "10">2.1 Data.</S>
			<S sid ="18" ssid = "11">The Europarl corpus (EPPS) and News Commentary (NC) corpus were used for training our translation models.</S>
			<S sid ="19" ssid = "12">We trained language models for each language on the monolingual part of the training corpora as well as the News Shuffle and the Gigaword corpora.</S>
			<S sid ="20" ssid = "13">The additional data such as web-crawled corpus, UN and Giga corpora were used after filtering.</S>
			<S sid ="21" ssid = "14">The filtering work for this data is discussed in Section 3.</S>
			<S sid ="22" ssid = "15">For the German↔English systems we use the news-test2010 set for tuning, while the news- test2011 set is used for the French↔English systems.</S>
			<S sid ="23" ssid = "16">For testing, news-test2012 set was used for all systems.</S>
			<S sid ="24" ssid = "17">2.2 Preprocessing.</S>
			<S sid ="25" ssid = "18">The training data is preprocessed prior to training the system.</S>
			<S sid ="26" ssid = "19">This includes normalizing special symbols, smart-casing the first word of each sentence and removing long sentences and sentence pairs with length mismatch.</S>
			<S sid ="27" ssid = "20">Compound splitting is applied to the German part of the corpus of the German→English system as described in Koehn and Knight (2003).</S>
	</SECTION>
	<SECTION title="Filtering of Noisy Pairs. " number = "3">
			<S sid ="28" ssid = "1">The filtering was applied on the corpora which are found to be noisy.</S>
			<S sid ="29" ssid = "2">Namely, the Giga English- French parallel corpus and the all the new web- crawled data . The operation was performed using 104 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104–108, Sofia, Bulgaria, August 89, 2013 Qc 2013 Association for Computational Linguistics an SVM classifier as in our past systems (Medi- ani et al., 2011).</S>
			<S sid ="30" ssid = "3">For each pair, the required lexica were extracted from Giza alignment of the corresponding EPPS and NC corpora.</S>
			<S sid ="31" ssid = "4">Furthermore, for the web-crawled data, higher precision classifiers were trained by providing a larger number of negative examples to the classifier.</S>
			<S sid ="32" ssid = "5">After filtering, we could still find English sentences in the other part of the corpus.</S>
			<S sid ="33" ssid = "6">Therefore, we performed a language identification (LID)- based filtering afterwards (performed only on the French-English corpora, in this participation).</S>
	</SECTION>
	<SECTION title="Word Reordering. " number = "4">
			<S sid ="34" ssid = "1">Word reordering was modeled based on POS sequences.</S>
			<S sid ="35" ssid = "2">For the German↔English system, reordering rules learned from syntactic parse trees were used in addition.</S>
			<S sid ="36" ssid = "3">4.1 POS-based Reordering Model.</S>
			<S sid ="37" ssid = "4">In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.</S>
			<S sid ="38" ssid = "5">As described in Rottmann and Vogel (2007), continuous reordering rules are extracted.</S>
			<S sid ="39" ssid = "6">This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with non- continuous rules (Niehues and Kolss, 2009), for German↔English systems.</S>
			<S sid ="40" ssid = "7">4.2 Tree-based Reordering Model.</S>
			<S sid ="41" ssid = "8">In addition to the POS-based reordering, we apply a tree-based reordering model for the German↔English translation to better address the differences in word order between German and English.</S>
			<S sid ="42" ssid = "9">We use the Stanford Parser (Rafferty and Manning, 2008) to generate syntactic parse trees for the source side of the training corpus.</S>
			<S sid ="43" ssid = "10">Then we use the word alignment between source and target language to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013).</S>
			<S sid ="44" ssid = "11">The POS-based and tree-based reordering rules are applied to each input sentence.</S>
			<S sid ="45" ssid = "12">The resulting reordered sentence variants as well as the original sentence order are encoded in a word lattice.</S>
			<S sid ="46" ssid = "13">The lattice is then used as input to the decoder.</S>
			<S sid ="47" ssid = "14">4.3 Lexicalized Reordering.</S>
			<S sid ="48" ssid = "15">The lexicalized reordering model stores the reordering probabilities for each phrase pair.</S>
			<S sid ="49" ssid = "16">Possible reordering orientations at the incoming and outgoing phrase boundaries are monotone, swap or discontinuous.</S>
			<S sid ="50" ssid = "17">With the POS- and tree-based reordering word lattices encode different reordering variants.</S>
			<S sid ="51" ssid = "18">In order to apply the lexicalized reordering model, we store the original position of each word in the lattice.</S>
			<S sid ="52" ssid = "19">At each phrase boundary at the end, the reordering orientation with respect to the original position of the words is checked.</S>
			<S sid ="53" ssid = "20">The probability for the respective orientation is included as an additional score.</S>
	</SECTION>
	<SECTION title="Translation Models. " number = "5">
			<S sid ="54" ssid = "1">In addition to the models used in the baseline system described above, we conducted experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation modeling process.</S>
			<S sid ="55" ssid = "2">5.1 Bilingual Language Model.</S>
			<S sid ="56" ssid = "3">During the decoding the source sentence is segmented so that the best combination of phrases which maximizes the scores is available.</S>
			<S sid ="57" ssid = "4">However, this causes some loss of context information at the phrase boundaries.</S>
			<S sid ="58" ssid = "5">In order to make bilingual context available, we use a bilingual language model (Niehues et al., 2011).</S>
			<S sid ="59" ssid = "6">In the bilingual language model, each token consists of a target word and all source words it is aligned to.</S>
			<S sid ="60" ssid = "7">5.2 Discriminative Word Lexicon.</S>
			<S sid ="61" ssid = "8">Mauser et al.</S>
			<S sid ="62" ssid = "9">(2009) introduced the Discriminative Word Lexicon (DWL) into phrase-based machine translation.</S>
			<S sid ="63" ssid = "10">In this approach, a maximum entropy model is used to determine the probability of using a target word in the translation.</S>
			<S sid ="64" ssid = "11">In this evaluation, we used two extensions to this work as shown in (Niehues and Waibel, 2013).</S>
			<S sid ="65" ssid = "12">First, we added additional features to model the order of the source words better.</S>
			<S sid ="66" ssid = "13">Instead of representing the source sentence as a bag-of-words, we used a bag-of-n-grams.</S>
			<S sid ="67" ssid = "14">We used n-grams up to the order of three and applied count filtering to the features for higher order n-grams.</S>
			<S sid ="68" ssid = "15">Furthermore, we created the training examples differently in order to focus on addressing errors of the other models of the phrase-based translation system.</S>
			<S sid ="69" ssid = "16">We first translated the whole corpus with a baseline system.</S>
			<S sid ="70" ssid = "17">Then we only used the words that occur in the N-Best List and not in the reference as negative examples instead of using all words that do not occur in the reference.</S>
			<S sid ="71" ssid = "18">5.3 Quasi-Morphological Operations.</S>
			<S sid ="72" ssid = "19">Because of the inflected characteristic of the German language, we try to learn quasi- morphological operations that change the lexical entry of a known word form to the out-of- vocabulary (OOV) word form as described in Niehues and Waibel (2012).</S>
			<S sid ="73" ssid = "20">5.4 Phrase Table Adaptation.</S>
			<S sid ="74" ssid = "21">For the French↔English systems, we built two phrase tables; one trained with all data and the other trained only with the EPPS and NC corpora.</S>
			<S sid ="75" ssid = "22">This is due to the fact that Giga corpus is big but noisy and EPPS and NC corpus are more reliable.</S>
			<S sid ="76" ssid = "23">The two models are combined log-linearly to achieve the adaptation towards the cleaner corpora as described in Niehues et al.</S>
			<S sid ="77" ssid = "24">(2010).</S>
	</SECTION>
	<SECTION title="Language Models. " number = "6">
			<S sid ="78" ssid = "1">The 4-gram language models generated by the SRILM toolkit are used as the main language models for all of our systems.</S>
			<S sid ="79" ssid = "2">For the English↔French systems, we use a good quality corpus as in-domain data to train in-domain language models.</S>
			<S sid ="80" ssid = "3">Additionally, we apply the POS and cluster language models in different systems.</S>
			<S sid ="81" ssid = "4">For the German→English system, we build separate language models using each corpus and combine them linearly before the decoding by minimizing the perplexity.</S>
			<S sid ="82" ssid = "5">Language models are integrated into the translation system by a log-linear combination and receive optimal weights during tuning by the MERT.</S>
			<S sid ="83" ssid = "6">6.1 POS Language Models.</S>
			<S sid ="84" ssid = "7">For the English→German system, we use the POS language model, which is trained on the POS sequence of the target language.</S>
			<S sid ="85" ssid = "8">The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.</S>
			<S sid ="86" ssid = "9">The RFTagger generates fine- grained tags which include person, gender, and case information.</S>
			<S sid ="87" ssid = "10">The language model is trained with up to 9-gram information, using the German side of the parallel EPPS and NC corpus, as well as the News Shuffle corpus.</S>
			<S sid ="88" ssid = "11">6.2 Cluster Language Models.</S>
			<S sid ="89" ssid = "12">In order to use larger context information, we use a cluster language model for all our systems.</S>
			<S sid ="90" ssid = "13">The cluster language model is based on the idea shown in Och (1999).</S>
			<S sid ="91" ssid = "14">Using the MKCLS algorithm, we cluster the words in the corpus, given a number of classes.</S>
			<S sid ="92" ssid = "15">Then words in the corpus are replaced with their cluster IDs.</S>
			<S sid ="93" ssid = "16">Using these cluster IDs, we train n-gram language models as well as a phrase table with this additional factor of cluster ID. Our submitted systems have diversed range of the number of clusters as well as n-gram.</S>
	</SECTION>
	<SECTION title="Results. " number = "7">
			<S sid ="94" ssid = "1">Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop.</S>
			<S sid ="95" ssid = "2">The results are reported as case- sensitive BLEU scores on one reference translation.</S>
			<S sid ="96" ssid = "3">7.1 German→English.</S>
			<S sid ="97" ssid = "4">The experiments for the German to English translation system are summarized in Table 1.</S>
			<S sid ="98" ssid = "5">The baseline system uses POS-based reordering, DWA with lattice phrase extraction and language models trained on the News Shuffle corpus and Giga corpus separately.</S>
			<S sid ="99" ssid = "6">Then we added a 5-gram cluster LM trained with 1,000 word classes.</S>
			<S sid ="100" ssid = "7">By adding a language model using the filtered crawled data we gained 0.3 BLEU on the test set.</S>
			<S sid ="101" ssid = "8">For this we combined all language models linearly.</S>
			<S sid ="102" ssid = "9">The filtered crawled data was also used to generate a phrase table, which brought another improvement of 0.85 BLEU.</S>
			<S sid ="103" ssid = "10">Applying tree-based reordering improved the BLEU score, and the performance had more gain by adding the extended DWL, namely using both bag-of-ngrams and n-best lists.</S>
			<S sid ="104" ssid = "11">While lexicalized reordering gave us a slight gain, we added morphological operation and gained more improvements.</S>
			<S sid ="105" ssid = "12">7.2 English→German.</S>
			<S sid ="106" ssid = "13">The English to German baseline system uses POS- based reordering and language models using parallel data (EPPS and NC) as shown in Table 2.</S>
			<S sid ="107" ssid = "14">Gradual gains were achieved by changing alignment from GIZA++ to DWA, adding a bilingual language model as well as a language model based on the POS tokens.</S>
			<S sid ="108" ssid = "15">A 9-gram cluster-based language model with 100 word classes gave us a System Dev Test in our best system.</S>
			<S sid ="109" ssid = "16">Baseline 24.15 22.79 + Cluster LM 24.18 22.84 System Dev Test + Cra wle d Dat a LM (C om b.)</S>
			<S sid ="110" ssid = "17">24 .5 3 23 .1 4 B as eli ne 30 .3 3 29 .3 5 + Cra wle d Dat a PT 25 .3 8 23 .9 9 + Cr aw le d D at a 30 .5 9 29 .9 3 + Tre e Rul es 25 .8 0 24 .1 6 + Bil in gu al an d Cl us ter L M s 30 .6 7 30 .0 1 + Ext end ed D WL 25 .5 9 24 .5 4 + In Do m ai n PT A da pt ati on 31 .1 7 30 .1 6 + Lex ical ize d Re ord eri ng 26 .0 4 24 .5 5 + D W L 31 .0 7 30 .4 0 + Morphological Operation - 24.62 Table 1: Translation results for German→English small gain.</S>
			<S sid ="111" ssid = "18">Improving the reordering using lexialized reordering gave us gain on the optimization set.</S>
			<S sid ="112" ssid = "19">Using DWL let us have more improvements on our test set.</S>
			<S sid ="113" ssid = "20">By using the filtered crawled data, we gained a big improvement of 0.46 BLEU on the test set.</S>
			<S sid ="114" ssid = "21">Then we extended the DWL with bag of n-grams and n-best lists to achieve additional improvements.</S>
			<S sid ="115" ssid = "22">Finally, the best system includes lattices generated using tree rules.</S>
			<S sid ="116" ssid = "23">Table 3: Translation results for French→English 7.4 English→French.</S>
			<S sid ="117" ssid = "24">In the baseline system, EPPS, NC, Giga and News Shuffle corpora are used for language modeling.</S>
			<S sid ="118" ssid = "25">The big phrase tables tailored EPPC, NC and Giga data.</S>
			<S sid ="119" ssid = "26">The system also uses short-range reordering trained on EPPS and NC.</S>
			<S sid ="120" ssid = "27">Adding parallel and filtered crawl data improves the system.</S>
			<S sid ="121" ssid = "28">It was further enhanced by the integration of a 4-gram bilingual language model.</S>
			<S sid ="122" ssid = "29">Moreover, the best configuration of 9-gram language model trained on 500 clusters of French texts gains 0.25 BLEU points improvement.</S>
			<S sid ="123" ssid = "30">We also conducted phrase-table adaptation from the general one into the domain covered by EPPS and NC data and it helps as well.</S>
			<S sid ="124" ssid = "31">The initial tryout with lexicalized reordering feature showed an improvement of 0.23 points on the development set, but a surprising reduction on the test set, thus we decided to take the system after adaptation as our best English→French system.</S>
			<S sid ="125" ssid = "32">Table 2: Translation results for English→German 7.3 French→English.</S>
			<S sid ="126" ssid = "33">Table 3 reports some remarkable improvements as we combined several techniques on the French→English direction.</S>
			<S sid ="127" ssid = "34">The baseline system was trained on parallel corpora such as EPPS, NC and Giga, while the language model was trained on the English part of those corpora plus News Shuffle.</S>
			<S sid ="128" ssid = "35">The newly presented web-crawled data helps to achieve almost 0.6 BLEU points more on test set.</S>
			<S sid ="129" ssid = "36">Adding bilingual language model and cluster language model does not show a significant impact.</S>
			<S sid ="130" ssid = "37">Further gains were achieved by the adaptation of in-domain data into general-theme phrase table, bringing 0.15 BLEU better on the test set.</S>
			<S sid ="131" ssid = "38">When we added the DWL feature, it notably improves the system by 0.25 BLEU points, resulting Table 4: Translation results for English→French</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "8">
			<S sid ="132" ssid = "1">We have presented the systems for our participation in the WMT 2013 Evaluation for English↔German and English↔French.</S>
			<S sid ="133" ssid = "2">All systems use a class-based language model as well as a bilingual language model.</S>
			<S sid ="134" ssid = "3">Using a DWL with source context improved the translation quality of English↔German systems.</S>
			<S sid ="135" ssid = "4">Also for these systems, we could improve even more with a tree-based reordering model.</S>
			<S sid ="136" ssid = "5">Special handling of OOV words improved German→English system, while for the inverse direction the language model with fine-grained POS tags was helpful.</S>
			<S sid ="137" ssid = "6">For English↔French, phrase table adaptation helps to avoid using wrong parts of the noisy Giga corpus.</S>
	</SECTION>
	<SECTION title="Acknowledgements. " number = "9">
			<S sid ="138" ssid = "1">This work was partly achieved as part of the Quaero Programme, funded by OSEO, French State agency for innovation.</S>
			<S sid ="139" ssid = "2">The research leading to these results has received funding from the European Union Seventh Framework Pro- gramme (FP7/20072013) under grant agreement n◦ 287658.</S>
	</SECTION>
</PAPER>
