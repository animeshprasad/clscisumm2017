<PAPER>
<S sid ="0">Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging</S>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.</S>
		<S sid ="2" ssid = "2">It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.</S>
		<S sid ="3" ssid = "3">In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tˆN = tˆ1, ..., tˆN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.</S>
			<S sid ="5" ssid = "5">Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.</S>
			<S sid ="6" ssid = "6">For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.</S>
			<S sid ="7" ssid = "7">The additional information may also help to disambiguate the (base) part of speech.</S>
			<S sid ="8" ssid = "8">Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das Realita¨ t?</S>
			<S sid ="9" ssid = "9">(Is that reality?).</S>
			<S sid ="10" ssid = "10">The word das is ambiguous between an article and a demonstrative.</S>
			<S sid ="11" ssid = "11">Because of the lack of gender agreement between das (neuter) and the noun Realita¨ t (feminine), the article reading must be wrong.</S>
			<S sid ="12" ssid = "12">The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).</S>
			<S sid ="13" ssid = "13">Large tagsets aggravate sparse data problems.</S>
			<S sid ="14" ssid = "14">As an example, take the German sentence Das zu versteuernde Einkommen sinkt (“The to be taxed income decreases”; The tˆN N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).</S>
			<S sid ="15" ssid = "15">This sentence should be tagged as shown in table 1.</S>
			<S sid ="16" ssid = "16">Das ART.Def.Nom.Sg.Neut zu PART.Zu versteuernde ADJA.Pos.Nom.Sg.Neut Einkommen N.Reg.Nom.Sg.Neut p(tN , wN ) = n 1 1 i=1 p(ti|ti−1 ) i−k p(wi|ti) le .</S>
			<S sid ="17" ssid = "17">(1) context prob.</S>
			<S sid ="18" ssid = "18">xical prob HMM taggers are fast and were successfully applied to a wide range of languages and training corpora.</S>
			<S sid ="19" ssid = "19">Qc 2008.</S>
			<S sid ="20" ssid = "20">Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).</S>
			<S sid ="21" ssid = "21">Some rights reserved.</S>
			<S sid ="22" ssid = "22">Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.</S>
			<S sid ="23" ssid = "23">Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.</S>
			<S sid ="24" ssid = "24">(Neither does the pair consisting of the first two tags.)</S>
			<S sid ="25" ssid = "25">The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777–784 Manchester, August 2008 context probability of the third POS tag is therefore 0.</S>
			<S sid ="26" ssid = "26">If the probability is smoothed with the backoff distribution p(•|P ART .Z u), the most probable tag is ADJA.Pos.Acc.Sg.Fem rather than ADJA.Pos.Nom.Sg.Neut.</S>
			<S sid ="27" ssid = "27">Thus, the agreement between the article and the adjective is not checked anymore.</S>
			<S sid ="28" ssid = "28">A closer inspection of the Tiger corpus reveals that it actually contains all the information needed to completely disambiguate each component of the POS tag ADJA.Pos.Nom.Sg.Neut: • All words appearing after an article (ART) and the infinitive particle zu (PART.zu) are attributive adjectives (ADJA) (10 of 10 cases).</S>
			<S sid ="29" ssid = "29">• All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).</S>
			<S sid ="30" ssid = "30">• All adjectives appearing after a nominative article and a particle have nominative case (11 of 11 cases).</S>
			<S sid ="31" ssid = "31">• All adjectives appearing after a singular article and a particle are singular (32 of 32 cases).</S>
			<S sid ="32" ssid = "32">• All adjectives appearing after a neuter article and a particle are neuter (4 of 4 cases).</S>
			<S sid ="33" ssid = "33">By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) ∗ p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) ∗ p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) ∗ p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) ∗ p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the ∗ p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) ∗ p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1.</S>
			<S sid ="34" ssid = "34">Hence the context probability of the whole tag is. also 1.</S>
			<S sid ="35" ssid = "35">Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context.</S>
			<S sid ="36" ssid = "36">These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute.</S>
			<S sid ="37" ssid = "37">Decision trees are ideal for this task because the identification of relevant attribute combinations is at the heart of this method.</S>
			<S sid ="38" ssid = "38">The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here.</S>
			<S sid ="39" ssid = "39">Discriminatively trained taggers, on the other hand, have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature.</S>
	</SECTION>
	<SECTION title="Decision Trees. " number = "2">
			<S sid ="40" ssid = "1">Decision trees (Breiman et al., 1984; Quinlan, 1993) are normally used as classifiers, i.e. they assign classes to objects which are represented as attribute vectors.</S>
			<S sid ="41" ssid = "2">The non-terminal nodes are labeled with attribute tests, the edges with the possible outcomes of a test, and the terminal nodes are labeled with classes.</S>
			<S sid ="42" ssid = "3">An object is classified by evaluating the test of the top node on the object, following the respective edge to a daughter node, evaluating the test of the daughter node, and so on until a terminal node is reached whose class is assigned to the object.</S>
			<S sid ="43" ssid = "4">Decision Trees are turned into probability estimation trees by storing a probability for each possible class at the terminal nodes instead of a single result class.</S>
			<S sid ="44" ssid = "5">Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.</S>
			<S sid ="45" ssid = "6">2.1 Induction of Decision Trees.</S>
			<S sid ="46" ssid = "7">Decision trees are incrementally built by first selecting the test which splits the manually annotated training sample into the most homogeneous subsets with respect to the class.</S>
			<S sid ="47" ssid = "8">This test, which maximizes the information gain1 wrt.</S>
			<S sid ="48" ssid = "9">the class, is following expression for the context probability: 1 The information gain measures how much the test de-.</S>
			<S sid ="49" ssid = "10">p(ADJA | ART, PART.Zu) ∗ p(Pos | 2:ART, 1:PART, 0:ADJA) ∗ p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA) creases the uncertainty about the class.</S>
			<S sid ="50" ssid = "11">It is the difference between the entropy of the empirical distribution of the class variable in the training set and the weighted average entropy yes 0:N.Name yes no 1:ART.Nom no 1:ADJA.Nom yes no which returns a probability of 0.3.</S>
			<S sid ="51" ssid = "12">The third tree for neuter has one non terminal and two terminal nodes returning a probability of 0.3 and 0.5, respectively.</S>
			<S sid ="52" ssid = "13">The sum of probabilities is therefore either 0.9 or 1.1, but never exactly 1.</S>
			<S sid ="53" ssid = "14">This problem 2:N.Reg p=0.999 0:N.Name 0:N.Name yes no p=0.571 p=0.938 yes no p=0.948 p=0.998 .... is solved by renormalizing the probabilities.</S>
			<S sid ="54" ssid = "15">The probability of an attribute (such as “Nom”) is always conditioned on the respective base POS (such as “N”) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns.</S>
			<S sid ="55" ssid = "16">The test 1:ART.Nom checks if the preceding word is a nominative article.</S>
			<S sid ="56" ssid = "17">assigned to the top node.</S>
			<S sid ="57" ssid = "18">The tree is recursively expanded by selecting the best test for each subset and so on, until all objects of the current subset belong to the same class.</S>
			<S sid ="58" ssid = "19">In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data.</S>
			<S sid ="59" ssid = "20">Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.</S>
			<S sid ="60" ssid = "21">The motivation was that a tree which predicts a single value (say verb) does not fragment the data with tests which are only relevant for the distinction of two other values (e.g. article and possessive pronoun).2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.) The tree induction algorithm only considers binary tests, which check whether some particular attribute is present or not.</S>
			<S sid ="61" ssid = "22">The best test for each node is selected with the standard information gain criterion.</S>
			<S sid ="62" ssid = "23">The recursive tree building process terminates if the information gain is 0.</S>
			<S sid ="63" ssid = "24">The decision tree is pruned with the pruning criterion described below.</S>
			<S sid ="64" ssid = "25">Since the tagger creates a separate tree for each attribute, the probabilities of a set of competing attributes such as masculine, feminine, and neuter will not exactly sum up to 1.</S>
			<S sid ="65" ssid = "26">To understand why, assume that there are three trees for the gender attributes.</S>
			<S sid ="66" ssid = "27">Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS.</S>
			<S sid ="67" ssid = "28">All context attributes other than the base POS are always used in combination with the base POS.</S>
			<S sid ="68" ssid = "29">A typical context attribute is “1:ART.Nom” which states that the preceding tag is an article with the attribute “Nom”.</S>
			<S sid ="69" ssid = "30">“1:ART” is also a valid attribute specification, but “1:Nom” is not.</S>
			<S sid ="70" ssid = "31">The tagger further restricts the set of possible test attributes by requiring that some attribute of the POS tag at position i-k (i=position of the predicted POS tag, k ≥ 1) must have been used be fore an attribute of the POS tag at position i-(k+1) may be examined.</S>
			<S sid ="71" ssid = "32">This restriction improved the tagging accuracy for large contexts.</S>
			<S sid ="72" ssid = "33">2.2 Pruning Criterion.</S>
			<S sid ="73" ssid = "34">The tagger applies3 the critical-value pruning strategy proposed by (Mingers, 1989).</S>
			<S sid ="74" ssid = "35">A node is pruned if the information gain of the best test multiplied by the size of the data subsample is below a given threshold.</S>
			<S sid ="75" ssid = "36">To illustrate the pruning, assume that D is the data of the current node with 50 positive and 25 negative elements, and that D1 (with 20 positive and 20 negative elements) and D2 (with 30 positive and 5 negative elements) are the two subsets induced by the best test.</S>
			<S sid ="76" ssid = "37">The entropy of D is −2/3 log22/3 − 1/3 log21/3 = 0.92, the entropy of D1 is −1/2 log21/2−1/2 log21/2 = 1, and the entropy of D2 is −6/7 log26/7 − 1/7 log21/7 = 0.59.</S>
			<S sid ="77" ssid = "38">The information gain is therefore 0.92 − (8/15 ∗ 1 − 7/15 ∗ 0.59) = 0.11.</S>
			<S sid ="78" ssid = "39">The resulting score is 75 ∗ 0.11 = 8.25.</S>
			<S sid ="79" ssid = "40">Given a threshold of 6, the node is therefore not pruned.</S>
			<S sid ="80" ssid = "41">We experimented with pre-pruning (where a node is always pruned if the gain is below the in the two subsets.</S>
			<S sid ="81" ssid = "42">The weight of each subset is proportional to its size.</S>
			<S sid ="82" ssid = "43">2 We did not directly compare the two alternatives (two- valued vs. multi-valued tests), because the implementational effort required would have been too large.</S>
			<S sid ="83" ssid = "44">3 We also experimented with a pruning criterion based on binomial tests, which returned smaller trees with a slightly lower accuracy, although the difference in accuracy was never larger than 0.1% for any context size.</S>
			<S sid ="84" ssid = "45">Thus, the simpler pruning strategy presented here was chosen.</S>
			<S sid ="85" ssid = "46">threshold) as well as post-pruning (where a node is only pruned if its sub-nodes are terminal nodes or pruned nodes).</S>
			<S sid ="86" ssid = "47">The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold.</S>
			<S sid ="87" ssid = "48">A threshold of 6 consistently produced optimal or near optimal results for pre-pruning.</S>
			<S sid ="88" ssid = "49">Thus, pre-pruning with a threshold of 6 was used in the experiments.</S>
	</SECTION>
	<SECTION title="Splitting of the POS Tags. " number = "3">
			<S sid ="89" ssid = "1">The tagger treats dots in POS tag labels as attribute separators.</S>
			<S sid ="90" ssid = "2">The first attribute of a POS tag is the main category.</S>
			<S sid ="91" ssid = "3">The number of additional attributes is fixed for each main category.</S>
			<S sid ="92" ssid = "4">The additional attributes are category-specific.</S>
			<S sid ="93" ssid = "5">The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature.</S>
			<S sid ="94" ssid = "6">The attributes occurring at a certain position constitute the value set of the feature.</S>
	</SECTION>
	<SECTION title="Our Tagger. " number = "4">
			<S sid ="95" ssid = "1">Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.</S>
			<S sid ="96" ssid = "2">The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p(ti|ti−1 ) is internally computed as a product of attribute probabili ties.</S>
			<S sid ="97" ssid = "3">In order to increase the speed, the tagger also applies a beam-search strategy which prunes all search paths whose probability is below the probability of the best path times a threshold.</S>
			<S sid ="98" ssid = "4">With athreshold of 10−3 or lower, the influence of prun ing on the tagging accuracy was negligible.</S>
			<S sid ="99" ssid = "5">4.1 Supplementary Lexicon.</S>
			<S sid ="100" ssid = "6">The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.</S>
			<S sid ="101" ssid = "7">If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags.</S>
			<S sid ="102" ssid = "8">The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities.</S>
			<S sid ="103" ssid = "9">If the word w was observed with N different tags, and f (w, t) is the joint frequency of w and POS tag t, and p(t|[w]) is the average probability of t among words with the same set of possible tags as w, then the smoothed probability of t given w is defined as follows: f (w, t) + N p(t|[w]) the preceding attributes of the predicted POS tag is estimated with a decision tree as described be p(t|w) = f (w) + N fore.</S>
			<S sid ="104" ssid = "10">The probabilities at the terminal nodes of the decision trees are smoothed with the parent node probabilities (which themselves were smoothed in the same way).</S>
			<S sid ="105" ssid = "11">The smoothing is implemented by adding the weighted class probabilities pp(c) of the parent node to the frequencies f (c) before normalizing them to probabilities: p(c) = f (c) + αpp(c) α + �c f (c) The weight α was fixed to 1 after a few experiments on development data.</S>
			<S sid ="106" ssid = "12">This smoothing strategy is closely related to Witten-Bell smoothing.</S>
			<S sid ="107" ssid = "13">The probabilities are normalized by dividing them by the total probability of all attribute values of the respective feature (see section 2.1).</S>
			<S sid ="108" ssid = "14">The best tag sequence is computed with the Viterbi algorithm.</S>
			<S sid ="109" ssid = "15">The main differences of our tag- ger to a standard trigram tagger are that the order of the Markov model (the k in equation 1) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom.The smoothed estimates of p(tag|word) are di vided by the prior probability p(tag) of the tag and used instead of p(word|tag).5 4.2 Unknown Words.</S>
			<S sid ="110" ssid = "16">The lexical probabilities of unknown words are obtained as follows: The unknown words are divided into four disjoint classes6 with numeric expressions, words starting with an uppercase letter, words starting with a lowercase letter, and a fourth class for the other words.</S>
			<S sid ="111" ssid = "17">The tagger builds a suffix trie for each class of unknown words using the known word types from that class.</S>
			<S sid ="112" ssid = "18">The maximal length of the suffixes is 7.</S>
			<S sid ="113" ssid = "19">The suffix tries are pruned until (i) all suffixes have a frequency of at least 5 and (ii) the information gain multiplied by the suffix frequency and di 5 p(word|tag) is equal to p(tag|word)p(word)/p(tag) and p(word) is a constant if the tokenization is unambiguous.</S>
			<S sid ="114" ssid = "20">Therefore dropping the factor p(word) has no influence on the ranking of the different tag sequences.</S>
			<S sid ="115" ssid = "21">6 In earlier experiments, we had used a much larger number of word classes.</S>
			<S sid ="116" ssid = "22">Decreasing their number to 4 turned out to be better.</S>
			<S sid ="117" ssid = "23">a threshold of 1.</S>
			<S sid ="118" ssid = "24">More precisely, if Tα is the set of POS tags that occurred with suffix α, |T | is the size of the set T , fα is the frequency of suffix α, and pα(t) is the probability of POS tag t among the words with suffix α, then the following condition must hold: tion between definite and indefinite articles, and the distinction between hyphens, slashes, left and right parentheses, quotation marks, and other symbols which the Tiger treebank annotates with “$(”.</S>
			<S sid ="119" ssid = "25">A supplementary lexicon was created by analyzing a word list which included all words from the faα paα (t) log paα(t) &lt; 1 training, development, and test data with a German computationa l morphology.</S>
			<S sid ="120" ssid = "26">The analyses gener |Taα| t∈Taα pα(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "5">
			<S sid ="121" ssid = "1">Our tagger was first evaluated on data from the German Tiger treebank.</S>
			<S sid ="122" ssid = "2">The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gime´nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.</S>
			<S sid ="123" ssid = "3">Therefore it was not possible to optimize the parameters systematically.</S>
			<S sid ="124" ssid = "4">We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (Hladka´ et al., 2007) and compared to the TnT tag- ger.</S>
			<S sid ="125" ssid = "5">5.1 Tiger Corpus.</S>
			<S sid ="126" ssid = "6">The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens.</S>
			<S sid ="127" ssid = "7">It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.</S>
			<S sid ="128" ssid = "8">After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left.</S>
			<S sid ="129" ssid = "9">The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.</S>
			<S sid ="130" ssid = "10">Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23.</S>
			<S sid ="131" ssid = "11">Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.</S>
			<S sid ="132" ssid = "12">Some lexically decidable distinctions missing in the Tiger corpus have been tagset.</S>
			<S sid ="133" ssid = "13">Note that only the words, but not the POS tags from the test and development data were used, here.</S>
			<S sid ="134" ssid = "14">Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed.</S>
			<S sid ="135" ssid = "15">In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown.</S>
			<S sid ="136" ssid = "16">This strategy returned the best results on the development data.</S>
			<S sid ="137" ssid = "17">In case of the SVM- Tool, we were not able to successfully integrate the supplementary lexicon.</S>
			<S sid ="138" ssid = "18">5.1.1 Refined Tagset Prepositions are not annotated with case in the Tiger treebank, although this information is important for the disambiguation of the case of the next noun phrase.</S>
			<S sid ="139" ssid = "19">In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc).</S>
			<S sid ="140" ssid = "20">Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in).</S>
			<S sid ="141" ssid = "21">The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or).</S>
			<S sid ="142" ssid = "22">For evaluation purposes, the refined tags are mapped back to the original tags.</S>
			<S sid ="143" ssid = "23">This mapping is unambiguous.</S>
			<S sid ="144" ssid = "24">7 It was planned to include also the Stanford tagger.</S>
			<S sid ="145" ssid = "25">(Toutanova et al., 2003) in this comparison, but it was not possible to train it on the Tiger data.</S>
			<S sid ="146" ssid = "26">8 In German, the genitive case of arguments is more and.</S>
			<S sid ="147" ssid = "27">more replaced by the dative.</S>
			<S sid ="148" ssid = "28">Table 2: Tagging accuracies on development data in percent.</S>
			<S sid ="149" ssid = "29">Results for 2 and for 10 preceding POS tags as context are reported for our tagger.</S>
			<S sid ="150" ssid = "30">much smaller.</S>
			<S sid ="151" ssid = "31">Table 3 shows the results of an evaluation based on the plain STTS tagset.</S>
			<S sid ="152" ssid = "32">The first result was obtained with TnT trained on Tiger data which was mapped to STTS before.</S>
			<S sid ="153" ssid = "33">The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output is mapped to STTS.</S>
			<S sid ="154" ssid = "34">The third row gives the corresponding figures for our tagger.</S>
			<S sid ="155" ssid = "35">5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data.</S>
			<S sid ="156" ssid = "36">The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.</S>
			<S sid ="157" ssid = "37">The TnT tagger achieves 86.3% accuracy on the default tagset.</S>
			<S sid ="158" ssid = "38">A tag is considered correct if all attributes are correct.</S>
			<S sid ="159" ssid = "39">The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%.</S>
			<S sid ="160" ssid = "40">The SVMTool is slightly better than the TnT tagger on the default tagset, but shows little improvement from the tagset refinement.</S>
			<S sid ="161" ssid = "41">Apparently, the lexical features used by the SVMTool encode most of the information of the tagset refinement.</S>
			<S sid ="162" ssid = "42">With a context of two preceding POS tags (similar to the trigram tagger TnT), our tagger outperforms TnT by 0.7% on the default tagset, by 1% on the refined tagset, and by 1.1% on the refined tagset plus the additional lexicon.</S>
			<S sid ="163" ssid = "43">A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively.</S>
			<S sid ="164" ssid = "44">de fa ult refined ref.+lexicon T n T S T T S T n T Ti g e r 1 0 t a g s 9 7.</S>
			<S sid ="165" ssid = "45">2 8 9 7.</S>
			<S sid ="166" ssid = "46">1 7 97.26 97.51 9 7.</S>
			<S sid ="167" ssid = "47">3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.</S>
			<S sid ="168" ssid = "48">These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features.</S>
			<S sid ="169" ssid = "49">This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization.</S>
			<S sid ="170" ssid = "50">Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size.</S>
			<S sid ="171" ssid = "51">The best results are obtained with a context size of 10.</S>
			<S sid ="172" ssid = "52">What type of information is relevant across a distance of ten words?</S>
			<S sid ="173" ssid = "53">A good example is the decision tree for the attribute first person of finite verbs, which looks for a first person pronoun at positions -1 through -10 (relative to the position of the current word) in this order.</S>
			<S sid ="174" ssid = "54">Since German is a verb-final language, these tests clearly make sense.</S>
			<S sid ="175" ssid = "55">Table 4 shows the performance on the test data.</S>
			<S sid ="176" ssid = "56">Our tagger was used with a context size of 10.</S>
			<S sid ="177" ssid = "57">The suffix length parameter of the TnT tagger was set to 6 without lexicon and to 3 with lexicon.</S>
			<S sid ="178" ssid = "58">These values were optimal on the development data.</S>
			<S sid ="179" ssid = "59">The accuracy of our tagger is lower than on the development data.</S>
			<S sid ="180" ssid = "60">This could be due to the higher rate of unknown words (10.0% vs. 7.7%).</S>
			<S sid ="181" ssid = "61">Relative to the TnT tagger, however, the accuracy is quite similar for test and development data.</S>
			<S sid ="182" ssid = "62">The differences between the two taggers are significant.10 ta gg er de fa ult refined ref.+lexicon Tn T ou r ta gg er 8 3.</S>
			<S sid ="183" ssid = "63">4 5 84.11 89.14 8 5.</S>
			<S sid ="184" ssid = "64">0 0 85.92 91.07 Table 4: Tagging accuracies on test data.</S>
			<S sid ="185" ssid = "65">By far the most frequent tagging error was the confusion of nominative and accusative case.</S>
			<S sid ="186" ssid = "66">If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger.</S>
			<S sid ="187" ssid = "67">The resulting score of a binomial test is below 0.001.</S>
			<S sid ="188" ssid = "68">this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.</S>
			<S sid ="189" ssid = "69">Our tagger is quite fast, although not as fast as the TnT tagger.</S>
			<S sid ="190" ssid = "70">With a context size of 3 (10), it annotates 7000 (2000) tokens per second on a computer with an Athlon X2 4600 CPU.</S>
			<S sid ="191" ssid = "71">The training with a context size of 10 took about 4 minutes.</S>
			<S sid ="192" ssid = "72">5.2 Czech Academic Corpus.</S>
			<S sid ="193" ssid = "73">We also evaluated our tagger on the Czech Academic corpus (Hladka´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.</S>
			<S sid ="194" ssid = "74">The data was divided into 80% training data, 10% development data and 10% test data.</S>
			<S sid ="195" ssid = "75">89 88.9 88.8 Provost &amp; Domingos (2003) noted that well- known decision tree induction algorithms such as C4.5 (Quinlan, 1993) or CART (Breiman et al., 1984) fail to produce accurate probability estimates.</S>
			<S sid ="196" ssid = "76">They proposed to grow the decision trees to their maximal size without pruning, and to smooth the probability estimates with add-1 smoothing (also known as the Laplace correction).</S>
			<S sid ="197" ssid = "77">Ferri et al.</S>
			<S sid ="198" ssid = "78">(2003) describe a more complex backoff smoothing method.</S>
			<S sid ="199" ssid = "79">Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0).</S>
			<S sid ="200" ssid = "80">Another difference is that we used N two- class trees with normalization to predict the probabilities of N classes.</S>
			<S sid ="201" ssid = "81">These two-class trees can be pruned with a fixed pruning threshold.</S>
			<S sid ="202" ssid = "82">Hence there is no need to put aside training data for parameter tuning.</S>
			<S sid ="203" ssid = "83">88.7 88.6 88.5 ’ c o n t e x t d a t a 2 ’ 2 3 4 5 6 7 8 9 10 A n ope n que stio n is wh eth er the SV MT ool (or oth er dis cri min ativ ely trai ned tag ger s) cou ld out - perf orm the pre sen ted tag ger if the sa me dec om positi on of PO S tag s and the sa me con text size wasFigure 3: Accuracy on development data depend ing on context size The best accuracy of our tagger on the development set was 88.9% obtained with a context of 4 preceding POS tags.</S>
			<S sid ="204" ssid = "84">The best accuracy of the TnT tagger was 88.2% with a maximal suffix length of 5.</S>
			<S sid ="205" ssid = "85">The corresponding figures for the test data are.</S>
			<S sid ="206" ssid = "86">89.53% for our tagger and 88.88% for the TnT tag- ger.</S>
			<S sid ="207" ssid = "87">The difference is significant.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "6">
			<S sid ="208" ssid = "1">Our tagger combines two ideas, the decomposition of the probability of complex POS tags into a product of feature probabilities, and the estimation of the conditional probabilities with decision trees.</S>
			<S sid ="209" ssid = "2">A similar idea was previously presented in Kempe (1994), but apparently never applied again.</S>
			<S sid ="210" ssid = "3">The tagging accuracy reported by Kempe was below that of a traditional trigram tagger.</S>
			<S sid ="211" ssid = "4">Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.</S>
			<S sid ="212" ssid = "5">Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability.</S>
			<S sid ="213" ssid = "6">Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model.</S>
			<S sid ="214" ssid = "7">used.</S>
			<S sid ="215" ssid = "8">We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree, but we doubt that it is possible to train the SVMTool (or other discriminatively trained tag- gers) without such a restriction given the difficulties to train it with the standard context size.</S>
			<S sid ="216" ssid = "9">Czech POS tagging has been extensively studied in the past (Hajicˇ and Vidova´-Hladka´, 1998; Hajicˇ et al., 2001; Votrubec, 2006).</S>
			<S sid ="217" ssid = "10">Spoustov et al.</S>
			<S sid ="218" ssid = "11">(2007) compared several POS taggers including an n-gram tagger and a discriminatively trained tagger (Morcˇe), and evaluated them on the Prague Dependency Treebank (PDT 2.0).</S>
			<S sid ="219" ssid = "12">Morcˇe’s tagging accuracy was 95.12%, 0.3% better than the n-gram tagger.</S>
			<S sid ="220" ssid = "13">A hybrid system based on four different tagging methods reached an accuracy of 95.68%.</S>
			<S sid ="221" ssid = "14">Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult.</S>
			<S sid ="222" ssid = "15">Furthermore, our tagger uses no corpus-specific heuristics, whereas Morcˇe e.g. is optimized for Czech POS tagging.</S>
			<S sid ="223" ssid = "16">The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.</S>
	</SECTION>
	<SECTION title="Summary. " number = "7">
			<S sid ="224" ssid = "1">We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.</S>
			<S sid ="225" ssid = "2">In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).</S>
	</SECTION>
</PAPER>
