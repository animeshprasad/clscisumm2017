<S sid ="2" ssid = "2">It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.</S>
<S sid ="3" ssid = "3">Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.</S>
<S sid ="4" ssid = "4">In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.</S>
<S sid ="6" ssid = "6">A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.</S>
<S sid ="9" ssid = "9">We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.</S>
<S sid ="11" ssid = "11">We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).</S>
S sid ="48" ssid = "7">Such constraints are derived from training data, expressing some relationship between features and outcome.</S>
<S sid ="61" ssid = "1">The features we used can be divided into 2 classes: local and global.</S> | <S sid ="62" ssid = "2">Local features are features that are based on neighboring tokens, as well as the token itself.</S>
<S sid ="62" ssid = "2">Local features are features that are based on neighboring tokens, as well as the token itself.</S>
<S sid ="63" ssid = "3">Global features are extracted from other occurrences of the same token in the whole document.</S> 
<S sid ="68" ssid = "8">In the maximum entropy framework, there is no such constraint.</S>
<S sid ="69" ssid = "9">Multiple features can be used for the same token.</S>
<S sid ="86" ssid = "26">Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.</S>
<S sid ="102" ssid = "42">For all lists except locations, the lists are processed into a list of tokens (unigrams).</S>
<S sid ="103" ssid = "43">Location list is processed into a list of unigrams and bigrams (e.g., New York).</S>
<S sid ="104" ssid = "44">For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.</S> 
<S sid ="136" ssid = "76">The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.</S>
<S sid ="137" ssid = "77">For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.</S>
<S sid ="196" ssid = "1">We have shown that the maximum entropy framework is able to use global information directly.</S>