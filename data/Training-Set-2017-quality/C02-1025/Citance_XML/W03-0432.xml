<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a named entity recognition and classification system that uses only probabilistic character-level features.</S>
		<S sid ="2" ssid = "2">Classifications by multiple orthographic tries are combined in a hidden Markov model framework to incorporate both internal and contextual evidence.</S>
		<S sid ="3" ssid = "3">As part of the system, we perform a preprocessing stage in which capitalisation is restored to sentence-initial and all-caps words with high accuracy.</S>
		<S sid ="4" ssid = "4">We report f-values of 86.65 and 79.78 for English, and 50.62 and 54.43 for the German datasets.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Language independent NER requires the development of a metalinguistic model that is sufficiently broad to accommodate all languages, yet can be trained to exploit the specific features of the target language.</S>
			<S sid ="6" ssid = "6">Our aim in this paper is to investigate the combination of a character- level model, orthographic tries, with a sentence-level hidden Markov model.</S>
			<S sid ="7" ssid = "7">The local model uses affix information from a word and its surrounds to classify each word independently, and relies on the sentence-level model to determine a correct state sequence.</S>
			<S sid ="8" ssid = "8">Capitalisation is an often-used discriminator for NER, but can be misleading in sentence-initial or all-caps text.</S>
			<S sid ="9" ssid = "9">We choose to use a model that makes no assumptions about the capitalisation scheme, or indeed the character set, of the target language.</S>
			<S sid ="10" ssid = "10">We solve the problem of misleading case in a novel way by removing the effects of sentence-initial or all-caps capitalisation.</S>
			<S sid ="11" ssid = "11">This results in a simpler language model and easier recognition of named entities while remaining strongly language independent.</S>
	</SECTION>
	<SECTION title="Probabilistic Classification using. " number = "2">
			<S sid ="12" ssid = "1">Orthographic Tries Tries are an efficient data structure for capturing statistical differences between strings in different categories.</S>
			<S sid ="13" ssid = "2">In an orthographic trie, a path from the root through n nodes represents a string a1 a2 . . .</S>
			<S sid ="14" ssid = "3">an . The nth node in the path stores the occurrences (frequency) of the string a1 a2 . . .</S>
			<S sid ="15" ssid = "4">an in each word category.</S>
			<S sid ="16" ssid = "5">These frequencies can be used to calculate probability estimatesP (c | a1 a2 . . .</S>
			<S sid ="17" ssid = "6">an ) for each category c. Tries have previ ously been used in both supervised (Patrick et al., 2002) and unsupervised (Cucerzan and Yarowsky, 1999) named entity recognition.</S>
			<S sid ="18" ssid = "7">Each node in an orthographic trie stores the cumulative frequency information for each category in which a given string of characters occurs.</S>
			<S sid ="19" ssid = "8">A heterogeneous node represents a string that occurs in more than one category, while a homogeneous node represents a string that occurs in only one category.</S>
			<S sid ="20" ssid = "9">If a string a1 a2 . . .</S>
			<S sid ="21" ssid = "10">an occurs in only one category, all longer strings a1 a2 . . .</S>
			<S sid ="22" ssid = "11">an . . .</S>
			<S sid ="23" ssid = "12">an+k are also of the same category.</S>
			<S sid ="24" ssid = "13">This redundancy can be exploited when constructing a trie.</S>
			<S sid ="25" ssid = "14">We build minimum- depth MD-tries which have the condition that all nodes are heterogeneous, and all leaves are homogeneous.</S>
			<S sid ="26" ssid = "15">MD- tries are only as large as is necessary to capture the differences between categories, and can be built efficiently to large depths.</S>
			<S sid ="27" ssid = "16">MD-tries have been shown to give better performance than a standard trie with the same number of nodes (Whitelaw and Patrick, 2002).</S>
			<S sid ="28" ssid = "17">Given a string a1 a2 . . .</S>
			<S sid ="29" ssid = "18">an and a category c an orthographic trie yields a set of relative probabilities P (c | a1 ), P (c | a1 a2 ), . . ., P (c | a1 a2 . . .</S>
			<S sid ="30" ssid = "19">an ).</S>
			<S sid ="31" ssid = "20">The probability that a string indicates a particular class is estimated along the whole trie path, which helps to smooth scores for rare strings.</S>
			<S sid ="32" ssid = "21">The contribution of each level in the trie is governed by a linear weighting function of the form n P (c | a1 a2 . . .</S>
			<S sid ="33" ssid = "22">an ) = ) λi P (c | a1 a2 . . .</S>
			<S sid ="34" ssid = "23">ai ) i=1 n where λi ∈ [0, 1] and ) λi = 1 i=1 Tries are highly language independent.</S>
			<S sid ="35" ssid = "24">They make no assumptions about character set, or the relative importance of different parts of a word or its context.</S>
			<S sid ="36" ssid = "25">Tries use a progressive back-off and smoothing model that is well suited to the classification of previously unseen words.</S>
			<S sid ="37" ssid = "26">While each trie looks only at a single context, multiple tries can be used together to capture both word-internal and external contextual evidence of class membership.</S>
	</SECTION>
	<SECTION title="Restoring Case Information. " number = "3">
			<S sid ="38" ssid = "1">In European languages, named entities are often distinguished through their use of capitalisation.</S>
			<S sid ="39" ssid = "2">However, capitalisation commonly plays another role, that of marking the first word in a sentence.</S>
			<S sid ="40" ssid = "3">In addition, some sentences such as newspaper headlines are written in all- capitals for emphasis.</S>
			<S sid ="41" ssid = "4">In these environments, the case information that has traditionally been so useful to NER systems is lost.</S>
			<S sid ="42" ssid = "5">Previous work in NER has been aware of this problem of dealing with words without accurate case information, and various workarounds have been exploited.</S>
			<S sid ="43" ssid = "6">Most commonly, feature-based classifiers use a set of capitalisation features and a sentence-initial feature (Bikel et al., 1997).</S>
			<S sid ="44" ssid = "7">Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a “weaker” classifier that did not use case information at all (Chieu and Ng, 2002b).</S>
			<S sid ="45" ssid = "8">We propose a different solution to the problem of case- less words.</S>
			<S sid ="46" ssid = "9">Rather than noting their lack of case and treating them separately, we propose to restore the correct capitalisation as a preprocessing step, allowing all words to be treated in the same manner.</S>
			<S sid ="47" ssid = "10">If this process of case restoration is sufficiently accurate, capitalisation should be more correctly associated with entities, resulting in better recognition performance.</S>
			<S sid ="48" ssid = "11">Restoring case information is not equivalent to distinguishing common nouns from proper nouns.</S>
			<S sid ="49" ssid = "12">This is particularly evident in German, where all types of nouns are written with an initial capital letter.</S>
			<S sid ="50" ssid = "13">The purpose of case restoration is simply to reveal the underlying capitalisation model of the language, allowing machine learners to learn more accurately from orthography.</S>
			<S sid ="51" ssid = "14">We propose two methods, each of which requires a corpus with accurate case information.</S>
			<S sid ="52" ssid = "15">Such a corpus is easily obtained; any unannotated corpus can be used once Table 1: Case restoration performance using an MDtrie, English.</S>
			<S sid ="53" ssid = "16">sentence-initial words and allcaps sentences have been excluded.</S>
			<S sid ="54" ssid = "17">For both languages, the training corpus consisted of the raw data, training and test data combined.</S>
			<S sid ="55" ssid = "18">The first method for case restoration is to replace a caseless word with its most frequent form.</S>
			<S sid ="56" ssid = "19">Word capitalisation frequencies can easily be computed for corpora of any size.</S>
			<S sid ="57" ssid = "20">The major weakness of this technique is that each word is classified individually without regard for its context.</S>
			<S sid ="58" ssid = "21">For instance, “new” will always be written in lowercase, even when it is part of a valid capitalised phrase such as “New York”.</S>
			<S sid ="59" ssid = "22">The second method uses an MDtrie which, if allowed to extend over word boundaries, can effectively capture the cases where a word has multiple possible forms.</S>
			<S sid ="60" ssid = "23">Since an MDtrie is only built as deep as is required to capture differences between categories, most paths will still be quite shallow.</S>
			<S sid ="61" ssid = "24">As in other word categorisation tasks, tries can robustly deal with unseen words by performing classification on the longest matchable prefix.</S>
			<S sid ="62" ssid = "25">To test these recapitalisation methods, the raw, training, and development sets were used as the training set.</S>
			<S sid ="63" ssid = "26">From the second test set, only words with known case information were used for testing, resulting in corpora of 30484 and 39639 words for English and German respectively.</S>
			<S sid ="64" ssid = "27">Each word was classified as either lowercase (“new”), initial-caps (“New”), all-caps(“U.S.”), or inner- caps (“ex-English”).</S>
			<S sid ="65" ssid = "28">On this test set, the word-frequency method and the trie-based method achieved accuracies of 93.9% and 95.7% respectively for English, and 95.4% and 96.3% in German.</S>
			<S sid ="66" ssid = "29">Table 1 shows the trie performance for English in more detail.</S>
			<S sid ="67" ssid = "30">In practice, it is usually possible to train on the same corpus as is being re- capitalised.</S>
			<S sid ="68" ssid = "31">This will give more accurate information for those words which appear in both known-case and unknown-case positions, and should yield higher accuracy.</S>
			<S sid ="69" ssid = "32">This process of restoring case information is language independent and requires only an unannotated corpus in the target language.</S>
			<S sid ="70" ssid = "33">It is a pre-processing step that can be ignored for languages where case information is either not present or is not lost.</S>
			<S sid ="71" ssid = "34">NER Table 2: Recognition performance.</S>
	</SECTION>
	<SECTION title="Classification Process. " number = "4">
			<S sid ="72" ssid = "1">The training data was converted to use the IOB2 phrase model (Tjong Kim Sang and Veenstra, 1999).</S>
			<S sid ="73" ssid = "2">This phrase model was found to be more appropriate to the nature of NE phrases in both languages, in that the first word in the phrase may behave differently to consecutive words.</S>
			<S sid ="74" ssid = "3">MD-Tries were trained on the prefix and suffix of the current word, and the left and right surrounding contexts.</S>
			<S sid ="75" ssid = "4">Each trie Tx produces an independent probability estimate, PTx (c | context).</S>
			<S sid ="76" ssid = "5">These probabilities are com bined to produce a single estimate n P (c | context) = n PT (c | context) i=0 These probabilities are then used directly as observation probabilities in a hidden Markov model (HMM) framework.</S>
			<S sid ="77" ssid = "6">An HMM uses probability matrices Π, A, and B for the initial state, state transitions, and symbol emissions respectively (Manning and Schu¨ tze, 1999).</S>
			<S sid ="78" ssid = "7">We derive Π and A from the training set.</S>
			<S sid ="79" ssid = "8">Rather than explicitly defining B, trie-based probability estimates are used directly within the standard Viterbi algorithm, which exploits dynamic programming to efficiently search the entire space of state assignments.</S>
			<S sid ="80" ssid = "9">Illegal assignments, such as an I-PER without a preceding B-PER, cannot arise due to the restrictions of the transition matrix.</S>
			<S sid ="81" ssid = "10">The datasets for both languages contained extra information including chunk and part-of-speech information, as well as lemmas for the German data.</S>
			<S sid ="82" ssid = "11">While these are rich sources of data, and may help especially in the recognition phase, our aim was to investigate the feasibility of a purely orthographic approach, and as such no extra information was used.</S>
	</SECTION>
	<SECTION title="Results. " number = "5">
			<S sid ="83" ssid = "1">Table 2 shows how the system performs in terms of recognition.</S>
			<S sid ="84" ssid = "2">There is a large discrepancy between recognition performance for English and German.</S>
			<S sid ="85" ssid = "3">For German, it appears that there is insufficient morphological information in a word and its immediate context to reliably discriminate between NEs and common nouns.</S>
			<S sid ="86" ssid = "4">Precision is markedly higher than recall across all tests.</S>
			<S sid ="87" ssid = "5">The most common error in English was the misclassification Table 3: Accuracy on seen and unseen tokens.</S>
			<S sid ="88" ssid = "6">Table 4: Improvement in f-score through restoring case.</S>
			<S sid ="89" ssid = "7">of a single-term entity as a nonentity, while multi-word entities were more successfully identified.</S>
			<S sid ="90" ssid = "8">Table 3 shows the overall performance difference between words present in the tagged training corpus and those that only occurred in the test set.</S>
			<S sid ="91" ssid = "9">For previously seen words, both recognition and classification perform well, aided by the variable depth of MD-tries.</S>
			<S sid ="92" ssid = "10">The progressive back-off model of tries is quite effective in classifying new tokens, achieving up to 85% accuracy in classification unseen entities.</S>
			<S sid ="93" ssid = "11">It is interesting to note that, given a successful recognition phase, German NEs are more successfully classified than English NEs.</S>
			<S sid ="94" ssid = "12">The effects of heuristically restoring case information can be seen in Table 4.</S>
			<S sid ="95" ssid = "13">The contribution of recapitalisation is limited by the proportion of entities in caseless positions.</S>
			<S sid ="96" ssid = "14">Both the word-based method and the trie-based method produced improvements.</S>
			<S sid ="97" ssid = "15">The higher accuracy of the trie-based approach gives better overall performance.</S>
			<S sid ="98" ssid = "16">The final results for each language and dataset are given in Table 5.</S>
			<S sid ="99" ssid = "17">Both English datasets have the same performance profile: results for the PER and LOC categories were markedly better than the MISC and ORG categories.</S>
			<S sid ="100" ssid = "18">Since seen and unseen performance remained quite stable, the lower results for the second test set can be explained by a higher percentage of previously unseen words.</S>
			<S sid ="101" ssid = "19">While MISC is traditionally the worst-performing category, the lowest results were for ORG.</S>
			<S sid ="102" ssid = "20">This pattern of performance was different to that for German, in which MISC was consistently identified less well than the other categories.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="103" ssid = "1">We have presented a very simple system that uses only internal and contextual character-level evidence.</S>
			<S sid ="104" ssid = "2">This highly language-independent model performs well on both seen and unseen tokens despite using only the su pervised training data.</S>
			<S sid ="105" ssid = "3">The incorporation of trie-based estimates into an HMM framework allows the optimal tag sequence to be found for each sentence.</S>
			<S sid ="106" ssid = "4">We have also shown that case information can be restored with high accuracy using simple machine learning techniques, and that this restoration is beneficial to named entity recognition.</S>
			<S sid ="107" ssid = "5">We would expect most NER systems to benefit from this recapitalisation process, especially in fields without accurate case information, such as transcribed text or allcaps newswire.</S>
			<S sid ="108" ssid = "6">Trie-based classification yields probability estimates that are highly suitable for use as features in a further machine learning process.</S>
			<S sid ="109" ssid = "7">This approach has the advantage of being highly language-independent, and requiring fewer features than traditional orthographic feature representations.</S>
	</SECTION>
</PAPER>
