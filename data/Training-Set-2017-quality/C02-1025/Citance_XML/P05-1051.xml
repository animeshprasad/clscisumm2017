<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Information extraction systems incorporate multiple stages of linguistic analysis.</S>
		<S sid ="2" ssid = "2">Although errors are typically compounded from stage to stage, it is possible to reduce the errors in one stage by harnessing the results of the other stages.</S>
		<S sid ="3" ssid = "3">We demonstrate this by using the results of coreference analysis and relation extraction to reduce the errors produced by a Chinese name tagger.</S>
		<S sid ="4" ssid = "4">We use an N-best approach to generate multiple hypotheses and have them re-ranked by subsequent stages of processing.</S>
		<S sid ="5" ssid = "5">We obtained thereby a reduction of 24% in spurious and incorrect name tags, and a reduction of 14% in missed tags.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Systems which extract relations or events from a document typically perform a number of types of linguistic analysis in preparation for information extraction.</S>
			<S sid ="7" ssid = "7">These include name identification and classification, parsing (or partial parsing), semantic classification of noun phrases, and coreference analysis.</S>
			<S sid ="8" ssid = "8">These tasks are reflected in the evaluation tasks introduced for MUC6 (named entity, coreference, template element) and MUC7 (template relation).</S>
			<S sid ="9" ssid = "9">In most extraction systems, these stages of analysis are arranged sequentially, with each stage using the results of prior stages and generating a single analysis that gets enriched by each stage.</S>
			<S sid ="10" ssid = "10">This provides a simple modular organization for the extraction system.Unfortunately, each stage also introduces a cer tain level of error into the analysis.</S>
			<S sid ="11" ssid = "11">Furthermore, these errors are compounded – for example, errors in name recognition may lead to errors in parsing.</S>
			<S sid ="12" ssid = "12">The net result is that the final output (relations or events) may be quite inaccurate.</S>
			<S sid ="13" ssid = "13">This paper considers how interactions between the stages can be exploited to reduce the error rate.</S>
			<S sid ="14" ssid = "14">For example, the results of coreference analysis or relation identification may be helpful in name classification, and the results of relation or event extraction may be helpful in coreference.</S>
			<S sid ="15" ssid = "15">Such interactions are not easily exploited in a simple sequential model … if name classification is performed at the beginning of the pipeline, it cannot make use of the results of subsequent stages.</S>
			<S sid ="16" ssid = "16">It may even be difficult to use this information implicitly, by using features which are also used in later stages, because the representation used in the initial stages is too limited.To address these limitations, some recent sys tems have used more parallel designs, in which a single classifier (incorporating a wide range of features) encompasses what were previously several separate stages (Kambhatla, 2004; Zelenko et al., 2004).</S>
			<S sid ="17" ssid = "17">This can reduce the compounding of errors of the sequential design.</S>
			<S sid ="18" ssid = "18">However, it leads to a very large feature space and makes it difficult to select linguistically appropriate features for particular analysis tasks.</S>
			<S sid ="19" ssid = "19">Furthermore, because these decisions are being made in parallel, it becomes much harder to express interactions between the levels of analysis based on linguistic intuitions.</S>
			<S sid ="20" ssid = "20">411 Proceedings of the 43rd Annual Meeting of the ACL, pages 411–418, Ann Arbor, June 2005.</S>
			<S sid ="21" ssid = "21">Qc 2005 Association for Computational Linguistics In order to capture these interactions more explicitly, we have employed a sequential design in which multiple hypotheses are forwarded from each stage to the next, with hypotheses being reranked and pruned using the information from later stages.</S>
			<S sid ="22" ssid = "22">We shall apply this design to show how named entity classification can be improved by ‘feedback’ from coreference analysis and relation extraction.</S>
			<S sid ="23" ssid = "23">We shall show that this approach can capture these interactions in a natural and efficient manner, yielding a substantial improvement in name identification and classification.</S>
	</SECTION>
	<SECTION title="Prior Work. " number = "2">
			<S sid ="24" ssid = "1">A wide variety of trainable models have been applied to the name tagging task, including HMMs (Bikel et al., 1997), maximum entropy models (Borthwick, 1999), support vector machines (SVMs), and conditional random fields.</S>
			<S sid ="25" ssid = "2">People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name.</S>
			<S sid ="26" ssid = "3">Some other named entity systems have explored global information for name tagging.</S>
			<S sid ="27" ssid = "4">(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.</S>
			<S sid ="28" ssid = "5">One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names.</S>
			<S sid ="29" ssid = "6">We attempted to recover some of these names by heuristic rules which are quite language specific.</S>
			<S sid ="30" ssid = "7">In addition, this single- hypothesis method placed an upper bound on recall.</S>
			<S sid ="31" ssid = "8">Traditional statistical name tagging methods have generated a single name hypothesis.</S>
			<S sid ="32" ssid = "9">BBN proposed the N-Best algorithm for speech recognition in (Chow and Schwartz, 1989).</S>
			<S sid ="33" ssid = "10">Since then N- Best methods have been widely used by other researchers (Collins, 2002; Zhai et al., 2004).In this paper, we tried to combine the advan tages of the prior work, and incorporate broader knowledge into a more general re-ranking model.</S>
	</SECTION>
	<SECTION title="Task and Terminology. " number = "3">
			<S sid ="34" ssid = "1">Our experiments were conducted in the context of the ACE Information Extraction evaluations, and we will use the terminology of these evaluations: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) name mention: a reference by name to an entity nominal mention: a reference by a common noun or noun phrase to an entity relation: one of a specified set of relationships between a pair of entities The 2004 ACE evaluation had 7 types of entities, of which the most common were PER (persons), ORG (organizations), and GPE (‘geopolitical entities’ – locations which are also political units, such as countries, counties, and cities).</S>
			<S sid ="35" ssid = "2">There were 7 types of relations, with 23 subtypes.</S>
			<S sid ="36" ssid = "3">Examples of these relations are “the CEO of Microsoft” (an employ-exec relation), “Fred’s wife” (a family relation), and “a military base in Germany” (a located relation).In this paper we look at the problem of identify ing name mentions in Chinese text and classifying them as persons, organizations, or GPEs.</S>
			<S sid ="37" ssid = "4">Because Chinese has neither capitalization nor overt word boundaries, it poses particular problems for name identification.</S>
	</SECTION>
	<SECTION title="Baseline System. " number = "4">
			<S sid ="38" ssid = "1">4.1 Baseline Name Tagger.</S>
			<S sid ="39" ssid = "2">Our baseline name tagger consists of a HMM tag- ger augmented with a set of post-processing rules.</S>
			<S sid ="40" ssid = "3">The HMM tagger generally follows the Nymble model (Bikel et al, 1997), but with multiple hypotheses as output and a larger number of states (12) to handle name prefixes and suffixes, and transliterated foreign names separately.</S>
			<S sid ="41" ssid = "4">It operates on the output of a word segmenter from Tsinghua University.</S>
			<S sid ="42" ssid = "5">Within each of the name class states, a statistical bigram model is employed, with the usual one- word-per-state emission.</S>
			<S sid ="43" ssid = "6">The various probabilities involve word co-occurrence, word features, and class probabilities.</S>
			<S sid ="44" ssid = "7">Then it uses A* search decoding to generate multiple hypotheses.</S>
			<S sid ="45" ssid = "8">Since these probabilities are estimated based on observations seen in a corpus, “back-off models” are used to reflect the strength of support for a given statistic, as for the Nymble system.</S>
			<S sid ="46" ssid = "9">We also add post-processing rules to correct some omissions and systematic errors using name lists (for example, a list of all Chinese last names; lists of organization and location suffixes) and particular contextual patterns (for example, verbs occurring with people’s names).</S>
			<S sid ="47" ssid = "10">They also deal with abbreviations and nested organization names.</S>
			<S sid ="48" ssid = "11">The HMM tagger also computes the margin – the difference between the log probabilities of the top two hypotheses.</S>
			<S sid ="49" ssid = "12">This is used as a rough measure of confidence in the top hypothesis (see sections 5.3 and 6.2, below).</S>
			<S sid ="50" ssid = "13">The name tagger used for these experiments identifies the three main ACE entity types: Person (PER), Organization (ORG), and GPE (names of the other ACE types are identified by a separate component of our system, not involved in the experiments reported here).</S>
			<S sid ="51" ssid = "14">4.2 Nominal Mention Tagger.</S>
			<S sid ="52" ssid = "15">Our nominal mention tagger (noun group recognizer) is a maximum entropy tagger trained on the Chinese TreeBank from the University of Pennsylvania, supplemented by list matching.</S>
			<S sid ="53" ssid = "16">4.3 Reference Resolver.</S>
			<S sid ="54" ssid = "17">Our baseline reference resolver goes through two successive stages: first, coreference rules will identify some high-confidence positive and negative mention pairs, in training data and test data; then the remaining samples will be used as input of a maximum entropy tagger.</S>
			<S sid ="55" ssid = "18">The features used in this tagger involve distance, string matching, lexical information, position, semantics, etc. We separate the task into different classifiers for different mention types (name / noun / pronoun).</S>
			<S sid ="56" ssid = "19">Then we in tions of interest1.</S>
			<S sid ="57" ssid = "20">Each training / test example consists of the pair of mentions and the sequence of intervening words.</S>
			<S sid ="58" ssid = "21">Associated with each training example is either one of the ACE relation types or no relation at all.</S>
			<S sid ="59" ssid = "22">We defined a distance metric between two examples based on  whether the heads of the mentions match  whether the ACE types of the heads of the mentions match (for example, both are people or both are or ganizations)  whether the intervening words match To tag a test example, we find the k nearest training examples (where k = 3) and use the distance to weight each neighbor, then select the most common class in the weighted neighbor set.</S>
			<S sid ="60" ssid = "23">To provide a crude measure of the confidence of our relation tagger, we define two thresholds, Dnear and Dfar.</S>
			<S sid ="61" ssid = "24">If the average distance d to the nearest neighbors d &lt; Dnear, we consider this a definite relation.</S>
			<S sid ="62" ssid = "25">If Dnear &lt; d &lt; Dfar, we consider this a possible relation.</S>
			<S sid ="63" ssid = "26">If d &gt; Dfar, the tagger assumes that no relation exists (regardless of the class of the nearest neighbor).</S>
	</SECTION>
	<SECTION title="Information from Coreference and Re-. " number = "5">
			<S sid ="64" ssid = "1">lations Our system is processing a document consisting of multiple sentences.</S>
			<S sid ="65" ssid = "2">For each sentence, the name recognizer generates multiple hypotheses, each of which is an NE tagging of the entire sentence.</S>
			<S sid ="66" ssid = "3">The names in the hypothesis, plus the nouns in the categories of interest constitute the mention set for that hypothesis.</S>
			<S sid ="67" ssid = "4">Coreference resolution links these mentions, assigning each to an entity.</S>
			<S sid ="68" ssid = "5">In symbols: Si is the i-th sentence in the document.</S>
			<S sid ="69" ssid = "6">Hi is the hypotheses set for Si hij is the j-th hypothesis in Si Mij is the mention set for hij mijk is the k-th mention in Mij corporate the results from the relation tagger to adjust the probabilities from the classifiers.</S>
			<S sid ="70" ssid = "7">Finally eijk is the entity which mijk belongs to according to we apply a clustering algorithm to combine them into entities (sets of coreferring mentions).</S>
			<S sid ="71" ssid = "8">4.4 Relation Tagger.</S>
			<S sid ="72" ssid = "9">The relation tagger uses a k-nearest-neighbor algorithm.</S>
			<S sid ="73" ssid = "10">For both training and test, we consider all pairs of entity mentions where there is at most one other mention between the heads of the two men the current reference resolution results 5.1 Coreference Features.</S>
			<S sid ="74" ssid = "11">For each mention we compute seven quantities based on the results of name tagging and reference resolution: 1 This constraint is relaxed for parallel structures such as “mention1, mention2,.</S>
			<S sid ="75" ssid = "12">[and] mention3….”; in such cases there can be more than one intervening mention.</S>
			<S sid ="76" ssid = "13">CorefNumijk is the number of mentions in eijk WeightSumijk is the sum of all the link weights between mijk and other mentions in eijk , 0.8 for name-name coreference; 0.5 for apposition; 0.3 for other name-nominal coreference FirstMentionijk is 1 if mijk is the first name mention in the entity; otherwise 0 Headijk is 1 if mijk includes the head word of name; otherwise 0 Withoutidiomijk is 1 if mijk is not part of an idiom; otherwise 0 PERContextijk is the number of PER context words around a PER name such as a title or an action verb involving a PER ORGSuffixijk is 1 if ORG mijk includes a suffix word; otherwise 0 The first three capture evidence of the correctness of a name provided by reference resolution; for example, a name which is coreferenced with more other mentions is more likely to be correct.</S>
			<S sid ="77" ssid = "14">The last four capture local or name-internal evidence; for instance, that an organization name includes an explicit, organization-indicating suffix.</S>
			<S sid ="78" ssid = "15">We then compute, for each of these seven quantities, the sum over all mentions k in a sentence, obtaining values for CorefNumij, WeightSumij, etc.: able to generalize from the examples in the training corpus to other words in the cluster.The set of ACE relations includes several in volving employment, social, and family relations.</S>
			<S sid ="79" ssid = "16">We gathered the words appearing as an argument of one of these relations in the training corpus, eliminated low-frequency terms and manually edited the ten resulting clusters to remove inappropriate terms.</S>
			<S sid ="80" ssid = "17">These were then combined with lists (of titles, organization name suffixes, location suffixes) used in the baseline tagger.</S>
			<S sid ="81" ssid = "18">5.3 Relation Features.</S>
			<S sid ="82" ssid = "19">Because the performance of our relation tagger is not as good as our coreference resolver, we have used the results of relation detection in a relatively simple way to enhance name detection.</S>
			<S sid ="83" ssid = "20">The basic intuition is that a name which has been correctly identified is more likely to participate in a relation than one which has been erroneously identified.</S>
			<S sid ="84" ssid = "21">For a given range of margins (from the HMM), the probability that a name in the first hypothesis is correct is shown in the following table, for names participating and not participating in a relation: CorefNumij = ∑ CorefNumijk k etc. Finally, we determine, for a given sentence and hypothesis, for each of these seven quantities, whether this quantity achieves the maximum of its values for this hypothesis: BestCorefNumij ≡ CorefNumij = maxq CorefNumiq etc. We will use these properties of the hypothesis as features in assessing the quality of a hypothesis.</S>
			<S sid ="85" ssid = "22">5.2 Relation Word Clusters.</S>
			<S sid ="86" ssid = "23">In addition to using relation information for reranking name hypotheses, we used the relation training corpus to build word clusters which could more directly improve name tagging.</S>
			<S sid ="87" ssid = "24">Name tag- gers rely heavily on words in the immediate context to identify and classify names; for example, specific job titles, occupations, or family relations can be used to identify people names.</S>
			<S sid ="88" ssid = "25">Such words are learned individually from the name tagger’s training corpus.</S>
			<S sid ="89" ssid = "26">If we can provide the name tagger with clusters of related words, the tagger will be Table 1 Probability of a name being correct Table 1 confirms that names participating in relations are much more likely to be correct than names that do not participate in relations.</S>
			<S sid ="90" ssid = "27">We also see, not surprisingly, that these probabilities are strongly affected by the HMM hypothesis margin (the difference in log probabilities) between the first hypothesis and the second hypothesis.</S>
			<S sid ="91" ssid = "28">So it is natural to use participation in a relation (coupled with a margin value) as a valuable feature for re- ranking name hypotheses.</S>
			<S sid ="92" ssid = "29">Let mijk be the k-th name mention for hypothesis hij of sentence; then we define: Inrelationijk = 1 if mijk is in a definite relation = 0 if mijk is in a possible relation = -1 if mijk is not in a relation Inrelationij = ∑ Inrelationijk k Mostrelatedij ≡ ( Inrelationij = max q Inrelationiq ) Finally, to capture the interaction with the margin, we let zi = the margin for sentence Si and divide the range of values of zi into six intervals Mar1, … Mar6.</S>
			<S sid ="93" ssid = "30">And we define the hypothesis ranking information: FirstHypothesisij = 1 if j =1; otherwise 0.</S>
			<S sid ="94" ssid = "31">We will use as features for ranking hij the conjunction of Mostrelatedij, zi ∈ Marp (p = 1, …, 6), and FirstHypothesisij .</S>
	</SECTION>
	<SECTION title="Using  the  Information  from  Corefer-. " number = "6">
			<S sid ="95" ssid = "1">ence and Relations 6.1 Word Clustering based on Relations.</S>
			<S sid ="96" ssid = "2">As we described in section 5.2, we can generate word clusters based on relation information.</S>
			<S sid ="97" ssid = "3">If a word is not part of a relation cluster, we consider it an independent (1-word) cluster.</S>
			<S sid ="98" ssid = "4">The Nymble name tagger (Bikel et al., 1999) relies on a multi-level linear interpolation model for backoff.</S>
			<S sid ="99" ssid = "5">We extended this model by adding a level from word to cluster, so as to estimate more reliable probabilities for words in these clusters.</S>
			<S sid ="100" ssid = "6">Table 2 shows the extended backoff model for each of the three probabilities used by Nymble.</S>
			<S sid ="101" ssid = "7">6.2 Pre-pruning by Margin.</S>
			<S sid ="102" ssid = "8">The HMM tagger produces the N best hypotheses for each sentence.</S>
			<S sid ="103" ssid = "9">2 In order to decide when we need to rely on global (coreference and relation) information for name tagging, we want to have some assessment of the confidence that the name tagger has in the first hypothesis.</S>
			<S sid ="104" ssid = "10">In this paper, we use the margin for this purpose.</S>
			<S sid ="105" ssid = "11">A large margin indicates greater confidence that the first hypothesis is correct.3 So if the margin of a sentence is above a threshold, we select the first hypothesis, dropping the others and bypassing the reranking.</S>
			<S sid ="106" ssid = "12">6.3 Re-ranking based on Coreference.</S>
			<S sid ="107" ssid = "13">We described in section 5.1, above, the coreference features which will be used for reranking the hypotheses after pre-pruning.</S>
			<S sid ="108" ssid = "14">A maximum entropy model for re-ranking these hypotheses is then trained and applied as follows: Training 1.</S>
			<S sid ="109" ssid = "15">Use K-fold cross-validation to generate multi-.</S>
			<S sid ="110" ssid = "16">ple name tagging hypotheses for each document in the training data Dtrain (in each of the K iterations, we use K-1 subsets to train the HMM and then generate hypotheses from the Kth subset).</S>
			<S sid ="111" ssid = "17">2.</S>
			<S sid ="112" ssid = "18">For each document d in Dtrain, where d includes.</S>
			<S sid ="113" ssid = "19">n sentences S1…Sn For i = 1…n, let m = the number of hypotheses for Si(1) Pre-prune the candidate hypotheses us ing the HMM margin (2) For each hypothesis hij, j = 1…m (a) Compare hij with the key, set the prediction Valueij “Best” or “Not Best” (b) Run the Coreference Resolver on hij and the best hypothesis for each of the other sentences, generate entity results for each candidate name in hij(c) Generate a coreference feature vec tor Vij for hij (d) Output Vij and Valueij Table2 Extended Backoff Model 2 We set different N = 5, 10, 20 or 30 for different margin ranges,.</S>
			<S sid ="114" ssid = "20">by cross- validation checking the training data about the ranking position of the best hypothesis for each sentence.</S>
			<S sid ="115" ssid = "21">With this N, optimal reranking (selecting the best hypothesis among the N best) would yield Precision = 96.9 Recall = 94.5 F = 95.7 on our test corpus.</S>
			<S sid ="116" ssid = "22">3 Similar methods based on HMM margins were used by (Scheffer.</S>
			<S sid ="117" ssid = "23">et al., 2001).</S>
			<S sid ="118" ssid = "24">3.</S>
			<S sid ="119" ssid = "25">Train Maxent Re-ranking system on all Vij and.</S>
			<S sid ="120" ssid = "26">Valueij Wij = exp( probij ) prob ∑ exp( q iq ) Test 1.</S>
			<S sid ="121" ssid = "27">Run the baseline name tagger to generate mul-.</S>
			<S sid ="122" ssid = "28">tiple name tagging hypotheses for each document in the test data Dtest 2.</S>
			<S sid ="123" ssid = "29">For each document d in Dtest, where d includes.</S>
			<S sid ="124" ssid = "30">n sentences S1…Sn(1) Initialize: Dynamic input of coreference re solver H = {hi-best | i = 1…n, hi-best is the current best hypothesis for Si} For each name mention mijk in hij , we define: Occurq (mijk ) = 1 if mijk occurs in hq = 0 otherwise Then we count its voting value as follows: Votingijk is 1 if ∑ Wiq × Occurq (mijk ) &gt;0.3; q otherwise 0.</S>
			<S sid ="125" ssid = "31">The voting value of hij is: (2) For i = 1…n, assume m = the number of hypotheses for Si Votingij = ∑Voting k ijk(a) Pre-prune the candidate hypotheses us ing the HMM margin (b) For each hypothesis hij, j = 1…m • hi-best = hij • Run the Coreference Resolver on H, generate entity results for each name candidate in hij• Generate a coreference feature vec tor Vij for hij • Run Maxent Re-ranking system on Vij, produce Probij of “Best” value (c) hi-best = the hypothesis with highest Probij of “Best” value, update H and output hi-best 6.4 Re-ranking based on Relations.</S>
			<S sid ="126" ssid = "32">From the above first-stage re-ranking by coreference, for each hypothesis we got the probability of its being the best one.</S>
			<S sid ="127" ssid = "33">By using these results and relation information we proceed to a second-stage re-ranking.</S>
			<S sid ="128" ssid = "34">As we described in section 5.3, the information of “in relation or not” can be used together with margin as another important measure of confidence.</S>
			<S sid ="129" ssid = "35">In addition, we apply the mechanism of weighted voting among hypotheses (Zhai et al., 2004) as an additional feature in this second-stage re-ranking.</S>
			<S sid ="130" ssid = "36">This approach allows all hypotheses to vote on a possible name output.</S>
			<S sid ="131" ssid = "37">A recognized name is considered correct only when it occurs in more than 30 percent of the hypotheses (weighted by their probability).</S>
			<S sid ="132" ssid = "38">In our experiments we use the probability produced by the HMM, probij , for hypothesis hij . We normalize this probability weight as: Finally we define the following voting feature: BestVotingij ≡ (Votingij = max q Votingiq ) This feature is used, together with the features described at the end of section 5.3 and the probability score from the first stage, for the second- stage maxent re-ranking model.</S>
			<S sid ="133" ssid = "39">One appeal of the above two re-ranking algorithms is its flexibility in incorporating features into a learning model: essentially any coreference or relation features which might be useful in discriminating good from bad structures can be included.</S>
	</SECTION>
	<SECTION title="System Pipeline. " number = "7">
			<S sid ="134" ssid = "1">Combining all the methods presented above, the flow of our final system is shown in figure 1.</S>
	</SECTION>
	<SECTION title="Evaluation Results. " number = "8">
			<S sid ="135" ssid = "1">8.1 Training and Test Data.</S>
			<S sid ="136" ssid = "2">We took 346 documents from the 2004 ACE training corpus and official test set, including both broadcast news and newswire, as our blind test set.</S>
			<S sid ="137" ssid = "3">To train our name tagger, we used the Beijing University Insititute of Computational Linguistics corpus – 2978 documents from the People’s Daily in 1998 – and 667 texts in the training corpus for the 2003 &amp; 2004 ACE evaluation.</S>
			<S sid ="138" ssid = "4">Our reference resolver is trained on these 667 ACE texts.</S>
			<S sid ="139" ssid = "5">The relation tagger is trained on 546 ACE 2004 texts, from which we also extracted the relation clusters.</S>
			<S sid ="140" ssid = "6">The test set included 11715 names: 3551 persons, 5100 GPEs and 3064 organizations.</S>
			<S sid ="141" ssid = "7">Input HMM Name Tagger, word clustering based on relations, pruned by margin Multiple name hypotheses Nominal Mention Tagger Nominal Mentions the-art baseline HMM trained on the same material.</S>
			<S sid ="142" ssid = "8">Furthermore, it helps to disambiguate many name type errors: the number of cases of type confusion in name classification was reduced from 191 to 1 0 2 . Coreference Resolver Maxent Re-ranking by coreference Table 3 Baseline Name Tagger Relation Tagger M a x e n t R e r a n k i n g b y r e l a t i o n Post-processing by heuristic rules Table 4 Baseline + Word Clustering by Relation Single name hypothesis Figure 1 System Flow 8.2 Overall Performance Comparison.</S>
			<S sid ="143" ssid = "9">Table 3 shows the performance of the baseline system; Table 4 is the system with relation word clusters; Table 5 is the system with both relation clusters and re-ranking based on coreference features; and Table 6 is the whole system with second-stage re-ranking using relations.</S>
			<S sid ="144" ssid = "10">The results indicate that relation word clusters help to improve the precision and recall of most name types.</S>
			<S sid ="145" ssid = "11">Although the overall gain in F-score is small (0.7%), we believe further gain can be achieved if the relation corpus is enlarged in the future.</S>
			<S sid ="146" ssid = "12">The re-ranking using the coreference features had the largest impact, improving precision and recall consistently for all types.</S>
			<S sid ="147" ssid = "13">Compared to our system in (Ji and Grishman, 2004), it helps to distinguish the good and bad hypotheses without any loss of recall.</S>
			<S sid ="148" ssid = "14">The second-stage re-ranking using the relation participation feature yielded a small further gain in F score for each type, improving precision at a slight cost in recall.The overall system achieves a 24.1% relative re duction on the spurious and incorrect tags, and14.3% reduction in the missing rate over a state-of Table 5 Baseline + Word Clustering by Relation + Re-ranking by Coreference Name Precision Recall F PER 90.7 91.0 90.8 GPE 91.2 86.9 89.0 ORG 91.7 89.1 90.4 ALL 91.2 88.6 89.9 Table 6 Baseline + Word Clustering by Relation + Re-ranking by Coreference + Re-ranking by Relation In order to check how robust these methods are, we conducted significance testing (sign test) on the 346 documents.</S>
			<S sid ="149" ssid = "15">We split them into 5 folders, 70 documents in each of the first four folders and 66 in the fifth folder.</S>
			<S sid ="150" ssid = "16">We found that each enhancement (word clusters, coreference reranking, relation reranking) produced an improvement in F score for each folder, allowing us to reject the hypothesis that these improvements were random at a 95% confidence level.</S>
			<S sid ="151" ssid = "17">The overall F-measure improvements (using all enhancements) for the 5 folders were: 2.3%, 1.6%, 2.1%, 3.5%, and 2.1%.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "9">
			<S sid ="152" ssid = "1">This paper explored methods for exploiting the interaction of analysis components in an information extraction system to reduce the error rate of individual components.</S>
			<S sid ="153" ssid = "2">The ACE task hierarchy provided a good opportunity to explore these interactions, including the one presented here between reference resolution/relation detection and name tagging.</S>
			<S sid ="154" ssid = "3">We demonstrated its effectiveness for Chinese name tagging, obtaining an absolute improvement of 2.4% in F-measure (a reduction of 19% in the (1 – F) error rate).</S>
			<S sid ="155" ssid = "4">These methods are quite low-cost because we don’t need any extra resources or components compared to the baseline information extraction system.</S>
			<S sid ="156" ssid = "5">Because no language-specific rules are involved and no additional training resources are required, we expect that the approach described here can be straightforwardly applied to other languages.</S>
			<S sid ="157" ssid = "6">It should also be possible to extend this re-ranking framework to other levels of analysis in information extraction –- for example, to use event detection to improve name tagging; to incorporate subtype tagging results to improve name tagging; and to combine name tagging, reference resolution and relation detection to improve nominal mention tagging.</S>
			<S sid ="158" ssid = "7">For Chinese (and other languages without overt word segmentation) it could also be extended to do character-based name tagging, keeping multiple segmentations among the N-Best hypotheses.</S>
			<S sid ="159" ssid = "8">Also, as information extraction is extended to capture cross-document information, we should expect further improvements in performance of the earlier stages of analysis, including in particular name identification.</S>
			<S sid ="160" ssid = "9">For some levels of analysis, such as name tagging, it will be natural to apply lattice techniques to organize the multiple hypotheses, at some gain in efficiency.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="161" ssid = "10">This research was supported by the Defense Advanced Research Projects Agency under Grant N6600104-18920 from SPAWAR San Diego, and by the National Science Foundation under Grant 0325657.</S>
			<S sid ="162" ssid = "11">This paper does not necessarily reflect the position or the policy of the U.S. Government.</S>
	</SECTION>
</PAPER>
