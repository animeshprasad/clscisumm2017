<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">&lt;NONE&gt;</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="2" ssid = "2">The named entity recognition (NER) task involves identifying noun phrases that are names, and assigning a class to each name.</S>
			<S sid ="3" ssid = "3">This task has its origin from the Message Understanding Conferences (MUC) in the 1990s, a series where o refers to the outcome, h the history (or context), and Z (h) is a normalization function.</S>
			<S sid ="4" ssid = "4">The features used in the maximum entropy framework are binary.</S>
			<S sid ="5" ssid = "5">An example of a feature function is 1 if o = org-B, word = PETER of conferences aimed at evaluating systems that extract information from natural language texts.</S>
			<S sid ="6" ssid = "6">It became evi fj (h, o) = 0 otherwise dent that in order to achieve good performance in information extraction, a system needs to be able to recognize names.</S>
			<S sid ="7" ssid = "7">A separate subtask on NER was created in MUC 6 and MUC7 (Chinchor, 1998).</S>
			<S sid ="8" ssid = "8">Much research has since been carried out on NER, using both knowledge engineering and machine learning approaches.</S>
			<S sid ="9" ssid = "9">At the last CoNLL in 2002, a common NER task was used to evaluate competing NER systems.</S>
			<S sid ="10" ssid = "10">In this year’s CoNLL, the NER task is to tag noun phrases with the following four classes: person (PER), organization (ORG), location (LOC), and miscellaneous (MISC).</S>
			<S sid ="11" ssid = "11">This paper presents a maximum entropy approach to the NER task, where NER not only made use of local context within a sentence, but also made use of other occurrences of each word within the same document to extract useful features (global features).</S>
			<S sid ="12" ssid = "12">Such global features enhance the performance of NER (Chieu and Ng, 2002b).</S>
	</SECTION>
	<SECTION title="A Maximum Entropy Approach" number = "2">
			<S sid ="13" ssid = "1">The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible, other than the constraints imposed.</S>
			<S sid ="14" ssid = "2">Such constraints are derived from training data, expressing some relationship between features and outcome.</S>
			<S sid ="15" ssid = "3">The probability distribution that satisfies the above property is the one with the highest entropy.</S>
			<S sid ="16" ssid = "4">It is unique, agrees with the maximum-likelihood distribution, and has the exponential form (Della Pietra et al., 1997): The parameters αj are estimated by a procedure calledGeneralized Iterative Scaling (GIS) (Darroch and Rat cliff, 1972).</S>
			<S sid ="17" ssid = "5">This is an iterative procedure that improves the estimation of the parameters at each iteration.</S>
			<S sid ="18" ssid = "6">The maximum entropy classifier is used to classify each word as one of the following: the beginning of a NE (B tag), a word inside a NE (C tag), the last word of a NE (L tag), or the unique word in a NE (U tag).</S>
			<S sid ="19" ssid = "7">During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., PER-B followed by LOC-L).</S>
			<S sid ="20" ssid = "8">To eliminate such sequences, we define a transition probability between word classes P (ci |cj ) to be equal to 1 if the sequence is admissible, and 0 otherwise.</S>
			<S sid ="21" ssid = "9">The probability of the classes c1 , . . .</S>
			<S sid ="22" ssid = "10">, cn assigned to the words in a sentence s in a document D is defined as follows: n P (c1 , . . .</S>
			<S sid ="23" ssid = "11">, cn |s, D) = n P (ci |s, D) ∗ P (ci |ci−1 ), i=1 where P (ci |s, D) is determined by the maximum entropy classifier.</S>
			<S sid ="24" ssid = "12">The Viterbi algorithm is then used to select the sequence of word classes with the highest probability.</S>
	</SECTION>
	<SECTION title="Feature Representation. " number = "3">
			<S sid ="25" ssid = "1">We present two systems: a system ME1 that does not make use of any external knowledge base other than the training data, and a system ME2 that makes use of additional features derived from name lists.</S>
			<S sid ="26" ssid = "2">ME1 is used for both English and German.</S>
			<S sid ="27" ssid = "3">For German, however, for 1 p(o|h) = Z (h) k n j=1 αfj (h,o) , features that made use of the word string, the lemma (provided in the German training and test data) is used instead of the actual word.</S>
			<S sid ="28" ssid = "4">3.1 Lists derived from training data.</S>
			<S sid ="29" ssid = "5">The training data is first preprocessed to compile a number of lists that are used by both ME1 and ME2.</S>
			<S sid ="30" ssid = "6">These lists are derived automatically from the training data.</S>
			<S sid ="31" ssid = "7">Frequent Word List (FWL) This list consists of words that occur in more than 5 different documents.</S>
			<S sid ="32" ssid = "8">Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.</S>
			<S sid ="33" ssid = "9">Useful Bigrams (UBI) This list consists of bigrams of words that precede a name class.</S>
			<S sid ="34" ssid = "10">Examples are “CITY OF”, “ARRIVES IN”, etc. The list is compiled by taking bigrams with higher probability to appear before a name class than the unigram itself (e.g., “CITY OF” has higher probability to appear before a location than “OF”).</S>
			<S sid ="35" ssid = "11">A list is collected for each name class.</S>
			<S sid ="36" ssid = "12">We have attempted to use bigrams that appear after a name class, but for English at least, we have been unable to compile any such meaningful bigrams.</S>
			<S sid ="37" ssid = "13">A possible explanation is that in writing, people tend to explain with bigrams such as “CITY OF” before mentioning the name itself.</S>
			<S sid ="38" ssid = "14">Useful Word Suffixes (SUF) For each word in a name class, three-letter suffixes with high correlation metric score are collected.</S>
			<S sid ="39" ssid = "15">This is especially important for the MISC class, where suffixes such as “IAN” and “ISH” often appear.</S>
			<S sid ="40" ssid = "16">Useful Name Class Suffixes (NCS) A suffix list is compiled for each name class.</S>
			<S sid ="41" ssid = "17">These lists capture tokens that frequently terminate a particular name class.</S>
			<S sid ="42" ssid = "18">For example, the ORG class often terminates with tokens such as INC and COMMITTEE, and the MISC class often terminates with CUP, OPEN, etc. Function Words (FUN) Lower case words that occur within a name class.</S>
			<S sid ="43" ssid = "19">These include “van der”, “of”, etc. 3.2 Local Features.</S>
			<S sid ="44" ssid = "20">The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).</S>
			<S sid ="45" ssid = "21">Local features of a token w are those that are derived from the sentence containing w. Global features are derived by looking up other occurrences of w within the same document.</S>
			<S sid ="46" ssid = "22">In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).</S>
			<S sid ="47" ssid = "23">Local features include: First Word, Case, and Zone For English, each document is segmented by simple rules into 4 zones: headline (HL), author (AU), dateline (DL), and text (TXT).</S>
			<S sid ="48" ssid = "24">To identify the zones, a DL sentence is first identified using a regular expression.</S>
			<S sid ="49" ssid = "25">The system then looks for an AU sentence that occurs before DL using another regular expression.</S>
			<S sid ="50" ssid = "26">All sentences other than AU that occur before the DL sentence are then taken to be in the HL zone.</S>
			<S sid ="51" ssid = "27">Sentences after the DL sentence are taken to be in the TXT zone.</S>
			<S sid ="52" ssid = "28">If no DL sentence can be found in a document, then the first sentence of the document is taken as HL, and the rest as TXT.</S>
			<S sid ="53" ssid = "29">For German, the first sentence of each document is taken as HL, and the rest as TXT.</S>
			<S sid ="54" ssid = "30">Zone is used as part of the following features: If w starts with a capital letter (i.e., initCaps), and it is the first word of a sentence, a feature (firstwordinitCaps, zone) is set to 1.</S>
			<S sid ="55" ssid = "31">If it is initCaps but not the first word, a feature (initCaps, zone) is set to 1.</S>
			<S sid ="56" ssid = "32">If it is the first word but not initCaps, (firstwordnotInitCaps, zone) is set to 1.</S>
			<S sid ="57" ssid = "33">If it is made up of all capital letters, then (allCaps, zone) is set to 1.</S>
			<S sid ="58" ssid = "34">If it starts with a lower case letter, and contains both upper and lower case letters, then (mixedCaps, zone) is set to 1.</S>
			<S sid ="59" ssid = "35">A token that is allCaps will also be initCaps.</S>
			<S sid ="60" ssid = "36">Case and Zone of w+1 and w−1 Similarly, if w+1 (or w−1 ) is initCaps, a feature (initCaps, zone)NEXT (or (initCaps, zone)PREV ) is set to 1, etc. Case Sequence Suppose both w−1 and w+1 are init- Caps.</S>
			<S sid ="61" ssid = "37">Then if w is initCaps, a feature I is set to 1, else a feature N I is set to 1.</S>
			<S sid ="62" ssid = "38">Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).</S>
			<S sid ="63" ssid = "39">Lexicon Feature The string of w is used as a feature.</S>
			<S sid ="64" ssid = "40">This group contains a large number of features (one for each token string present in the training data).</S>
			<S sid ="65" ssid = "41">Lexicon Feature of Previous and Next Token The string of the previous token w−1 and the next token w+1 is used with the initCaps information of w. If w has init- Caps, then a feature (initCaps, w+1 )NEXT is set to 1.</S>
			<S sid ="66" ssid = "42">If w is not initCaps, then (not-initCaps, w+1 )NEXT is set to 1.</S>
			<S sid ="67" ssid = "43">Same for w−1 ..</S>
			<S sid ="68" ssid = "44">Hyphenated Words Hyphenated words w of the form s1s2 have a feature U -U set to 1 if both s1 and s2 are initCaps.</S>
			<S sid ="69" ssid = "45">If s1 is initCaps but not s2, then the features U =s1, L=s2, and U -L are set to 1.</S>
			<S sid ="70" ssid = "46">If s2 is initCaps but not s1, then the features U =s2, L=s1, and L-U are set to 1.</S>
			<S sid ="71" ssid = "47">Within Quotes/Brackets Sequences of tokens within quotes or brackets have a feature to indicate that they are within quotes.</S>
			<S sid ="72" ssid = "48">We found this feature useful for MISC class, where names such as movie names often appear within quotes.</S>
			<S sid ="73" ssid = "49">Rare Words If w is not found in FWL, then this feature is set to 1.</S>
			<S sid ="74" ssid = "50">Bigrams If (w−2 , w−1 ) is found in UBI for the name class nc, then the feature BInc is set to 1.</S>
			<S sid ="75" ssid = "51">Word Suffixes If w has a 3-letter suffix that can be found in SUF for the name class nc, then the feature SU F -nc is set to 1.</S>
			<S sid ="76" ssid = "52">Class Suffixes For w in a consecutive sequence of initCaps tokens (w, w+1 , . . .</S>
			<S sid ="77" ssid = "53">, w+n ), if any of the tokens from w+1 to w+n is found in the NCS list of the name class nc, then the feature N C S-nc is set to 1.</S>
			<S sid ="78" ssid = "54">Function Words If w is part of a sequence found in FUN, then this feature is set to 1.</S>
			<S sid ="79" ssid = "55">3.3 Global Features.</S>
			<S sid ="80" ssid = "56">The global features include: Unigrams If another occurrence of w in the same document has a previous word wp that can be found in UNI, then these words are used as features Other- occurrence-prev=wp.</S>
			<S sid ="81" ssid = "57">Bigrams If another occurrence of w has the feature BInc set to 1, then w will have the feature OtherBInc set to 1.</S>
			<S sid ="82" ssid = "58">Class Suffixes If another occurrence of w has the feature N C S-nc set to 1, then w will have the feature OtherN C S-nc set to 1.</S>
			<S sid ="83" ssid = "59">InitCaps of Other Occurrences This feature checks for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT zone) in the same document is initCaps or not.</S>
			<S sid ="84" ssid = "60">For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.</S>
			<S sid ="85" ssid = "61">Acronyms Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM).</S>
			<S sid ="86" ssid = "62">The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.</S>
			<S sid ="87" ssid = "63">Such sequences are given additional features of A begin, A continue, or A end, and the acronym is given a feature A unique.</S>
			<S sid ="88" ssid = "64">For example, if FCC and Federal Communications Commission are both found in a document, then Federal has A begin set to 1, Communications has A continue set to 1, Commission has A end set to 1, and FCC has A unique set to 1.</S>
			<S sid ="89" ssid = "65">Sequence of InitCaps In the sentence Even News Broadcasting Corp., noted for its accurate reporting, made the erroneous announcement., a NER may mistake Even News Broadcasting Corp. as an organization name.</S>
			<S sid ="90" ssid = "66">However, it is unlikely that other occurrences of News Broadcasting Corp. in the same document also co-occur with Even.</S>
			<S sid ="91" ssid = "67">This group of features attempts to capture such information.</S>
			<S sid ="92" ssid = "68">For every sequence of initial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified.</S>
			<S sid ="93" ssid = "69">For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest sub- string that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature Name Class of Previous Occurrences The name class of previous occurrences of w is used as a feature, similar to (Zhou and Su, 2002).</S>
			<S sid ="94" ssid = "70">We use the occurrence where w is part of the longest name class phrase (name class with the most number of tokens).</S>
			<S sid ="95" ssid = "71">For example, if w is the second token in a person name class phrase of 5 tokens, then a feature 2P erson5 is set to 1.</S>
			<S sid ="96" ssid = "72">During training, the name classes are known.</S>
			<S sid ="97" ssid = "73">During testing, the name classes are the ones already assigned to tokens in the sentences already processed.</S>
			<S sid ="98" ssid = "74">This last feature makes the order of processing important.</S>
			<S sid ="99" ssid = "75">As HL sentences usually contain less context, they are processed after the other sentences.</S>
			<S sid ="100" ssid = "76">3.4 Name List.</S>
			<S sid ="101" ssid = "77">In additional to the above features used by both ME1 and ME2, ME2 uses additional features derived from name lists compiled from a variety of sources.</S>
			<S sid ="102" ssid = "78">These sources are the Internet and the list provided by the organizers of this shared task.</S>
			<S sid ="103" ssid = "79">The list is a mapping of sequences of words to name classes.</S>
			<S sid ="104" ssid = "80">An example of an entry in the list is “JOHN KENNEDY : PERSON”.</S>
			<S sid ="105" ssid = "81">Words that are part of a sequence of words mapped to a name class nc will have a feature C LASS=nc set to 1.</S>
			<S sid ="106" ssid = "82">Another list of weekdays and month names is also used in the same way.</S>
			<S sid ="107" ssid = "83">For ME2, we have also manually added additional entries into the automatically compiled NCS lists.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
			<S sid ="108" ssid = "1">The English training and test data are part of the Reuters Corpus, Volume 11 . The German training and test data are part of the European Corpus Initiative, Multilingual Corpus 1.</S>
			<S sid ="109" ssid = "2">The best results obtained on the developement and test sets of the 2 languages are as shown in Table 2.</S>
			<S sid ="110" ssid = "3">Results in Table 1 are obtained by applying ME1, without the help of name lists, on the 2 languages.</S>
			<S sid ="111" ssid = "4">The best results for English are obtained using ME2, which made use of name lists compiled from the Internet and the list provided with the training set (See Section 3.4).</S>
			<S sid ="112" ssid = "5">The best results on German are obtained by using part-of-speech tags (provided in both training and test data) as an additional feature to the features used by ME1.</S>
			<S sid ="113" ssid = "6">For all experiments, features that occur only once in the training data are not used, and the GIS algorithm is run for 600 iterations.</S>
			<S sid ="114" ssid = "7">Running more iterations does not bring about any significant improvement to the accuracy.</S>
			<S sid ="115" ssid = "8">Our system usually does well for the LOC and PER class, but fails to do as well for the MISC and ORG class.</S>
			<S sid ="116" ssid = "9">The bad performance on the MISC class agrees with the observations of (Carreras et al., 2002).</S>
			<S sid ="117" ssid = "10">We felt that the of I continue set to 1, and Corp. has an additional feature of I end set to 1.</S>
			<S sid ="118" ssid = "11">1 http://about.reuters.</S>
			<S sid ="119" ssid = "12">com/researchandsta ndards/corpus/ English devel.</S>
			<S sid ="120" ssid = "13">English devel.</S>
			<S sid ="121" ssid = "14">English test English test German devel.</S>
			<S sid ="122" ssid = "15">German devel.</S>
			<S sid ="123" ssid = "16">German test German test Table 1: Results for development and test set for the two languages by ME1 MISC class is particularly difficult due to its generality (it can refer to anything from movie titles to sports events).</S>
			<S sid ="124" ssid = "17">Acknowledgements We would like to thank Yoong Keok Lee for helping us to apply boosting and feature selection to the maximum entropy algorithm, although these were not used in the final system.</S>
	</SECTION>
</PAPER>
