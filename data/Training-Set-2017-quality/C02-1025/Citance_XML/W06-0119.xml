<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes a Chinese word segmentor (CWS) for the third International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2006).</S>
		<S sid ="2" ssid = "2">We participate in the word segmentation task at the Microsoft Research (MSR) closed testing track.</S>
		<S sid ="3" ssid = "3">Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.</S>
		<S sid ="4" ssid = "4">From the scored results and our experimental results, it shows WSM can improve our previous CWS, which was reported at the SIGHAN Bakeoff 2005, about 1% of F-measure.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">A high-performance Chinese word segmentor (CWS) is a critical processing stage to produce an intermediate result for later processes, such as search engines, text mining, word spell checking, text-to-speech and speech recognition, etc. As per (Lin et al. 1993; Tsai et al. 2003; Tsai, 2005), the bottleneck for developing a high- performance CWS is to comprise of high performance Chinese unknown word identification (UWI).</S>
			<S sid ="6" ssid = "6">It is because Chinese is written without any separation between words and more than 50% words of the Chinese texts in web corpus are out-of-vocabulary (Tsai et al. 2003).</S>
			<S sid ="7" ssid = "7">In our report for the SIGHAN Bakeoff 2005 (Tsai, 2005), we have shown that a highly performance of 99.1% F-measure can be achieved while a BMM-based CWS using a perfect system dictionary (Tsai, 2005).</S>
			<S sid ="8" ssid = "8">A perfect system dictionary means all word types of the dictionary are extracted from training and testing gold standard corpus.</S>
			<S sid ="9" ssid = "9">Conventionally, there are four approaches to develop a CWS: (1) Dictionary-based approach (Cheng et al. 1999), especial forward and backward maximum matching (Wong and Chan, 1996); (2) Linguistic approach based on syntax-semantic knowledge (Chen et al. 2002); (3) Statistical approach based on statistical language model (SLM) (Sproat and Shih, 1990; Teahan et al. 2000; Gao et al. 2003); and (4) Hybrid approach trying to combine the benefits of dictionary-based, linguistic and statistical approaches (Tsai et al. 2003; Ma and Chen, 2003).</S>
			<S sid ="10" ssid = "10">In practice, statistical approaches are most widely used because their effective and reasonable performance.</S>
			<S sid ="11" ssid = "11">To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction; (2) Linguistic approach, three major types of linguistic rules (knowledge): morphology, syntax, and semantics, are used to identify unknown words; and (3) Hybrid approach, recently, one important trend of UWI follows a hybrid approach so as to take advantage of both merits of statistical and linguistic approaches.</S>
			<S sid ="12" ssid = "12">Statistical approaches are simple and efficient whereas linguistic approaches are effective in identifying low frequency unknown words (Chen et al. 2002).</S>
			<S sid ="13" ssid = "13">To develop WSD, there are two major types of word segmentation ambiguities while there are no unknown word problems with them: (1) Overlap Ambiguity (OA).</S>
			<S sid ="14" ssid = "14">Take string C1C2C3 130 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 130–133, Sydney, July 2006.</S>
			<S sid ="15" ssid = "15">Qc 2006 Association for Computational Linguistics comprised of three Chinese characters C1, C2 and C3 as an example.</S>
			<S sid ="16" ssid = "16">If its segmentation can be either C1C2/C3 or C1/C2C3 depending on context meaning, the C1C2C3 is called an overlapambiguity string (OAS), such as “將軍(a gen eral)/用(use)” and “將(to get)/軍用(for military use)” (the symbol “/” indicates a word bound ary).</S>
			<S sid ="17" ssid = "17">(2) Combination Ambiguity (CA).</S>
			<S sid ="18" ssid = "18">Take string C1C2 comprised of two Chinese characters C1 and C2 as an example.</S>
			<S sid ="19" ssid = "19">If its segmentation can be either C1/C2 or C1C2 depending on context meaning, the C1C2 is called a combina tion ambiguity string (CAS), such as “才(just)/ 能(can)” and “才能(ability).” Besides the OA and CA problems, the other two types of word segmentation errors are caused by unknown word problems.</S>
			<S sid ="20" ssid = "20">They are: (1) Lack of unknown word (LUW), it means segmentation error occurred by lack of an unknown word in the system dictionary, and (2) Error identified word (EIW), it means segmentation error occurred by an error identified unknown words.</S>
			<S sid ="21" ssid = "21">The goal of this paper is to report the approach and experiment results of our backward maximum matching-based (BMM-based) CWS with word support model (WSM) for the SIGHAN Bakeoff 2006.</S>
			<S sid ="22" ssid = "22">In (Tsai, 2006), WSM has been shown effectively to improve Chinese input system.</S>
			<S sid ="23" ssid = "23">In the third Bakeoff, our CWS is mainly addressed on improving its performance of OA/CA disambiguation by WSM.</S>
			<S sid ="24" ssid = "24">We show that WSM is able to improve our BMM-based CWS, which reported at the SIGHAN Bakeoff 2005, about 1% of F-measure.</S>
			<S sid ="25" ssid = "25">The remainder of this paper is arranged as follows.</S>
			<S sid ="26" ssid = "26">In Section 2, we present the details of our BMM-based CWS comprised of WSM.</S>
			<S sid ="27" ssid = "27">In Section 3, we present the scored results of the CWS at the Microsoft Research closed track and give our experiment results and analysis.</S>
			<S sid ="28" ssid = "28">Finally, in Section 4, we give our conclusions and future research directions.</S>
	</SECTION>
	<SECTION title="BMM-based CWS with WSM. " number = "2">
			<S sid ="29" ssid = "1">From our work (Tsai et al. 2004), the Chinese word segmentation performance of BMM technique is about 1% greater than that of forward maximum matching (FMM) technique.</S>
			<S sid ="30" ssid = "2">Thus, we adopt BMM technique as base to develop our CWS.</S>
			<S sid ="31" ssid = "3">In this Bakeoff, we use context-based Chinese unknown word identification (CCUWI) (Tsai, 2005) to resolve unknown word problem.</S>
			<S sid ="32" ssid = "4">The CCUWI uses template matching technique to extract unknown words from sentences.</S>
			<S sid ="33" ssid = "5">The context template includes triple context template (TCT) and word context template (WCT).</S>
			<S sid ="34" ssid = "6">The details of the CCUWI can be found in (Tsai, 2005).</S>
			<S sid ="35" ssid = "7">In (Tsai, 2006), we propose a new language model named word support model (WSM) and shown it can effectively perform homophone selection and word-syllable segmentation to improve Chinese input system.</S>
			<S sid ="36" ssid = "8">For this Bake- off, we use WSM to resolve OA/CA problems.</S>
			<S sid ="37" ssid = "9">The two steps of our BMM-based CWS with WSM are as below: Step 1.</S>
			<S sid ="38" ssid = "10">Generate the BMM segmentation for the given Chinese sentence by system dictionary.</S>
			<S sid ="39" ssid = "11">Step 2.</S>
			<S sid ="40" ssid = "12">Use WSM to resolve OA/CA problems for the BMM segmentation of Step 1.</S>
			<S sid ="41" ssid = "13">Now, we give a brief description of how we use WSM to resolve OA/CA problem.</S>
			<S sid ="42" ssid = "14">Firstly, we pre-collect OA/CA pattern-pairs (such as “就/ 是 ”-“ 就是 ”) by compare each training gold segmentation and its corresponding BMMsegmentation.</S>
			<S sid ="43" ssid = "15">The pattern of OA/CA pattern pairs can be a segmentation pattern, such as“就/是,” or just a word, such as “就是.” Sec ondly, for a BMM segmentation of Step 1, if one pattern matching (matching pattern) with at least one pattern of those pre-collected OA/CA pattern-pairs (matching OA/CA pattern-pairs), CWS will compute the word support degree for each pattern of the matching OA/CA pattern-pair.</S>
			<S sid ="44" ssid = "16">Finally, select out the pattern with maximum word support degree as its segmentation for the matching pattern.</S>
			<S sid ="45" ssid = "17">If the patterns of the matching OA/CA pattern- pair having the same word support degree, randomly select one to be its segmentation.</S>
			<S sid ="46" ssid = "18">The details of WSM can be found in (Tsai, 2006).</S>
	</SECTION>
	<SECTION title="Scored Results and Our Experiments. " number = "3">
			<S sid ="47" ssid = "1">In the SIGHAN Bakeoff 2006, there are four training corpus for word segmentation (WS) task: AS (Academia Sinica) and CU (City University of Hong Kong) are traditional Chinese corpus; PU (Peking University) and Microsoft Research (MSR) are simplified Chinese corpus.</S>
			<S sid ="48" ssid = "2">And, for each corpus, there are closed and open track.</S>
			<S sid ="49" ssid = "3">In the Bakeoff 2006, we attend the Microsoft Research closed (MSR_C) track.</S>
			<S sid ="50" ssid = "4">3.1 Scored Results and our Experiments.</S>
			<S sid ="51" ssid = "5">Tables 1a and 1b show the details of MSR training and testing corpus for 2nd (2005) and 3rd (2006) bakeoff.</S>
			<S sid ="52" ssid = "6">From Table 1a and 1b, it indi From Tables 2 and 3, we conclude that our CWS of 3rd bakeoff improve the CWS of 2nd bakeoff about 1.8% of F-measure.</S>
			<S sid ="53" ssid = "7">Among the 1.8% F-measure improvement, 1% is contributed by WSM for resolving OA/CA problems and the other 0.8% is contributed by CCUWI for resolving UWI problem.</S>
			<S sid ="54" ssid = "8">cates that MSR track of 3rd bakeoff seems to be a more difficult WS task than that of 2nd bakeoff, since (1) the training size of 2nd bakeoff is two times as great as that of 3rd bakeoff; (2) in training data, the word type number of 3rd bakeoff is less than that of 2nd bakeoff, and (3) in testing data, the word type number of 3rd bakeoff is greater than that of 2nd bakeoff.</S>
			<S sid ="55" ssid = "9">System R P F ROOV RIV a 0.949 0.897 0.922 0.022 0.982 b 0.954 0.921 0.937 0.163 0.981 c 0.950 0.930 0.940 0.272 0.974 Table 2.</S>
			<S sid ="56" ssid = "10">The scored results of our CWS in the MSR_C track (OOV is 0.034) for 3rd bakeoff.</S>
			<S sid ="57" ssid = "11">System R P F Improve Training Testing Sentences 86,924 3,985 Word types 88,119 12,924 Words 2,368,391 109,002 Character types 5,167 2,839 Characters 4,050,469 184,356 Table 1a.</S>
			<S sid ="58" ssid = "12">Details of MSR_C corpus of 2nd bake- off.</S>
			<S sid ="59" ssid = "13">Training Testing Sentences 46,364 4356 Word types 63,494 13,461 Words 1,266,169 100,361 Character types 4,767 3,103 Characters 2,169879 172,601 Table 1b.</S>
			<S sid ="60" ssid = "14">Details of MSR_C corpus of 3rd bake- off.</S>
			<S sid ="61" ssid = "15">Table 2 shows the scored results of our CWS at the MSR_C track of this bakeoff.</S>
			<S sid ="62" ssid = "16">In Table 2, the symbols a, b and c stand for the CWS with a, b and c system dictionary.</S>
			<S sid ="63" ssid = "17">The system dictionary “a” is the dictionary comprised of all word types found in the MSR training corpus.</S>
			<S sid ="64" ssid = "18">The system dictionary “b” is the dictionary comprised of “a” system dictionary and the word types found in the testing corpus by CCUWI with TCT knowledge.</S>
			<S sid ="65" ssid = "19">The system dictionary “c” is the dictionary comprised of “a” system dictionary and the word types found in the testing corpus by CCUWI with TCT and WCT knowledge.</S>
			<S sid ="66" ssid = "20">Table 3 is F-measure differences between the BMM-based CWS system and it with WSM and CCUWI using “a”, “b” and “c” system dictionary in the MSR_C track.</S>
			<S sid ="67" ssid = "21">a1.BMM 0.949 0.897 0.922 a2.BMM+WSM 0.958 0.907 0.932 0.010 b1.BMM 0.946 0.911 0.928 b2.BMM+WSM 0.954 0.921 0.937 0.009 c1.BMM 0.938 0.920 0.929 c2.BMM+WSM 0.950 0.930 0.940 0.011 Table 3.</S>
			<S sid ="68" ssid = "22">The F-measure improvement between the BMM-based CWS and it with WSM in the MSR_C track (OOV is 0.034) using a, b, and c system dictionary.</S>
			<S sid ="69" ssid = "23">3.2 Error Analysis.</S>
			<S sid ="70" ssid = "24">Table 4 shows the F-measure and ROOV differences between each result of our CWS with a, b and c system dictionaries.</S>
			<S sid ="71" ssid = "25">From Table 4, it indicates that the most contribution for increasing the overall performance (F-measure) of our CWS is occurred while our CWS comprised of WSM and CCUWI with TCT knowledge.</S>
			<S sid ="72" ssid = "26">System F F(d) ROOV ROOV(d) a 0.922 0.022 b 0.937 0.015 0.163 0.141 c 0.940 0.003 0.272 0.109 Table 4.</S>
			<S sid ="73" ssid = "27">The differences of F-measure and ROOV between nearby steps of our CWS.</S>
			<S sid ="74" ssid = "28">OA CA LUW EIW Table 5.</S>
			<S sid ="75" ssid = "29">The number of OAS (types), CAS (types), LUW (types) and EIW (types) for our CWS.</S>
			<S sid ="76" ssid = "30">Table 5 shows the distributions of four segmentation error types (OA, CA, LUW and EIW) for each result of our CWS with a and c system dictionaries.</S>
			<S sid ="77" ssid = "31">From Table 5, it shows CCUWI with the knowledge of TCT and WCT can be used to optimize the LUWEIW tradeoff.</S>
			<S sid ="78" ssid = "32">Moreover, it shows that WSM can effectively to reduce the number of OA/CA segmentation errors from 1,070 to 391.</S>
	</SECTION>
	<SECTION title="Conclusions and Future Directions. " number = "4">
			<S sid ="79" ssid = "1">In this paper, we have applied a BMM-based CWS comprised of a context-based UWI and word support model to the Chinese word segmentation.</S>
			<S sid ="80" ssid = "2">While we repeat the CWS with the MSR_C track data of 2nd bakeoff, we obtained 96.3% F-measure, which is 0.8% greater than that (95.5%) of our CWS at 2nd bakeoff.</S>
			<S sid ="81" ssid = "3">To sum up the results of this study, we have following conclusions and future directions: (1) UWI and OA/CA problems could be independent tasks for developing a CWS.</S>
			<S sid ="82" ssid = "4">The experiment results of this study support this observation.</S>
			<S sid ="83" ssid = "5">It is because we found 1% improvement is stable contributed by WSM and the other 0.8% improvement is stable contributed by the CCUWI while the BMM- based CWS with difference a, b and c system dictionaries and different MSR_C training and testing data of 2nd and 3rd bakeoff.</S>
			<S sid ="84" ssid = "6">(2) About 89% of segmentation errors of our CWS caused by unknown word problem.</S>
			<S sid ="85" ssid = "7">In the 89%, we found 66% is LUW problem and 23% is EIW problem.</S>
			<S sid ="86" ssid = "8">This result indicates that the major target to improve our CWS is CCUWI.</S>
			<S sid ="87" ssid = "9">The result also supports that a high performance CWS is relied on a high performance Chinese UWI (Tsai, 2005).</S>
			<S sid ="88" ssid = "10">(3) We will continue to expand our CWS with other unknown word identification tech niques, especially applying n-gram extractor with the TCT and WCT template matching technique to improve our CCUWI for attending the fourth SIGHAN Bakeoff.</S>
	</SECTION>
</PAPER>
