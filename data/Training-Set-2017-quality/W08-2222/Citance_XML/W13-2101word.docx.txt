Aligning Formal Meaning Representations with Surface Strings for
Wide-coverage Text Generation

Valerio Basile	Johan Bos

{v.basile,johan.bos}@rug.nl
Center for Language and Cognition Groningen (CLCG) 
University of Groningen, The Netherlands




Abstract

Statistical natural language generation 
from  abstract meaning representations 
presupposes  large corpora  consisting  of 
text–meaning  pairs.  Even though such 
corpora exist nowadays, or could be con- 
structed using robust semantic parsing, the 
simple alignment between text and mean- 
ing representation  is too coarse  for de- 
veloping robust (statistical) NLG systems. 
By reformatting  semantic representations 
as graphs, fine-grained  alignment  can be 
obtained. Given a precise alignment at the 
word level, the complete surface form of 
a meaning representations can be deduced 
using a simple declarative rule.

1   Introduction

Surface Realization is the task of producing flu- 
ent text from some kind of formal, abstract rep- 
resentation of meaning (Reiter and Dale, 2000). 
However, while it is obvious what the output of 
a natural language generation component should 
be, namely text, there is little to no agreement 
on what its input formalism should be (Evans et 
al., 2002). Since open-domain  semantic parsers 
are able to produce formal semantic representa- 
tions nowadays  (Bos, 2008; Butler and Yoshi- 
moto, 2012), it would be natural to see generation 
as a reversed process, and consider  such seman- 
tic representations as input of a surface realization 
component.
  The idea of using large text corpora annotated 
with formal semantic  representations for robust 
generation has been presented recently (Basile and 
Bos, 2011; Wanner et al., 2012). The need for for- 
mal semantic  representations  as a basis for NLG 
was  expressed  already much earlier by Power 
(1999), who derives semantic networks enriched 
with scope information from knowledge represen- 
tations for content planning. In this paper we take


a further  step towards the goal of generating text 
from deep semantic representations, and consider 
the issue of aligning  the representations with sur- 
face strings that capture their meaning.
  First we describe  the basic idea of  align- 
ing semantic representations (logical  forms) with 
surface strings in a formalism-independent  way 
(Section 2).   Then we apply our method  to a 
well-known  and widely-used semantic formalism, 
namely Discourse Representation Theory (DRT), 
first demonstrating  how to represent Discourse 
Representation Structures (DRSs) as graphs (Sec- 
tion 3) and showing that the resulting Discourse 
Representation Graphs (DRGs)  are equivalent  to 
DRSs but are  more convenient to fulfill  word- 
level alignment (Section 4). Finally, in Section 5 
we present a method that generates partial surface 
strings for each discourse referent occurring  in the 
semantic representation of a text, and composes 
them into a complete  surface form. All in all, we 
think this would be a first and important  step in 
surface realization from formal semantic represen- 
tations.

2   Aligning Logic with Text

Several different formal semantic representations 
have been proposed in the literature, and although 
they might differ in various aspects, they also have 
a lot in common. Many semantic representations 
(or logical forms  as they are sometimes referred 
to) are variants of first-order logic and share basic 
building  blocks such as entities, properties, and re- 
lations, complemented with quantifiers, negation 
and further scope operators.
  A simple snapshot of a formal meaning repre- 
sentation is the following (with symbols composed 
out of WordNet  (Fellbaum,  1998) synset identi- 
fiers to abstract away from natural language):
blue#a#1(x) ∧ cup#n#1(x)
  How could this logical  form be expressed in nat- 
ural language?  Or put differently, how could we




1

Proceedings of the 14th European Workshop on Natural  Language Generation,  pages 1–9, 
Sofia, Bulgaria, August 8-9 2013. Qc 2013 Association for Computational Linguistics


realize the variable x in text? As simple  as it is, x 
describes “a blue cup”, or if your target language 
is Italian, “una tazza blu”, or variants hereof, e.g. 
“every blue cup” (if x happens to be bound by uni- 
versally quantified) or perhaps  as “una tazza az- 
zurra”, using a different adjective to express blue- 
ness.   This works for simple examples, but how 
does it scale up to larger and more complex  se- 
mantic representations?
  In a way, NLG can be viewed as a machine 
translation (MT) task, but unlike translating from 
one natural language into another, the task is here 
to translate a formal (unambiguous) language into 
a natural language like English or Italian. Current 
statistical MT techniques are based on large paral- 
lel corpora of aligned source and target text. In this 
paper we introduce  a method for precise alignment 
of formal semantic representations and text, with 
the purpose of creating a large corpus that could 
be used in NLG research, and one that opens the 
way for statistical  approaches, perhaps similar to 
those used in MT.
  Broadly  speaking, alignments between seman- 
tic representations and surface strings can be made 
in three different ways. The simplest strategy, but 
also the least informative,  is to align a semantic 
representation with a sentence  or complete text 
without further information on which part of the 
representation produces what part of the surface 
form.  This might be enough to develop statisti- 
cal NLG systems for small sentences, but proba- 
bly does not scale up to handle larger texts. Alter- 
natively, one could devise more complex schemes 
that allow for a more fine-grained  alignment  be- 
tween parts of the semantic representation and sur-


a better  starting  point for the development of a 
statistical NLG component.  With sufficient  data 
in the form of aligned texts with semantic repre- 
sentations, these alignments  can be automatically 
learned, thus creating a model to generate surface 
forms from abstract, logical representations.
  However, aligning semantic representations 
with words is a difficult enterprise, primarily be- 
cause formal  semantic representations are not flat 
like  a  string of words and often form complex 
structures.  To overcome this issue we represent 
formal semantic  representations   as a set  of tu- 
ples. For instance, returning to our earlier exam- 
ple representation for “blue cup”, we could repre-
sent part of it by the tuples (blue#a#1,arg,x) and
(cup#n#1,arg,x). For convenience we can display
this as a graph (Figure 1).


blue#a#1 

                     x 
cup#n#1 

Figure 1: Logical form graph.

  Note that in this example several tuples are not 
shown  for clarity (such  as conjunction   and the 
quantifier). We show below that we can indeed 
represent every bit of semantic information in this 
format without sacrificing the capability of align- 
ment with the text. The important thing now is to 
show how alignments  between tuples and words 
can be realized, which is done by adding an ele- 
ment to each tuple denoting the surface string, for
instance (cup#n#1,arg,x,”tazza”),  as in Figure 2.


face strings (words  and phrases).  Here there are 
two routes to follow, which we call the minimal 
and maximal alignment.
In maximal alignment, each single piece of the


blue#a#1 




cup#n#1 



"blue"

"a" 	x
"cup" 


blue#a#1 




cup#n#1 



"blu"

"una" 	x
"tazza" 


semantic representation corresponds to the words 
that express that part of the meaning. Possible 
problems with this approach are that perhaps not 
every  bit of the semantic representation corre- 
sponds to a surface form, and a single word could 
also correspond to various  pieces in the seman- 
tic representation.   This is an interesting option 
to explore,  but in this paper we present the al- 
ternative approach, minimal alignment, which is 
a method where every word in the surface string 
points to exactly one part of the semantic repre- 
sentation. We think this alignment method forms


Figure 2: Logical form graphs aligned with sur- 
face forms in two languages.

  We can further refine the alignment by saying 
something about the local order of surface expres- 
sions. Again, this is done by adding an element 
to the tuple, in this case one that denotes the local 
order of a logical term. We will make this clear by 
continuing with our example, where we add word 
order encoded as numerical  indices  in the tuple,
e.g. (cup#n#1,arg,x,”tazza”,2),  as Figure 3 shows.
From these graphs  we can associate the term



blue#a#1 




cup#n#1 




"blue"
2
"a"     1       x
"cup"  3



blue#a#1 




cup#n#1 



"blu"
        3 "una"   1       
x "tazza"  2


cific 
tailoring  
the 
semantic 
representa
tion to 
one’s 
(technical)  
needs.  
Further, 
the 
formalism 
should 
have a 
model-
theoretic  
backbone, 
to ensure 
that the 
semantic 
representa
tions one 
works 
with actu- 
ally have 
an 
interpretati
on,  and 
can 
consequen
tly


Figure 3: Encoding local word order.


x with the surface strings “a blue cup” and “una 
tazza  blu”.   But the way we express  local or- 
der is not limited to words and can be employed 
for partial phrases  as well, if one adopts  a neo- 
Davidsonian  event semantics with explicit the- 
matic roles. This can be achieved by using the 
same kind of numerical  indices already used for 
the alignment of words. The example in Figure 4 
shows how to represent an event “hit” with its the- 
matic roles, preserving their relative order. We call 
surface forms “partial” or “incomplete” when they 
contain variables, and “complete”  when they only 
contain tokens. The corresponding partial surface 
form is then “y hit z”, where y and z are place- 
holders for surface strings.


agent 	y

1
hit#v#1 	"hit" 2	x
3


theme 	z
Figure 4: Graph for a neo-Davidsonian structure. 
This is the basic idea of aligning surface strings
with parts of a deep semantic representation.  Note
that precise alignment is only possible for words 
with a  lexical semantics that include first-order 
variables. For words that introduce  scope oper- 
ators (negation particles, coordinating conjuncts) 
we can’t have the cake and eat it: specifying the 
local order with respect to an entity or event vari- 
able directly and at the same time associating it 
with an operator isn’t always possible.  To solve 
this we introduce surface tuples that complement a 
semantic representation to facilitate perfect align- 
ment. We will explain this in more detail in the 
following sections.

3   Discourse Representation Graphs

The choice of semantic formalism should ideally 
be independent from the application of natural 
language generation itself, to avoid bias and spe-


be used in inference tasks using, for instance, auto- 
mated deduction for first-order logic. Given these 
criteria,  a good candidate is Discourse Represen- 
tation Theory, DRT (Kamp and Reyle, 1993), that 
captures the meaning of texts in the form of Dis- 
course Representation Structures (DRSs).
  DRSs  are capable  of effectively representing 
the meaning of natural language, covering many 
linguistic phenomena including  pronouns, quanti- 
fier scope, negation, modals, and presuppositions. 
DRSs are recursive structures put together by logi- 
cal and non-logical symbols, as in predicate logic, 
and in fact can be translated into first-order logic 
formulas (Muskens, 1996). The way DRSs  are 
nested inside each other give DRT the ability to 
explain the behaviour of pronouns and presuppo- 
sitions (Van der Sandt, 1992).
  Aligning DRSs with texts with fine granularity 
is hard because words and phrases introduce  dif- 
ferent kinds of semantic objects in a DRS: dis- 
course  referents,  predicates,  relations, but also 
logical operators such as negation, disjunction and 
implication that introduce  embedded DRSs.  A 
precise alignment of a DRS with its text on the 
level of words is therefore a non-trivial  task.
  To overcome this issue, we apply the idea pre- 
sented in the previous section to DRSs, making all 
recursion implicit by representing them as directed 
graphs. We call a graph  representing   a DRS a 
Discourse Representation Graph (DRG, in short). 
DRGs encode the same information  as DRSs, but 
are expressed  as a set of tuples. Essentially, this 
is done by reification over DRSs — every DRSs 
gets a unique label,  and the arity of DRS condi- 
tions increases by one for accommodating  a DRS 
label. This allows us to reformulate a DRS as a set 
of tuples.
  A DRS is an ordered pair of discourse refer- 
ents (variables over entities) and DRS-conditions. 
DRS-conditions  are basic (representing properties 
or relations) or complex (to handle negation and 
disjunction). To reflect these different  constructs, 
we distinguish three types of tuples in DRGs:
• (K,referent,X)  means that X is a discourse 
referent in K (referent tuples);
• (K,condition,C)  means that C is a condition


k1                        unary   
                 scope  k2 
k2                      referent e1 
k2                      referent x1 
k2                        event     pay
k2                      concept customer






k1 	unary 	






scope	k2 





event role 



referent 
 pay agent 
referent 




instance	e1 
internal 


external 


k2 	role	agent



concept 


x1 
instance 


customer  instance x1 
pay           instance e1 
agent        internal e1 
agent        external  x1



customer 



Figure 5: DRS and corresponding DRG (in tuples and in graph format) for “A customer did not pay.”




in K (condition  tuples), with various  sub- 
types: concept, event, relation, role, named, 
cardinality, attribute, unary, and binary;
• (C,argument,A) means that C is a condition 
with argument A (argument  tuples), with
the sub-types internal, external,  instance, 
scope, antecedent, and consequence.

  With the help of a concrete example, it is easy to 
see that DRGs have the same expressive power as 
DRSs. Consider for instance a DRS with negation, 
before and after labelling it (Figure 6):
  

Note that labelling conditions is crucial to dis- 
tinguish between syntactically  equivalent condi- 
tions occurring in different (embedded) DRSs. 
Unlike  Power’s  scoped semantic network for 
DRSs, we don’t make the assumption that condi- 
tions appear in the DRS in which their discourse 
referents are introduced  (Power,  1999). The ex- 
ample in Figure 6 illustrates that this assumption 
is not sound: the condition p(x) is in a different 
DRS than where its discourse referent x is intro- 
duced. Further note that our reification  proce- 
dure yields “flatter” representations than similar


x y 
r(x,y)
z





K1 :


x y c1 
:r(x,y)
z


formalisms 
(Copestake 
et al., 
1995; 
Reyle, 
1993), and 
this makes 
it more 
convenient 
to align 
surface 
strings 
with DRSs 
with a 
high 
granularity,  
as we


   p(x)
s(z,y)


c2 :K2 :


c3 :p(x)
c4 :s(z,y)


will show 
below.

4	Word-
Aligned 
DRGs


Figure 6: From DRS to DRG: labelling.


  Now, from the labelled DRS we can derive the 
following three referent  tuples: (K1,referent,x), 
(K1,referent,y),  and (K2,referent,z);  the follow- 
ing four condition tuples:  (K1,relation,c1:r), 
(K1,unary,c2:), (K2,concept,c3:p), and 
(K2,relation,c4:s); and the following argu- 
ment tuples:  (c1:r,internal,x),  (c1:r,external,y), 
(c2:,scope,K2), (c3:p,instance,x), 
(c4:s,internal,z),   and (c4:s,external,y).      From
these  tuples, it  is  straightforward  to  recreate 
a  labelled DRS, and by  dropping the labels 
subsequently, the original  DRS resurfaces again.
  For the sake of readability we sometimes leave 
out labels in examples throughout  this paper. In 
addition, we also show DRGs in graph-like pic- 
tures, where the tuples that form a DRG are the 
edges, and word-alignment  information  attached 
at the tuple level is shown as labels on the graph 
edges, as in Figure 9. In such graphs, nodes repre- 
senting discourse referents are square shaped, and 
nodes representing conditions are oval shaped.



In this section we show how the alignment be- 
tween surface text and its logical representation is 
realized by adding information of the tuples that 
make up a DRG. This sounds more straightfor- 
ward than it is. For some word classes this is in- 
deed easy to do.  For others we need additional 
machinery in the formalism. Let’s start with the 
straightforward cases. Determiners  are usually  as- 
sociated with referent tuples. Content words, such 
as nouns,  verbs,  adverbs and adjectives,  are typ- 
ically directly associated with one-place relation 
symbols, and can be naturally  aligned with argu- 
ment tuples. Verbs  are assigned to instance tu- 
ples linking its event condition; likewise,  nouns 
are typically  aligned to instance tuples which link 
discourse referents to the concepts they express; 
adjectives are aligned to tuples of attribute con- 
ditions. Finally, words expressing relations (such 
as prepositions),   are attached to the external ar- 
gument tuple linking the relation to the discourse 
referent playing the role of external argument.
Although the strategy presented for DRG–text


alignment is intuitive and straightforward  to im- 
plement, there are surface strings that don’t corre- 
spond to something explicit in the DRS. To this 
class belong punctuation  symbols,  and semanti- 
cally  empty  words such as (in English) the infiniti- 
val particle, pleonastic pronouns, auxiliaries, there 
insertion, and so on. Furthermore, function words 
such as “not”, “if”,  and “or”, introduce semantic 
material, but for the sake of surface string gener- 
ation could be better aligned with the event that 
they take the scope of. To deal with all these cases, 
we extend DRGs with surface tuples of the form
(K,surface,X), whose edges are decorated with the
required surface strings. Figure 7 shows an exam-
ple of a DRG extended with such surface tuples.

k1	unary	
	scope	k2
k2	referent e1
k2	referent x1
k2	event 	pay
k2	concept customer k2		role	agent customer  instance x1
p
a
y
           
i
n
s
t
a
n
c
e
 
e
1
 
a
g
e
n
t
       
i
n
t
e
r
n
a
l
 
e
1
 
a
g
e
n
t
        
e
x
t
e
r
n
a
l
   
x
1




1




2
4
1




A




cu
st
o
m
er 
pa
y
k2	surface e1
k2	surface e1
k2	surface e1
2
3
5
d
i
d
n
o
t
.

Figure 7: Word-aligned DRG for “A customer did 
not pay.”   All  alignment information  (including 
surface tuples) is highlighted.

  Note that surface tuples don’t have any influ- 
ence on the meaning of the original DRS – they 
just serve for the purpose of alignment of the re- 
quired surface strings. Also note in Figure 7 the 
indices that were added to some tuples. They serve 
to express the local order of surface information.
  Following the idea sketched in Section 2, the to- 
tal order of words is transformed into a local rank- 
ing of edges relative  to discourse referents. This is 
possible because the tuples that have word tokens 
aligned to them always have a discourse referent 
as third element (the head of the directed edge, in 
terms of graphs).  We group tuples that share the 
same discourse referent and then assign indices re- 
flecting the relative order of how these tuples are 
realized in the original text.
Illustrating this with our example in Figure 7,


we got two discourse referents:  x1 and e1.  The 
discourse referent x1 is associated with three tu- 
ples, of which two are indexed  (with indices 1 
and 2). Generating the surface string for x1 suc- 
ceeds by traversing the edges in the order speci- 
fied, resulting in [A,customer] for x1. The refer- 
ent e1  associates with six tuples, of which four 
are indexed  (with indices 1–4).   The order of 
these tuples would yield the partial surface string 
[x1,did,not,pay,.] for e1.
  Note that the manner in which DRSs are con- 
structed during  analysis ensures that all discourse 
referents  are linked to each other by taking the 
transitive closure of all binary relations appearing 
in a DRS, and therefore we can reconstruct the to- 
tal order from composing the local orders. In the 
next section we explain how this is done.

5   Surface Composition

In  this section we show in  detail how sur- 
face strings can be generated from word-aligned 
DRGs. It consists of two subsequent steps. First, 
a surface  form is associated with each discourse 
referent. Secondly, surface forms are put together 
in a bottom-up  fashion, to generate the complete 
output. During the composition, all of the dis- 
course referents are associated with their own sur- 
face representation.  The surface form associated 
with the discourse unit that contains all other dis- 
course units is then the text aligned with the origi- 
nal DRG.
  Surface forms of discourse referents are lists of 
tokens and other discourse referents.  Recall that 
the order of the elements of a discourse referent’s 
surface form is reflected by the local ordering of 
tuples,  as explained  in the previous section, and 
tuples with no index are simply ignored when re- 
constructing surface strings.
  The surface form is composed by taking  each 
tuple belonging to a specific discourse referent, in 
the correct order, and adding the tokens aligned 
with the tuple to a list representing the surface 
string for that discourse referent. An important 
part of this process is that binary DRS relations, 
represented in the DRG by a pair of internal and 
external  argument tuple, are  followed unidirec- 
tionally: if the tuple is of the internal type, then 
the discourse referent on the other end of the re- 
lation (i.e.  following its external tuple edge) is 
added to the list.  Surface forms for embedded 
DRSs include the discourse referents of the events




x1  : Michelle	e1 : x1  thinks p1
e1 : Michelle thinks p1




p1 : that e2


x2  : Obama	e2 : x2  smokes .
e2 : Obama smokes .
p1 : that Obama smokes .


k1  : e4


e
1
 
:
 
M
i
c
h
e
l
l
e
  
t
h
i
n
k
s
 
t
h
a
t
 
O
b
a
m
a
 
s
m
o
k
e
s
 
.
k
1
 
 
:
 
M
i
c
h
e
l
l
e
 
 
t
h
i
n
k
s
 
t
h
a
t
 
O
b
a
m
a
 
s
m
o
k
e
s
 
.

F
i
g
u
r
e
 
8
:
 
S
u
r
f
a
c
e
 
c
o
m
p
o
s
i
t
i
o
n
 
o
f
 
e
m
b
e
d
d
e
d
 
s
t
r
u
c
t
u
r
e
s
.






they contain.
  Typically, discourse units contain exactly one 
event (the main event of the clause).  Phenomena 
such as gerunds (e.g. “the laughing girl”) and rel- 
ative clauses (e.g. “the man who smokes”) may 
introduce more than one event in a discourse unit. 
To ensure correct order and grouping, we borrow 
a technique  from description logic (Horrocks and 
Sattler, 1999) and invert roles in DRGs. Rather
than representing “the laughing girl” as [girl(x) ∧
agent(e,x) ∧ laugh(e)],  we represent it as [girl(x)
∧ agent−1(x,e) ∧ laugh(e)],  making use of R(x,y)
≡ R−1(y,x) to preserve meaning. This “trick” en-
sures that we can describe the local order of noun
phrases with relative clauses and alike.
  To wrap things up, the composition operation is 
used to derive complete surface forms for DRGs. 
Composition  puts together  two surface  forms, 
where one of them is complete, and one of them 
is incomplete. It is formally defined as follows:


6   Selected 
Phenomena

We implemented  a first prototype using our 
align- ment and realization  method and 
tested it on ex- amples taken from the 
Groningen Meaning Bank, a large annotated 
corpus of texts paired with DRSs (Basile et 
al., 2012). Naturally, we came across 
phenomena that are notoriously  hard to 
analyze. Most of these we can handle 
adequately, but some we can’t currently 
account for and require further work.

6.1   Embedded 
Clauses

In the variant of DRT that we are using, 
propo- sitional arguments of verbs introduce  
embedded DRSs associated with a discourse  
referent. This is a good  test for our surface 
realization  formal- ism, because it would 
show that it is capable of re- cursively  
generating embedded clauses. Figure 9 
shows the DRG for the sentence “Michelle 
thinks that Obama smokes.”


ρ1 : τ 	ρ2 : Λ1ρ1Λ2
ρ2 : Λ1τ Λ2



(1)



referent 


michelle 




instance 
"Michelle"  1       x1 
ext 


where ρ1 and ρ2 are discourse referents, τ is a list
of tokens, and Λ1 and Λ2 are lists of word tokens 
and discourse referents. In the example from Fig- 
ure 7, the complete surface form for the discourse 
unit k1 is derived by means of composition  as for- 
mulated in (1) as follows:


x1  : A customer	e1 : x1  did not pay


named 

role 

k1	event role 



Agent 

referent 

think 

Theme 

referent 
"that" 


subordinates:prop 




int 
            1 instance	e1 
"thinks"    2
         3 int 


ext 

1       p1 










  role named 







referent 
Patient obama 

referent 








ext	x2 

int	1

instance 
"Obama" 


1


k2  : e1


e1 : A 
custom
er did 
not 
pay .
k2  : A 
customer 
did not pay 
.





event 


punctuation 
"."



3       e2 
             2 
instance 
"smokes" 


  The procedure for generation described here is 
reminiscent of the work of (Shieber, 1988) who 
also employs a deductive approach.  In particular 
our composition operation can be seen as a simpli- 
fied completion.
  Going back to the example in Section 4, sub- 
stituting the value of x1  in the incomplete sur- 
face form of e1  produces  the surface string 
[A,customer,did,not,pay,.] for e1.


smoke 


Figure 9:  Word-aligned DRG for the 
sentence
“Michelle thinks that Obama smokes.”

  Here the surface forms of two discourse 
units (main  and embedded) are generated.  
In order to generate the complete surface 
form, first the em- bedded clause is 
generated, and then composed


with the incomplete  surface form of the main 
clause.  As noted earlier, during the composition 
process, the complete surface form for each dis- 
course referent is generated (Figure  8), showing 
a clear alignment  between the entities of the se- 
mantic representation and the surface forms they 
represent.

6.2   Coordination

Coordination is another good test case for a lin- 
guistic formalism. Consider for instance “Sub- 
sistence  fishing and commercial trawling occur 
within refuge waters”,  where two noun phrases are 
coordinated, giving rise to either a distributive (in- 
troducing two events in the DRS) or a collective 
interpretation  (introducing  a set formation  of dis- 
course referents in the DRS). We can account for 
both interpretations (Figure 10).
  Note that, interestingly,  using the distributive 
interpretation DRG as input to the surface realiza- 
tion component could result, depending on how 
words are aligned, in a surface  form “fishing oc- 
curs and trawling  occurs”, or as “fishing  and trawl- 
ing occur”.

6.3   Long-Distance Dependencies

Cases   of  extraction,   for  instance with  WH- 
movement, could be problematic to capture with 
our formalism. This is in particular  an issue when 
extraction  crosses more than one clause boundary, 
as in “Which car does Bill believe John bought”. 
Even though  these cases are rare in the real world, 
a complete formalism  for surface realization must 
be able to deal with such cases. The question is 
whether this is a separate generation  task in the 
domain of syntax (White et al., 2007), or whether 
the current formalism needs to be adapted to cover 
such long-distance dependencies. Another  range 
of complications are caused by discontinuous con- 
stituents,  as in the Dutch sentence “Ik heb kaart- 
jes gekocht voor Berlijn” (literally: “I have tick- 
ets bought  for Berlin”), where the prepositional 
phrase “voor Berlijn” is an argument of the noun 
phrase “kaartjes”. In our formalism the only align- 
ment possible would result in the sentence “Ik 
heb kaartjes voor Berlijn gekocht”, which is ar- 
guably a more fluent realization  of the sentence, 
but doesn’t correspond to the original text. If one 
uses the original  text as gold standard, this could 
cause problems  in evaluation.   (One could also 
benefit from this deficiency, and use it to generate


more than one gold standard surface string. This 
is something to explore in future work.)

6.4   Control Verbs

In constructions like “John wants to swim”, the 
control verb “wants” associates its own subject 
with the subject of the infinitival clause that it has 
as argument. Semantically, this is realized by vari- 
able binding. Generating an appropriate surface 
form for semantic representation with controlled 
variables is a challenge:   a naive approach would 
generate “John wants John to swim”. One possi- 
ble solution is to add another derivation  rule for 
surface composition  dedicated to deal with cases 
where a placeholder variable occurs in more than 
one partial surface form, substituting  a null string 
for a variable  following some heuristic rules. A 
second, perhaps more elegant solution  is to inte- 
grate a language model into the surface composi- 
tion process.

7   Related work

Over the years, several systems have emerged that 
aim at generate surface forms from different kind 
of abstract input representations. An overview of 
the state-of-the-art is showcased by the submis- 
sions to the Surface Realization Shared Task (Belz 
et al., 2012). Bohnet et al. (2010), for instance, 
employ deep structures derived from the CoNLL
2009 shared task, essentially sentences annotated 
with shallow semantics, lemmata and dependency 
trees; as the authors state, these annotations are not 
made with generation in mind, and they necessi- 
tate complex  preprocessing steps in order to derive 
syntactic trees, and ultimately  surface forms. The 
format presented in this work has been especially 
developed with statistical approaches in mind.
  Nonetheless,  there is very little work on ro- 
bust, wide-scale generation from DRSs, surpris- 
ingly perhaps given the large body of theoretical 
research carried out in the framework of Discourse 
Representation Theory, and practical implemen- 
tations and annotated corpora  of DRSs that are 
nowadays available (Basile et al., 2012). This is 
in contrast to the NLG work in the framework of 
Lexical Functional Grammar (Guo et al., 2011).
  Flat  representation   of  semantic representa- 
tions, like the DRGs that we present, have also 
been put forward to facilitate machine translation 
(Schiehlen et al., 2000) and for evaluation pur- 
poses (Allen et al., 2008), and semantic parsing



event 



occur 

referent 



instance 
"occur"

2
e1 


referent 



occur 





instance 
"occur"     2       e1 


internal    1


event 


internal    1






  role 
concept 


theme 


fishing 

referent 


external 
instance 
"fishing"    1       x1 



role 


theme 


referent surface 



external 




k1	event 





occur 




instance 


"and"	2       x3 
int
er
na
l    
1
3


referent 
surface 


"occur"


3

e2 


k1	relation 
concept 

relation 


superset 


fishing 


external 
internal instance 




role 


"and" 	1
            2
 
in
te
rn
al 



superset 

referent 


"fishing"
1       x1 


external 


concept 


theme 


external 




trawling 

referent 



instance 
"trawling"  1       x2 





concept 


referent 




trawling 




x2 
instance      1 "trawling"




Figure 10: Analysis of NP coordination, in a distributive  (left) and a collective  interpretation (right).




(Le and Zuidema, 2012) just because they’re eas- 
ier and more efficient to process.  Packed seman- 
tic representations (leaving scope underspecified) 
also resemble flat representations (Copestake et 
al., 1995; Reyle, 1993) and can be viewed as 
graphs, however they show less elaborated reifica- 
tion than the DRGs presented in this paper, and are 
therefore less suitable for precise alignment with 
surface strings.


8   Conclusion

We presented a formalism  to align logical forms, 
in particular Discourse Representation Structures, 
with surface text strings. The resulting graph rep- 
resentations (DRGs),  make recursion implicit by 
reification over nested DRSs. Because of their 
“flat”  structure, DRGs can be precisely aligned 
with the text they represent at the word level. This 
is key to open-domain statistical  Surface Real- 
ization, where words are learned  from abstract, 
syntactic or semantic,  representations,  but also 
useful for other applications  such as learning  se- 
mantic representations directly  from text (Le and 
Zuidema, 2012). The actual alignment  between


the tuples that form a DRG and the surface forms 
they represent is not trivial, and requires to make 
several choices.
  Given the alignment with text, we show that it 
is possible to directly  generate surface forms from 
automatically  generated word-aligned  DRGs. To 
do so, a declarative  procedure  is presented, that 
generates complete  surface  forms from aligned 
DRGs in a compositional  fashion. The method 
works in a bottom-up  way, using discourse ref- 
erents  as starting  points, then generating  a sur- 
face form for each of them, and finally compos- 
ing all of the surface form together into a com- 
plete text. We are currently  building  a large corpus 
of word-aligned  DRSs, and are investigating  ma- 
chine learning methods that could automatically 
learn the alignments.
  Surprisingly, given that DRT is one of the best 
studied formalisms in formal semantics, there isn’t 
much work on generation from DRSs so far. The 
contribution of this paper  presents a method  to 
align DRSs with surface strings,  that paves the 
way for robust, statistical methods for surface gen- 
eration from deep semantic representations.


References

James F. Allen, Mary Swift, and Will de Beaumont.
2008. Deep Semantic Analysis  of Text.  In Johan 
Bos and Rodolfo Delmonte,  editors,  Semantics in 
Text Processing.  STEP 2008 Conference Proceed- 
ings, volume 1 of Research in Computational  Se- 
mantics, pages 343–354. College Publications.

Valerio Basile and Johan Bos. 2011. Towards generat- 
ing text from discourse representation structures. In 
Proceedings of the 13th European Workshop on Nat- 
ural Language Generation  (ENLG),  pages 145–150, 
Nancy, France.

Valerio Basile, Johan Bos, Kilian Evang, and Noortje 
Venhuizen.  2012. Developing a large semantically 
annotated corpus. In Proceedings of the Eight In- 
ternational Conference on Language Resources and 
Evaluation  (LREC 2012), pages 3196–3200, Istan- 
bul, Turkey.

Anja Belz, Bernd Bohnet, Simon Mille, Leo Wanner, 
and Michael White. 2012. The surface realisation 
task: Recent developments and future plans. In Bar- 
bara Di Eugenio,  Susan McRoy, Albert Gatt, Anja 
Belz, Alexander Koller, and Kristina Striegnitz, ed- 
itors, INLG, pages 136–140.  The Association for 
Computer Linguistics.

Bernd Bohnet,  Leo Wanner,  Simon Mille,  and Ali- 
cia Burga.    2010.   Broad coverage  multilingual 
deep sentence generation with a stochastic  multi- 
level realizer.  In Proceedings of the 23rd Inter- 
national Conference on Computational Linguistics, 
pages 98–106.

Johan Bos.  2008. Wide-Coverage Semantic Analy- 
sis with Boxer. In J. Bos and R. Delmonte, editors, 
Semantics in Text Processing. STEP 2008 Confer- 
ence Proceedings, volume 1 of Research in Compu- 
tational Semantics, pages 277–286. College Publi- 
cations.

Alastair Butler and Kei Yoshimoto. 2012. Banking 
meaning representations from treebanks. Linguis- 
tic Issues in Language Technology - LiLT, 7(1):1– 
âA˘ S¸ 22.

Ann Copestake, Dan Flickinger,  Rob Malouf, Susanne 
Riehemann, and Ivan Sag. 1995. Translation us- 
ing Minimal Recursion Semantics.  In Proceedings 
of the Sixth International  Conference on Theoretical 
and Methodological Issues in Machine Translation, 
pages 15–32, University of Leuven, Belgium.

Roger Evans, Paul Piwek, and Lynne Cahill.  2002.
What is NLG? In Proceedings of the Second Inter- 
national Conference on Natural Language Genera- 
tion, pages 144–151.

Christiane Fellbaum, editor. 1998. WordNet. An Elec- 
tronic Lexical Database. The MIT Press.


Yuqing Guo, Haifeng  Wang, and Josef van Genabith.
2011. Dependency-based n-gram models for gen- 
eral purpose sentence realisation. Natural Language 
Engineering, 17:455–483.

Ian Horrocks and Ulrike  Sattler.  1999.  A descrip- 
tion logic with transitive and inverse roles and role 
hierarchies.	Journal of logic and computation,
9(3):385–410.

Hans Kamp and Uwe Reyle. 1993. From Discourse 
to Logic; An Introduction to Modeltheoretic  Seman- 
tics of Natural Language, Formal Logic and DRT. 
Kluwer, Dordrecht.

Phong Le and Willem Zuidema. 2012. Learning com- 
positional semantics for open domain semantic pars- 
ing. Forthcoming.

Reinhard Muskens.  1996. Combining Montague Se- 
mantics and Discourse Representation.  Linguistics 
and Philosophy, 19:143–186.

R. Power. 1999. Controlling logical scope in text gen- 
eration. In Proceedings of the 7th European Work- 
shop on Natural Language Generation,  Toulouse, 
France.

E. Reiter and R. Dale. 2000. Building Natural Lan- 
guage Generation  Systems.  Cambridge  University 
Press.

Uwe Reyle. 1993. Dealing with Ambiguities by Un- 
derspecification:  Construction, Representation and 
Deduction. Journal of Semantics, 10:123–179.

Michael Schiehlen,  Johan Bos, and Michael Dorna.
2000. Verbmobil interface terms (vits). In Wolfgang 
Wahlster, editor, Verbmobil: Foundations of Speech- 
to-Speech Translation. Springer.

Stuart M. Shieber. 1988. A uniform architecture for 
parsing and generation.  In Proceedings of the 12th 
conference on Computational linguistics - Volume
2, COLING ’88, pages 614–619,  Stroudsburg,  PA, 
USA. Association for Computational Linguistics.

Rob A. Van der Sandt. 1992. Presupposition Projec- 
tion as Anaphora Resolution.  Journal  of Semantics,
9:333–377.

Leo Wanner, Simon Mille, and Bernd Bohnet. 2012.
Towards a surface realization-oriented  corpus anno- 
tation. In Proceedings of the Seventh International 
Natural Language Generation  Conference,  INLG
’12, pages 22–30, Stroudsburg, PA, USA. Associ- 
ation for Computational Linguistics.

Michael White, Rajakrishnan  Rajkumar, and Scott 
Martin. 2007. Towards broad coverage surface real- 
ization with CCG. In Proceedings of the Workshop 
on Using Corpora for NLG: Language Generation 
and Machine Translation (UCNLG+MT).

