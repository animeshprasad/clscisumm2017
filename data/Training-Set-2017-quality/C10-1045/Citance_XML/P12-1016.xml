<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">(1) ._}...-ll . When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors.</S>
		<S sid ="2" ssid = "2">To address this issue, we present a target-side, class-based agreement model.</S>
		<S sid ="3" ssid = "3">Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis.</S>
		<S sid ="4" ssid = "4">For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline.</S>
		<S sid ="5" ssid = "5">The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Languages vary in the degree to which surface forms reflect grammatical relations.</S>
			<S sid ="7" ssid = "7">English is a weakly inflected language: it has a narrow verbal paradigm, restricted nominal inflection (plurals), and only the vestiges of a case system.</S>
			<S sid ="8" ssid = "8">Consequently, translation into English—which accounts for much of the machine translation (MT) literature (Lopez, 2008)—often involves some amount of morpho-syntactic dimension- ality reduction.</S>
			<S sid ="9" ssid = "9">Less attention has been paid to what happens during translation from English: richer grammatical features such as gender, dual number, and overt case are effectively latent variables that must be inferred during decoding.</S>
			<S sid ="10" ssid = "10">Consider the output of Google Translate for the simple English sentence in Fig.</S>
			<S sid ="11" ssid = "11">1.</S>
			<S sid ="12" ssid = "12">The correct translation is a monotone mapping of the input.</S>
			<S sid ="13" ssid = "13">However, in Arabic, SVO word order requires both gender and number agreement between the subject ._}...-ll ‘the car’ and verb . ‘go’.</S>
			<S sid ="14" ssid = "14">The MT system selects the correct verb stem, but with masculine inflection.</S>
			<S sid ="15" ssid = "15">Although the translation has the-carsg.def.fem gosg.masc with-speedsg.fem The car goes quickly Figure 1: Ungrammatical Arabic output of Google Translate for the English input The car goes quickly.</S>
			<S sid ="16" ssid = "16">The subject should agree with the verb in both gender and number, but the verb has masculine inflection.</S>
			<S sid ="17" ssid = "17">For clarity, the Arabic tokens are arranged left-to-right.</S>
			<S sid ="18" ssid = "18">the correct semantics, it is ultimately ungrammatical.</S>
			<S sid ="19" ssid = "19">This paper addresses the problem of generating text that conforms to morpho-syntactic agreement rules.</S>
			<S sid ="20" ssid = "20">Agreement relations that cross statistical phrase boundaries are not explicitly modeled in most phrase- based MT systems (Avramidis and Koehn, 2008).</S>
			<S sid ="21" ssid = "21">We address this shortcoming with an agreement model that scores sequences of fine-grained morpho- syntactic classes.</S>
			<S sid ="22" ssid = "22">First, bound morphemes in translation hypotheses are segmented.</S>
			<S sid ="23" ssid = "23">Next, the segments are labeled with classes that encode both syntactic category information (i.e., parts of speech) and grammatical features such as number and gender.</S>
			<S sid ="24" ssid = "24">Finally, agreement is promoted by scoring the predicted class sequences with a generative Markov model.</S>
			<S sid ="25" ssid = "25">Our model scores hypotheses during decoding.</S>
			<S sid ="26" ssid = "26">Unlike previous models for scoring syntactic relations, our model does not require bitext annotations, phrase table features, or decoder modifications.</S>
			<S sid ="27" ssid = "27">The model can be implemented using the feature APIs of popular phrase-based decoders such as Moses (Koehn et al., 2007) and Phrasal (Cer et al., 2010).</S>
			<S sid ="28" ssid = "28">Intuition might suggest that the standard n-gram language model (LM) is sufficient to handle agreement phenomena.</S>
			<S sid ="29" ssid = "29">However, LM statistics are sparse, and they are made sparser by morphological variation.</S>
			<S sid ="30" ssid = "30">For English-to-Arabic translation, we achieve a +1.04 BLEU average improvement by tiling our model on top of a large LM.</S>
			<S sid ="31" ssid = "31">146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 146–155, Jeju, Republic of Korea, 814 July 2012.</S>
			<S sid ="32" ssid = "32">Qc 2012 Association for Computational Linguistics It has also been suggested that this setting requires morphological generation because the bitext may not Pron+Fem+Sg Verb+Masc+3+Pl Prt Conj contain all inflected variants (Minkov et al., 2007; Toutanova et al., 2008; Fraser et al., 2012).</S>
			<S sid ="33" ssid = "33">However, using lexical coverage experiments, we show that it there is ample room for translation quality improvements through better selection of forms that already exist in the translation model.</S>
			<S sid ="34" ssid = "34">they write will and</S>
	</SECTION>
	<SECTION title="A Class-based Model of Agreement" number = "2">
			<S sid ="35" ssid = "1">2.1 Morpho-syntactic Agreement.</S>
			<S sid ="36" ssid = "2">Morpho-syntactic agreement refers to a relationship between two sentence elements a and b that must have at least one matching grammatical feature.1 Agreement relations tend to be defined for particular syntactic configurations such as verb-subject, noun-adjective, and pronoun-antecedent.</S>
			<S sid ="37" ssid = "3">In some languages, agreement affects the surface forms of the words.</S>
			<S sid ="38" ssid = "4">For example, from the perspective of generative grammatical theory, the lexicon entry for the Arabic nominal ._}...-ll ‘the car’ contains a femininegender feature.</S>
			<S sid ="39" ssid = "5">When this nominal appears in the sub ject argument position, the verb-subject agreement relationship triggers feminine inflection of the verb.</S>
			<S sid ="40" ssid = "6">Our model treats agreement as a sequence of scored, pairwise relations between adjacent words.</S>
			<S sid ="41" ssid = "7">Of course, this assumption excludes some agreement phenomena, but it is sufficient for many common cases.</S>
			<S sid ="42" ssid = "8">We focus on EnglishArabic translation as an example of a translation direction that expresses substantially more morphological information in the target.</S>
			<S sid ="43" ssid = "9">These relations are best captured in a target- side model because they are mostly unobserved (from lexical clues) in the English source.</S>
			<S sid ="44" ssid = "10">The agreement model scores sequences of morpho- syntactic word classes, which express grammatical features relevant to agreement.</S>
			<S sid ="45" ssid = "11">The model has three components: a segmenter, a tagger, and a scorer.</S>
			<S sid ="46" ssid = "12">2.2 Morphological Segmentation.</S>
			<S sid ="47" ssid = "13">Segmentation is a procedure for converting raw surface forms to component morphemes.</S>
			<S sid ="48" ssid = "14">In some languages, agreement relations exist between bound morphemes, which are syntactically independent yet phonologically dependent morphemes.</S>
			<S sid ="49" ssid = "15">For example, 1 We use morpho-syntactic and grammatical agreement inter-.</S>
			<S sid ="50" ssid = "16">Figure 2: Segmentation and tagging of the Arabic token};._....S:...�_, ‘and they will write it’.</S>
			<S sid ="51" ssid = "17">This token has four seg ments with conflicting grammatical features.</S>
			<S sid ="52" ssid = "18">For example, the number feature is singular for the pronominal object and plural for the verb.</S>
			<S sid ="53" ssid = "19">Our model segments the raw token, tags each segment with a morpho-syntactic class (e.g., “Pron+Fem+Sg”), and then scores the class sequences.</S>
			<S sid ="54" ssid = "20">the single raw token in Fig.</S>
			<S sid ="55" ssid = "21">2 contains at least four grammatically independent morphemes.</S>
			<S sid ="56" ssid = "22">Because the morphemes bear conflicting grammatical features and basic parts of speech (POS), we need to segment the token before we can evaluate agreement relations.2 Segmentation is typically applied as a bitext pre- processing step, and there is a rich literature on the effect of different segmentation schemata on translation quality (Koehn and Knight, 2003; Habash and Sadat, 2006; El Kholy and Habash, 2012).</S>
			<S sid ="57" ssid = "23">Unlike previous work, we segment each translation hypothesis as it is generated (i.e., during decoding).</S>
			<S sid ="58" ssid = "24">This permits greater modeling flexibility.</S>
			<S sid ="59" ssid = "25">For example, it may be useful to count tokens with bound morphemes as a unit during phrase extraction, but to score segmented morphemes separately for agreement.</S>
			<S sid ="60" ssid = "26">We treat segmentation as a character-level sequence modeling problem and train a linear-chain conditional random field (CRF) model (Lafferty et al., 2001).</S>
			<S sid ="61" ssid = "27">As a pre-processing step, we group contiguous non-native characters (e.g., Latin characters in Arabic text).</S>
			<S sid ="62" ssid = "28">The model assigns four labels: • I: Continuation of a morpheme • O: Outside morpheme (whitespace) • B: Beginning of a morpheme • F: Non-native character(s) 2 Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey Translation Model e Target sequence of I words f Source sequence of J words a Sequence of K phrase alignments for (e, f ) Π Permutation of the alignments for target word order e h Sequence of M feature functions λ Sequence of learned weights for the M features H A priority queue of hypotheses Class-based Agreement Model t ∈ T Set of morpho-syntactic classes s ∈ S Set of all word segments θseg Learned weights for the CRF-based segmenter θtag Learned weights for the CRF-based tagger φo , φt CRF potential functions (emission and transition) τ Sequence of I target-side predicted classes π T dimensional (log) prior distribution over classesSet of Classes The tagger assigns morpho syntactic classes, which are coarse POS categories refined with grammatical features such as gender and definiteness.</S>
			<S sid ="63" ssid = "29">The coarse categories are the universal POS tag set described by Petrov et al.</S>
			<S sid ="64" ssid = "30">(2012).</S>
			<S sid ="65" ssid = "31">More than 25 tree- banks (in 22 languages) can be automatically mapped to this tag set, which includes “Noun” (nominals), “Verb” (verbs), “Adj” (adjectives), and “ADP” (pre- and post-positions).</S>
			<S sid ="66" ssid = "32">Many of these treebanks also contain per-token morphological annotations.</S>
			<S sid ="67" ssid = "33">It is easy to combine the coarse categories with selected grammatical annotations.</S>
			<S sid ="68" ssid = "34">sˆ Sequence of l word segments σ Model state: a tagged segment (s, t) Figure 3: Notation used in this paper.</S>
			<S sid ="69" ssid = "35">The convention eI indicates a subsequence of a length I sequence.</S>
			<S sid ="70" ssid = "36">The features are indicators for (character, position, label) triples for a five character window and bigram label transition indicators.</S>
			<S sid ="71" ssid = "37">This formulation is inspired by the classic “IOB” text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al., 2004).</S>
			<S sid ="72" ssid = "38">It can be learned from gold-segmented data, generally applies to languages with bound morphemes, and does not require a hand- compiled lexicon.3 Moreover, it has only four labels, so Viterbi decoding is very fast.</S>
			<S sid ="73" ssid = "39">We learn the parameters θseg using a quasi-Newton (QN) procedure with l1 (lasso) regularization (Andrew and Gao, 2007).</S>
			<S sid ="74" ssid = "40">2.3 Morpho-syntactic Tagging.</S>
			<S sid ="75" ssid = "41">After segmentation, we tag each segment with a fine- grained morpho-syntactic class.</S>
			<S sid ="76" ssid = "42">For this task we also train a standard CRF model on full sentences with gold classes and segmentation.</S>
			<S sid ="77" ssid = "43">We use the same QN procedure as before to obtain θtag . A translation derivation is a tuple (e, f, a) where e is the target, f is the source, and a is an alignment between the two.</S>
			<S sid ="78" ssid = "44">The CRF tagging model predicts a target-side class sequence τ ∗ I τ ∗ = arg max ) θtag · {φo(τi, i, e) + φt(τi, τi−1)} i=1 where further notation is defined in Fig.</S>
			<S sid ="79" ssid = "45">3.</S>
	</SECTION>
	<SECTION title="Mada, the standard tool for Arabic segmentation (Habash. " number = "3">
			<S sid ="80" ssid = "1">For Arabic, we used the coarse POS tags plus definiteness and the so-called phi features (gender, number, and person).4 For example, ._}...-ll ‘the car’ would be tagged “Noun+Def+Sg+Fem”.</S>
			<S sid ="81" ssid = "2">We restricted the set of classes to observed combinations in the training data, so the model implicitly disallows incoherent classes like “Verb+Def”.</S>
			<S sid ="82" ssid = "3">Features The tagging CRF includes emission features φo that indicate a class τi appearing with various orthographic characteristics of the word sequence being tagged.</S>
			<S sid ="83" ssid = "4">In typical CRF inference, the entire observation sequence is available throughout inference, so these features can be scored on observed words in an arbitrary neighborhood around the current position i. However, we conduct CRF inference in tandem with the translation decoding procedure (§3), creating an environment in which subsequent words of the observation are not available; the MT system has yet to generate the rest of the translation when the tagging features for a position are scored.</S>
			<S sid ="84" ssid = "5">Therefore, we only define emission features on the observed words at the current and previous positions of a class: φo(τi, ei, ei−1).</S>
			<S sid ="85" ssid = "6">The emission features are word types, prefixes and suffixes of up to three characters, and indicators for digits and punctuation.</S>
			<S sid ="86" ssid = "7">None of these features are language specific.</S>
			<S sid ="87" ssid = "8">Bigram transition features φt encode local agreement relations.</S>
			<S sid ="88" ssid = "9">For example, the model learns that the Arabic class “Noun+Fem” is followed by “Adj+Fem” and not “Adj+Masc” (noun-adjective gender agreement).</S>
			<S sid ="89" ssid = "10">2.4 Word Class Sequence Scoring.</S>
			<S sid ="90" ssid = "11">The CRF tagger model defines a conditional distribution p(τ |e; θtag ) for a class sequence τ given a sentence e and model parameters θtag . That is, the sample space is over class—not word—sequences.</S>
			<S sid ="91" ssid = "12">However, in MT, we seek a measure of sentence quality q(e) that is comparable across different hypotheses on the beam (much like the n-gram language model score).</S>
			<S sid ="92" ssid = "13">Discriminative model scores have been used as MT features (Galley and Manning, 2009), but we obtained better results by scoring the 1-best class sequences with a generative model.</S>
			<S sid ="93" ssid = "14">We trained a simple add-1 smoothed bigram language model over gold class sequences in the same treebank training data: I Input: implicitly defined search space generate initial hypotheses and add to H set Hf inal to ∅ while H is not empty: set Hext to ∅ for each hypothesis η in H : if η is a goal hypothesis: add η to Hf inal else Extend η and add to Hext ..,Score agreement Recombine and Prune Hext set H to Hext Output: argmax of Hf inal Figure 4: Breadth-first beam search algorithm of Och and Ney (2004).</S>
			<S sid ="94" ssid = "15">Typically, a hypothesis stack H is maintained for each unique source coverage set.</S>
			<S sid ="95" ssid = "16">Input: (eI , n, is_goal) run segmenter on attachment eI to get sˆL q(e) = p(τ ) = n p(τi|τi−1) et model state σ = s, t for translation prefix en i=1 We chose a bigram model due to the aggressive g initialize π to −∞ set π(t) = 0 n+1 1 ) 1 recombination strategy in our phrase-based decoder.</S>
			<S sid ="96" ssid = "17">compute τ ∗ from parameters s, sˆL , π, is_goal) For contexts in which the LM is guaranteed to back off (for instance, after an unseen bigram), our decoder compute q(eI ) = p(τ ∗) under the generative LM L ) for prefix eI maintains only the minimal state needed (perhaps only Output : q(eI ) a single word).</S>
			<S sid ="97" ssid = "18">In less restrictive decoders, higher order scoring models could be used to score longer- distance agreement relations.</S>
			<S sid ="98" ssid = "19">We integrate the segmentation, tagging, and scoring models into a self-contained component in the translation decoder.</S>
			<S sid ="99" ssid = "20">3 Inference during Translation Decoding.</S>
			<S sid ="100" ssid = "21">Scoring the agreement model as part of translation decoding requires a novel inference procedure.</S>
			<S sid ="101" ssid = "22">Crucially, the inference procedure does not measurably affect total MT decoding time.</S>
			<S sid ="102" ssid = "23">3.1 Phrase-based Translation Decoding.</S>
			<S sid ="103" ssid = "24">We consider the standard phrase-based approach to MT (Och and Ney, 2004).</S>
			<S sid ="104" ssid = "25">The distribution p(e|f ) is modeled directly using a log-linear model, yielding the following decision rule: Figure 5: Procedure for scoring agreement for each hypothesis generated during the search algorithm of Fig.</S>
			<S sid ="105" ssid = "26">4.</S>
			<S sid ="106" ssid = "27">In the extended hypothesis eI , the index n + 1 indicates the start of the new attachment.</S>
			<S sid ="107" ssid = "28">• Extend a hypothesis with a new phrase pair • Recombine hypotheses with identical states We assume familiarity with these operations, which are described in detail in (Och and Ney, 2004).</S>
			<S sid ="108" ssid = "29">3.2 Agreement Model Inference.</S>
			<S sid ="109" ssid = "30">The class-based agreement model is implemented as a feature function hm in Eq.</S>
			<S sid ="110" ssid = "31">(1).</S>
			<S sid ="111" ssid = "32">Specifically, when Extend generates a new hypothesis, we run the algorithm shown in Fig.</S>
			<S sid ="112" ssid = "33">5.</S>
			<S sid ="113" ssid = "34">The inputs are a translation hypothesis eI , an index n distinguishing the prefix from the attachment, and a flag indicating if their concatenation is a goal hypothesis.The beam search maintains state for each deriva e∗ = arg max e,a,Π ( M ) ) λmhm(e, f, a, Π) m=1 (1) tion, the score of which is a linear combination of the feature values.</S>
			<S sid ="114" ssid = "35">States in this program depend on some amount of lexical history.</S>
			<S sid ="115" ssid = "36">With a trigram lan This decoding problem is NP-hard, thus a beam search is often used (Fig.</S>
			<S sid ="116" ssid = "37">4).</S>
			<S sid ="117" ssid = "38">The beam search relies on three operations, two of which affect the agreement model: guage model, the state might be the last two words of the translation prefix.</S>
			<S sid ="118" ssid = "39">Recombine can be applied to any two hypotheses with equivalent states.</S>
			<S sid ="119" ssid = "40">As a result, two hypotheses with different full prefixes— and thus potentially different sequences of agreement relations—can be recombined.</S>
			<S sid ="120" ssid = "41">Incremental Greedy Decoding Decoding with the CRF-based tagger model in this setting requires some slight modifications to the Viterbi algorithm.</S>
			<S sid ="121" ssid = "42">We make a greedy approximation that permits recombination and works well in practice.</S>
			<S sid ="122" ssid = "43">The agreement model state is the last tagged segment (s, t) of the concatenated hypothesis.</S>
			<S sid ="123" ssid = "44">We tag a new attachment by assuming a prior distribution π over the starting position such that π(t) = 0 and −∞ for all other classes, a deterministic distribution in the tropical semiring.</S>
			<S sid ="124" ssid = "45">This forces the Viterbi path to go through t. We only tag the final boundary symbol for goal hypotheses.</S>
			<S sid ="125" ssid = "46">To accelerate tagger decoding in our experiments, we also used tagging dictionaries for frequently observed word types.</S>
			<S sid ="126" ssid = "47">For each word type observed more than 100 times in the training data, we restricted the set of possible classes to the set of observed classes.</S>
			<S sid ="127" ssid = "48">3.3 Translation Model Features.</S>
			<S sid ="128" ssid = "49">The agreement model score is one decoder feature function.</S>
			<S sid ="129" ssid = "50">The output of the procedure in Fig.</S>
			<S sid ="130" ssid = "51">5 is the log probability of the class sequence of each attachment.</S>
			<S sid ="131" ssid = "52">Summed over all attachments, this gives the log probability of the whole class sequence.</S>
			<S sid ="132" ssid = "53">We also add a new length penalty feature.</S>
			<S sid ="133" ssid = "54">To discriminate between hypotheses that might have the same number of raw tokens, but different underlying segmentations, we add a penalty equal to the length difference between the segmented and unsegmented attachments |sˆL| − |eI |.</S>
			<S sid ="134" ssid = "55">solution is to manually extract unification rules from phrase-structure trees.</S>
			<S sid ="135" ssid = "56">Williams and Koehn (2011) annotated German trees, and extracted translation rules from them.</S>
			<S sid ="136" ssid = "57">They then specified manual unification rules, and applied a penalty according to the number of unification failures in a hypothesis.</S>
			<S sid ="137" ssid = "58">In contrast, our class-based model does not require any manual rules and scores similar agreement phenomena as probabilistic sequences.</S>
			<S sid ="138" ssid = "59">Factored Translation Models Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling.</S>
			<S sid ="139" ssid = "60">Words are represented as a vector of features such as lemma and POS.</S>
			<S sid ="140" ssid = "61">The bitext is annotated with separate models, and the annotations are saved during phrase extraction.</S>
			<S sid ="141" ssid = "62">Hassan et al.</S>
			<S sid ="142" ssid = "63">(2007) noticed that the target- side POS sequences could be scored, much as we do in this work.</S>
			<S sid ="143" ssid = "64">They used a target-side LM over Combinatorial Categorial Grammar (CCG) supertags, along with a penalty for the number of operator violations, and also modified the phrase probabilities based on the tags.</S>
			<S sid ="144" ssid = "65">However, Birch et al.</S>
			<S sid ="145" ssid = "66">(2007) showed that this approach captures the same reordering phenomena as lexicalized reordering models, which were not included in the baseline.</S>
			<S sid ="146" ssid = "67">Birch et al.</S>
			<S sid ="147" ssid = "68">(2007) then investigated source-side CCG supertag features, but did not show an improvement for DutchEnglish.</S>
			<S sid ="148" ssid = "69">Subotin (2011) recently extended factored translation models to hierarchical phrase-based translation and developed a discriminative model for predicting target-side morphology in EnglishCzech.</S>
			<S sid ="149" ssid = "70">His model benefited from gold morphological annotations on the target-side of the 8M sentence bitext.</S>
			<S sid ="150" ssid = "71">1</S>
	</SECTION>
	<SECTION title="Related Work. " number = "4">
			<S sid ="151" ssid = "1">n+1 In contrast to these methods, our model does not affect phrase extraction and does not require annotated translation rules.We compare our class-based model to previous ap proaches to scoring syntactic relations in MT. Unification-based Formalisms Agreement rules impose syntactic and semantic constraints on the structure of sentences.</S>
			<S sid ="152" ssid = "2">A principled way to model these constraints is with a unification-based grammar (UBG).</S>
			<S sid ="153" ssid = "3">Johnson (2003) presented algorithms for learning and parsing with stochastic UBGs.</S>
			<S sid ="154" ssid = "4">However, training data for these formalisms remains extremely limited, and it is unclear how to learn such knowledge- rich representations from unlabeled data.</S>
			<S sid ="155" ssid = "5">One partial Class-based LMs Class-based LMs (Brown et al., 1992) reduce lexical sparsity by placing words in equivalence classes.</S>
			<S sid ="156" ssid = "6">They have been widely used for speech recognition, but not for MT. Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class.</S>
			<S sid ="157" ssid = "7">To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs.</S>
			<S sid ="158" ssid = "8">They used a classical exchange algorithm for clustering, and learned 512 classes from a large monolingual corpus.</S>
			<S sid ="159" ssid = "9">Then they mixed the classes into a word-based LM.</S>
			<S sid ="160" ssid = "10">However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes.</S>
			<S sid ="161" ssid = "11">It is unclear if their classes captured agreement information.</S>
			<S sid ="162" ssid = "12">Monz (2011) recently investigated parameter estimation for POS-based language models, but his classes did not include inflectional features.</S>
			<S sid ="163" ssid = "13">Target-Side Syntactic LMs Our agreement model is a form of syntactic LM, of which there is a long history of research, especially in speech processing.5 Syntactic LMs have traditionally been too slow for scoring during MT decoding.</S>
			<S sid ="164" ssid = "14">One exception was the quadratic-time dependency language model presented by Galley and Manning (2009).</S>
			<S sid ="165" ssid = "15">They applied a quadratic time dependency parser to every hypothesis during decoding.</S>
			<S sid ="166" ssid = "16">However, to achieve quadratic running time, they permitted ill-formed trees (e.g., parses with multiple roots).</S>
			<S sid ="167" ssid = "17">More recently, Schwartz et al.</S>
			<S sid ="168" ssid = "18">(2011) integrated a right-corner, incremental parser into Moses.</S>
			<S sid ="169" ssid = "19">They showed a large improvement for UrduEnglish, but decoding slowed by three orders of magnitude.6 In contrast, our class-based model encodes shallow syntactic information without a noticeable effect on decoding time.</S>
			<S sid ="170" ssid = "20">Our model can be viewed as a way to score local syntactic relations without extensive decoder modifications.</S>
			<S sid ="171" ssid = "21">For long-distance relations, Shen et al.</S>
			<S sid ="172" ssid = "22">(2010) proposed a new decoder that generates target-side dependency trees.</S>
			<S sid ="173" ssid = "23">The target-side structure enables scoring hypotheses with a trigram dependency LM.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="174" ssid = "1">We first evaluate the Arabic segmenter and tagger components independently, then provide EnglishArabic translation quality results.</S>
			<S sid ="175" ssid = "2">5.1 Intrinsic Evaluation of Components.</S>
			<S sid ="176" ssid = "3">Experimental Setup All experiments use the Penn Full (%) Incremental (%) Segmenter 98.6 – Tagger 96.3 96.2 Table 1: Intrinsic evaluation accuracy [%] (development set) for Arabic segmentation and tagging.</S>
			<S sid ="177" ssid = "4">The ATB contains clitic-segmented text with per- segment morphological analyses (in addition to phrase-structure trees, which we discard).</S>
			<S sid ="178" ssid = "5">For training the segmenter, we used markers in the vocalized section to construct the IOB character sequences.</S>
			<S sid ="179" ssid = "6">For training the tagger, we automatically converted the ATB morphological analyses to the fine-grained class set.</S>
			<S sid ="180" ssid = "7">This procedure resulted in 89 classes.</S>
			<S sid ="181" ssid = "8">For the segmentation evaluation, we report per- character labeling accuracy.8 For the tagger, we report per-token accuracy.</S>
			<S sid ="182" ssid = "9">Results Tbl.</S>
			<S sid ="183" ssid = "10">1 shows development set accuracy for two settings.</S>
			<S sid ="184" ssid = "11">Full is a standard evaluation in which features may be defined over the whole sentence.</S>
			<S sid ="185" ssid = "12">This includes next-character segmenter features and next- word tagger features.</S>
			<S sid ="186" ssid = "13">Incremental emulates the MT setting in which the models are restricted to current and previous observation features.</S>
			<S sid ="187" ssid = "14">Since the segmenter operates at the character level, we can use the same feature set.</S>
			<S sid ="188" ssid = "15">However, next-observation features must be removed from the tagger.</S>
			<S sid ="189" ssid = "16">Nonetheless, tagging accuracy only decreases by 0.1%.</S>
			<S sid ="190" ssid = "17">5.2 Translation Quality.</S>
			<S sid ="191" ssid = "18">Experimental Setup Our decoder is based on the phrase-based approach to translation (Och and Ney, 2004) and contains various feature functions including phrase relative frequency, word-level alignment statistics, and lexicalized reordering models (Till- mann, 2004; Och et al., 2004).</S>
			<S sid ="192" ssid = "19">We tuned the feature weights on a development set using lattice-based min imum error rate training (MERT) (Macherey et al., Arabic Treebank (ATB) (Maamouri et al., 2004) parts 1–3 divided into training/dev/test sections according to the canonical split (Rambow et al., 2005).7 5 See (Zhang, 2009) for a comprehensive survey..</S>
			<S sid ="193" ssid = "20">6 In principle, their parser should run in linear time.</S>
			<S sid ="194" ssid = "21">An implementation issue may account for the decoding slowdown.</S>
			<S sid ="195" ssid = "22">(p.c.) 7 LDC catalog numbers: LDC2008E61 (ATBp1v4), LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).</S>
			<S sid ="196" ssid = "23">The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).</S>
			<S sid ="197" ssid = "24">The corpus split is available at http://nlp.stanford.edu/projects/arabic.shtml.</S>
			<S sid ="198" ssid = "25">8 We ignore orthographic re-normalization performed by the annotators.</S>
			<S sid ="199" ssid = "26">For example, they converted the contraction ‘ Jl’ ll back to ‘ Jl J’ l Al. As a result, we can report accuracy since the guess and gold segmentations have equal numbers of non- whitespace characters.</S>
			<S sid ="200" ssid = "27">MT04 (tune) MT02 MT03 MT05 Avg Ba sel ine 18 .1 4 23 .8 7 1 8.</S>
			<S sid ="201" ssid = "28">8 8 2 2.</S>
			<S sid ="202" ssid = "29">6 0 + PO S 18 .1 1 − 0 . 0 3 23 .6 5 − 0 . 2 2 1 8.</S>
			<S sid ="203" ssid = "30">9 9 + 0 . 1 1 2 2.</S>
			<S sid ="204" ssid = "31">2 9 − 0 . 3 1 − 0.</S>
			<S sid ="205" ssid = "32">1 7 + PO S + Ag r 18 .8 6 + 0 . 7 2 24 .8 4 + 0 . 9 7 2 0.</S>
			<S sid ="206" ssid = "33">2 6 + 1 . 3 8 2 3.</S>
			<S sid ="207" ssid = "34">4 8 + 0 . 8 8 + 1.</S>
			<S sid ="208" ssid = "35">0 4 ge nre s n w nw nw n w #s ent en ce s 13 53 72 8 66 3 10 56 2 4 4 7 Table 2: Translation quality results (BLEU4 [%]) for newswire (nw) sets.</S>
			<S sid ="209" ssid = "36">Avg is the weighted averaged (by number of sentences) of the individual test set gains.</S>
			<S sid ="210" ssid = "37">All improvements are statistically significant at p ≤ 0.01.</S>
			<S sid ="211" ssid = "38">MT06 MT08 Avg Baseline 14.68 14.30 +POS 14.57 −0.11 14.30 +0.0 −0.06 +POS+Agr 15.04 +0.36 14.49 +0.19 +0.29 genres nw,bn,ng nw,ng,wb #sentences 1797 1360 3157 Table 3: Mixed genre test set results (BLEU4 [%]).</S>
			<S sid ="212" ssid = "39">The MT06 result is statistically significant at p ≤ 0.01; MT08 is significant at p ≤ 0.02.</S>
			<S sid ="213" ssid = "40">The genres are: nw, broadcast news (bn), newsgroups (ng), and weblog (wb).</S>
			<S sid ="214" ssid = "41">2008).</S>
			<S sid ="215" ssid = "42">For each set of results, we initialized MERT with uniform feature weights.</S>
			<S sid ="216" ssid = "43">We trained the translation model on 502 million words of parallel text collected from a variety of sources, including the Web.</S>
			<S sid ="217" ssid = "44">Word alignments were induced using a hidden Markov model based alignment model (Vogel et al., 1996) initialized with bilexical parameters from IBM Model 1 (Brown et al., 1993).</S>
			<S sid ="218" ssid = "45">Both alignment models were trained using two iterations of the expectation maximization algorithm.</S>
			<S sid ="219" ssid = "46">Our distributed 4-gram language model was trained on 600 million words of Arabic text, also collected from many sources including the Web (Brants et al., 2007).</S>
			<S sid ="220" ssid = "47">For development and evaluation, we used the NIST ArabicEnglish data sets, each of which contains one set of Arabic sentences and multiple English references.</S>
			<S sid ="221" ssid = "48">To reverse the translation direction for each data set, we chose the first English reference as the source and the Arabic as the reference.</S>
			<S sid ="222" ssid = "49">The NIST sets come in two varieties: newswire (MT0205) and mixed genre (MT06,08).</S>
			<S sid ="223" ssid = "50">Newswire contains primarily Modern Standard Arabic (MSA), while the mixed genre data sets also contain transcribed speech and web text.</S>
			<S sid ="224" ssid = "51">Since the ATB contains MSA, and significant lexical and syntactic differences may exist between MSA and the mixed genres, we achieved best results by tuning on MT04, the largest newswire set.</S>
			<S sid ="225" ssid = "52">We evaluated translation quality with BLEU4 (Pa- pineni et al., 2002) and computed statistical significance with the approximate randomization method of Riezler and Maxwell (2005).9</S>
	</SECTION>
	<SECTION title="Discussion of Translation Results. " number = "6">
			<S sid ="226" ssid = "1">Tbl.</S>
			<S sid ="227" ssid = "2">2 shows translation quality results on newswire, while Tbl.</S>
			<S sid ="228" ssid = "3">3 contains results for mixed genres.</S>
			<S sid ="229" ssid = "4">The baseline is our standard system feature set.</S>
			<S sid ="230" ssid = "5">For comparison, +POS indicates our class-based model trained on the 11 coarse POS tags only (e.g., “Noun”).</S>
			<S sid ="231" ssid = "6">Finally, +POS+Agr shows the class-based model with the fine-grained classes (e.g., “Noun+Fem+Sg”).</S>
			<S sid ="232" ssid = "7">The best result—a +1.04 BLEU average gain— was achieved when the class-based model training data, MT tuning set, and MT evaluation set contained the same genre.</S>
			<S sid ="233" ssid = "8">We realized smaller, yet statistically significant, gains on the mixed genre data sets.</S>
			<S sid ="234" ssid = "9">We tried tuning on both MT06 and MT08, but obtained insignificant gains.</S>
			<S sid ="235" ssid = "10">In the next section, we investigate this issue further.</S>
			<S sid ="236" ssid = "11">Tuning with a Treebank-Trained Feature The class-based model is trained on the ATB, which is predominantly MSA text.</S>
			<S sid ="237" ssid = "12">This data set is syntactically regular, meaning that it does not have highly dialectal content, foreign scripts, disfluencies, etc. Conversely, the mixed genre data sets contain more irregularities.</S>
			<S sid ="238" ssid = "13">For example, 57.4% of MT06 comes from non- newswire genres.</S>
			<S sid ="239" ssid = "14">Of the 764 newsgroup sentences, 112 contain some Latin script tokens, while others contain very little morphology: 9 With the implementation of Clark et al.</S>
			<S sid ="240" ssid = "15">(2011), available at:.</S>
			<S sid ="241" ssid = "16">http://github.com/jhclark/multeval.</S>
			<S sid ="242" ssid = "17">(2) . l 1/2 _ J } Human Evaluation We also manually evaluated mix 1/2 cup vinegar apple the MT05 output for improvemen ts in agreement.</S>
			<S sid ="243" ssid = "18">11 Mix 1/2 cup apple vinegar Our system produced different output from the base line for 785 (74.3%) sentences.</S>
			<S sid ="244" ssid = "19">We randomly sam (3) } __...</S>
			<S sid ="245" ssid = "20">} Music Match pled 100 of these sentences and counted agreeme nt start program miozik maatsh MusicMatch Start the program music match (MusicMatch) In these imperatives, there are no lexically marked agreement relations to score.</S>
			<S sid ="246" ssid = "21">Ex.</S>
			<S sid ="247" ssid = "22">(2) is an excerpt from a recipe that appears in full in MT06.</S>
			<S sid ="248" ssid = "23">Ex.</S>
			<S sid ="249" ssid = "24">(3) is part of usage instructions for the MusicMatch software.</S>
			<S sid ="250" ssid = "25">The ATB contains few examples like these, so our class-based model probably does not effectively discriminate between alternative hypotheses for these types of sentences.</S>
			<S sid ="251" ssid = "26">Phrase Table Coverage In a standard phrase- based system, effective translation into a highly inflected target language requires that the phrase table contain the inflected word forms necessary to construct an output with correct agreement.</S>
			<S sid ="252" ssid = "27">If the requisite words are not present in the search space of the decoder, then no feature function would be sufficient to enforce morpho-syntactic agreement.</S>
			<S sid ="253" ssid = "28">During development, we observed that the phrase table of our large-scale EnglishArabic system did often contain the inflected forms that we desired the system to select.</S>
			<S sid ="254" ssid = "29">In fact, correctly agreeing alternatives often appeared in n-best translation lists.</S>
			<S sid ="255" ssid = "30">To verify this observation, we computed the lexical coverage of the MT05 reference sentences in the decoder search space.</S>
			<S sid ="256" ssid = "31">The statistics below report the token- level recall of reference unigrams:10 errors of all types.</S>
			<S sid ="257" ssid = "32">The baseline contained 78 errors, while our system produced 66 errors, a statistically significant 15.4% error reduction at p ≤ 0.01 according to a paired t-test.</S>
			<S sid ="258" ssid = "33">In our output, a frequent source of remaining errors was the case of so-called “deflected agreement”: inanimate plural nouns require feminine singular agreement with modifiers.</S>
			<S sid ="259" ssid = "34">On the other hand, animate plural nouns require the sound plural, which is indicated by an appropriate masculine or feminine suffix.For example, the inanimate plural }.�_ll ’states’ re quires the singular feminine adjective . ;.,..</S>
			<S sid ="260" ssid = "35">Jl ‘united’,not the sound plural l ;.,..</S>
			<S sid ="261" ssid = "36">Jl.</S>
			<S sid ="262" ssid = "37">The ATB does not con tain animacy annotations, so our agreement model cannot discriminate between these two cases.</S>
			<S sid ="263" ssid = "38">However, Alkuhlani and Habash (2011) have recently started annotating the ATB for animacy, and our model could benefit as more data is released.</S>
	</SECTION>
	<SECTION title="Conclusion and Outlook. " number = "7">
			<S sid ="264" ssid = "1">Our class-based agreement model improves translation quality by promoting local agreement, but with a minimal increase in decoding time and no additional storage requirements for the phrase table.</S>
			<S sid ="265" ssid = "2">The model can be implemented with a standard CRF package, trained on existing treebanks for many languages, and integrated easily with many MT feature APIs.</S>
			<S sid ="266" ssid = "3">We achieved best results when the model training data, MT tuning set, and MT evaluation set con The bottom category includes all lexical items that the decoder could produce in a translation of the source.</S>
			<S sid ="267" ssid = "4">This large gap between the unigram recall of the actual translation output (top) and the lexical coverage of the phrase-based model (bottom) indicates that translation performance can be improved dramatically by altering the translation model through features such as ours, without expanding the search space of the decoder.</S>
			<S sid ="268" ssid = "5">mixed genre evaluation sets.</S>
			<S sid ="269" ssid = "6">In principle, our class-based model should be more robust to unseen word types and other phenomena that make non-newswire genres challenging.</S>
			<S sid ="270" ssid = "7">However, our analysis has shown that for Arabic, these genres typically contain more Latin script and transliterated words, and thus there is less morphology to score.</S>
			<S sid ="271" ssid = "8">One potential avenue of future work would be to adapt our component models to new genres by self-training them on the target side of a large bitext.</S>
			<S sid ="272" ssid = "9">10 To focus on possibly inflected word forms, we excluded.</S>
			<S sid ="273" ssid = "10">numbers and punctuation from this analysis.</S>
			<S sid ="274" ssid = "11">11 The annotator was the first author..</S>
			<S sid ="275" ssid = "12">Acknowledgments We thank Zhifei Li and Chris Manning for helpful discussions, and Klaus Macherey, Wolfgang Macherey, Daisy Stanton, and Richard Zens for engineering support.</S>
			<S sid ="276" ssid = "13">This work was conducted while the first author was an intern at Google.</S>
			<S sid ="277" ssid = "14">At Stanford, the first author is supported by a National Science Foundation Graduate Research Fellowship.</S>
	</SECTION>
</PAPER>
