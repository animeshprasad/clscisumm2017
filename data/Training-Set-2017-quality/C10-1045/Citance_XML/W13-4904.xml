<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes cross-task flexible transition models (CTFTMs) and demonstrates their effectiveness for Arabic natural language processing (NLP).</S>
		<S sid ="2" ssid = "2">NLP pipelines often suffer from error propagation, as errors committed in lower-level tasks cascade through the remainder of the processing pipeline.</S>
		<S sid ="3" ssid = "3">By allowing a flexible order of operations across and within multiple NLP tasks, a CTFTM can mitigate both cross-task and within-task error propagation.</S>
		<S sid ="4" ssid = "4">Our Arabic CTFTM models to- kenization, affix detection, affix labeling, part- of-speech tagging, and dependency parsing, achieving state-of-the-art results.</S>
		<S sid ="5" ssid = "5">We present the details of our general framework, our Arabic CTFTM, and the setup and results of our experiments.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Natural Language Processing (NLP) systems often consist of a series of NLP components, each trained to perform a specific task such as parsing.</S>
			<S sid ="7" ssid = "7">These pipelines tend to suffer from error propagation— errors introduced by early components cascade through the remainder of the pipeline causing subsequent components to commit additional errors.</S>
			<S sid ="8" ssid = "8">Partial solutions from higher-level tasks (e.g., parsing) can aid in resolving the difficult decisions that must be made in solving lower-level tasks, as with part- of-speech tagging the classic “garden path” sentence example “The horse raced past the barn fell.” To this end, this paper presents cross-task flexible transition models (CTFTMs), which model multiple tasks and solve these tasks in a more flexible order than pipeline approaches.</S>
			<S sid ="9" ssid = "9">We implement and experiment with a CTFTM for Arabic1 language processing and report experimental results for it on Arabic tokenization (i.e., clitic separation), affix detection, affix labeling, part-of-speech tagging, and dependency parsing.</S>
			<S sid ="10" ssid = "10">In addition to error propagation between modules within a parsing pipeline, errors may propagate within the parsing process itself due to the fixed order of operations of the parser.</S>
			<S sid ="11" ssid = "11">This is common for standard transition-based dependency parsing models (McDonald and Nivre, 2007), such as shift-reduce parsers, which incrementally construct a parse by processing the input in a fixed left-to-right or right-to-left fashion.</S>
			<S sid ="12" ssid = "12">However, using a transition model that allows a more flexible order of operations, such as Goldberg and Elhadad’s (2010) parser, allows difficult decisions to be postponed until later, when more of the solution has been constructed.</S>
			<S sid ="13" ssid = "13">CTFTMs extend this approach by modeling multiple tasks and providing this flexibility across tasks so that no one task needs to be complete before another can be partially solved.</S>
			<S sid ="14" ssid = "14">As a morphologically rich language, Arabic requires a significant number of processing steps.</S>
			<S sid ="15" ssid = "15">Arabic uses a variety of affixes to inflect for case, gender, number (including dual), and mood, has clitics that attach to other words, permits both VSO and SVO constructions, and rarely includes short vowels in written form.</S>
			<S sid ="16" ssid = "16">The presence of clitics and the absence of written short vowels are particularly significant sources of ambiguity.</S>
			<S sid ="17" ssid = "17">As Tsarfaty (2006) argues for Modern Hebrew, a Semitic language that shares these characteristics, we contend that mor 1 This paper focuses on Modern Standard Arabic rather than any of the dialects.</S>
			<S sid ="18" ssid = "18">34 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 34–45, Seattle, Washington, USA, 18 October 2013.</S>
			<S sid ="19" ssid = "19">Qc 2013 Association for Computational Linguistics phological analysis and parsing should be done in a unified framework, such as a CTFTM, rather than by separate components.</S>
			<S sid ="20" ssid = "20">In this paper, we describe CTFTMs, which can roots using the same templates and, thus, look similar.</S>
			<S sid ="21" ssid = "21">A single Arabic token may permit a variety of different analyses, as the example in Table 1 illustrates.</S>
			<S sid ="22" ssid = "22">be used for a wide variety of NLP tasks, and present our Arabic CTFTM for Arabic tokenization, affix detection, affix labeling, part-of-speech tagging, and dependency parsing as well as the results obtained in applying it to our dependency conversion of the Penn Arabic Treebank (ATB) (Maamouri et al., 2004; Maamouri and Bies, 2004).</S>
			<S sid ="23" ssid = "23">We find that our Arabic CTFTM for tokenization, affix detection, affix labeling, POS tagging, and parsing achieves slightly better results than a similar CTFTM that performs all the tasks except parsing.</S>
			<S sid ="24" ssid = "24">The CTFTM that supports parsing appears to be more accurate at distinguishing between passive and active verbs as well as between nouns and adjectives— cases where the context is crucial for proper interpretation due to Arabic’s ambiguities.</S>
			<S sid ="25" ssid = "25">Our system achieves tokenization accuracy similar to Kulick’s (2011) state-of-the-art system for a standard split of the ATB part 3, and, in our experiments using ATB parts 1–3, our system achieves the highest labeled attachment, unlabeled attachment, and clitic separation figures (including pronomial clitics) for Arabic yet reported (although no other work can be compared directly).</S>
	</SECTION>
	<SECTION title="Relevant Arabic Linguistics. " number = "2">
			<S sid ="26" ssid = "1">Arabic has rich morphology, with a wide array of affixes and clitics and inflecting for case, number, gender, and, occasionally, mood.</S>
			<S sid ="27" ssid = "2">Coordinating conjunctions, pronouns, and most true prepositions, along with some other particles and the definite article, usually occur as clitics in Arabic.</S>
			<S sid ="28" ssid = "3">Thus, a space- delimited2 sequence of Arabic characters may consist of multiple words, and identifying the boundaries between these must be done in order to produce syntactic parses.</S>
			<S sid ="29" ssid = "4">These boundaries can’t be detected perfectly using simple deterministic rules.</S>
			<S sid ="30" ssid = "5">Significantly, short vowels, which are expressed using diacritics, are not typically written in Arabic, resulting in pervasive ambiguity.</S>
			<S sid ="31" ssid = "6">For example, active and passive forms of verbs vary only in their diacritics, and nouns and adjectives are both derived from Arabic 2 Technically, space-and-punctuation-deliminated..</S>
			<S sid ="32" ssid = "7">_,JI_ wAlY ‘ruler’ ..s- + _,JI+_ w+AlY+y ‘and to me’ _,J +_ w+&lt;ly ‘and I follow’ ..s- + +_ w+|l+y ‘and my clan’ + Table 1: Possible interpretations for the text wAlY (Habash and Rambow, 2005).</S>
	</SECTION>
	<SECTION title="CTF-TM Framework. " number = "3">
			<S sid ="33" ssid = "1">Error propagation is not simply a problem that occurs between components in a pipeline but one that often occurs within a single component’s processing.</S>
			<S sid ="34" ssid = "2">Since transition systems can use the partially built solution for feature generation, incorrect actions taken early on result not only in an invalid final solution, but the invalid partial solution may dissuade the system from making correct decisions with respect to other parts of the solution.</S>
			<S sid ="35" ssid = "3">If a transition system can postpone decisions it is not confident of until later, the partial solution created by performing other actions may provide more or better information that enables the system to properly resolve more difficult decisions.</S>
			<S sid ="36" ssid = "4">This “easy- first” strategy is adopted by Goldberg and Elhadad’s (2010) parsing system, which starts with an ordered list of unattached words and, in each iteration, creates a new arc between any of the adjacent pairs of words in the list and removes the daughter node (word) from the list.</S>
			<S sid ="37" ssid = "5">This strategy is much more flexible than shift- reduce style parsing because the system has more options available to it at any one step for building up the solution.</S>
			<S sid ="38" ssid = "6">However, simply having flexibility within a single component does not reduce error propagation to or from other components in a pipeline and, to mitigate the potential for this, one may use a cross-task flexible transition model (CTFTM) that does not have to wait for lower level tasks to be 100% complete before starting work on higher level tasks.</S>
			<S sid ="39" ssid = "7">McDonald and Nivre (2007) define a transition system as follows: 1.</S>
			<S sid ="40" ssid = "8">a set C of parse configurations, each of which defines a (partially built) dependency graph G 2.</S>
			<S sid ="41" ssid = "9">a set T of transitions, each a function t : C → C 3.</S>
			<S sid ="42" ssid = "10">for every sentence x = w0, w1, ..., wn (a) a unique initial configuration cx (b) a set Cx of terminal configurations These systems start at the initial configuration and use a scoring function s : C × T → R to repeatedly select and follow the locally optimal transition, stopping when a terminal configuration is reached.</S>
			<S sid ="43" ssid = "11">We make a few changes to McDonald and Nivre’s transition system definition in order to explain our framework.</S>
			<S sid ="44" ssid = "12">First, to support modeling of multiple tasks, instead of referring to parse configurations, we simply use the term configuration, defining it to represent a partially built solution rather than a dependency graph.</S>
			<S sid ="45" ssid = "13">Second, we specify that there exists a routine for enumerating a set of anchors for any given configuration.</S>
			<S sid ="46" ssid = "14">Anchors are an organizational concept for dealing with arbitrary data structures; each anchor acts as a hook into some portion of the configuration that may be changed.</S>
			<S sid ="47" ssid = "15">Finally, there exist routines for enumerating legal actions that can be performed in relation to any anchor and, for training, a routine for verifying that performing a given action will lead to a configuration consistent with the final solution.</S>
			<S sid ="48" ssid = "16">The performance of an action constitutes a transition between configurations.3 It is quite straightfoward to adapt Goldberg and Elhadad’s (2010) parsing approach to any configuration that is indexable by anchors, and in so doing we are able to create cross-task flexible transition models.</S>
			<S sid ="49" ssid = "17">3 For example, in a fixed order, one-word-at-a-time POS tagging system, there would be only one anchor—the word currently being labeled—but, for a one-at-a-time POS tagger capable of tagging words in any order, the anchor set would contain the entire list of still-unlabeled words.</S>
			<S sid ="50" ssid = "18">The POS labeling actions for the anchors in each of these cases constitute transitions to new configurations.</S>
	</SECTION>
	<SECTION title="Our Arabic CTF-TM. " number = "4">
			<S sid ="51" ssid = "1">4.1 Tasks.</S>
			<S sid ="52" ssid = "2">Our Arabic CTFTM system performs the following tasks: split a series of space-delimited Arabic tokens into words (tokenization), identify the bounds of affixes within the words (affix detection), label the affixes (affix labeling), label the words with their parts of speech (POS tagging), and construct a labeled dependency tree (dependency parsing).</S>
			<S sid ="53" ssid = "3">Tokenization, part-of-speech tagging, and dependency parsing are frequent topics in NLP literature.</S>
			<S sid ="54" ssid = "4">Affix identification and labeling are parts of morphological analysis that are sometimes completely ignored or are performed using an external morphological analyzer.</S>
			<S sid ="55" ssid = "5">Identifying affixes and labeling them can help the overall system contend with lexical sparsity issues as well as utilize the information encoded by the affixes (e.g., person).</S>
			<S sid ="56" ssid = "6">4.2 Anchors and Actions.</S>
			<S sid ="57" ssid = "7">The configurations that the system deals with have anchors of two types, token anchors and affix anchors.</S>
			<S sid ="58" ssid = "8">The initial configuration consists of an ordered list of neighboring token anchors (neighborhood), each of which corresponds to one of the original space-delimited tokens.</S>
			<S sid ="59" ssid = "9">As processing continues, new token anchors may be created by splitting off clitics, new affix anchors may be created by identifying substrings of tokens as affixes, and token anchors will be removed from the ordered list to become daughter nodes of their neighbors, attached via labeled dependency arcs.</S>
			<S sid ="60" ssid = "10">The complete list of actions that can be performed on the anchors, which, as described earlier, constitute the transitions between configurations, are as follows: Tokenization 1.</S>
			<S sid ="61" ssid = "11">Separate a proclitic of length l from a token anchor, cre-.</S>
			<S sid ="62" ssid = "12">ating a new token anchor for the clitic and reducing the width of the original token 2.</S>
			<S sid ="63" ssid = "13">Separate an enclitic of length l from a token anchor, cre-.</S>
			<S sid ="64" ssid = "14">ating a new token anchor for the clitic and reducing the width of the original token Affix Detection 3.</S>
			<S sid ="65" ssid = "15">Create an affix (prefix) anchor from the first l characters.</S>
			<S sid ="66" ssid = "16">of a token anchor that are not labeled as part of an affix (If the affix is the definite determiner Al, which we treat as an affix for consistency with the ATB’s tokenization scheme, it is automatically labeled as DET and removed from further processing for the sake of efficiency.)</S>
			<S sid ="67" ssid = "17">4.</S>
			<S sid ="68" ssid = "18">Create an affix (suffix) anchor from the last l characters.</S>
			<S sid ="69" ssid = "19">of a token anchor that are not labeled as part of an affix POS and affix labeling</S>
	</SECTION>
	<SECTION title="Assign a label l to the anchor (Affixes are automatically. " number = "5">
			<S sid ="70" ssid = "1">removed from further processing after labeling) Dependency parsing chor and the preceding unattached neighbor token anchor and remove the attached anchor from the neighborhood 7.</S>
			<S sid ="71" ssid = "2">Create a dependency arc with label d between a token.</S>
			<S sid ="72" ssid = "3">anchor and the following unattached neighbor token anchor with label l and remove the attached anchor from the neighborhood 8.</S>
			<S sid ="73" ssid = "4">Swap the position of two neighboring token anchors (this.</S>
			<S sid ="74" ssid = "5">adds Nivre-style (2009) non-projectivity support as described by Tratz and Hovy (2011)) General 9.</S>
			<S sid ="75" ssid = "6">Mark an anchor as fully processed and remove it from.</S>
			<S sid ="76" ssid = "7">further processing The dependency labels, POS labels, clitic lengths, and affix lengths used to define the actions are all collected automatically from the training data.</S>
			<S sid ="77" ssid = "8">4 The actions are subject to the following constraints/preconditions: 1.</S>
			<S sid ="78" ssid = "9">Labeling is only valid if the anchor has not been labeled.</S>
			<S sid ="79" ssid = "10">2.</S>
			<S sid ="80" ssid = "11">Tokens may only be labeled with token labels, prefixes.</S>
			<S sid ="81" ssid = "12">with prefix labels, and suffixes with suffix labels (as determined by the training data) 3.</S>
			<S sid ="82" ssid = "13">Affix strings observed in the training data may not be la-.</S>
			<S sid ="83" ssid = "14">beled with any label not used with them in the training data 4.</S>
			<S sid ="84" ssid = "15">Token anchors may not be assigned labels that do not co-.</S>
			<S sid ="85" ssid = "16">occur with the labels of any already-labeled affixes and vice versa 5.</S>
			<S sid ="86" ssid = "17">A prefix creation action may only be applied to a token anchor that doesn’t yet have a prefix already has a prefix, and enclitics are similarly restricted by the presence of a suffix 7.</S>
			<S sid ="87" ssid = "18">Clitics may not be detached from a token that has already.</S>
			<S sid ="88" ssid = "19">been attached to another token via a dependency arc 8.</S>
			<S sid ="89" ssid = "20">A dependency arc with label x may not be created between token anchors T1 and T2 if 1) one or both are labeled and 2) no arc between similarly POS tagged anchors exists in the training data 9.</S>
			<S sid ="90" ssid = "21">Swap actions may not undo previous swaps.</S>
			<S sid ="91" ssid = "22">10.</S>
			<S sid ="92" ssid = "23">Marking a token anchor as fully processed may only oc-.</S>
			<S sid ="93" ssid = "24">cur if it has already been labeled, and it must either be 1) the last unattached token or 2) already attached 4 Training examples with clitics that are invalid (i.e., too long) are discarded at the beginning of training.</S>
			<S sid ="94" ssid = "25">4.3 Scoring Function.</S>
			<S sid ="95" ssid = "26">For our scoring function, like Goldberg and Elhadad, we use the structured perceptron algorithm (Collins, 2002) with parameter averaging.</S>
			<S sid ="96" ssid = "27">This has previously been shown to produce strong results when modeling multiple NLP tasks (Zhang and Clark, 2008).</S>
			<S sid ="97" ssid = "28">4.4 Features.</S>
			<S sid ="98" ssid = "29">For a given anchor5, the system extracts features from the partially built solution (e.g., the text, affixes, POS tags, and syntactic dependencies of the anchor and nearby anchors).</S>
			<S sid ="99" ssid = "30">The same feature templates are used for all action types except the affix labeling actions—affix labeling is applied to affix anchors instead of word-level anchors, and, since all templates are defined relative to an anchor, the templates must be different.</S>
			<S sid ="100" ssid = "31">The system uses no external resources (e.g., lexicons, morphological analyzers).</S>
			<S sid ="101" ssid = "32">We leave out a more exhaustive listing and description of the features due to space limitations6, the fact that the focus of this paper is not on the value of any particular feature template but rather on our overall approach and experimental results, and because we plan to release our code, which will be more helpful for reproducibility efforts.</S>
			<S sid ="102" ssid = "33">4.5 Data Preparation.</S>
			<S sid ="103" ssid = "34">For our experiments, we use the original written form of the data from the latest versions of the first three parts of the Penn Arabic Treebank (ATB) (Maamouri et al., 2004; Maamouri and Bies, 2004) as well as the new broadcast news collection (Maamouri et al., 2012).7 We convert the constituent trees into dependency trees and adjust the part-of- speech tags.</S>
			<S sid ="104" ssid = "35">5 ‘A given action’ may be more correct technically, but our implementation is set up to share the same set of string-based features across all actions associated with a given anchor.</S>
			<S sid ="105" ssid = "36">with minimal (insufficient) explanation would require well over a page.</S>
			<S sid ="106" ssid = "37">The set of feature templates is based upon the templates used by Tratz and Hovy’s (2011) English parser, which are given in Tratz’s (2011) thesis.</S>
			<S sid ="107" ssid = "38">7 We use version 4.1 of ATB part 1, 3.1 of part 2, 3.2 of part.</S>
			<S sid ="108" ssid = "39">3, and 1.0 of the broadcast news transcriptions.</S>
			<S sid ="109" ssid = "40">4.5.1 Dependency Conversion The two main Modern Standard Arabic dependency treebanks currently available are the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009) and the Prague Arabic Dependency Treebank (PADT) (Hajicˇ et al., 2004).</S>
			<S sid ="110" ssid = "41">CATiB has over 228,000 manually annotated words as well as an automatic ATB conversion.</S>
			<S sid ="111" ssid = "42">It uses only 8 dependency relations (subject, object, predicate, topic, idafa, tamyiz, modifier, and flat) and 6 part-of- speech tags, and it has not yet been publicly released by the LDC.</S>
			<S sid ="112" ssid = "43">The PADT, which was used in the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), is much smaller, with only about 148,000 annotated tokens.</S>
			<S sid ="113" ssid = "44">Since we want a large annotated corpus with fine-grained labels, we create our own ATB conversion.</S>
			<S sid ="114" ssid = "45">4.5.2 Transformations In addition to converting the ATB’s constituent parses to dependency trees, we make a handful of other changes.</S>
			<S sid ="115" ssid = "46">Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous.</S>
			<S sid ="116" ssid = "47">Following Rambow et al.</S>
			<S sid ="117" ssid = "48">(2005), Treebank sentences headed by TOP elements containing multiple S daughters are split into separate sentences.8 Additionally, if the dependency converter concludes that an S node without treebank functional tags is dependent upon another S node and is separated from it via sentence-final punctuation (e.g., an exclamation point), these S nodes are separated into distinct sentences as well.</S>
			<S sid ="118" ssid = "49">For the broadcast news data, we remove all subtrees headed by EDITED tags to make it more closely resemble newswire text.9 Since we adhere to the tokenization scheme used by the ATB, and we do not split off the determiner Al as its own tree token.</S>
			<S sid ="119" ssid = "50">Instead, we treat it as a prefix.</S>
			<S sid ="120" ssid = "51">The words referred to as inna and her sisters are annotated using two different part-of-speech categories and syntactic structures in the ATB.</S>
			<S sid ="121" ssid = "52">In our conversion, both ATB structures are converted to 8 The ATB often has multiple sentences, or even entire paragraphs, annotated under a single TOP element.</S>
			<S sid ="122" ssid = "53">9 The EDITED tag “is used to show the repetition and restart-.</S>
			<S sid ="123" ssid = "54">ing of constituents that are repaired by subsequent speech” (Maamouri et al., 2012).</S>
			<S sid ="124" ssid = "55">the same dependency structure headed by the INNA word, similar to CATiB (Habash and Roth, 2009).</S>
			<S sid ="125" ssid = "56">We treat the focus particle AmmA like a preposition in our dependency structure, following CATiB.</S>
			<S sid ="126" ssid = "57">4.5.3 Dependency Label Scheme Our dependency scheme consists of a total of 35 labels.</S>
			<S sid ="127" ssid = "58">Many of these are similar to those of Stanford’s basic dependency scheme for English (de Marneffe and Manning, 2008), although they are somewhat closer to a similar scheme used by (Tratz and Hovy, 2011).</S>
			<S sid ="128" ssid = "59">The list of relations is presented in Table 2.</S>
			<S sid ="129" ssid = "60">Most of the relations are self-explanatory or correspond to similar labels in either Tratz and Hovy’s (2011) scheme for English or CATiB’s (Habash and Roth, 2009) scheme for Arabic.</S>
			<S sid ="130" ssid = "61">A few are new or significantly different from their similarly named counterparts in other schemes and are described in greater detail below.</S>
			<S sid ="131" ssid = "62">• adjnom — connects the head of an NP to that of a sister NP (occurs with apposition and preposition-like nouns) • advcl — connects verbal nouns to their syntactic governor in what resemble English’s adverbial participle clauses • advnp — connects NPs with treebank adverbial function tags (e.g., -LOC, -TMP, -DIR), which are often headed by preposition-like nouns, to what they modify • fidafa — for false idafa (idafa-like structures that are headed by adjectives instead of nouns) • kccmp — connects a clausal complement that is part of a past progressive or habitual construction to the head verb kana • lakinna — similar to cc but used with the sister of inna lakinna instead of coordinating conjunctions • part — particle modifier; connects particles (other than FOCUS PART) to their governors • rcmod — connects a bare relative clause to its head • reladv — connects an adverbial WH- clause to its governor • relmod — connects the head of a WH- node to the relativized word • ripcmp — connects a clause to the relative or interrogative pronoun that heads it 4.5.4 Part-of-Speech Tag Scheme The Penn Arabic Treebank uses complex part of speech tags for the entire tree token such as DET+NOUN+NSUFF FEM SG+CASE DEF GEN. Across the treebank data used in our experiments, there are a total of 579 such tags, which are composed of 179 different parts separated by plus signs.</S>
			<S sid ="132" ssid = "63">Each part corresponds to a substring of the adj no m ad vcl adv mo d ad vn p cc c ci ni t cc o m p co m b o c o nj c o p d e p d e t adju nct nom inal a d v e r b i a l c l a u s e a d v e r b i a l m o d i f i e r a d v e r b i a l n o u n p h r a s e c o o r d i n a t i n g c o n j u n c t i o n init ial co or din ati ng co nju nct ion cla us al co m pl e m en t co mb ina tio n ter m conj unct ion c o p u l a c o m p l e m e n t u n s p e c i f i e d d e p e n d e n c y d e t e r m i n e r intj io bj id af a fi d af a fla t kc c m p la ki nn a ne g obj obj co mp pa rt pc om p inter jecti on i n d i r e c t o b j e c t i d a f a f a l s e i d a f a f l a t s t r u c t u r e kana clau sal com ple men t s e e t e x t n e g a t i o n o b j e c t o bj e ct c o m pl e m e nt p ar ti cl e m o di fi e r pr e p o si ti o n c o m pl e m e nt pre p pu nct rc mo d rel ad v rel mo d rip cm p sc s u b j t m z t p c v o c prep ositi on mod ifier punc tuati on mod ifier ( b a r e ) r e l a t i v e c l a u s e m o d i f i e r r e l a t i v e p r o n o u n a d v e r b i a l r e l a t i v e p r o n o u n m o d i f i e r rel ativ e/i nte rro gati ve pro no un co mp le me nt su bor din ati ng co nju nct ion mo difi er s u b j e c t t a m y i z t o p i c a l i z e d e l e m e n t v o c a t i v e Table 2: Syntactic dependency scheme used in this work.</S>
			<S sid ="133" ssid = "64">Labels that aren’t self-explanatory or similar to the labels used by Tratz and Hovy (2011) for English or CATiB for Arabic (Habash and Roth, 2009) are in bold (for completely new relations) or italics (for similarly named but semantically different relations) vowelized version of the word.10 Due at least in part to the enormity of this label set, simpler schemes are often preferred, such as the “Bies” labels (Bikel, 2004; Kulick et al., 2006), Diab’s (2007) labels, Kulick’s (2011) labels, and CATiB’s labels (Habash and Roth, 2009).</S>
			<S sid ="134" ssid = "65">Marton et al.</S>
			<S sid ="135" ssid = "66">(2010) find that using simpler schemes allow them to get better parsing results when using predicted POS tags due to the relatively poor performance of taggers trained using the full ATB scheme.</S>
			<S sid ="136" ssid = "67">The part-of-speech tag scheme we use is quite similar to that of the original ATB but has several simplifications.</S>
			<S sid ="137" ssid = "68">These changes are listed below.</S>
			<S sid ="138" ssid = "69">1.</S>
			<S sid ="139" ssid = "70">Possessive and direct object pronoun clitics are all given.</S>
			<S sid ="140" ssid = "71">the same label (PRON OPP) (50 fewer tags; mapping back to the originals is trival in almost all cases) 2.</S>
			<S sid ="141" ssid = "72">.VN forms of NOUN and ADJ are merged with their respective more generic categories 3.</S>
			<S sid ="142" ssid = "73">Interrogative and relative adverbial and pronoun labels.</S>
			<S sid ="143" ssid = "74">are merged together into RI ADV and RI PRON 4.</S>
			<S sid ="144" ssid = "75">Noun suffix labels (e.g., NSUFF MASC PL GEN,.</S>
			<S sid ="145" ssid = "76">NSUFF MASC PL ACC) with genitive or accusative case distinctions are merged because there is no distinction in unvowelized form 5.</S>
			<S sid ="146" ssid = "77">Labels for dual masculine noun suffixes are merged with.</S>
			<S sid ="147" ssid = "78">their plural counterparts (no distinction in the unvowelized forms)</S>
	</SECTION>
	<SECTION title="Demonstrative    pronoun  labels  are   collapsed    to. " number = "6">
			<S sid ="148" ssid = "1">DEM PRON (person and number information is easily recovered)</S>
	</SECTION>
	<SECTION title="The words called inna and her sisters are labeled INNA. " number = "7">
			<S sid ="149" ssid = "1">instead of PSEUDO VERB or SUB CONJ 10 Since we use the original written form of the data and the internal segmentation of the words are only provided for the vowelized versions, we project the segmentation into the original written forms, discarding any parts that weren’t actually Since our system splits off clitics and identifies the affixes, the tagging is performed at the individual morpheme level instead of producing a single all- encompassing tag for the entire token.</S>
			<S sid ="150" ssid = "2">Some of the part-of-speech tags (mostly instances of DIALECT, TYPO, TRANSERR, and NOT IN LEXICON tags) are automatically corrected/improved during the dependency conversion based upon the original constituent parse.</S>
			<S sid ="151" ssid = "3">4.6 Filtering.</S>
			<S sid ="152" ssid = "4">Sentences containing invalid clitics are not used in training both because they are erroneous and because including them would require allowing the system to perform actions that should not occur (i.e., splitting off a clitic of length 8); similarly, training examples with more than 20% of their tokens tagged as DIALECT, TRANSERR, LATIN, PARTIAL, GRAMMAR PROBLEM, and/or TYPO are ignored on the assumption that including them would harm the model.</S>
			<S sid ="153" ssid = "5">This filtering process is not applied in testing.</S>
			<S sid ="154" ssid = "6">4.7 Data Split.</S>
			<S sid ="155" ssid = "7">We train and test models using three different splits of the data.</S>
			<S sid ="156" ssid = "8">The first split is based upon the split used by Zitouni et al.</S>
			<S sid ="157" ssid = "9">(2006) in their diacritization work and is the same as that used by Marton et al.</S>
			<S sid ="158" ssid = "10">(2013) in their parsing work and by Kulick (2011) in his to- kenization and part-of-speech tagging work, in order to facilitate better comparison.</S>
			<S sid ="159" ssid = "11">However, Marton et al. use the CATiB conversion of a slightly earlier version of the data (3.1, not 3.2), and, thus, the re Table 3: Counts of the number of files, sentences (Sent), original space-delimited tokens (Tok), ATB tree tokens (Tree Toks), and affixes in the experimental data.</S>
			<S sid ="160" ssid = "12">the first (in name and chronological order) 85% of the documents in ATB part 3 in training, the next 7.5% in development, and the final 7.5% in test.</S>
			<S sid ="161" ssid = "13">In the second split, we use data from the first three parts of the ATB, each of which consists of documents coming from a different newswire source.</S>
			<S sid ="162" ssid = "14">Parts 1 and 2 are split 70%/15%/15% training/dev/test, and we reuse the split of part 3 just mentioned.</S>
			<S sid ="163" ssid = "15">Under this setup, we train two different CTFTMs, one that performs all of the tasks and one that performs all of the tasks except parsing.</S>
			<S sid ="164" ssid = "16">This enables us to test whether modeling parsing task improves performance on the lower level tasks.</S>
			<S sid ="165" ssid = "17">In the final split, we use the splits for parts 1–3 plus the data in LDC’s annotated broadcast news transcripts (Maamouri et al., 2012).</S>
			<S sid ="166" ssid = "18">Unlike parts 1–3, the broadcast news data are drawn from a variety of sources.</S>
			<S sid ="167" ssid = "19">Files from sources with three or more files are split across training, development, and test, with the latest documents being placed in test.</S>
			<S sid ="168" ssid = "20">11 This experiment illustrates how the system per-.</S>
			<S sid ="169" ssid = "21">forms when additional, out-of-domain data are included.</S>
			<S sid ="170" ssid = "22">Statistics for the data are given in Table 3.</S>
			<S sid ="171" ssid = "23">4.8 Evaluation Measures.</S>
			<S sid ="172" ssid = "24">Dependency parsing quality is measured in terms of labeled and unlabeled attachment scores (LAS and UAS), which indicate the percentage of words attached to their correct parent and, in the case of dependency.</S>
			<S sid ="173" ssid = "25">Since a given space-delimited token may not be tokenized into words correctly, the dependency arcs are only counted as correct if they occur between the correct words (spans of character indices).</S>
			<S sid ="174" ssid = "26">We measure part-of-speech tagging in terms of F-score (F1) and require that the tree token have the correct bounds (was tokenized correctly) and have the correct label.</S>
			<S sid ="175" ssid = "27">Normally, we would choose LAS on the development set as the measure for determining the version of the model to keep for testing because it measures performance on the highest-level task (labeled dependency parsing).</S>
			<S sid ="176" ssid = "28">However, since one of the CTFTMs does not perform parsing, we instead use POS tagging F1.</S>
			<S sid ="177" ssid = "29">In general, we observe that the scores are highly correlated, making the point moot.</S>
			<S sid ="178" ssid = "30">For the ATB part 3 experiment, POS tagging F1 peaks on iteration 437.12 For the second experiment, POS tagging F1 peaks at iteration 301 for the CTFTM with parsing and iteration 278 for the one without.</S>
			<S sid ="179" ssid = "31">For the third experiment, the highest score occurs on iteration 431.</S>
			<S sid ="180" ssid = "32">4.9 Results and Discussion.</S>
			<S sid ="181" ssid = "33">The results for the various experimental setups are presented in Table 4.</S>
			<S sid ="182" ssid = "34">ATB 3 Experiment When using the same split of ATB part 3 as Kulick (2011) and Marton et al.</S>
			<S sid ="183" ssid = "35">(2013), the system correctly tokenizes 99.3% of the space-delimited tokens, similar to Kulick’s (2011) accuracy (99.3%) and slightly higher than the 99.0% figure Kulick calculates for MADA.</S>
			<S sid ="184" ssid = "36">Though these results are obtained using our dependency conversion of the ATB rather than the original, we use the same tokenization scheme.</S>
			<S sid ="185" ssid = "37">The POS labeling F1 score of 95.8 can’t be compared well with any other work due to differences in tag schemes, which vary greatly, as well as use of gold tokenization and other differences.</S>
			<S sid ="186" ssid = "38">Our system obtains 84.9 UAS and 82.0 LAS, which are higher than Marton et al.’s best results of 84.0 UAS and 81.0 LAS, but they were using a different conversion (CATiB) of a different version of the data (3.1, not 3.2) as well as gold tokenization, so the results are not directly comparable.</S>
			<S sid ="187" ssid = "39">Framework Internal Experiment The CTFTM LAS, whose attachment is labeled with the correct 12 We run 500 iterations for each experiment, which can take.</S>
			<S sid ="188" ssid = "40">11 We will make the exact list of files used in the training, development, and test sets available.</S>
			<S sid ="189" ssid = "41">as long as a week using a quad-core machine.</S>
			<S sid ="190" ssid = "42">However, little improvement is seen after the first 100 iterations.</S>
			<S sid ="191" ssid = "43">Tra in Eva l Dat a Tok Ac c PO S F1 Affi x Bou nds F1 Affi x Lab el F1 UA S LA S 3 3 De v 9 9 . 5 9 6 . 6 98.</S>
			<S sid ="192" ssid = "44">7 9 8 . 4 86.</S>
			<S sid ="193" ssid = "45">3 83.</S>
			<S sid ="194" ssid = "46">8 3 3 Tes t 9 9 . 3 9 5 . 8 98.</S>
			<S sid ="195" ssid = "47">4 9 7 . 9 84.</S>
			<S sid ="196" ssid = "48">9 82.</S>
			<S sid ="197" ssid = "49">0 1,2 ,3 1,2, 3 De v 9 9 . 6 9 7 . 1 99.</S>
			<S sid ="198" ssid = "50">1 9 8 . 9 88.</S>
			<S sid ="199" ssid = "51">3 86.</S>
			<S sid ="200" ssid = "52">0 1,2 ,3 1,2, 3 Tes t 9 9 . 6 9 6 . 8 99.</S>
			<S sid ="201" ssid = "53">0 9 8 . 7 87.</S>
			<S sid ="202" ssid = "54">4 84.</S>
			<S sid ="203" ssid = "55">8 1,2 ,3, BN 1,2, 3 De v 9 9 . 6 9 7 . 1 99.</S>
			<S sid ="204" ssid = "56">1 9 8 . 9 88.</S>
			<S sid ="205" ssid = "57">5 86.</S>
			<S sid ="206" ssid = "58">2 1,2 ,3, BN 1,2, 3 Tes t 9 9 . 6 9 6 . 8 99.</S>
			<S sid ="207" ssid = "59">0 9 8 . 8 87.</S>
			<S sid ="208" ssid = "60">5 85.</S>
			<S sid ="209" ssid = "61">0 1,2 ,3, BN 1,2, 3,B N De v 9 9 . 5 9 6 . 0 98.</S>
			<S sid ="210" ssid = "62">8 9 8 . 5 87.</S>
			<S sid ="211" ssid = "63">4 84.</S>
			<S sid ="212" ssid = "64">6 1,2 ,3, BN 1,2, 3,B N Tes t 9 9 . 3 9 5 . 7 98.</S>
			<S sid ="213" ssid = "65">7 9 8 . 4 86.</S>
			<S sid ="214" ssid = "66">6 83.</S>
			<S sid ="215" ssid = "67">8 Without Parsing 1,2 ,3 1,2, 3 De v 9 9 . 6 9 6 . 9 99.</S>
			<S sid ="216" ssid = "68">1 9 8 . 9 N A N A 1,2 ,3 1,2, 3 Tes t 9 9 . 5 9 6 . 5 98.</S>
			<S sid ="217" ssid = "69">9 9 8 . 6 N A N A Table 4: Results for the various experiments (Exp) for both the development and test portions of the data, including per- token clitic separation (tokenization) accuracy, part-of-speech tagging F1, affix boundary detection F1, affix labeling F1, and both unlabeled and labeled attachment scores.</S>
			<S sid ="218" ssid = "70">that does parsing and the CTFTM that doesn’t achieve similar overall results for the different tasks (other than parsing, of course).</S>
			<S sid ="219" ssid = "71">However, when looking deeper at the individual POS tagging mistakes that one system made more often by one system than the other, (see Tables 5 and 6), we observe that the parsing CTFTM does a better job with labeling some parts-of-speech.</S>
			<S sid ="220" ssid = "72">For instance, the non-parsing system mismarks passive verbs as active more than 29% more often than the other.</S>
			<S sid ="221" ssid = "73">In Arabic, passive and active forms of verbs are only distinguished by their short vowels, which are typically unwritten, and, thus, the context is of particular importance in distinguishing between the two.</S>
			<S sid ="222" ssid = "74">The non-parsing system also has more trouble with the distinction between nouns and adjectives, which is likely because adjectives are derived using the same templatic structures as nouns (Attia et al., 2010) and, thus, context is, once again, of great importance.</S>
			<S sid ="223" ssid = "75">Broadcast News Experiment The scores obtained in the experiment with the broadcast news data are slightly lower than in the second experiment.</S>
			<S sid ="224" ssid = "76">However, this appears to be because the broadcast news portions of the development and test sections are more difficult to parse than the remainder.</S>
			<S sid ="225" ssid = "77">If we apply the model to the development and test sections of parts 1, 2, and 3, we observe that the results, which are given in Table 4, are higher than those of the model trained without the broadcast news data.</S>
			<S sid ="226" ssid = "78">Gold Prediction Errors Diff Table 5: Top 10 POS mistakes made more often by either the CTFTM with parsing or the CTFTM without on the ATB part 1, 2, and 3 development set.</S>
			<S sid ="227" ssid = "79">Ta g #G old Ta g #G old NO UN 26 19 5 IN NA 14 56 AD J 74 91 SU B CO NJ 6 4 1 NO UN PR OP 59 13 VB PV PA SS 2 3 1 VB PV 34 78 VB IV PA SS 2 0 7 VB IV 26 82 Table 6: Counts for the POS tags mentioned in Table 5.</S>
			<S sid ="228" ssid = "80">5 Related Work.</S>
			<S sid ="229" ssid = "81">5.1 Semitic Language Parsing.</S>
			<S sid ="230" ssid = "82">Much of the Arabic parsing research to date uses the pipeline approach, either running a tokenizer prior to parsing or simply assuming the existence of gold to- kenization (Bikel, 2004; Buchholz and Marsi, 2006; Kulick et al., 2006; Nivre et al., 2007; Marton et al., 2010; Marton et al., 2011; Marton et al., 2013).</S>
			<S sid ="231" ssid = "83">Of course, using gold tokenization results in optimistic evaluation figures.13 Other methods exist however.</S>
			<S sid ="232" ssid = "84">For example, to parse Modern Hebrew, Cohen and Smith (2007) combine a morphological model with a syntactic model using a product of experts.</S>
			<S sid ="233" ssid = "85">Another alternative is lattice parsing, which can be used to jointly model both tokenization and parsing (Chappelier et al., 1999).</S>
			<S sid ="234" ssid = "86">Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013), Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).</S>
			<S sid ="235" ssid = "87">Why lattice parsing may help in some cases but not others is not clear.</S>
			<S sid ="236" ssid = "88">Some Arabic parsing work focuses on the usefulness of various features and part-of-speech tagsets.</S>
			<S sid ="237" ssid = "89">Marton et al.</S>
			<S sid ="238" ssid = "90">(2013) examine various morphological features and part-of-speech tagsets, employing MADA (Habash and Rambow, 2005; Habash et al., 2009) to predict form-based morphological features and an in-house system (Alkuhlani and Habash, 2012) to predict functional morphological features.</S>
			<S sid ="239" ssid = "91">Dehdari et al.</S>
			<S sid ="240" ssid = "92">(2011) investigate the best set of features for Arabic constituent parsing and try several approaches for selecting an optimal feature set, finding that the best-first with backtracking algorithm is the most effective in their experiments.</S>
			<S sid ="241" ssid = "93">5.2 Other Languages.</S>
			<S sid ="242" ssid = "94">There has been a flurry of recent research involving the joint modeling of dependency parsing and lower-level tasks14 for a variety of languages, with most of the attention focused on Chinese.</S>
			<S sid ="243" ssid = "95">While lacking Arabic’s morphological richness, Chinese has its own challenges, such as word segmentation and part-of-speech ambiguities, which have led researchers to develop new unified approaches for processing it.</S>
			<S sid ="244" ssid = "96">Qian and Liu (2012) train independent models for word segmentation, POS tagging, and 13 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.</S>
			<S sid ="245" ssid = "97">14 Systems that jointly model POS tagging and constituent.</S>
			<S sid ="246" ssid = "98">parsing have existed for some time.</S>
			<S sid ="247" ssid = "99">parsing but then incorporate them together during decoding.</S>
			<S sid ="248" ssid = "100">Li et al.</S>
			<S sid ="249" ssid = "101">(2011), Li and Zhou (2012), Ha- tori et al.</S>
			<S sid ="250" ssid = "102">(2011), and Ma et al.</S>
			<S sid ="251" ssid = "103">(2012) present systems that jointly model Chinese POS tagging and dependency parsing.</S>
			<S sid ="252" ssid = "104">Li et al.</S>
			<S sid ="253" ssid = "105">(2011) use a dynamic programming approach similar to Koo and Collins (2010), Li and Zhou (2012) present a shift- reduce style system that uses structured perceptron and beam search, Hatori et al.</S>
			<S sid ="254" ssid = "106">(2011) implement a shift-reduce style algorithm that utilizes dynamic programming and beam search in the manner of Huang and Sagae (2010), and Ma et al.</S>
			<S sid ="255" ssid = "107">(2012) extend Goldberg and Elhadad’s (2010) easy-first approach to support both dependency parsing and POS tagging and is thus similar to our work.</S>
			<S sid ="256" ssid = "108">Hatori et al.</S>
			<S sid ="257" ssid = "109">(2012) extend their previous system to tackle word segmentation, and Ma et al.</S>
			<S sid ="258" ssid = "110">(2013) build upon earlier work by implementing beam search to get better results.</S>
			<S sid ="259" ssid = "111">Li and Zhou (2012) side step some of the issues of Chinese word segmentation by parsing structures of words, phrases, and sentences in a unified framework using a structured perceptron and beam search.</S>
			<S sid ="260" ssid = "112">Some researchers focus their work on other languages.</S>
			<S sid ="261" ssid = "113">Lee et al.</S>
			<S sid ="262" ssid = "114">(2011) present a graphical model for morphological disambiguation and dependency parsing that they apply to Latin, Ancient Greek, Hungarian, and Czech.</S>
			<S sid ="263" ssid = "115">Bohnet and Nivre (2012) present a shift-reduce style system similar to Li and Zhou’s (2012) system that jointly models POS tagging and labeled dependency parsing, achieving state-of-the-art accuracy on Czech, German, Chinese, and English.</S>
			<S sid ="264" ssid = "116">6 Conclusion.</S>
			<S sid ="265" ssid = "117">In this paper, we described cross-task flexible transition models (CTFTMs) and demonstrated their viability for Arabic tokenization, affix detection, affix labeling, part-of-speech labeling, and dependency parsing, obtaining very strong results in each tasks.</S>
			<S sid ="266" ssid = "118">We plan to release our software in the near future, including the software for converting the ATB to dependency parses, and would like to release our dependency conversion of the Penn Arabic Treebank via the LDC.</S>
			<S sid ="267" ssid = "119">7 Future Work.</S>
			<S sid ="268" ssid = "120">In the future, we plan to integrate beam search into the training and decoding.</S>
			<S sid ="269" ssid = "121">We want to add support for the recovery of diacritics, roots, and derivation templates, and we would like to apply modified versions of our system to other languages.</S>
			<S sid ="270" ssid = "122">Our choice of anchors, operations, and constraints represent one possible design for an Arabic CTFTM.</S>
			<S sid ="271" ssid = "123">Other options, such as creating unlabeled dependencies and adding labels in subsequent operations, restricting clitic separation to a handcrafted list of clitics, utilizing information from a dictionary or morphological analyzer, or following some sort of coarse-to-fine labeling scheme, are also possible, and we hope to investigate more of these options.</S>
	</SECTION>
</PAPER>
