<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We explore the contribution of morphological features – both lexical and inflectional – to dependency parsing of Arabic, a morphologically rich language.</S>
		<S sid ="2" ssid = "2">Using controlled experiments, we find that definiteness, person, number, gender, and the undiacritzed lemma are most helpful for parsing on automatically tagged input.</S>
		<S sid ="3" ssid = "3">We further contrast the contribution of form-based and functional features, and show that functional gender and number (e.g., “broken plurals”) and the related rationality feature improve over form-based features.</S>
		<S sid ="4" ssid = "4">It is the first time functional morphological features are used for Arabic NLP.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Parsers need to learn the syntax of the modeled language in order to project structure on newly seen sentences.</S>
			<S sid ="6" ssid = "6">Parsing model design aims to come up with features that best help parsers to learn the syntax and choose among different parses.</S>
			<S sid ="7" ssid = "7">One aspect of syntax, which is often not explicitly modeled in parsing, involves morphological constraints on syntactic structure, such as agreement, which often plays an important role in morphologically rich languages.</S>
			<S sid ="8" ssid = "8">In this paper, we explore the role of morphological features in parsing Modern Standard Arabic (MSA).</S>
			<S sid ="9" ssid = "9">For MSA, the space of possible morphological features is fairly large.</S>
			<S sid ="10" ssid = "10">We determine which morphological features help and why.</S>
			<S sid ="11" ssid = "11">We also explore going beyond the easily detectable, regular form-based (“surface”) features, by representing functional values for some morphological features.</S>
			<S sid ="12" ssid = "12">We expect that representing lexical abstrac tions and inflectional features participating in agreement relations would help parsing quality, but other inflectional features would not help.</S>
			<S sid ="13" ssid = "13">We further expect functional features to be superior to surface- only features.</S>
			<S sid ="14" ssid = "14">The paper is structured as follows.</S>
			<S sid ="15" ssid = "15">We first present the corpus we use (Section 2), then relevant Arabic linguistic facts (Section 3); we survey related work (Section 4), describe our experiments (Section 5), and conclude with an analysis of parsing error types (Section 6).</S>
	</SECTION>
	<SECTION title="Corpus. " number = "2">
			<S sid ="16" ssid = "1">We use the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009).</S>
			<S sid ="17" ssid = "2">Specifically, we use the portion converted automatically from part 3 of the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.</S>
			<S sid ="18" ssid = "3">CATiB’s dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations.</S>
			<S sid ="19" ssid = "4">It has a reduced POS tagset (with six tags only – henceforth CATIB6), but a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively, (whether they appear pre- or post-verbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and other less common relations that we will not discuss here.</S>
			<S sid ="20" ssid = "5">For more information, see Habash et al.</S>
			<S sid ="21" ssid = "6">(2009).</S>
			<S sid ="22" ssid = "7">The CATiB treebank uses the word segmentation of the PATB: it splits off several categories of orthographic clitics, but not the definite article JI Al. In all of the experiments reported in this paper, we use the gold 1586 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1586–1596, Portland, Oregon, June 1924, 2011.</S>
			<S sid ="23" ssid = "8">Qc 2011 Association for Computational Linguistics VRB J tς ml ‘work’ posed of the root . the pattern 1A2i3.1 � k-t-b ‘writing related’ and MOD PRT fy ‘in’ OBJ SBJ NOM I. HfydAt ‘granddaughters’ Lexeme and features Alternatively, Arabic words can be described in terms of lexemes and inflectional features.</S>
			<S sid ="24" ssid = "9">The set of word forms that only vary inflectionally among each other is called the lexeme.</S>
			<S sid ="25" ssid = "10">A lemma is a specific word form chosen to represent the lexeme word set; for example, Arabic verb lemmas are third person masculine sin NOM &apos;-&quot;_I.�..1I AlmdArs MOD NOM IDF NOM gular perfective.</S>
			<S sid ="26" ssid = "11">We explore using both the diacritized lemma and the undiacritized lemma (here ‘the-schools’ MOD NOM I AlHkwmy ‘the-governmental’ ..</S>
			<S sid ="27" ssid = "12">I AlðkyAt ‘smart’ ....</S>
			<S sid ="28" ssid = "13">C I AlkAtb ‘the-writer’ after LMM).</S>
			<S sid ="29" ssid = "14">Just as the lemma abstracts over inflectional morphology, the root abstracts over both inflectional and derivational morphology and thus provides a deeper level of lexical abstraction, indicating the “core” meaning of the word.</S>
			<S sid ="30" ssid = "15">The pat Figure 1: CATiB Annotation example (tree display from righttern is a generally complementary abstraction some to left).</S>
			<S sid ="31" ssid = "16">I &apos; &quot;_I.�..1I ..</S>
			<S sid ="32" ssid = "17">I ....</S>
			<S sid ="33" ssid = "18">C I I. J times indicati ng sematic notions such causati on and tς ml HfydAt AlkAtb AlðkyAt fy AlmdArs AlHkwmy ‘The writer’s smart granddaughters work for public schools.’ segmentation.</S>
			<S sid ="34" ssid = "19">An example CATiB dependency tree is shown in Figure 1.</S>
	</SECTION>
	<SECTION title="Relevant Linguistic Concepts. " number = "3">
			<S sid ="35" ssid = "1">In this section, we present the linguistic concepts relevant to our discussion of Arabic parsing.</S>
			<S sid ="36" ssid = "2">Orthography The Arabic script uses optional diacritics to represent short vowels, consonantal doubling and the indefininteness morpheme (nunation).For example, the word ..._. kataba ‘he wrote’ is of ten written as . ktb, which can be ambiguous with other words such as ..... kutubu˜ ‘books’.</S>
			<S sid ="37" ssid = "3">In newstext, only around 1.6% of all words have any dia critic (Habash, 2010).</S>
			<S sid ="38" ssid = "4">As expected, the lack of diacritics contributes heavily to Arabic’s morphological ambiguity.</S>
			<S sid ="39" ssid = "5">In this work, we only use undiacritized text; however, some of our parsing features which are derived through morphological disambiguation include diacritics (specifically, lemmas, see below).</S>
			<S sid ="40" ssid = "6">reflexiveness.</S>
			<S sid ="41" ssid = "7">We use the pattern of the lemma, not of the word form.</S>
			<S sid ="42" ssid = "8">We group the ROOT, PATTERN, LEMMA and LMM in our discussion as lexical features.</S>
			<S sid ="43" ssid = "9">Nominal lexemes can also be classified into two groups: rational (i.e., human) or irrational (i.e.„ nonhuman).2 The rationality feature interacts with syntactic agreement and other inflectional features (discussed next); as such, we group it with those features in this paper’s experiments.</S>
			<S sid ="44" ssid = "10">The inflectional features define the the space of variations of the word forms associated with a lexeme.</S>
			<S sid ="45" ssid = "11">PATBtokenized words vary along nine dimensions: GENDER and NUMBER (for nominals and verbs); PERSON, ASPECT, VOICE and MOOD (for verbs); and CASE, STATE, and the attached definite article proclitic DET (for nominals).</S>
			<S sid ="46" ssid = "12">Inflectional features abstract away from the specifics of morpheme forms.</S>
			<S sid ="47" ssid = "13">Some inflectional features affect more than one morpheme in the same word.</S>
			<S sid ="48" ssid = "14">For example, changing the value of the ASPECT feature in the example above from imperfective to perfectiveyields the word form I . S kAtab+uwA ‘they corre sponded’, which differs in terms of prefix, suffix and pattern.</S>
			<S sid ="49" ssid = "15">Morphemes Words can be described in terms of their morphemes; in Arabic, in addition to concate- native prefixes and suffixes, there are templatic morphemes called root and pattern.</S>
			<S sid ="50" ssid = "16">For example, the word v . C yu+kAtib+uwn ‘they correspond’ hasone prefix and one suffix, in addition to a stem com 1 The digits in the pattern correspond to the positions root.</S>
			<S sid ="51" ssid = "17">radicals are inserted.</S>
			<S sid ="52" ssid = "18">2 Note that rationality (‘humanness’ ‘ J ..</S>
			<S sid ="53" ssid = "19">/ J ..’) is. nar rower than animacy; its expression is wide-spead in Arabic, but less so English, where it mainly shows in pronouns (he/she vs. it) and relativizers (the student who... vs. the desk/bird which...).</S>
			<S sid ="54" ssid = "20">Surface vs. functional features Additionally, some inflectional features, specifically gender and number, are expressed using different morphemes in different words (even within the same part-of- speech).</S>
			<S sid ="55" ssid = "21">There are four sound gender-number suffixes in Arabic:3 +φ (null morpheme) for masculine singular, o+ + for feminine singular, v_+ +wn for masculine plural and I+ +At for feminine plural.Plurality can be expressed using sound plural suf fixes or using a pattern change together with singular suffixes.</S>
			<S sid ="56" ssid = "22">A sound plural example is the word pairI.</S>
			<S sid ="57" ssid = "23">/ o. Hafiyd+a /Hafiyd+At ‘granddaugh ter/granddaughters’.</S>
			<S sid ="58" ssid = "24">On the other hand, the plural of the inflectionally and morphemically feminine singular word �_. madras+a ‘school’ is the word &apos;-&quot;_I.</S>
			<S sid ="59" ssid = "25">madAris+φ ‘schools’, which is feminine and plural inflectionally, but has a masculine singular suffix.</S>
			<S sid ="60" ssid = "26">This irregular inflection, known as broken plural, is similar to the English mouse/mice, but is much more common in Arabic (over 50% of plurals in our training data).</S>
			<S sid ="61" ssid = "27">A similar inconsistency appears in feminine nouns that are not inflected using sound gender suffixes, e.g., the feminine form of the masculine singular adjective &apos;-__ Âzraq+φ ‘blue’ is ber and rationality features for Arabic.</S>
			<S sid ="62" ssid = "28">We use this resource in modeling these features in Section 5.5.</S>
			<S sid ="63" ssid = "29">Morpho-syntactic interactions Inflectional features and rationality interact with syntax in two ways.</S>
			<S sid ="64" ssid = "30">In agreement relations, two words in a specific syntactic configuration have coordinated values for specific sets of features.</S>
			<S sid ="65" ssid = "31">MSA has standard (i.e., matching value) agreement for subject-verb pairs on PERSON, GENDER, and NUMBER, and for noun- adjective pairs on NUMBER, GENDER, CASE, and DET. There are three very common cases of exceptional agreement: verbs preceding subjects are always singular, adjectives of irrational plural nouns are always feminine singular, and verbs whose subjects are irrational plural are also always feminine singular.</S>
			<S sid ="66" ssid = "32">See the example in Figure 1: the adjective, ..</S>
			<S sid ="67" ssid = "33">I AlðkyAt ‘smart’, of the feminine plural (andrational) I. HafiydAt ‘granddaughters’ is fem inine plural; but the adjective, I AlHkwmy ‘the-governmental’, of the feminine plural (and irrational) &apos;-&quot;_I.</S>
			<S sid ="68" ssid = "34">madAris ‘schools’ is feminine singu lar.</S>
			<S sid ="69" ssid = "35">These agreement rules always refer to functional morphology categories; they are orthogonal to the morpheme-feature inconsistency discussed above.</S>
			<S sid ="70" ssid = "36">, __ zarqA’+φ not __ * *Âzraq+a . To address MSA exhibits marking relations in CASE and this inconsistency in the correspondence between inflectional features and morphemes, and inspired by (Smrž, 2007), we distinguish between two types of inflectional features: surface (or form-based)4 features and functional features.</S>
			<S sid ="71" ssid = "37">Most available Arabic NLP tools and resources model morphology using surface inflectional features and do not mark rationality; this includes the PATB (Maamouri et al., 2004), the Buckwalter morphological analyzer (BAMA) (Buckwalter, 2004) and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) system (Habash and Rambow, 2005).</S>
			<S sid ="72" ssid = "38">The Elixir- FM analyzer (Smrž, 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.</S>
			<S sid ="73" ssid = "39">Most recently, Alkuhlani and Habash (2011) present a version of the PATB (part 3) that is annotated for functional gender, num 3 We ignore duals, which are regular in Arabic, and case/state variations in this discussion for simplicity.</S>
	</SECTION>
	<SECTION title="Smrž (2007) uses the term illusory for surface features.. " number = "4">
			<S sid ="74" ssid = "1">STATE marking.</S>
			<S sid ="75" ssid = "2">Different types of dependents have different CASE, e.g., verbal subjects are always marked NOMINATIVE.</S>
			<S sid ="76" ssid = "3">CASE and STATE are rarely explicitly manifested in undiacritized MSA.</S>
			<S sid ="77" ssid = "4">The DET feature plays an important role in distinguishing between the N-N idafa (possessive) construction, in which only the last noun may bear the definite article, and the N-A modifier construction, in which both elements generally exhibit agreement in definiteness.</S>
			<S sid ="78" ssid = "5">Lexical features do not constrain syntactic structure as inflectional features do.</S>
			<S sid ="79" ssid = "6">Instead, bilexical dependencies are used to model semantic relations which often are the only way to disambiguate among different possible syntactic structures.</S>
			<S sid ="80" ssid = "7">Lexical abstraction also reduces data sparseness.</S>
			<S sid ="81" ssid = "8">The core POS tagsets Words also have associated part-of-speech (POS) tags, e.g., “verb”, which further abstract over morphologically and syntactically similar lexemes.</S>
			<S sid ="82" ssid = "9">Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals and particles.</S>
			<S sid ="83" ssid = "10">In com parison, the tagset of the Buckwalter Morphological Analyzer (Buckwalter, 2004) used in the PATB has a core POS set of 44 tags (before morphological extension).</S>
			<S sid ="84" ssid = "11">Cross-linguistically, a core set containing around 12 tags is often assumed, including: noun, proper noun, verb, adjective, adverb, preposition, particles, connectives, and punctuation.</S>
			<S sid ="85" ssid = "12">Henceforth, we reduce CORE44 to such a tagset, and dub it CORE12.</S>
			<S sid ="86" ssid = "13">The CATIB6 tagset can be viewed as a further reduction, with the exception that CATIB6 contains a passive voice tag; however, this tag constitutes only 0.5% of the tags in the training.</S>
			<S sid ="87" ssid = "14">Extended POS tagsets The notion of “POS tagset” in natural language processing usually does not refer to a core set.</S>
			<S sid ="88" ssid = "15">Instead, the Penn English Treebank (PTB) uses a set of 46 tags, including not only the core POS, but also the complete set of morphological features (this tagset is still fairly small since English is morphologically impoverished).</S>
			<S sid ="89" ssid = "16">In PATBtokenized MSA, the corresponding type of tagset (core POS extended with a complete description of morphology) would contain upwards of 2,000 tags, many of which are extremely rare (in our training corpus of about 300,000 words, we encounter only 430 of such POS tags with complete morphology).</S>
			<S sid ="90" ssid = "17">Therefore, researchers have proposed tagsets for MSA whose size is similar to that of the English PTB tagset, as this has proven to be a useful size computationally.</S>
			<S sid ="91" ssid = "18">These tagsets are hybrids in the sense that they are neither simply the core POS, nor the complete morphologically enriched tagset, but instead they selectively enrich the core POS tagset with only certain morphological features.</S>
			<S sid ="92" ssid = "19">A full dicussion of how these tagsets affect parsing is presented in Marton et al.</S>
			<S sid ="93" ssid = "20">(2010); we summarize the main points here.</S>
			<S sid ="94" ssid = "21">The following are the various tagsets we use in this paper: (a) the core POS tagset CORE12; (b) the CATiB treebank tagset CATIBEX, a newly introduced extension of CATIB6 (Habash and Roth, 2009) by simple regular expressions of the word form, indicating particular morphemes such as the prefix JI Al+ or the suffix v_ +wn; this tagsetis the best-performing tagset for Arabic on pre dicted values.</S>
			<S sid ="95" ssid = "22">(c) the PATB full tagset (BW), size ≈2000+ (Buckwalter, 2004); We only discuss here the best performing tagsets (on predicted values), and BW for comparison.</S>
			<S sid ="96" ssid = "23">4 Related Work.</S>
			<S sid ="97" ssid = "24">Much work has been done on the use of morphological features for parsing of morphologically rich languages.</S>
			<S sid ="98" ssid = "25">Collins et al.</S>
			<S sid ="99" ssid = "26">(1999) report that an optimal tagset for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).</S>
			<S sid ="100" ssid = "27">This tagset (size 58) outperforms the basic Czech POS tagset (size 13) and the complete tagset (size ≈3000+).</S>
			<S sid ="101" ssid = "28">They also report that the use of gender, number and person features did not yield any improvements.</S>
			<S sid ="102" ssid = "29">We got similar results for CASE in the gold experimental setting (Marton et al., 2010) but not when using predicted POS tags (POS tagger output).</S>
			<S sid ="103" ssid = "30">This may be a result of CASE tagging having a lower error rate in Czech (5.0%) (Hajicˇ and Vidová-Hladká, 1998) compared to Arabic (≈14.0%, see Table 2).</S>
			<S sid ="104" ssid = "31">Similarly, Cowan and Collins (2005) report that the use of a subset of Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations.</S>
			<S sid ="105" ssid = "32">Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.</S>
			<S sid ="106" ssid = "33">We also find that the number feature helps for Arabic.</S>
			<S sid ="107" ssid = "34">Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima’an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.</S>
			<S sid ="108" ssid = "35">As for work on Arabic, results have been reported on PATB (Kulick et al., 2006; Diab, 2007; Green and Manning, 2010), the Prague Dependency Tree- bank (PADT) (Buchholz and Marsi, 2006; Nivre, 2008) and the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009).</S>
			<S sid ="109" ssid = "36">Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses.</S>
			<S sid ="110" ssid = "37">Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al., 2007), trained on the PADT.</S>
			<S sid ="111" ssid = "38">His results are not directly comparable to ours because of the different treebanks’ representations, even though all the experiments reported here were performed using MaltParser.</S>
			<S sid ="112" ssid = "39">Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.</S>
			<S sid ="113" ssid = "40">However, we go beyond previous work in that we also extend this morphologically enhanced feature set to include additional lexical and inflectional features.</S>
			<S sid ="114" ssid = "41">Previous work with MaltParser in Russian, Turkish and Hindi showed gains with case but not with agreement features (Nivre et al., 2008; Eryigit et al., 2008; Nivre, 2009).</S>
			<S sid ="115" ssid = "42">Our work is the first using MaltParser to show gains using agreement-oriented features (Marton et al., 2010), and the first to use functional features for this task (this paper).</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="116" ssid = "1">Throughout this section, we only report results using predicted input feature values (e.g., generated automatically by a POS tagger).</S>
			<S sid ="117" ssid = "2">After presenting the parser we use (Section 5.1), we examine a large space of settings in the following order: the contribution of numerous inflectional features in a controlled fashion (Section 5.2);5 the contribution of the lexical features in a similar fashion, as well as the combination of lexical and inflectional features (Section 5.3); an extension of the DET feature (Section 5.4); using functional NUMBER and GENDER feature values, as well as the RATIONALITY feature (Section 5.5); finally, putting best feature combinations to test with the best-performing POS tagset, and on an unseen test set (Section 5.6).</S>
			<S sid ="118" ssid = "3">All results are reported mainly in terms of labeled attachment accuracy score (parent word and the dependency re velopment and testing, we follow the splits used by Roth et al.</S>
			<S sid ="119" ssid = "4">(2008) for PATB part 3 (Maamouri et al., 2004).</S>
			<S sid ="120" ssid = "5">We kept the test unseen during training.</S>
			<S sid ="121" ssid = "6">There are five default attributes, in the MaltParser terminology, for each token in the text: word ID (ordinal position in the sentence), word form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).</S>
			<S sid ="122" ssid = "7">There are default MaltParser features (in the machine learning sense),7 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.</S>
			<S sid ="123" ssid = "8">The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).</S>
			<S sid ="124" ssid = "9">Hence MaltParser features are defined as POS tag at stk[0], word form at buf[0], etc. Kübler et al.</S>
			<S sid ="125" ssid = "10">(2009) describe a “typical” MaltParser model configuration of attributes and features.8 Starting with it, in a series of initial controlled experiments, we settled on using buf[01] + stk[01] for wordforms, and buf[03] + stk[02] for POS tags.</S>
			<S sid ="126" ssid = "11">For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].</S>
			<S sid ="127" ssid = "12">We did not change the features for deprel.</S>
			<S sid ="128" ssid = "13">This new MaltParser configuration resulted in gains of 0.31.1% in labeled attachment accuracy (depending on the POS tagset) over the default Malt 9lation to it, a.k.a. LAS).</S>
			<S sid ="129" ssid = "14">Unlabeled attachment ac Parser configuration.All experiments reported be curacy score (UAS) is also given.</S>
			<S sid ="130" ssid = "15">We use McNe- mar’s statistical significance test as implemented by Nilsson and Nivre (2008), and denote p &lt; 0.05 and p &lt; 0.01 with + and ++, respectively.</S>
			<S sid ="131" ssid = "16">5.1 Parser.</S>
			<S sid ="132" ssid = "17">For all experiments reported here we used the syntactic dependency parser MaltParser v1.3 (Nivre, 2003; Nivre, 2008; Kübler et al., 2009) – a transition-based parser with an input buffer and a stack, using SVM classifiers to predict the next state in the parse derivation.</S>
			<S sid ="133" ssid = "18">All experiments were done using the Nivre &quot;eager&quot; algorithm.6 For training, de 5 In this paper, we do not examine the contribution of different POS tagsets, see Marton et al.</S>
			<S sid ="134" ssid = "19">(2010) for details.</S>
	</SECTION>
	<SECTION title="Nivre  (2008)  reports that  non-projective  and pseudo-. " number = "6">
			<S sid ="135" ssid = "1">projective algorithms outperform the &quot;eager&quot; projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies.</S>
			<S sid ="136" ssid = "2">The Nivre &quot;standard&quot; algorithm low were conducted using this new configuration.</S>
			<S sid ="137" ssid = "3">5.2 Inflectional features.</S>
			<S sid ="138" ssid = "4">In order to explore the contribution of inflectional and lexical information in a controlled manner, we focused on the best performing core (“morphology- free”) POS tagset, CORE12, as baseline; using three is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the &quot;eager” one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set – an observation already noted in Nivre (2008).</S>
	</SECTION>
	<SECTION title="The terms “feature” and “attribute” are overloaded in the. " number = "7">
			<S sid ="139" ssid = "1">literature.</S>
			<S sid ="140" ssid = "2">We use them in the linguistic sense, unless specifically noted otherwise, e.g., “MaltParser feature(s)”.</S>
	</SECTION>
	<SECTION title="It is slightly different from the default configuration.. " number = "8">
			<S sid ="141" ssid = "1">9 We also experimented with normalizing word forms (Alif Maqsura conversion to Ya, and hamza removal from Alif forms) as is common in parsing and statistical machine translation literature – but it resulted in a similar or slightly decreased performance, so we settled on using non-normalized word forms.</S>
			<S sid ="142" ssid = "2">setup LAS LASdif f UAS CORE12 78.68 — 82.48 + all inflectional features 77.910.77 82.14 +DET 79.82++ 1.14 83.18 +STATE 79.34++ 0.66 82.85 +GENDER 78.75 0.07 82.35 +PERSON 78.74 0.06 82.45 +NUMBER 78.660.02 82.39 +VOICE 78.640.04 82.41 +ASPECT 78.600.08 82.39 +MOOD 78.540.14 82.35 +CASE 75.812.87 80.24 +DET+STATE 79.42++ 0.74 82.84 +DET+GENDER 79.90++ 1.22 83.20 +DET+GENDER+PERSON 79.94++ 1.26 83.21 +DET+PNG 80.11++ 1.43 83.29 +DET+PNG+VOICE 79.96++ 1.28 83.18 +DET+PNG+ASPECT 80.01++ 1.33 83.20 +DET+PNG+MOOD 80.03++ 1.35 83.21 Table 1: CORE12 with inflectional features, predicted input.</S>
			<S sid ="143" ssid = "3">Top: Adding all nine features to CORE12.</S>
			<S sid ="144" ssid = "4">Second part: Adding each feature separately, comparing difference from CORE12.</S>
			<S sid ="145" ssid = "5">Third part: Greedily adding best features from second part.</S>
			<S sid ="146" ssid = "6">different setups, we added nine morphological features with values predicted by MADA: DET (presence of the definite determiner), PERSON, ASPECT, VOICE, MOOD, GENDER, NUMBER, STATE (morphological marking as head of an idafa construction), and CASE.</S>
			<S sid ="147" ssid = "7">In setup All , we augmented the baseline model with all nine MADA features (as nine additional MaltParser attributes); in setup Sep , we augmented the baseline model with the MADA features, one at a time; and in setup Greedy , we combined them in a greedy heuristic (since the entire feature space is too vast to exhaust): starting with the most gainful feature from Sep, adding the next most gainful feature, keeping it if it helped, or discarding it otherwise, and continuing through the least gainful feature.</S>
			<S sid ="148" ssid = "8">See Table 1.</S>
			<S sid ="149" ssid = "9">Somewhat surprisingly, setup All hurts performance.</S>
			<S sid ="150" ssid = "10">This can be explained if one examines the prediction accuracy of each feature (top of Table 2).</S>
			<S sid ="151" ssid = "11">Features which are not predicted with very high accuracy, such as CASE (86.3%), can dominate the negative contribution, even though they are top contributors when provided as gold input (Marton et al., 2010); when all features are provided as gold input, All actually does better than individual features, which puts to rest a concern that its decrease here feature acc set size DET 99.6 3* PERSON 99.1 4* ASPECT 99.1 5* VOICE 98.9 4* MOOD 98.6 5* GENDER 99.3 3* NUMBER 99.5 4* STATE 95.6 4* CASE 86.3 5* ROOT 98.4 9646 PATTERN 97.0 338 LEMMA (diacritized) 96.7 16837 LMM (undiacritized lemma) 98.3 15305 normalized word form (A,Y) 99.3 29737 non-normalized word form 98.9 29980 Table 2: Feature prediction accuracy and set sizes.</S>
			<S sid ="152" ssid = "12">* = The set includes a &quot;N/A&quot; value.</S>
			<S sid ="153" ssid = "13">setup LAS LASdif f UAS CORE12 (repeated) 78.68 — 82.48 + all lexical features 78.85 0.17 82.46 +LMM 78.96+ 0.28 82.54 +ROOT 78.94+ 0.26 82.64 +LEMMA 78.80 0.12 82.42 +PATTERN 78.590.09 82.39 +LMM+ROOT 79.04++ 0.36 82.63 +LMM+ROOT+LEMMA 79.05++ 0.37 82.63 +LMM+ROOT+PATTERN 78.93 0.25 82.58 Table 3: Lexical features.</S>
			<S sid ="154" ssid = "14">Top part: Adding each feature separately; difference from CORE12 (predicted).</S>
			<S sid ="155" ssid = "15">Bottom part: Greedily adding best features from previous part.</S>
			<S sid ="156" ssid = "16">is due to data sparseness.</S>
			<S sid ="157" ssid = "17">Here, when features are predicted, the DET feature (determiner), followed by the STATE (construct state, idafa) feature, are top individual contributors in setup Sep. Adding DET and the so-called φ-features (PERSON, NUMBER, GENDER, also shorthanded PNG) in the Greedy setup, yields 1.43% gain over the CORE12 baseline.</S>
			<S sid ="158" ssid = "18">5.3 Lexical features.</S>
			<S sid ="159" ssid = "19">Next, we experimented with adding the lexical features, which involve semantic abstraction to some degree: LEMMA, LMM (the undiacritized lemma), and ROOT.</S>
			<S sid ="160" ssid = "20">We experimented with the same setups as above: All, Sep, and Greedy.</S>
			<S sid ="161" ssid = "21">Adding all four features yielded a minor gain in setup All.</S>
			<S sid ="162" ssid = "22">LMM was the best single contributor, closely followed by ROOT in Sep. CORE12+LMM+ROOT (with or with CORE12 + . . .</S>
			<S sid ="163" ssid = "23">LAS LASdif f UAS +DET+PNG (repeated) 80.11++ 1.43 83.29 +DET+PNG+LMM 80.23++ 1.55 83.34 +DET+PNG+LMM +ROOT 80.10++ 1.42 83.25 +DET+PNG+LMM +PATTERN 80.03++ 1.35 83.15 Table 4: Inflectional+lexical features together.</S>
			<S sid ="164" ssid = "24">CORE12 + . . .</S>
			<S sid ="165" ssid = "25">LAS LASdif f UAS +DET (repeated) 79.82++ — 83.18 +DET2 80.13++ 0.31 83.49 CORE12 + . . .</S>
			<S sid ="166" ssid = "26">LAS LASdif f UAS CORE12 (repeated) 78.68 – 82.48 +PERSON (repeated) 78.74 0.06 82.45 +GENDER (repeated) 78.75 0.07 82.35 +NUMBER (repeated) 78.660.02 82.39 +FN*GENDER 78.96++ 0.28 82.53 +FN*NUMBER 78.88+ 0.20 82.53 +FN*NUMDGTBIN 78.87 0.19 82.53 +FN*RATIONALITY 78.91+ 0.23 82.60 +FN*GNR 79.32++ 0.64 82.78 +PERSON+FN*GNR 79.34++ 0.66 82.82 +DET+LMM+PERSON+FN*NGR 80.47++ 1.79 83.57 ++ +DET+PNG+LMM (repeated) 80.23++ — 83.34 +DET2+PNG+LMM 80.21++ -0.02 83.39 +DET2+LMM+PERSON+FN*NGR 80.53 +DET2+LMM+PERSON+FN*NG 80.43++ 1.85 83.66 1.75 83.56 Table 5: Extended inflectional features.</S>
			<S sid ="167" ssid = "27">out LEMMA) was the best greedy combination in setup Greedy.</S>
			<S sid ="168" ssid = "28">See Table 3.</S>
			<S sid ="169" ssid = "29">All lexical features are predicted with high accuracy (bottom of Table 2).</S>
			<S sid ="170" ssid = "30">Following the same greedy heuristic, we augmented the best inflection-based model CORE12+DET+PNG with lexical features, and found that only the undiacritized lemma (LMM) alone improved performance (80.23%).</S>
			<S sid ="171" ssid = "31">See Table 4.</S>
			<S sid ="172" ssid = "32">5.4 Inflectional feature engineering.</S>
			<S sid ="173" ssid = "33">So far we experimented with morphological feature values as predicted by MADA.</S>
			<S sid ="174" ssid = "34">However, it is likely that from a machine-learning perspective, representing similar categories with the same tag may be useful for learning.</S>
			<S sid ="175" ssid = "35">Therefore, we next experimented with modifying inflectional features that proved most useful.</S>
			<S sid ="176" ssid = "36">As DET may help distinguish the N-N idafa construction from the N-A modifier construction, we attempted modeling also the DET values of previous and next elements (as MaltParser’s stk[1] + buf[1], in addition to stk[0] + buf[0]).</S>
			<S sid ="177" ssid = "37">This variant, denoted DET2, indeed helps: when added to the CORE12, DET2 improves non-gold parsing quality by more than 0.3%, compared to DET (Table 5).</S>
			<S sid ="178" ssid = "38">This improvement unfortunately does not carry over to our best feature combination to date, CORE12+DET+PNG+LMM.</S>
			<S sid ="179" ssid = "39">However, in subsequent feature combinations, we see that DET2 helps again, or at least, doesn’t hurt: LAS goes up by 0.06% in conjunction with features LMM+PERSON +FN*NGR in Table 6.</S>
			<S sid ="180" ssid = "40">+DET2+LMM+PNG+FN*NGR 80.51++ 1.83 83.66 CATIBEX 79.74 – 83.30 +DET2+LMM +PERSON+FN*NGR 80.83++ 1.09 84.02 BW 72.64 – 77.91 +DET2+LMM +PERSON+FN*NGR 74.40++ 1.76 79.40 Table 6: Functional features: gender, number, rationality.</S>
			<S sid ="181" ssid = "41">We also experimented with PERSON.</S>
			<S sid ="182" ssid = "42">We changed the values of proper names from “N/A” to “3” (third person), but it resulted in a similar or slightly decreased performance, so it was abandoned.</S>
			<S sid ="183" ssid = "43">5.5 Functional feature values.</S>
			<S sid ="184" ssid = "44">The NUMBER and GENDER features we have used so far only reflect surface (as opposed to functional) values, e.g., broken plurals are marked as singular.</S>
			<S sid ="185" ssid = "45">This might have a negative effect on learning generalizations over the complex agreement patterns in MSA (see Section 3), beyond memorization of word pairs seen together in training.</S>
			<S sid ="186" ssid = "46">Predicting functional features To predict functional GENDER, functional NUMBER and RATIONALITY, we build a simple maximum likelihood estimate (MLE) model using these annotations in the corpus created by Alkuhlani and Habash (2011).</S>
			<S sid ="187" ssid = "47">We train using the same training data we use throughout this paper.</S>
			<S sid ="188" ssid = "48">For all three features, we select the most seen value in training associated with the triple word-CATIBEX-lemma; we back off to CATIBEX- lemma and then to lemma.</S>
			<S sid ="189" ssid = "49">For gender and number, we further back off to the surface values; for rationality, we back off to the most common value (irrational).</S>
			<S sid ="190" ssid = "50">On our predicted dev set, the overall accuracy baseline of predicting correct functional gender-number-rationality using surface features is the error by two thirds reaching an overall accuracy of 95.5%.</S>
			<S sid ="191" ssid = "51">The high accuracy may be a result of the low percentage of words in the dev set that do not appear in training (around 4.6%).</S>
			<S sid ="192" ssid = "52">Digit tokens (e.g., “4”) are also marked singular by default.</S>
			<S sid ="193" ssid = "53">They don’t show surface agreement, even though the corresponding number-word token( . _I Arbς ‘four.fem.sing’) would.</S>
			<S sid ="194" ssid = "54">We further observe that MSA displays complex agreement pat terns with numbers (Dada, 2007).</S>
			<S sid ="195" ssid = "55">Therefore, we alternatively experimented with binning the digit tokens’ NUMBER value accordingly: • the number 0 and numbers ending with 00 • the number 1 and numbers ending with 01 • the number 2 and numbers ending with 02 • the numbers 310 and those ending with 0310 • the numbers, and numbers ending with, 1199 • all other number tokens (e.g., 0.35 or 7/16) and denoted these experiments with NUMDGTBIN.</S>
			<S sid ="196" ssid = "56">Almost 1.5% of the tokens are digit tokens in the training set, and 1.2% in the dev set.10 Results using these new features are shown in Table 6.</S>
			<S sid ="197" ssid = "57">The first part repeats the CORE12 baseline.</S>
			<S sid ="198" ssid = "58">The second part repeats previous experiments with surface morphological features.</S>
			<S sid ="199" ssid = "59">The third part uses the new functional morphological features instead.</S>
			<S sid ="200" ssid = "60">The performance using NUMBER and GENDER increases by 0.21% and 0.22%, respectively, as we replace surface features with functional features.</S>
			<S sid ="201" ssid = "61">(Recall that there is no functional PERSON.)</S>
			<S sid ="202" ssid = "62">We then see that the change in the representation of digits does not help; in the large space of experiments we have performed, we saw some improvement through the use of this alternative representation, but not in any of the feature combinations that performed best and that we report on in this paper.</S>
			<S sid ="203" ssid = "63">We then use just the RATIONALITY feature, which results in an increase over the baseline.</S>
			<S sid ="204" ssid = "64">The combination of all three functional features (NUMBER, GENDER, RATIONALITY) provides for a nice cumulative effect.</S>
			<S sid ="205" ssid = "65">Adding PERSON hardly improves further.</S>
			<S sid ="206" ssid = "66">In the fourth part of the table, we include the other features which we found previously to be helpful, 10 We didn’t mark the number-words since in our training data there were less than 30 lemmas of less than 2000 such tokens, so presumably their agreement patterns can be more easily learned.</S>
			<S sid ="207" ssid = "67">DET (see Section 5.4) gives us a slight improvement, providing our best result using the CORE12 POS tagset: 80.53%.</S>
			<S sid ="208" ssid = "68">This is a 1.85% improvement over using only the CORE12 POS tags (an 8.7% error reduction); of this improvement, 0.3% absolute (35% relative) is due to the use of functional features.</S>
			<S sid ="209" ssid = "69">We then use the best configuration, but without the RATIONALITY feature; we see that this feature on its own contributes 0.1% absolute, confirming its place in Arabic syntax.</S>
			<S sid ="210" ssid = "70">In gold experiments which we do not report here, the contribution was even higher (0.60.7%).</S>
			<S sid ="211" ssid = "71">The last row in the fourth part of Table 6 shows that using both surface and functional variants of NUMBER and GENDER does not help (hurts, in fact); the functional morphology features carry sufficient information for syntactic disambiguation.</S>
			<S sid ="212" ssid = "72">The last part of the table revalidates the gains achieved of the best feature combination using the two other POS tagsets mentioned in Section 3: CAT- IBEX (the best performing tagset with predicted values), and BW (the best POS tagset with gold values in Marton et al.</S>
			<S sid ="213" ssid = "73">(2010), but results shown here are with predicted values).</S>
			<S sid ="214" ssid = "74">The CATIBEX result of 80.83% is our overall best result.</S>
			<S sid ="215" ssid = "75">The result using BW reconfirms that BW is not the best tagset to use for parsing Arabic with current prediction ability.</S>
			<S sid ="216" ssid = "76">5.6 Validating results on unseen test set.</S>
			<S sid ="217" ssid = "77">Once experiments on the development set were done, we ran the best performing models on the previously unseen test set (Section 5.1).</S>
			<S sid ="218" ssid = "78">Table 7 shows that the same trends hold on this set as well.</S>
			<S sid ="219" ssid = "79">Model LAS LASdif f UAS CA TIB EX 7 8.</S>
			<S sid ="220" ssid = "80">4 6 — 81 .8 1 +D ET2 +L MM +PE R+ FN *N GR 7 9.</S>
			<S sid ="221" ssid = "81">4 5 + + 0.</S>
			<S sid ="222" ssid = "82">9 9 82 .5 6 Table 7: Results on unseen test set for models which performed best on dev set – predicted input.</S>
			<S sid ="223" ssid = "83">6 Error Analysis.</S>
			<S sid ="224" ssid = "84">We analyze the attachment accuracy by attachment type.</S>
			<S sid ="225" ssid = "85">We show the accuracy for selected attachment types in Table 8.</S>
			<S sid ="226" ssid = "86">Using just CORE12, we see that some attachments (subject, modifications) are harder than others (objects, idafa).</S>
			<S sid ="227" ssid = "87">We see that by Table 8: Error analysis: Accuracy by attachment type (selected): subject, object, modification by a noun, modification (of a verb or a noun) by a preposition, idafa, and overall results (which match previously shown results) adding LMM, all attachment types improve a little bit; this is as expected, since this feature provides a slight lexical abstraction.</S>
			<S sid ="228" ssid = "88">We then add features designed to improve idafa and those relations subject to agreement, subject and nominal modification (DET2, PERSON, NUMBER, GENDER).</S>
			<S sid ="229" ssid = "89">We see that as expected, subject, nominal modification (MN), and idafa reduce error by substantial margins (error reduction over CORE12+LMM greater than 8%, in the case of idafa the error reduction is 16.7%), while object and prepositional attachment (MP) improve to a lesser degree (error reduction of 6.2% or less).</S>
			<S sid ="230" ssid = "90">We assume that the relations not subject to agreement (object and prepositional attachment) improve because of the overall improvement in the parse due to the improvements in the other relations.</S>
			<S sid ="231" ssid = "91">When we move to the functional features, we again see a reduction in the attachments which are subject to agreement, namely subject and nominal modification (error reductions over surface features of 2.1% and 4.4%, respectively).</S>
			<S sid ="232" ssid = "92">Idafa decreases slightly (since this relation is not affected by the functional features), while object stays the same.</S>
			<S sid ="233" ssid = "93">Surprisingly, prepositional attachment also improves, with an error reduction of 3.3%.</S>
			<S sid ="234" ssid = "94">Again, we can only explain this by proposing that the improvement in nominal modification attachment has the indirect effect of ruling out some bad prepositional attachments as well.</S>
			<S sid ="235" ssid = "95">In summary, we see that not only do morphological features – and functional morphology features in particular – improve parsing, but they improve parsing in the way that we expect: those relations subject to agreement improve more than those that are not.</S>
			<S sid ="236" ssid = "96">Last, we point out that MaltParser does not model i.e., it has not learned that certain syntactic relations require identical (functional) morphological feature values.</S>
			<S sid ="237" ssid = "97">The gains in parsing quality reflect that the MaltParser SVM classifier has learned that the pairing of specific morphological feature values – e.g., fem.sing.</S>
			<S sid ="238" ssid = "98">for both the verb and its subject – is useful, with no generalization from each specific value to other values, or to general pairwise value matching.</S>
			<S sid ="239" ssid = "99">7 Conclusions and Future Work.</S>
			<S sid ="240" ssid = "100">We explored the contribution of different morphological (inflectional and lexical) features to dependency parsing of Arabic.</S>
			<S sid ="241" ssid = "101">We find that definiteness (DET), φ-features (PERSON, NUMBER, GENDER), and undiacritized lemma (LMM) are most helpful for Arabic dependency parsing on predicted input.</S>
			<S sid ="242" ssid = "102">We further find that functional morphology features and rationality improve over surface morphological features, as predicted by the complex agreement rules of Arabic.</S>
			<S sid ="243" ssid = "103">To our knowledge, this is the first result in Arabic NLP that uses functional morphology features, and that shows an improvement over surface features.</S>
			<S sid ="244" ssid = "104">In future work, we intend to improve the prediction of functional morphological features in order to improve parsing accuracy.</S>
			<S sid ="245" ssid = "105">We also intend to investigate how these features can be integrated into other parsing frameworks; we expect them to help independently of the framework.</S>
			<S sid ="246" ssid = "106">We plan to make our parser available to other researchers.</S>
			<S sid ="247" ssid = "107">Please contact the authors if interested.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="248" ssid = "108">This work was supported by the DARPA GALE program, contract HR001108-C-0110.</S>
			<S sid ="249" ssid = "109">We thank Joakim Nivre for his useful remarks, Otakar Smrž for his help with Elixir-FM, Ryan Roth and Sarah Alkuhlani for their help with data, and three anonymous reviewers for useful comments.</S>
			<S sid ="250" ssid = "110">Part of the work was done while the first author was at Columbia University.</S>
	</SECTION>
</PAPER>
