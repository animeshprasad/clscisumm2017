<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Generating coherent discourse is an important aspect in natural language generation.</S>
		<S sid ="2" ssid = "2">Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level.</S>
		<S sid ="3" ssid = "3">We present an important subtask for this overall goal, in which we align predicates across comparable texts, admitting partial argument structure correspondence.</S>
		<S sid ="4" ssid = "4">The contribution of this work is twofold: We first construct a large corpus resource of comparable texts, including an evaluation set with manual predicate alignments.</S>
		<S sid ="5" ssid = "5">Secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts.</S>
		<S sid ="6" ssid = "6">Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1 -score.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Discourse coherence is an important aspect in natural language generation (NLG) applications.</S>
			<S sid ="8" ssid = "8">A number of theories have investigated coherence inducing factors.</S>
			<S sid ="9" ssid = "9">A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse.</S>
			<S sid ="10" ssid = "10">A data-driven model based on this theory is the entity-based approach by Barzilay and Lap- ata (2008), which models coherence phenomena by observing sentence-to-sentence transitions of entity occurrences.</S>
			<S sid ="11" ssid = "11">Barzilay and Lapata show that their approach can discriminate between a coherent and a non-coherent set of ordered sentences.</S>
			<S sid ="12" ssid = "12">However, their model is not able to generate alternative entity realizations by itself.</S>
			<S sid ="13" ssid = "13">Furthermore, the entity-based approach only investigates realization patterns for individual entities in discourse in terms of core grammatical functions.</S>
			<S sid ="14" ssid = "14">It does not investigate the interplay between entity transitions and realization patterns for full- fledged semantic structures.</S>
			<S sid ="15" ssid = "15">This interplay, however, is an important factor for a semantics-based, generative model of discourse coherence.</S>
			<S sid ="16" ssid = "16">The main hypothesis of our work is that we can automatically learn context-specific realization patterns for predicate argument structures (PAS) from a semantically parsed corpus of comparable text pairs.</S>
			<S sid ="17" ssid = "17">Our assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010).</S>
			<S sid ="18" ssid = "18">For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants.</S>
			<S sid ="19" ssid = "19">By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments.</S>
			<S sid ="20" ssid = "20">These include the specific forms of argument realization, as a pronoun or a specific type of referential expression, as studied in prior work in NLG (Belz et al., 2009, inter alia).</S>
			<S sid ="21" ssid = "21">The specific setup we examine, however, allows us to further investi 171 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 171–182, Jeju Island, Korea, 12–14 July 2012.</S>
			<S sid ="22" ssid = "22">Qc 2012 Association for Computational Linguistics gate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse.</S>
			<S sid ="23" ssid = "23">Example (1), extracted from our corpus of aligned texts,illustrates this point: Both texts report on the same event of locating victims in an avalanche.</S>
			<S sid ="24" ssid = "24">While (1.a) explicitly talks about the location of this event, the role remains implicit in the second sentence of (1.b), given that it can be recovered from the preceding sentence.</S>
			<S sid ="25" ssid = "25">In fact, realization of this argument role would impede the fluency of discourse by being overly repetitive.</S>
			<S sid ="26" ssid = "26">(1) a. . . .</S>
			<S sid ="27" ssid = "27">The official said that [no bodies]Arg1 had been recovered [from the avalanches]Arg2 which occurred late Friday in the Central Asian country near the Afghan border some 300 kilometers (185 miles) southeast of the capital Dushanbe.</S>
			<S sid ="28" ssid = "28">b. Three other victims were trapped in an avalanche in the village of Khichikh.</S>
			<S sid ="29" ssid = "29">[None of the victims bodies]Arg1 have been found [ ]Argmloc . This phenomenon clearly relates to the problem of discourse-linking of implicit roles, a very challenging task in discourse processing.1 In our work, we consider this problem from a content-based generation perspective, concentrating on the discourse factors that allow for the omission of a role.</S>
			<S sid ="30" ssid = "30">Thus, our aim is to identify comparable predications across aligned texts, and to study the discourse coherence factors that determine the realization patterns of arguments in the respective discourses.</S>
			<S sid ="31" ssid = "31">This can be achieved by considering the full set of arguments that can be recovered from the aligned predications.</S>
			<S sid ="32" ssid = "32">This paper focuses on the first of these tasks, henceforth called predicate alignment.2 In line with data-driven approaches in NLP, we automatically align predicates in a suitable corpus of paired texts.</S>
			<S sid ="33" ssid = "33">The induced alignments will (i) serve to identify events described in both comparable texts, and (ii) provide information about the underlying argument structures and how they are realized in each context to establish a coherent discourse.</S>
			<S sid ="34" ssid = "34">We investigate a graph-based clustering method for induc 1 See the recent SemEval 2010 task: Linking Events and their Participants in Discourse, (Ruppenhofer et al., 2010).</S>
	</SECTION>
	<SECTION title="Note that we provide details regarding the construction of. " number = "2">
			<S sid ="35" ssid = "1">a suitable data set and further examples involving non-realized arguments in a complementary paper (Roth and Frank, 2012).</S>
			<S sid ="36" ssid = "2">ing such alignments as clustering provides a suitable framework to implicitly relate alignment decisions to one another, by exploiting global information encoded in a graph.</S>
			<S sid ="37" ssid = "3">The remainder of this paper is structured as follows: In Section 2, we discuss previous work in related tasks.</S>
			<S sid ="38" ssid = "4">Section 3 describes our task and a suitable data set.</S>
			<S sid ="39" ssid = "5">Section 4 introduces a graph-based clustering model using Mincuts for the alignment of predicates.</S>
			<S sid ="40" ssid = "6">Section 5 outlines the experiments and presents evaluation results.</S>
			<S sid ="41" ssid = "7">Finally, we conclude in Section 6 and discuss future work.</S>
			<S sid ="42" ssid = "8">2 Related Work.</S>
			<S sid ="43" ssid = "9">The task of aligning words in general has been studied extensively in previous work, for example as part of research in statistical machine translation (SMT).</S>
			<S sid ="44" ssid = "10">Typically, alignment models in SMT are trained by observing and (re-)estimating co-occurrence counts of word pairs in parallel sentences (Brown et al., 1993).</S>
			<S sid ="45" ssid = "11">The same methods have also been applied in monolingual settings, for example to align words in paraphrases (Cohn et al., 2008).</S>
			<S sid ="46" ssid = "12">In contrast to traditional word alignment tasks, our focus is not on pairs of isolated sentences but on aligning predicates within the discourse contexts in which they are situated.</S>
			<S sid ="47" ssid = "13">Furthermore, text pairs for our task should not be strictly parallel as we are specifically interested in the impact of different discourse contexts.</S>
			<S sid ="48" ssid = "14">In Section 5, we will show that this particular setting indeed constitutes a more challenging task compared to traditional word alignment in parallel or paraphrasing sentences.</S>
			<S sid ="49" ssid = "15">Another set of related tasks is found in the area of textual inference.</S>
			<S sid ="50" ssid = "16">Since 2006, there have been regular challenges on the task of Recognizing Textual Entailment (RTE).</S>
			<S sid ="51" ssid = "17">In the original task description, Dagan et al.</S>
			<S sid ="52" ssid = "18">(2006) define textual entailment “as a directional relationship between pairs of text expressions, denoted by T - the entailing ‘Text’ -, and H - the entailed ‘Hypothesis’.</S>
			<S sid ="53" ssid = "19">) T entails H if the meaning of H can be inferred from the meaning of T, as would typically be interpreted by people.” Although this relation does not necessarily require the presence of corresponding predicates, previous work by MacCartney et al.</S>
			<S sid ="54" ssid = "20">(2008) shows that word alignments can serve as a good indicator of entailment.</S>
			<S sid ="55" ssid = "21">As a matter of fact, the same holds true for the task of detecting paraphrases.</S>
			<S sid ="56" ssid = "22">In contrast to RTE, this latter task requires bidirectional entailments, i.e., each of the two phrases must entail the other.</S>
			<S sid ="57" ssid = "23">Wan et al.</S>
			<S sid ="58" ssid = "24">(2006) show that a simple approach solely based on word (and lemmatized n-gram) overlap can already achieve an F1-score of up to 83% for detecting paraphrases in the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005, MSRPC).</S>
			<S sid ="59" ssid = "25">In fact, this is just 0.6% points below the state-of-the-art results recently reported by Socher et al.</S>
			<S sid ="60" ssid = "26">(2011).</S>
			<S sid ="61" ssid = "27">The MSRPC and data sets from the first RTE challenges only consisted of isolated pairs of sentences.</S>
			<S sid ="62" ssid = "28">The Fifth PASCAL Recognizing Textual Entailment Challenge (Bentivogli et al., 2009) introduced a “Search Task”, where entailing sentences for a hypothesis have to be found in a set of full documents.</S>
			<S sid ="63" ssid = "29">This new task first opened the doors for assessing the role of discourse (Mirkin et al., 2010a; Mirkin et al., 2010b) in RTE.</S>
			<S sid ="64" ssid = "30">However, this setting is still limited as discourse contexts are only provided for the entailing part (T ) of each text pair but not for the hypothesis H . A further task related to ours is the detection of event coreference.</S>
			<S sid ="65" ssid = "31">The goal of this task is to identify all mentions of the same event within a document and, in some settings, also across documents.</S>
			<S sid ="66" ssid = "32">However, the task setting is typically more restricted than ours in that its focus lies on identical events/references (cf.</S>
			<S sid ="67" ssid = "33">Walker et al.</S>
			<S sid ="68" ssid = "34">(2006), Weischedel et al.</S>
			<S sid ="69" ssid = "35">(2011), inter alia).</S>
			<S sid ="70" ssid = "36">In particular, verbalizations of different aspects of an event (e.g., ‘buy’–‘sell’, ‘kill’–‘die’, ‘recover’–‘find’) are generally not linked in this paradigm.</S>
			<S sid ="71" ssid = "37">In contrast to co- reference methods that identify chains of events, we are interested in pairs of corresponding predicates (and their argument structure), for which we can observe alternative realizations in discourse.</S>
	</SECTION>
	<SECTION title="Aligning Predicates Across Texts. " number = "3">
			<S sid ="72" ssid = "1">This section summarizes how we built a large corpus of comparable texts, as a basis for the predicate alignment task.</S>
			<S sid ="73" ssid = "2">We motivate the choice of the corpus and present a strategy for extracting comparable text pairs.</S>
			<S sid ="74" ssid = "3">Subsequently, we report on the preparation of an evaluation data set with manual predicate alignments across the paired texts.</S>
			<S sid ="75" ssid = "4">We conclude this section with an example that showcases the potential of using aligned predicates for the study of coherence phenomena.</S>
			<S sid ="76" ssid = "5">More detailed information regarding corpus creation, annotation guidelines and additional examples illustrating the potential of this corpus can be found in Roth and Frank (2012).</S>
			<S sid ="77" ssid = "6">3.1 Corpus Creation.</S>
			<S sid ="78" ssid = "7">The goal of our work is to investigate coherence factors for argument structure realization, using comparable texts that describe the same events, but that include variation in textual presentation.</S>
			<S sid ="79" ssid = "8">This requirement fits well with the news domain, for which we can trace varying textual sources that describe the same underlying events.</S>
			<S sid ="80" ssid = "9">The English Gigaword Fifth Edition (Parker et al., 2011) corpus (henceforth just Gigaword) is one of the largest corpus collections for English.</S>
			<S sid ="81" ssid = "10">It comprises a total of 9.8 million newswire articles from seven distinct sources.</S>
			<S sid ="82" ssid = "11">In previous work (Roth and Frank, 2012), we introduced GigaPairs, a sub-corpus extracted from Gigaword that includes over 160,000 pairs of newswire articles from distinct sources.</S>
			<S sid ="83" ssid = "12">GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al.</S>
			<S sid ="84" ssid = "13">(2009).</S>
			<S sid ="85" ssid = "14">In addition to calculating the similarity of news titles, we impose an additional date constraint to further increase the precision of extracted pairs of texts.</S>
			<S sid ="86" ssid = "15">Random inspection of about 100 documents revealed only two texts describing different events.</S>
			<S sid ="87" ssid = "16">Overall, we extracted 167,728 document pairs containing a total of 50 million word tokens.</S>
			<S sid ="88" ssid = "17">Each document in this corpus consists of up to 7.564 words with a mean and median of 301 and 213 words, respectively.</S>
			<S sid ="89" ssid = "18">All texts have been pre-processed using MATE tools (Bjo¨ rkelund et al., 2010; Bohnet, 2010), a pipeline of NLP modules including a state- of-the-art semantic role labeler that computes Prop- Bank/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008).</S>
			<S sid ="90" ssid = "19">3.2 Gold Standard Annotation.</S>
			<S sid ="91" ssid = "20">We selected 70 text pairs from the GigaPairs corpus for manual predicate alignment.</S>
			<S sid ="92" ssid = "21">All document pairs were randomly chosen with the constraint that each text consists of 100 to 300 words.3 Predi 3 This constraint is satisfied by 75.3% of all documents in.</S>
			<S sid ="93" ssid = "22">GigaPairs.</S>
			<S sid ="94" ssid = "23">cates identified by the semantic parser are provided as pre-labeled annotations for alignment.</S>
			<S sid ="95" ssid = "24">We asked two students4 to tag corresponding predicates across each text pair.</S>
			<S sid ="96" ssid = "25">Following standard practice in word alignment tasks (cf.</S>
			<S sid ="97" ssid = "26">Cohn et al.</S>
			<S sid ="98" ssid = "27">(2008)) the annota- tors were instructed to distinguish between sure and possible alignments, depending on how certainly, in their opinion, two predicates describe verbalizations of the same event.</S>
			<S sid ="99" ssid = "28">The following examples show predicate pairings marked as sure (2) and as possible alignments (3).</S>
			<S sid ="100" ssid = "29">(2) a. The regulator ruled on September 27 that Nasdaq too was qualified to bid for OMX [.</S>
			<S sid ="101" ssid = "30">] b. The authority [.</S>
			<S sid ="102" ssid = "31">] had already approved a similar application by Nasdaq.</S>
			<S sid ="103" ssid = "32">(3) a. Myanmar’s military government said earlier this year it has released some 220 political prisoners [.</S>
			<S sid ="104" ssid = "33">] b. The government has been regularly releasing members of Suu Kyi’s National League for Democracy party [.</S>
			<S sid ="105" ssid = "34">] In total, the annotators (A/B) aligned 487/451 sure and 221/180 possible alignments with a Kappa score (Cohen, 1960) of 0.86.5 For the construction of a gold standard, we merged the alignments from both annotators by taking the union of all possible alignments and the intersection of all sure alignments.</S>
			<S sid ="106" ssid = "35">Cases which involved a sure alignment on which the annotators disagreed were resolved in a group discussion with the first author.</S>
			<S sid ="107" ssid = "36">We split the final corpus into a development set of 10 document pairs and a test set of 60 document pairs.</S>
			<S sid ="108" ssid = "37">The test set contains a total of 3,453 predicates (1,531 nouns and 1,922 verbs).</S>
			<S sid ="109" ssid = "38">Its gold standard annotation consists of 446 sure and 361 possible alignments, which corresponds to an average of 7.4 sure (6.0 possible) alignments per document pair.</S>
			<S sid ="110" ssid = "39">Most of the gold alignments (82.4%) are between predicates of the same part-of-speech (242 noun and 423 verb pairs).</S>
			<S sid ="111" ssid = "40">A total of 383 gold alignments (47.5%) have been annotated between predicates with identical lemma form.</S>
			<S sid ="112" ssid = "41">Diverging numbers of realized arguments can be observed in 320 pairs (39.7%).</S>
			<S sid ="113" ssid = "42">4 Both annotators are students in computational linguistics, one undergraduate (A) and one postgraduate (B) student.</S>
			<S sid ="114" ssid = "43">5 Following Brockett (2007), we computed agreement on la-.</S>
			<S sid ="115" ssid = "44">beled annotations, including unaligned predicate pairs as an additional null category.</S>
			<S sid ="116" ssid = "45">3.3 Potential for Discourse Coherence.</S>
			<S sid ="117" ssid = "46">This section presents an example of an aligned predicate pair from our development set that illustrates the potential of aggregating corresponding PAS across comparable texts.</S>
			<S sid ="118" ssid = "47">The example represents one of eleven cases involving unrealized arguments that can be found in our development set of only ten document pairs.</S>
			<S sid ="119" ssid = "48">(4) a. The Chadians said theyArg0 had fled in fear of their lives.</S>
			<S sid ="120" ssid = "49">b. The United Nations says some 20,000 refugeesArg0 have fled into CameroonArg1 . In both sentences, the Arg0 role of the predicate flee is filled, but Arg1 (here: the goal) has not been realized in (4.a).</S>
			<S sid ="121" ssid = "50">However, sentence (4.a) is still part of a coherent discourse, as a role filler for the omitted argument can be inferred from the preceding context.</S>
			<S sid ="122" ssid = "51">For the goal of our work, we are interested in factors that license such omissions of an argument.</S>
			<S sid ="123" ssid = "52">Potential factors on the discourse level include the information status of the entity filling an argument position, and its salience at the corresponding point in discourse.</S>
			<S sid ="124" ssid = "53">Roth and Frank (2012) discuss additional examples that demonstrate the importance of factors on further linguistic levels, e.g., lexical choice of predicates and their syntactic realization.</S>
			<S sid ="125" ssid = "54">In the example above, the aggregation of aligned PAS presents an effective means to identify appropriate fillers for unrealized roles.</S>
			<S sid ="126" ssid = "55">Hence, we can utilize each such pair as one positive and one negative training instance for a model of discourse coherence that controls the omissibility of arguments.</S>
			<S sid ="127" ssid = "56">In what follows, we introduce an alignment approach that can be used to automatically acquire more training data using the entire GigaPairs corpus.</S>
	</SECTION>
	<SECTION title="Model. " number = "4">
			<S sid ="128" ssid = "1">For the automatic induction of predicate alignments across texts, we opt for an unsupervised graph-based clustering method.</S>
			<S sid ="129" ssid = "2">In this section, we first define a graph representation for pairs of documents.</S>
			<S sid ="130" ssid = "3">In particular, predicates are represented as nodes in such a graph and similarities between predicates as edges.</S>
			<S sid ="131" ssid = "4">We then proceed to describe various similarity measures that can be used to identify similar predicate instances.</S>
			<S sid ="132" ssid = "5">Finally, we introduce the clustering algorithm that we apply to graphs (representing pairs of documents) in order to induce alignments between corresponding predicates.</S>
			<S sid ="133" ssid = "6">4.1 Graph representation.</S>
			<S sid ="134" ssid = "7">We build a bipartite graph representation for each (Fellbaum, 1998) to find the synset of the least common subsumer (lcs) and uses the pre-computed Information Content (IC) files from Pedersen et al.</S>
			<S sid ="135" ssid = "8">(2004) to compute Lin’s measure: I C (lcs(s1, s2))pair of texts, using as vertices the predicate argument structures assigned in pre-processing (cf.</S>
			<S sid ="136" ssid = "9">Sec simWN(p1, p2) = I C (s1) ∗ I C (s2) (3) tion 3.1).</S>
			<S sid ="137" ssid = "10">We represent each predicate as a node and integrate information about arguments only implicitly.</S>
			<S sid ="138" ssid = "11">Given the sets of predicates P1 and P2 of two comparable texts T1 and T2, respectively, we formally define an undirected graph GP1 ,P2 as follows: V = P1 ∪ P2 In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun represents a nominalization of a verbal predicate, we resort to the corresponding verb synset.</S>
			<S sid ="139" ssid = "12">If no relation can be found between two predicates, we set a default value of simWN = 0.</S>
			<S sid ="140" ssid = "13">This applies GP1 ,P2 = (V, E) where E = P1 × P2 (1) in particular to all cases that involve a predicate not present in WordNet.</S>
			<S sid ="141" ssid = "14">Edge weights.</S>
			<S sid ="142" ssid = "15">We specify the edge weight between two nodes representing predicates p1 ∈ P1 and p2 ∈ P2 as a weighted linear combination of four similarity measures described in the next section: WordNet and VerbNet similarity, Distributional similarity and Argument similarity.</S>
			<S sid ="143" ssid = "16">wp1 p2 = λ1 ∗ simWN(p1, p2) VerbNet similarity.</S>
			<S sid ="144" ssid = "17">To overcome systematic problems with the WordNet verb hierarchy (cf.</S>
			<S sid ="145" ssid = "18">Richens (2008)), we further compute similarity between verbal predicates using VerbNet (Kipper et al., 2008).</S>
			<S sid ="146" ssid = "19">Verbs in VerbNet are categorized into semantic classes according to their syntactic behavior.</S>
			<S sid ="147" ssid = "20">A class C can recursively embed sub-classes (C ) that represent finer semantic and + λ2 ∗ simVN(p1, p2) + λ3 ∗ simDist(p1, p2) + λ4 ∗ simArg(p1, p2) (2) Cs ∈ sub syntactic distinctions.</S>
			<S sid ="148" ssid = "21">We define a simple similarity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1, p2 depending on Initially we set all weighting parameters λ1 . . .</S>
			<S sid ="149" ssid = "22">λ4 to have uniform weights by default.</S>
			<S sid ="150" ssid = "23">In Section 5, we their relatedness within the VerbNet class hierarchy:define an optimized weighting setting for the indi  1.0 if C : p , p C vidual similarity measures.</S>
			<S sid ="151" ssid = "24">∃ 1 2 ∈  4.2 Similarity Measures.</S>
			<S sid ="152" ssid = "25">simVN (p1 , p2 ) =  0.8 if ∃C, Cs : Cs ∈ sub(C ) (4)  ∧ p1 , p2 ∈ C ∪ Cs We employ a number of similarity measures that make use of complementary information that is type-based (simWN/VN/Dist) or token-based (simArg ).6 Given two lemmatized predicates p1, p2 and their set of arguments A1 = args(p1), A2 = args(p2), we define the following measures.</S>
			<S sid ="153" ssid = "26">WordNet similarity.</S>
			<S sid ="154" ssid = "27">Given all pairs of synsets s1, s2 that contain the predicates p1, p2, respectively, we compute the maximal similarity using the information theoretic measure described in Lin (1998).</S>
			<S sid ="155" ssid = "28">Our implementation exploits the WordNet hierarchy 6 All token-based frequency counts (i.e., f req() and idf ()) are computed over all documents from the AFP and APW parts of the English Gigaword Fifth Edition.</S>
			<S sid ="156" ssid = "29"> 0.0 else Distributional similarity.</S>
			<S sid ="157" ssid = "30">As some predicates may not be covered by the WordNet and VerbNet hierarchies, we additionally calculate similarity based on distributional meaning in a semantic space (Landauer and Dumais, 1997).</S>
			<S sid ="158" ssid = "31">Following the traditional bag-of-words approach that has been applied in related tasks (Guo and Diab, 2011; Mitchell and La- pata, 2010), we consider the 2,000 most frequent context words c1, . . .</S>
			<S sid ="159" ssid = "32">, c2000 ∈ C as dimensions of a vector space and define predicates as vectors using their Pointwise Mutual Information (PMI): p__ = (PMI(p, c1), . . .</S>
			<S sid ="160" ssid = "33">, PMI(p, c2000) (5) with PMI(x, y) = freq(x, y) freq(x) ∗ freq(y) Given the vector representations of two predicates, we calculate their similarity as the cosine of the angle between the two vectors: p__1 · p__2 function CLUSTER(G) clusters ← ∅ E ← GETEDGES(G) 1&gt; Step 1 e ← GETEDGEWITHLOWESTWEIGHT(E) s ← GETSOURCENODE(e) t ← GETTARGETNODE(e) G ← MINCUT(G, s, t) 1&gt; Step 2 simDist(p1, p2) = |p__1| ∗ |p__2| (6) C ← GETCONNECTEDCOMPONEN TS(G )Argument similarity.</S>
			<S sid ="161" ssid = "34">While the previous similar ity measures are purely type-based, argument similarity integrates token-based, i.e., discourse-specific, similarity information about predications by taking into account the similarity of their arguments.</S>
			<S sid ="162" ssid = "35">This measure calculates the association between the arguments A1 of the first and the arguments A2 of the second predicate by determining the ratio of overlapping words in both argument sets.</S>
			<S sid ="163" ssid = "36">for all Gs ∈ C do 1&gt; Step 3 if SIZE(Gs) &lt;= 2 then clusters ← clusters ∪ Gs else clusters ← clusters ∪ CLUSTER(Gs) end if end for return clusters; end function simArg(p1, p2) = w∈A1 ∩A2 idf(w) Figure 2: Pseudo code of our clustering algorithm w∈A1 idf(w) + w∈A2 idf(w) (7) As our goal is to induce clusters that correspond to In order to give higher weight to (rare) content words, we weight each word by its Inverse Document Frequency (IDF), which we calculate over all documents d from the AFP and APW sections of the Gigaword corpus: pairs of similar predicates, we set a maximum number of two nodes per cluster as stopping criterion.</S>
			<S sid ="164" ssid = "37">Given an input graph G, our algorithm recursively applies Mincuts in three steps as described in Figure 2.</S>
			<S sid ="165" ssid = "38">Step 1 identifies the edge e with lowest weight in.</S>
			<S sid ="166" ssid = "39">the given graph G. Step 2 performs the actual Min idf(w) = log |D| |{d : w ∈ D|} (8) cut operation on G. Finally, the stopping criterion and recursion are applied in Step 3.</S>
			<S sid ="167" ssid = "40">An example of Normalization.</S>
			<S sid ="168" ssid = "41">In order to make the outputs of all similarity measures comparable, we normalize their value ranges on the development set to have a mean and standard deviation of 1.0.</S>
			<S sid ="169" ssid = "42">4.3 Mincut-based Clustering.</S>
			<S sid ="170" ssid = "43">Our graph clustering method uses minimum cuts (or Mincut) in order to partition the bipartite text graph into clusters of aligned predicates.</S>
			<S sid ="171" ssid = "44">A Mincut operation divides a given graph into two disjoint sub- graphs.</S>
			<S sid ="172" ssid = "45">Each minimum cut is performed as a cut between some source node s and some target node t, such that (i) each of the two nodes will be in a different sub-graph and (ii) the sum of weights of all removed edges will be as small as possible.</S>
			<S sid ="173" ssid = "46">Our system determines each Mincut using an implementation of the method by Goldberg and Tarjan (1986).7 7 Basic graph operations are performed using the freely available Java library JGraph, cf.</S>
			<S sid ="174" ssid = "47">http://jgrapht.org/.</S>
			<S sid ="175" ssid = "48">a clustered graph is illustrated in Figure 1.</S>
			<S sid ="176" ssid = "49">The advantage of our method compared to off- the-shelf clustering techniques is twofold: On the one hand, the clustering algorithm is free of any parameters, such as the number of clusters or a clustering threshold, that require fine-tuning.</S>
			<S sid ="177" ssid = "50">On the other hand, the approach makes use of a termination criterion that very well represents the nature of the goal of our task, namely to align pairs of predicates across comparable texts.</S>
			<S sid ="178" ssid = "51">The next section provides empirical evidence for the advantage of this approach.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="179" ssid = "1">This section evaluates our graph-clustering model on the task of aligning predicates across comparable texts.</S>
			<S sid ="180" ssid = "2">For comparison to related tasks and methods, we describe different evaluation settings, vari Figure 1: The predicates of two sentences (white: “The company has said it plans to restate its earnings for 2000 through 2002.”; grey: “The company had announced in January that it would have to restate earnings (.</S>
			<S sid ="181" ssid = "3">)”) from the Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.</S>
			<S sid ="182" ssid = "4">ous baselines, as well as results for these baselines and the model presented above.</S>
			<S sid ="183" ssid = "5">5.1 Settings.</S>
			<S sid ="184" ssid = "6">In order to benchmark our model against traditional methods for word alignment, we first apply our graph-based alignment model (Full) on three sentence-based paraphrase corpora.</S>
			<S sid ="185" ssid = "7">This model uses the similarity measures defined in Section 4.2 and the clustering algorithm introduced in Section 4.3.</S>
			<S sid ="186" ssid = "8">In a second experiment, we evaluate Full on our novel task of inducing predicate alignments across comparable monolingual texts, using the GigaPairs data set described in Section 3.</S>
			<S sid ="187" ssid = "9">We evaluate against the manually annotated gold alignments in the test data set described in Section 3.2.</S>
			<S sid ="188" ssid = "10">To gain more insight into the performance of the various similarity measures included in the Full model, we evaluate simplified versions that omit individual similarity measures (Full–[measure name]).</S>
			<S sid ="189" ssid = "11">The relative differences in performance against various baselines will help us quantify the differences and difficulties between a traditional sentence- based word alignment setting and our novel alignment task that operates on full texts.</S>
			<S sid ="190" ssid = "12">5.1.1 Sentence-level Alignment Setting For sentence-based predicate alignment we make use of the following three corpora that are word- aligned subsets of the paraphrase collections described in (Cohn et al., 2008): MTC consists of 100 sentence pairs from the Multiple-Translation Chinese Corpus (Huang et al., 2002), Leagues contains 100 sentential paraphrases from two translations of Jules Verne’s “Twenty Thousand Leagues Under the Sea”, and MSR is a subset of the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 130 sentence pairs.</S>
			<S sid ="191" ssid = "13">All three paraphrase collections are in English.</S>
			<S sid ="192" ssid = "14">Results for these experiments are reported in Section 5.3.1.</S>
			<S sid ="193" ssid = "15">Note that in order to determine alignment candidates, we apply the same pre-processing steps as used for the annotation of our corpus.</S>
			<S sid ="194" ssid = "16">The semantic parser identified an average number of 3.8, 5.1 and 4.7 predicates per text (i.e., per paraphrase sentence) in MTC, Leagues and MSR, respectively.</S>
			<S sid ="195" ssid = "17">All models are evaluated against the subset of gold standard alignments (cf.</S>
			<S sid ="196" ssid = "18">Cohn et al.</S>
			<S sid ="197" ssid = "19">(2008)) between pairs of words marked as predicates.</S>
			<S sid ="198" ssid = "20">5.1.2 Text-level Alignment Setting Results for our own data set, GigaPairs, are reported in Section 5.3.2.</S>
			<S sid ="199" ssid = "21">In this setting, models are evaluated against the annotated gold standard alignments between predicates as described in Section 3.2.</S>
			<S sid ="200" ssid = "22">Since all text pairs in GigaPairs comprise multiple sentences each, the average number of predicates per text to consider (27.5) is much higher than in the paraphrase settings.</S>
			<S sid ="201" ssid = "23">As the full graph representation becomes rather inefficient to handle (by default, edges are inserted between all predicate pairs), we use the development set of 10 text pairs to estimate M T C Pr eci sio n Recall F1 L e a g u e s Pr eci sio n Recall F1 M S R Pr eci sio n Recall F1 L e m m a I d G r e e d y W or d Al ig n 2 5 . 1 * * 74.9 37.6** 7 4 . 8 * * 88.3** 81.0 9 9 . 3 8 6 . 6 9 2 . 5 3 1 . 5 * * 67.2 42.9** 7 5 . 0 * * 86.0** 80.1 9 8 . 7 7 8 . 5 8 7 . 4 4 2 . 3 * * 90.8 57.7** 8 0 . 7 * * 97.0** 88.1 9 9 . 5 9 6 . 0 * 9 7 . 7 * F u l l 9 2 . 3 72.2 81.1 9 2 . 7 69.4 79.4 9 4 . 5 88.3 91.3 Table 1: Results for sentence-based predicte alignment in the three benchmark settings MTC, Leagues and MSR (all numbers in %); results that significantly differ from Full are marked with asterisks (* p&lt;0.05; ** p&lt;0.01).</S>
			<S sid ="202" ssid = "24">a threshold on predicate similarity for adding edges.</S>
			<S sid ="203" ssid = "25">We tested all thresholds from 1.5 to 4.0 with a step- size of 0.25 and found 2.5 to perform best.</S>
			<S sid ="204" ssid = "26">This threshold is applied in the evaluation of all graph- based models.</S>
			<S sid ="205" ssid = "27">5.2 Baselines.</S>
			<S sid ="206" ssid = "28">A simple baseline for both settings is to align all predicates whose lemmas are identical.</S>
			<S sid ="207" ssid = "29">This baseline, henceforth called LemmaId, is computed as a lower bound for all settings.</S>
			<S sid ="208" ssid = "30">In order to assess the benefits of the clustering step, we propose a second baseline that uses the same similarity measures and thresholds as our Full model, but omits the clustering step described in Section 4.3.</S>
			<S sid ="209" ssid = "31">Instead, it greedily computes as many 1-to-1 alignments as possible, starting from the highest similarity to the learned threshold (Greedy).</S>
			<S sid ="210" ssid = "32">As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT).</S>
			<S sid ="211" ssid = "33">For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al.</S>
			<S sid ="212" ssid = "34">(2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus.</S>
			<S sid ="213" ssid = "35">For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-the- art word alignment tool Berkeley Aligner (Liang et al., 2006).</S>
			<S sid ="214" ssid = "36">As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al.</S>
			<S sid ="215" ssid = "37">(2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign.</S>
			<S sid ="216" ssid = "38">5.3 Results.</S>
			<S sid ="217" ssid = "39">We measure precision as the number of predicted alignments that are annotated in the gold standard divided by the total number of predictions.</S>
			<S sid ="218" ssid = "40">Recall is measured as the number of correctly predicted sure alignments divided by the total number of sure alignments in the gold standard.</S>
			<S sid ="219" ssid = "41">This conforms to evaluation measures used for word alignment models in SMT (Och and Ney, 2003).</S>
			<S sid ="220" ssid = "42">Following Cohn et al.</S>
			<S sid ="221" ssid = "43">(2008), we subsequently compute the F1-score as the harmonic mean between precision and recall.</S>
			<S sid ="222" ssid = "44">We compute statistical significance of result differences with a paired t-test (Cohen, 1995) over the affected test set documents and provide corresponding significance levels where appropriate.</S>
			<S sid ="223" ssid = "45">5.3.1 Sentence-level Predicate Alignment The results for MTC, Leagues and MSR are presented in Table 1.</S>
			<S sid ="224" ssid = "46">The numbers indicate that WordAlign consistently outperforms all other models on the three data sets in terms of F1-score.</S>
			<S sid ="225" ssid = "47">Statistical significance of result differences between WordAlign and Full can only be observed for recall and F1-score on the MSR data set (p&lt;0.05).</S>
			<S sid ="226" ssid = "48">Other differences are not significant due to high variance of results compared to data set sizes.</S>
			<S sid ="227" ssid = "49">The overall performance of WordAlign does not come much as a surprise, seeing that all three data sets consist of highly parallel sentence pairs.</S>
			<S sid ="228" ssid = "50">In fact, the results for LemmaId show that by aligning all predicates with identical lemmas, most of the sure alignments in the three settings are already covered.</S>
			<S sid ="229" ssid = "51">The reason for the low precision lies in the fact that the same lemma can occur multiple times in the same paraphrase, a phenomenon that is bet 8 Note that the performance of this system lies slightly be-.</S>
			<S sid ="230" ssid = "52">low the state-of-the-art results reported by Socher et al.</S>
			<S sid ="231" ssid = "53">(2011)ter handled by WordAlign, Greedy and Full.</S>
			<S sid ="232" ssid = "54">In terestingly, the Greedy model achieves the highest recall in all settings but it performs below our Full model in terms of precision and F1-score.</S>
			<S sid ="233" ssid = "55">The performance differences between Greedy and Full are statistically significant (p&lt;0.01) regarding precision and recall.</S>
			<S sid ="234" ssid = "56">However, we were not able to reproduce the results of Socher et al. using the publicly available release of their software.</S>
			<S sid ="235" ssid = "57">5.3.2 Text-level Predicate Alignment We now turn to the experiments on our own data set, GigaPairs, which comprises full documents of unequal lengths instead of pairs of single sentences.</S>
			<S sid ="236" ssid = "58">Table 2 presents the results for our full model and the three baselines.</S>
			<S sid ="237" ssid = "59">From all four approaches, WordAlign yields lowest performance.</S>
			<S sid ="238" ssid = "60">We observe two main reasons for this: On the one hand, sentence paraphrase detection does not perform perfectly.</S>
			<S sid ="239" ssid = "61">Hence, the extracted sentence pairs do not always contain gold alignments.</S>
			<S sid ="240" ssid = "62">On the other hand, even sentence pairs that contain gold alignments are generally less parallel than in the previous settings, which make them harder to align.</S>
			<S sid ="241" ssid = "63">The increased difficulty can also be seen in the results for the Greedy baseline, which only achieves an F1-score of 20.1% in this setting.</S>
			<S sid ="242" ssid = "64">In contrast, we observe that the majority of all sure alignments (60.3%) can be retrieved by applying the LemmaId model.</S>
			<S sid ="243" ssid = "65">The Full model achieves a recall of 46.6%, but it significantly outperforms LemmaId (p&lt;0.01) in terms of precision (58.7%, +18.4 percentage points).</S>
			<S sid ="244" ssid = "66">This is an important factor for us, as we plan to use the alignments in subsequent tasks.</S>
			<S sid ="245" ssid = "67">With 52.0%, Full achieves the best overall F1-score.</S>
			<S sid ="246" ssid = "68">Ablating similarity measures.</S>
			<S sid ="247" ssid = "69">All aforementioned results were conducted in experiments with a uniform weighting scheme of similarity measures as introduced in Section 4.3.</S>
			<S sid ="248" ssid = "70">Table 3 shows the performance impact of individual similarity measures by removing them completely (i.e., setting their weight to 0.0).</S>
			<S sid ="249" ssid = "71">The numbers indicate that not all measures contribute positively to the overall performance when using equal weights.</S>
			<S sid ="250" ssid = "72">However, a significant difference can only be observed when removing the argument similarity measure, which drastically reduces the results.</S>
			<S sid ="251" ssid = "73">This clearly highlights the importance of incorporating the context of individual predications in this task.</S>
			<S sid ="252" ssid = "74">Tuning weights.</S>
			<S sid ="253" ssid = "75">Subsequently, we tested various combinations of weights on our development set in order to estimate a good overall weighting scheme.</S>
			<S sid ="254" ssid = "76">Table 2: Results for GigaPairs (all numbers in %); results that significantly differ from Full are marked with asterisks (* p&lt;0.05; ** p&lt;0.01).</S>
			<S sid ="255" ssid = "77">Precision Recall F1 F u l l – W N 5 8 . 9 4 8 . 0 5 2 . 9 F u l l – V N 5 7 . 3 4 8 . 7 5 2 . 6 F u l l – D i s t F u ll – A r g s 5 4 . 3 4 0 . 1 * * 4 2 . 8 2 4.</S>
			<S sid ="256" ssid = "78">0* * 4 7 . 9 3 0.</S>
			<S sid ="257" ssid = "79">0* * F u l l F ull +t u ne d 5 8 . 7 5 9 . 7 * * 4 6 . 6 5 0.</S>
			<S sid ="258" ssid = "80">7* * 5 2 . 0 5 4.</S>
			<S sid ="259" ssid = "81">8* * Table 3: Impact of removing individual measures and using a tuned weighting scheme (all numbers in %); results that significantly differ from Full are marked with asterisks (* p&lt;0.05; ** p&lt;0.01).</S>
			<S sid ="260" ssid = "82">This tuning procedure is implemented as a brute- force technique, in which we fix the weight of one similarity measure and allow all other measures to receive a weight assignment between 0.25 to 5.0 times the fixed weight.</S>
			<S sid ="261" ssid = "83">Finally, the resulting weights are normalized to sum to 1.0.</S>
			<S sid ="262" ssid = "84">We found the best performing weighting scheme to be 0.09, 0.48, 0.24 and 0.19 for λ1, . . .</S>
			<S sid ="263" ssid = "85">, λ4, respectively (cf.</S>
			<S sid ="264" ssid = "86">Eq.</S>
			<S sid ="265" ssid = "87">(2), Section 4).</S>
			<S sid ="266" ssid = "88">The performance gains of the resulting model (Full+tuned) can be seen in Table 3.</S>
			<S sid ="267" ssid = "89">Computing statistical significance of the result differences between Full+tuned and all baseline models confirmed significant improvements (p&lt;0.01) for both precision and F1-score.</S>
			<S sid ="268" ssid = "90">5.4 Error Analysis.</S>
			<S sid ="269" ssid = "91">We perform an error analysis on the output of Full+tuned on the development set of GigaPairs in order to determine reoccurring problems.</S>
			<S sid ="270" ssid = "92">In total, the model missed 13 out of 35 sure alignments (Type I errors) and predicted 23 alignments not annotated in the gold standard (Type II errors).</S>
			<S sid ="271" ssid = "93">Six Type I errors (46%) occurred when the lemma of an affected predicate occurred more than once in a text and the model missed a correct link.</S>
			<S sid ="272" ssid = "94">Vice versa, identical predicates that refer to different events have been the source of 8 Type II errors (35%).</S>
			<S sid ="273" ssid = "95">We observe that these errors are frequently related to predicates, such as “say” and “appear”, that often occur in news texts.</S>
			<S sid ="274" ssid = "96">Altogether, we find 15 Type II errors (65%) that are due to high predicate similarity despite low argument overlap (cf.</S>
			<S sid ="275" ssid = "97">Example (5)).</S>
			<S sid ="276" ssid = "98">(5) a. The US alert (.</S>
			<S sid ="277" ssid = "99">) followed intelligence reports that . . .</S>
			<S sid ="278" ssid = "100">b. The Foreign Ministry announcement called on Japanese citizens to be cautious . . .</S>
			<S sid ="279" ssid = "101">We observe that argument overlap itself can be low even for correct alignments.</S>
			<S sid ="280" ssid = "102">This clearly indicates that a better integration of context is needed.</S>
			<S sid ="281" ssid = "103">Example (6.a) illustrates a case in which the agent of a warning event is not realized.</S>
			<S sid ="282" ssid = "104">Here, contextual information is required to correctly align it to the first warning event in (6.b).</S>
			<S sid ="283" ssid = "105">This involves inference beyond the local PAS.</S>
			<S sid ="284" ssid = "106">(6) a. The US alert (.</S>
			<S sid ="285" ssid = "107">) is one step down from a full [travel]Arg1 w arning [ ]Arg0 . b. Japan has issued a travel alert . . .</S>
			<S sid ="286" ssid = "108">(which) follows similar warnings [from American and British authorities]Arg0 .</S>
			<S sid ="287" ssid = "109">) An official said it was highly unusual for [Tokyo]Arg0 to issue such a warning . . .</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="288" ssid = "1">We presented a novel task for predicate alignment across comparable monolingual texts, which we address using graph-based clustering with Mincuts.</S>
			<S sid ="289" ssid = "2">The motivation for this task is to acquire empirical data for studying discourse coherence factors related to argument structure realization.</S>
			<S sid ="290" ssid = "3">As a first step, we constructed a data set of comparable texts that provide full discourse contexts for alternative verbalizations of the same underlying events.</S>
			<S sid ="291" ssid = "4">The data set is derived from all newswire pairs found in the English Gigaword Fifth Edition and contains a total of more than 160,000 paired documents.</S>
			<S sid ="292" ssid = "5">A subset of these pairs forms an evaluation set, annotated with gold alignments that relate predications, which exhibit a (possibly partial) corresponding argument structure.</S>
			<S sid ="293" ssid = "6">We established that the annotation task, while difficult, can be performed with good inter-annotator agreement (κ at 0.86).</S>
			<S sid ="294" ssid = "7">Our main contribution is a novel clustering approach using Mincuts for aligning predications across comparable texts.</S>
			<S sid ="295" ssid = "8">Our experiments established that recursive clustering improves on greedy selection methods by profiting from global information encoded in the graph representation.</S>
			<S sid ="296" ssid = "9">While the Mincut-based method is in itself unsupervised, a small amount of development data is needed to tune parameters for the construction of particularly suitable input graphs.</S>
			<S sid ="297" ssid = "10">We tested our full model against two additional baselines: simple heuristic alignment based on identical lemma forms and a combination of techniques from SMT and paraphrase detection.</S>
			<S sid ="298" ssid = "11">The evaluation for our novel task was complemented by a traditional word alignment task using established paraphrase data sets.</S>
			<S sid ="299" ssid = "12">We determined clear differences in performance for all models for the two types of task settings.</S>
			<S sid ="300" ssid = "13">While word alignment methods from SMT outperform the competing models in the sentence- based alignment tasks, they perform poorly in the discourse setting.</S>
			<S sid ="301" ssid = "14">In future work, we will enhance our model by incorporating more refined similarity measures including discourse-based criteria.</S>
			<S sid ="302" ssid = "15">We will further explore tuning techniques, e.g., a more suitable pre- selection method for edges in graph construction, in order to increase either precision or recall.</S>
			<S sid ="303" ssid = "16">The decision of optimizing towards one measure or another is clearly task-dependent.</S>
			<S sid ="304" ssid = "17">In our case, high precision is favorable as we plan to learn accurate discourse model parameters from the computed alignments.</S>
			<S sid ="305" ssid = "18">Even though such an optimization will result in an overall lower recall, application of the alignment model on the entire GigaPairs corpus can still provide us with a large amount of precise predicate alignments.</S>
			<S sid ="306" ssid = "19">Using this set of alignments, we will then proceed to exploit contextual information in order to learn a semantic model for discourse coherence in argument structure realization.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="307" ssid = "20">We are grateful to the Landesgraduiertenfo¨ rderung BadenWu¨ rttemberg for funding within the research initiative “Coherence in language processing” at Heidelberg University.</S>
			<S sid ="308" ssid = "21">We thank Danny Rehl and Lukas Funk for annotation.</S>
	</SECTION>
</PAPER>
