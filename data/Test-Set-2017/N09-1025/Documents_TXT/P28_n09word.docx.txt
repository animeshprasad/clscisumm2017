Diversify and Combine: Improving Word Alignment for Machine
Translation on Low-Resource Languages



Bing Xiang, Yonggang Deng, and Bowen Zhou 
IBM T. J. Watson Research Center Yorktown 
Heights, NY 10598
{bxiang,ydeng,zhou}@us.ibm.com


Abstract

We  present   a  novel  method   to  improve 
word alignment quality and eventoally  the 
translation performance by producing  and 
combining complementary word align­ 
ments for low-resource languages. Instead 
of focusing on the improvement of a single 
set of word alignments, we generate  mul­ 
tiple  sets  of  diversified  alignments based 
on different  motivations, such  as linguis­ 
tic  knowledge,  morphology  and  heuris­ 
tics.  We demonstrate this approach  on an 
English-to-Pashto translation task by com­ 
bining  the alignments obtained  from syn­ 
tactic reordering, stemming,  and partial 
words.   The combined alignment  outper­ 
forms the baseline alignment, with signif­ 
icantly  higher  F-scores and better  transla­ 
tion performance.

1   Introduction

Word  alignment  usually   serves   as  the  starting 
point and foundation for a statistical machine 
translation (SMT) system. It has received a signif­ 
icant amount of research  over the years, notably in 
(Brown eta!., 1993; Ittycheriah and Roukos, 2005; 
Fraser and Marcu, 2007; Hermjakob, 2009). They 
all focused on the improvement of word alignment 
models.  In this work, we leverage  existing  align­ 
ers and generate  multiple  sets of word alignments 
based on complementary information, then com­ 
bine them to get the final alignment for phrase 
training.   The resource  required  for this approach 
is little, compared to what is needed to build a rea­ 
sonable discriminative alignment model,  for ex­ 
ample.    1bis makes  the  approach  especially  ap­
pealing for SMT on low-resource languages.
  

Most of the research  on alignment  combination 
in the past has focused on how to combine the 
alignments from  two different  directions,  source­ 
to-target and target-to-source. Usually people start 
from  the  intersection of  two  sets  of  alignments, 
and  gradually add links in  the  union  based  on 
certain heuristics,  as in  (Koehn et a!., 2003), to 
achieve a better  balance  compared to using either 
intersection {high precision) or union {high recall). 
In{Ayan and Dorr, 2006) a maximum entropy  ap­ 
proach  was  proposed  to combine  multiple  align­ 
ments based on a set of linguistic and alignment 
features.   A different  approach  was presented  in 
(Deng and Zhou, 2009), which again concentrated 
on the combination of two sets of alignments, but 
with a different  criterion.  It tries to maximize the 
number of phrases that can be extracted in the 
combined alignments.  A  greedy  search  method 
was utilized and it achieved higher translation per­ 
formance  than the baseline.
  More recently, an alignment selection  approach 
was  proposed   in    (Huang,   2009),   which   com­ 
putes  confidence  scores  for each  liuk and prones 
the links from multiple  sets  of alignments using a  
hand-picked threshold.     The  alignments used
in that work were generated  from different  align­ 
ers {HMM, block model, and maximum entropy 
model).    In this  work,  we  use  soft  voting  with 
weighted   confidence scores,  where  the  weights 
can  be  tuned  with  a specific  objective  function. 
There  is no need  for  a pre-determined threshold 
as used  in   (Huang,  2009).   Also,  we utilize  var­ 
ious knowledge sources  to eurich  the alignments 
instead of using different  aligners.  Our strategy  is 
to diversify and then combine in order to catch any 
complementary information captured  in the word 
alignments for low-resource languages.
The  rest  of the  paper  is organized  as follows.




22
Proceedings uf the ACL 2010 C01'!{erence Short Papers, pages 22-26,
Uppsala, Sweden, 11-16 July 2010. @2010  Association for Computational  Unguistics



We present three different sets of alignments  in 
Section 2 for an English-to-Pashto MT task.  In 
Section 3, we propose the alignment combination 
algorithm.  The experimental results are reported 
in Section 4. We conclude the paper in Section 5.

2    Diversified Word Alignments

We take an English-to-Pashto MT task as an exam­ 
ple and create three sets of additional alignments 
on top of the baseline alignment.

2.1   Syntactic Reordering
Pashto is a subject-object-verb  (SOV) language, 
which puts verbs after objects.  People have pro­ 
posed different syntaetic rules to pre-reorder SOY 
languages, either based on a constituent parse tree 
(Dn\bek and Yarowsky, 2004; Wang eta!., 2007) 
or dependency parse tree   (Xu eta!., 2009).   In 
this work, we apply syntactic reordering for verb 
phrases  (VP)  based  on  the  English  constituent 
parse.  The VP-based reordering rule we apply in 
the work is:

 

where VBuepresents VB, VBD,VBG, VBN, 
VBPandVBZ.
  In Figure  I, we show the reference alignment 
between an English sentence and the correspond­ 
ing Pashto translation, where E is the original En­ 
glish sentence, P is the Pashto sentence (in ro­ 
manized text), and E' is the English sentence after 
reordering. As we can see, after the VP-based re­ 
ordering, the alignment between the two sentences 
becomes monotone, which makes it easier for the 
aligner to get the alignment correct.  During the 
reordering of English sentences, we store the in­ 
dex changes for the English words. After getting 
the alignment trained on the reordered English and 
original Pashto sentence pairs, we map the English 
words back to the original order, along with the 
learned alignment links.   In this way, the align­ 
ment is ready to be combined with the baseline 
alignment and any other alternatives.

2.2   Stemming

Pashto is one of the morphologically rich lan­ 
guages. Inaddition to the linguistic koowledge ap­ 
plied in the syntaetic reordering described above, 
we also utilize morphological analysis by applying 
stemming on both the English and Pashto sides. 
For  English,  we  use  Porter  stemming    (Porter,


•













Figure  1:   Alignment  before/after  VP-based  re­
ordering.


1980), a widely applied algorithm to remove the 
common morphological and inflexional endings 
from words  in English.   For Pashto,  we utilize 
a morphological decompostion algorithm that has 
been shown to be effective for Arabic speech 
recognition  (Xiang et a!., 2006). We start from a 
fixed set of affixes with 8 prefixes and 21 suffixes. 
The  prefixes and  suffixes are  stripped  off from 
the Pashto words under the two constraints:(!) 
Longest matched affixes first; (2) Remaining stem 
must be at least two characters long.

2.3   Partial Word

For  low-resource  languages,  we  usually  suffer 
from the data sparsity issue.   Recently, a simple 
method was presented in  (Chiang eta!., 2009), 
which keeps partial English and Urdu words in the 
training data for alignment training. This is similar 
to the stenuning method, but is more heoristics­ 
based, and does not rely on a set of available af­ 
fixes. With the same motivation, we keep the first
4 characters of each English and Pashto word to 
generate one more alternative for the word align­ 
ment.

3    Confidence-Based Alignment
Combination

Now we describe the algorithm to combine mul­ 
tiple sets of word alignments based on weighted 
confidence scores.  Suppose  a;;k is an alignment 
link in the i-th set of alignments between the j -th 
source word and k-th target word in sentence pair 
(S,T). Similar to  (Huang, 2009), we define the 
confidence of a;;k as

c(a;;kiS,T) = Vq•2t(a;;kiS,T)qt2s(a;;kiT, 8),
(1)


where the source-to-target link posterior  probabil­
ity

 

and the target-to-source link posterior  probability 
qt2s(<>i;kiT, S)  is defined  similarly.    p;(tkis;) is 
the lexical  translation probability between  source 
word s; and target word tk in the i-th set of align­ 
ments.
Our alignment  combination algorithm  is as fol­
lows.

1. Each candidate link a;k gets soft votes from 
N sets of alignments via weighted  confidence 
scores:

N
v(a;kiS,T) = Lw; oc(a;;kiS,T),  (3)
i=l

where the weight w; for each set of alignment 
can  be optimized under  various  criteria.   In 
this  work,  we tune it on a hand-aligned de­ 
velopment set to maximize the alignment  F­ 
score.

2.  All candidates  are sorted by soft votes in de­ 
scending  order and evaluated sequentially. A 
candidate link  a;k is included  if one  of the 
following  is true:

• Neither  s; nor tk is aligned so far;
• s1 is  not  aligned  and  its  left  or  right 
neighboring word is aligned to tk so far;
• tk  is  not  aligned  and  its  left  or  right 
neighboring word is aligned to s1 so far.

3.  Repeat  scanning  all candidate links until no 
more links can be added.

  In this way, those  alignment  links  with  higher 
confidence  scores  have  higher  priority  to  be  in­ 
cluded in the combined alignment.

4   Experiments

4.1   Baseline
Our training data contains around 70K English­ 
Pashto sentence pairs released under the DARPA 
TRANSTAC project,  with  about  900K  words  on 
the English  side.   The baseline  is a phrase-based 
MT system  similar  to   (Koehn  et a!., 2003).   We 
use  GIZA++ (Och  and  Ney,  2000)  to  generate 
the baseline  alignment  for each direction  and then


apply grow-diagonal-final (gdf).  The decoding 
weights  are optimized with minimum error rate 
training  (MERT) (Och, 2003) to maximize BLEU 
scores (Papineni eta!., 2002).  There are 2028 sen­ 
tences in the toning set and  I019 sentences  in the 
test set, both with one reference.  We use another
!50 sentence  pairs  as a heldout  hand-aligned set 
to measure  the word alignment  quality.  The three 
sets of alignments described in Section  2 are gen­ 
erated  on the  same  training  data  separately with 
GIZA++ and enhanced  by gdf as for the baseline 
alignment.  The English parse tree used for the 
syntactic  reordering  was produced by a maximum 
entropy based parser (Ratnaparkhi, 1997).

4.2   Improvement in Word Alignment

In Table  I  we  show  the  precision,  recall  and F­ 
score of each set of word alignments for the !50- 
sentence set. Using partial word provides the high­ 
est F-score  among  all individual  alignments. The 
F-score  is 5% higher  than for the baseline  align­ 
ment.  The VP-based reordering  itself does not im­ 
prove the F-score, which could be due to the parse 
errors on the conversational training  data.  We ex­ 
periment  with three options {co, c1. c2) when com­ 
bining  the  baseline   and  reordering-based align­ 
ments. Inco, the weights w; and confidence scores 
c(a;;kiS,T) in  Eq.   (3)  are  all  set  to 1.   In c1, 
we  set  confidence  scores  to  I, while  toning  the 
weights   with  hill climbing   to  maximize the  F­ 
score on a hand-aligned toning set. In c2 , we com­ 
pute the confidence  scores  as in Eq. {I) and tune 
the weights as in c1. The numbers  in Table I show 
the effectiveness of having both weights  and con­ 
fidence scores during the combination.
  Similarly,  we  combine  the baseline  with  each 
of  the  other  sets  of  alignments using  c2.   They 
all  result  in  significantly  higher  F-scores.    We 
also generate  alignments on VP-reordered partial 
words  (X  in Table  I) and compared B +X  and
B + V + P. The better  results  with B + V + P
show  the benefit of keeping  the alignments as di­ 
versified  as possible  before  the combination. Fi­ 
nally, we compare the proposed  alignment combi­ 
nation  c2 with  the heoristics-based method  (gdj), 
where the latter starts from the intersection of all 4 
sets of alignments and then applies grow-diagonal­ 
final  (Koehn  et a!.,  2003)  based  on  the  links  in 
the union. The proposed combination approach  on
B + V + S + P results in close to 7% higher F­
scores  than the baseline  and also 2% higher  than


gdf	We  also  notice  that  its  higher  F-score   is 	I Alignment I Comb I Links  I Phrase I BLEU I
mainly  due to the higher precision, which  should 	Baseline 	963K 	565K 	12.67 
result from the consideration of confidence scores. 	v 	965K 	624K 	12.82


I Alignment I Comb I 	p 	I	R 	I	F 	I


s 	915K 	692K 	13.04
p	906K 	716K 	13.30


Baseline                          0.6923     0.6414     0.6659        X                                      911K        689K        13.00 
v                       0.6934     0.6388     0.6650        B+V                    co         870K        890K        13.20 
s                       0.7376     0.6495     0.6907        B+V                    Cl    865K        899K        13.32
p                        0.7665     0.6643     0.7118        B+V                    C2    874K        879K        13.60
X                                     0.7615     0.6641     0.7095        B+S                    C2    864K        948K        13.41
B+V                    co         0.7639     0.6312     0.6913        B+P                    C2    863K        942K        13.40
B+V                    Cl   0.7645     0.6373     0.6951        B+X                    C2    871K        905K        13.37
B+V                    C2          0.7895     0.6505     0.7133        B+V+P               C2    880K        914K        13.60
B+S                    C2   0.7942     0.6553     0.7181        B+V+S+P        cat      3749K      1258K       13.01
B+P                    C2          0.8006     0.6612     0.7242        B+V+S+P        gdf       1021K      653K        13.14
B+X                    C2   0.7827     0.6670     0.7202        B+V+S+P         C2    907K        771K        13.73
B+V+P               C2   0.7912     0.6755     0.7288
B+V+S+P       gdf      0.7238     0.7042     0.7138      Table 2: Improvement in BLEU  scores (B: base-
B+V+S+P	C2	0.7906 	0.6852 	0.7342 	line; V: VP-hased reordering; S: stemming;  P: par- 
tial word; X: VP-reordered partial word).


Table  I: Alignment precision,  recall  and F-score
(B: baseline;  V: VP-based  reordering; S: stem­ 
ming; P: partial word; X: VP-reordered partial 
word).


4.3    Improvement in MT Performance
In Table  2,  we  show  the  corresponding BLEU 
scores on the test set for the systems built on each 
set of word  alignment  in Table I. Similar  to the
observation  from  Table I, c2  outperforms co and
C1> and  B + V + S + P with  c2   outperforms
B + V + S + P with  gdf   We also ran one ex­
perinlent  in which  we  concatenated all 4 sets  of
alignments into one big set (shown  as cat). Over­ 
all, the BLEU  score  with confidence-based com­ 
bination  was increased  by Ipoint compared to the 
baseline,  0.6 compared to gdf, and 0.7 compared 
to cat. All results  are statistically significant  with
p < 0.05 using the sign-test  described  in (Collins
eta!., 2005).

5   Conclusions

In this work, we have presented a word alignment 
combination method  that improves both the align­ 
ment quality and the translation performance. We 
generated multiple sets of diversified alignments 
based   on  linguistics,  morphology,  and  heuris­ 
tics,  and  denmnstrated the  effectiveness of  com­ 
bination  on the English-to-Pashto translation task. 
We showed  that  the  combined alignment signif­ 
icantly  outperforms the  baseline  alignment with




both higher F-score and higher BLEU score. The 
combination approach  itself  is not limited  to 
any specific  alignment.  It provides  a general  
franle­ work  that  can  take advantage  of  as  
many  align­ ments as possible, which could 
differ in prepro­ cessing, alignment modeling,  or 
any other aspect.

Acknowledgmen
ts

This  work  was  supported  by  the  DARPA 
TRANSTAC program.    We would  like  to 
thank Upendra  Chaudhari, Sameer  Maskey  
and  Xiao­ qiang Luo for providing useful  
resources and the anonymous reviewers  for 
their  constructive  com­ ments.


Referenc
es

Necip Fazil Ayan and Bonnie J. Dorr. 2006.  A 
max­ imum entropy approach to combining 
word align­ ments. InProc. HLTINAACL, June.

Peter Brown, Vmcent Della Pietra, Stephen Della 
Pietra, and Robert Mercer. 1993. The 
mathen1atics of statistical machine translation: 
paran1eter estima­ tion. 
Computationa/Linguistics, 19(2):263-311.

David Chiang, Kevin Knight, San1ad Echihabi, et 
al.
2009.  Isi/langusge weaver nist 2009 systems.  In
Presentation at NIST MT 2009 Workshop, August.

Michael Collins, Philipp Koehu, and Ivona 
Kueerova.
2005.   Clause restructuriug for statistical 
machine translation. InProc. of ACL, pages 531-
540.


Yonggang Deng and Bowen Zhou.  2009.  Optimizing 
word alignment combination  for phrase table train­ 
ing. In Proc. ACL, pages 229-232, August

Elliott Franco Dn\bek and David Yarowsky. 2004. Im­ 
proving bitext word alignments via syntax-based re­ 
ordering of english. In Proc. ACL.

Alexander Fraser and Daniel Marcu. 2007. Getting the 
structure right for word alignment: Leaf. InProc. of 
EMNLP, pages 51-{)0,June.

UlfHermjakob. 2009.  Improved word alignment with 
statistics and linguistic heuristics. In Proc. EMNLP, 
pages 229-237, August

Fei Huang. 2009. Confidence measure for word aligu­
ment In Proc. ACL, pages 932-940, August

AhraharnittycheriahandSalimRoukos. 2005. A max­ 
imum entropy word aliguer for arabic-english ma­ 
chine translation.   In Proc. of HLTIEMNLP, pages
89-96, October.

Philipp Koehn, Franz Och, and Daniel Marcu.  2003.
Statistical   phrase-based   translation. 	In Proc.
NAACVHLT.

Franz Josef Och and Hermann Ney. 2000.  Improved 
statistical alignment models. InProc. of ACL, pages
440-447, Hong Kong, China, October.

Franz Josef Och.  2003.  Minimum  error rate training 
in statistical machine translation.  In Proc. of ACL, 
pages 160-167.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei­ 
jing Zhu. 2002. Bleu: a method for automatic evalu­ 
ation of machine translation. InProc. of ACL, pages
311-318.

Martin Porter. 1980. An algorithm for suffix stripping.
InProgram, volume 14, pages 130-137.

Adwait Ratnaparkhi. 1997.  A linear observed time sta­ 
tistical parser based on maximum entropy models. 
lnProc. ofEMNLP, pages 1-10.

Chao  Wang,  Michael  Collins,   and  Philipp  Koehn.
2007.    Chinese  syntactic  reordering  for  statistical
machine translation.  In Proc. EMNLP, pages 737-
745.

Bing Xiang, Kham Nguyen, Long Nguyen, Richard 
Schwartz, and John Makhoul. 2006. Morphological 
decomposition for arabic broadcast news transcrip­ 
tion. In Proc. ICASSP.

Peng Xu, Jaeho Kang, Michael Ringgaard,  and Franz 
Och.  2009.  Using a dependency parser to improve 
smt  for  subject-object-verb languages.     In Proc. 
NAACVHLT, pages 245-253, June.

