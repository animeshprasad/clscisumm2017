<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a new translation model that include undecorated hierarchical-style phrase rules, decorated source-syntax rules, and partially decorated rules.</S>
		<S sid ="2" ssid = "2">Results show an increase in translation performance of up to 0.8% BLEU for German–English translation when trained on the news-commentary corpus, using syntactic annotation from a source language parser.</S>
		<S sid ="3" ssid = "3">We also experimented with annotation from shallow taggers and found this increased performance by 0.5% BLEU.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Hierarchical decoding is usually described as a formally syntactic model without linguistic commitments, in contrast with syntactic decoding which constrains rules and production with linguistically motivated labels.</S>
			<S sid ="5" ssid = "5">However, the decoding mechanism for both hierarchical and syntactic systems are identical and the rule extraction are similar.</S>
			<S sid ="6" ssid = "6">Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field.</S>
			<S sid ="7" ssid = "7">Both use synchronous context free grammar (SCFG) formalism, consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence.</S>
			<S sid ="8" ssid = "8">The most common algorithm for decoding with SCFG is currently CKY+ with cube pruning works for both hierarchical and syntactic systems, as implemented in Hiero (Chiang, 2005), Joshua (Li et al., 2009), and Moses (Hoang et al., 2009) Rewrite rules in hierarchical systems have general applicability as their non-terminals are undec- orated, giving hierarchical system broad coverage.</S>
			<S sid ="9" ssid = "9">However, rules may be used in inappropriate situations without the labeled constraints.</S>
			<S sid ="10" ssid = "10">The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses.</S>
			<S sid ="11" ssid = "11">Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of limiting model coverage.</S>
			<S sid ="12" ssid = "12">This paper presents work on combining hierarchical and syntax translation, utilizing the high coverage of hierarchical decoding and the insights that syntactic information can bring.</S>
			<S sid ="13" ssid = "13">We seek to balance the generality of using undeco- rated non-terminals with the specificity of labeled non-terminals.</S>
			<S sid ="14" ssid = "14">Specifically, we will use syntactic labels from a source language parser to label non-terminal in production rules.</S>
			<S sid ="15" ssid = "15">However, other source span information, such as chunk tags, can also be used.</S>
			<S sid ="16" ssid = "16">We investigate two methods for combining the hierarchical and syntactic approach.</S>
			<S sid ="17" ssid = "17">In the first method, syntactic translation rules are used concurrently with a hierarchical phrase rules.</S>
			<S sid ="18" ssid = "18">Each ruleset is trained independently and used concurrently to decode sentences.</S>
			<S sid ="19" ssid = "19">However, results for this method do not improve.</S>
			<S sid ="20" ssid = "20">The second method uses one translation model containing both hierarchical and syntactic rules.</S>
			<S sid ="21" ssid = "21">Moreover, an individual rule can contain both decorated syntactic non-terminals, and undeco- rated hierarchical-style non-terminals (also, the left-hand-side non-terminal may, or may not be decorated).</S>
			<S sid ="22" ssid = "22">This results in a 0.8% improvement over the hierarchical baseline and analysis suggest that long-range ordering has been improved.</S>
			<S sid ="23" ssid = "23">We then applied the same methods but using linguistic annotation from a chunk tagger (Abney, 1991) instead of a parser and obtained an improvement of 0.5% BLEU over the hierarchical baseline, showing that gains with additional source- side annotation can be obtained with simpler tools.</S>
	</SECTION>
	<SECTION title="Past Work. " number = "2">
			<S sid ="24" ssid = "1">Hierarchical machine translation (Chiang, 2005) extends the phrase-based model by allowing the use of non-contiguous phrase pairs (’production rules’).</S>
			<S sid ="25" ssid = "2">It promises better reordering of translation as the reordering rules are an implicit part of the translation model.</S>
			<S sid ="26" ssid = "3">Also, hierarchical rules follow the recursive structure of the sentence, reflecting the linguistic notion of language.</S>
			<S sid ="27" ssid = "4">However, the hierarchical model has several limitations.</S>
			<S sid ="28" ssid = "5">The model makes no use of linguistic information, thus creating a simple model with broad coverage.</S>
			<S sid ="29" ssid = "6">However, (Chiang, 2005) also describe heuristic constraints that are used during 409 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 409–417, Uppsala, Sweden, 1516 July 2010.</S>
			<S sid ="30" ssid = "7">Qc 2010 Association for Computational Linguistics rule extraction to reduce spurious ambiguity.</S>
			<S sid ="31" ssid = "8">The resulting translation model does reduces spurious ambiguity but also reduces the search space in an arbitrary manner which adversely affects translation quality.</S>
			<S sid ="32" ssid = "9">Syntactic labels from parse trees can be used to annotate non-terminals in the translation model.</S>
			<S sid ="33" ssid = "10">This reduces incorrect rule application by restricting rule extraction and application.</S>
			<S sid ="34" ssid = "11">However, as noted in (Ambati and Lavie, 2008) and elsewhere,the na¨ıve approach of constraining every non-terminal to a syntactic constituent severely limits the coverage of the resulting grammar, therefore, several approaches have been used to improve coverage when using syntactic information.</S>
			<S sid ="35" ssid = "12">Zollmann and Venugopal (2006) allow rules to be extracted where non-terminals do not exactly span a target constituent.</S>
			<S sid ="36" ssid = "13">The non-terminals are then labeled with complex labels which amalgamates multiple labels in the span.</S>
			<S sid ="37" ssid = "14">This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically.</S>
			<S sid ="38" ssid = "15">Huang and Chiang (2008) use parse information of the source language, production rules consists of source tree fragments and target languages strings.</S>
			<S sid ="39" ssid = "16">During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed forest.</S>
			<S sid ="40" ssid = "17">Liu et al.</S>
			<S sid ="41" ssid = "18">(2009) uses joint decoding with a hierarchical and tree-to-string model and find that translation performance increase for a ChineseEnglish task.</S>
			<S sid ="42" ssid = "19">Galley et al.</S>
			<S sid ="43" ssid = "20">(2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or coverage in any SMT system.</S>
			<S sid ="44" ssid = "21">This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words.</S>
			<S sid ="45" ssid = "22">These measures are essential for creating good SMT systems, but again, the rules syntax are strictly constrained by a parser.</S>
			<S sid ="46" ssid = "23">Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions.</S>
			<S sid ="47" ssid = "24">Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence.</S>
			<S sid ="48" ssid = "25">This follows on from earlier work in (Chiang, 2005) but they see gains when finer grain feature functions which different constituency types.</S>
			<S sid ="49" ssid = "26">The weights for feature function is tuned in batches due to the deficiency of MERT when presented with many features.</S>
			<S sid ="50" ssid = "27">Chiang et al.</S>
			<S sid ="51" ssid = "28">(2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination.</S>
			<S sid ="52" ssid = "29">However, the translation model continues to be hierarchical.</S>
			<S sid ="53" ssid = "30">Chiang et al.</S>
			<S sid ="54" ssid = "31">(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.</S>
			<S sid ="55" ssid = "32">The translation model remain constant but the parameterization changes.</S>
			<S sid ="56" ssid = "33">Shen et al.</S>
			<S sid ="57" ssid = "34">(2009) discusses soft syntax constraints and context features in a dependency tree translation model.</S>
			<S sid ="58" ssid = "35">The POS tag of the target head word is used as a soft constraint when applying rules.</S>
			<S sid ="59" ssid = "36">Also, a source context language model and a dependency language model are also used as features.</S>
			<S sid ="60" ssid = "37">Most SMT systems uses the Viterbi approximation whereby the derivations in the log-linear model is not marginalized, but the maximum derivation is returned.</S>
			<S sid ="61" ssid = "38">String-to-tree models build on this so that the most probable derivation, including syntactic labels, is assumed to the most probable translation.</S>
			<S sid ="62" ssid = "39">This fragments the derivation probability and the further partition the search space, leading to pruning errors.</S>
			<S sid ="63" ssid = "40">Venugopal et al.</S>
			<S sid ="64" ssid = "41">(2009) attempts to address this by efficiently estimating the score over an equivalent unlabeled derivation from a target syntax model.</S>
			<S sid ="65" ssid = "42">Ambati and Lavie (2008); Ambati et al.</S>
			<S sid ="66" ssid = "43">(2009) notes that tree-to-tree often underperform models with parse tree only on one side due to the non- isomorphic structure of languages.</S>
			<S sid ="67" ssid = "44">This motivates the creation of an isomorphic backbone into the target parse tree, while leaving the source parse unchanged.</S>
	</SECTION>
	<SECTION title="Model. " number = "3">
			<S sid ="68" ssid = "1">In extending the phrase-based model to the hierarchical model, non-terminals are used in translation rules to denote subphrases.</S>
			<S sid ="69" ssid = "2">Hierarchical non- terminals are undecorated so are unrestricted to the span they cover.</S>
			<S sid ="70" ssid = "3">In contrast, SCFG-based syntactic models restrict the extraction and application of non-terminals, typically to constituency spans of a parse tree or forest.</S>
			<S sid ="71" ssid = "4">Our soft syntax model combine the hierarchical and source-syntactic approaches, allowing translation rules with undeco- rated and decorated non-terminals with information from a source language tool.</S>
			<S sid ="72" ssid = "5">We give an example of the rules extracted from an aligned sentence in Figure 1, with a parse tree on the source side.</S>
			<S sid ="73" ssid = "6">Lexicalized rules with decorated non-terminals are extracted, we list five (non-exhaustive) examples below.</S>
			<S sid ="74" ssid = "7">Figure 1: Aligned parsed sentence N P → M usharraf s letzter Akt # M usharraf s Last Act N P → N E1 letzter Akt # X1 Last Act N P → N E1 ADJ A2 Akt # X1 X2 Act N P → N E1 letzter N N2 # X1 Last X2 T OP → N E1 ADJ A2 Akt ? # X1 X2 Act ? Hierarchical style rules are also extracted where the span doesn’t exactly match a parse constituent.</S>
			<S sid ="75" ssid = "8">We list 2 below.</S>
			<S sid ="76" ssid = "9">X → letzter Akt # Last Act X → letzter X1 # Last X1 Unlexicalized rules with decorated non- terminals are also extracted: T OP → N P1 P U N C2 # X1 X2 N P → N E1 ADJ A2 N N3 # X1 X2 X3 sentence is available to the decoder.</S>
			<S sid ="77" ssid = "10">Decorated non-terminals in rules must match the constituent span in the input sentence but the undecorated X symbol can match any span.</S>
			<S sid ="78" ssid = "11">Formally, we model translation as a string- to-string translation using a synchronous CFG that constrain the application of non-terminals to matching source span labels.</S>
			<S sid ="79" ssid = "12">The source words and span labels are represented as an unweighted word lattice, &lt; V, E &gt;, where each edge in the lattice correspond to a word or non-terminal label over the corresponding source span.</S>
			<S sid ="80" ssid = "13">In the soft syntax experiments, edges with the default source label, X , are also created for all spans.</S>
			<S sid ="81" ssid = "14">Nodes in the lattice represent word positions in the sentence.</S>
			<S sid ="82" ssid = "15">We encode the lattice in a chart, as described in (Dyer et al., 2008).</S>
			<S sid ="83" ssid = "16">A chart is is a tuple of 2- dimensional matrices &lt; F, R &gt;.</S>
			<S sid ="84" ssid = "17">Fi,j is the word or non-terminal label of the jth transition starting word position i. Ri,j is the end word position of the node on the right of the jth transition leaving word position i. The input sentence is decoded with a set of translation rules of the form X →&lt; αLs, γ, ∼&gt; where α and γ and strings of terminals and non- terminals.</S>
			<S sid ="85" ssid = "18">Ls and the string α are drawn from the same source alphabet, ∆s. γ is the target string, also consisting of terminals and non-terminals.</S>
			<S sid ="86" ssid = "19">∼is the one-to-one correspondence between non terminals in α and γ.</S>
			<S sid ="87" ssid = "20">Ls is the left-hand-side of the source.</S>
			<S sid ="88" ssid = "21">As a string-to-string model, the left- hand-side of the target is always the default target non-terminal label, X . Decoding follows the CKY+ algorithms which process contiguous spans of the source sentence bottom up.</S>
			<S sid ="89" ssid = "22">We describe the algorithm as inference rules, below, omitting the target side for brevity.</S>
			<S sid ="90" ssid = "23">Initialization Rules are also extracted which contains a mixture of decorated and undecorated non-terminals.</S>
			<S sid ="91" ssid = "24">These rules can also be lexicalized or unlexicalized.</S>
			<S sid ="92" ssid = "25">A non-exhaustive sample is given below: [X → •αLs, i, i] Terminal Symbol (X → αLs) ∈ G X → ADJ A1 Akt # X1 Act N P → N E1 X2 # X1 X2 T OP → N E1 letzter X2 # X1 Last X2 [X → α • Fj,k βLs, i, j] [X → αFj,k • βLs, i, j + 1] Non-Terminal Symbol [X → α • Fj,k βLs, i, j] [X, j, Rj,k ] [X → αFj,k • βLs, i, Rj,k ] [X → α • Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X → αLs•, i, Ri,j ] log p(t|s) = λm hm(t, s) (3) m Goal [X → αLs•, 0, |V | − 1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.</S>
			<S sid ="93" ssid = "26">The presence of default label edges between every node allows undecorated non-terminals to be applied to any span, allowing flexibility in the translation model.</S>
			<S sid ="94" ssid = "27">This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation.</S>
			<S sid ="95" ssid = "28">Rather than creating ad-hoc schemes to categories non-terminals with syntactic labels when they do not span syntactic constituencies, we only use labels that are presented by the parser or shallow tagger.</S>
			<S sid ="96" ssid = "29">Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al., 2008), but we build ambiguity into the translation rule.</S>
			<S sid ="97" ssid = "30">The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.</S>
			<S sid ="98" ssid = "31">The soft constraint in our model pertain not to a additional feature functions based on syntactic information, but to the availability of syntactic and non- syntactic informed rules.</S>
	</SECTION>
	<SECTION title="Parameterization. " number = "4">
			<S sid ="99" ssid = "1">In common with most current SMT systems, the decoding goal of finding the most probable target language sentence ˆt, given a source language sentence s ˆt = argmaxt p(t|s) (1) The argmax function defines the search objective of the decoder.</S>
			<S sid ="100" ssid = "2">We estimate p(t|s) by decom posing it into component models therefore, the tuning algorithm does not need to be replaced; we continue to use MERT (Och, 2003).</S>
	</SECTION>
	<SECTION title="Rule Extraction. " number = "5">
			<S sid ="101" ssid = "1">Rule extraction follows the algorithm described in (Chiang, 2005).</S>
			<S sid ="102" ssid = "2">We note the heuristics used for hierarchical phrases extraction include the following constraints: 1.</S>
			<S sid ="103" ssid = "3">all rules must be at least partially lexicalized, 2.</S>
			<S sid ="104" ssid = "4">non-terminals cannot be consecutive, 3.</S>
			<S sid ="105" ssid = "5">a maximum of two non-terminals per rule, 4.</S>
			<S sid ="106" ssid = "6">maximum source and target span width of 10 word 5.</S>
			<S sid ="107" ssid = "7">maximum of 5 source symbols In the source syntax model, non-terminals are restricted to source spans that are syntactic phrases which severely limits the rules that can be extracted or applied during decoding.</S>
			<S sid ="108" ssid = "8">Therefore, we can adapt the heuristics, dropping some of the constraints, without introducing too much complexity.</S>
			<S sid ="109" ssid = "9">1.</S>
			<S sid ="110" ssid = "10">consecutive non-terminals are allowed 2.</S>
			<S sid ="111" ssid = "11">a maximum of three non-terminals, 3.</S>
			<S sid ="112" ssid = "12">all non-terminals and LHS must span a parse constituent In the soft syntax model, we relax the constraint of requiring all non-terminals to span parse constituents.</S>
			<S sid ="113" ssid = "13">Where there is no constituency spans, the default symbol X is used to denote an undeco- rated non-terminal.</S>
			<S sid ="114" ssid = "14">This gives rise to rules which mixes decorated and undecorated non-terminals.</S>
			<S sid ="115" ssid = "15">To maintain decoding speed and minimize spurious ambiguity, item (1) in the syntactic extraction heuristics is adapted to prohibit consecutive undecorated non-terminals.</S>
			<S sid ="116" ssid = "16">This combines the p(t s) = 1 Z m hm(t, s)λm (2) strength of syntactic rules but also gives the translation model more flexibility and higher coverage from having undecorated non terminals.</S>
			<S sid ="117" ssid = "17">Therefore, the heuristics become:where hm(t, s) is the feature function for compo nent m and λm is the weight given to component m. Z is a normalization factor which is ignored in practice.</S>
			<S sid ="118" ssid = "18">Components are translation model scoring functions, language model, and other features.</S>
			<S sid ="119" ssid = "19">The problem is typically presented in log-space, which simplifies computations, but otherwise does 1.</S>
			<S sid ="120" ssid = "20">consecutive non-terminals are allowed, but consecutive undecorated non-terminals are prohibited 2.</S>
			<S sid ="121" ssid = "21">a maximum of three non-terminals, 3.</S>
			<S sid ="122" ssid = "22">all non-terminals and LHS must span a parse constituent 5.1 Rule probabilities.</S>
			<S sid ="123" ssid = "23">Maximum likelihood phrase probabilities, p(¯t|¯s), are calculated for phrase pairs, using fractional counts as described in (Chiang, 2005).</S>
			<S sid ="124" ssid = "24">The maximum likelihood estimates are smoothed using Good-Turing discounting (Foster et al., 2006).</S>
			<S sid ="125" ssid = "25">A phrase count feature function is also create for each translation model, however, the lexical and backward probabilities are not used.</S>
	</SECTION>
	<SECTION title="Decoding. " number = "6">
			<S sid ="126" ssid = "1">We use the Moses implementation of the SCFG- based approach (Hoang et al., 2009) which support hierarchical and syntactic training and decoding used in this paper.</S>
			<S sid ="127" ssid = "2">The decoder implements the CKY+ algorithm with cube pruning, as well as histogram and beam pruning, all pruning parameters were identical for all experiments for fairer comparison.</S>
			<S sid ="128" ssid = "3">All non-terminals can cover a maximum of 7 source words, similar to the maximum rule span feature other hierarchical decoders to speed up decoding time.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "7">
			<S sid ="129" ssid = "1">We trained on the New Commentary 2009 corpus1, tuning on a holdout set.</S>
			<S sid ="130" ssid = "2">Table 1 gives more details on the corpus.</S>
			<S sid ="131" ssid = "3">nc test2007 was used for testing.</S>
			<S sid ="132" ssid = "4">German English Train Sentences Words 82,306 2,034,373 1,965,325 Tune Sentences 2000 Test Sentences 1026 Table 1: Training, tuning, and test conditions The training corpus was cleaned and filtered using standard methods found in the Moses toolkit (Koehn et al., 2007) and aligned using GIZA++ (Och and Ney, 2003).</S>
			<S sid ="133" ssid = "5">Standard MERT weight tuning was used throughout.</S>
			<S sid ="134" ssid = "6">The English half of the training data was also used to create a trigram language model which was used for each experiment.</S>
			<S sid ="135" ssid = "7">All experiments use truecase data and results are reported in case-sensitive BLEU scores (Papineni et al., 2001).</S>
			<S sid ="136" ssid = "8">The German side was parsed with the Bitpar parser2.</S>
			<S sid ="137" ssid = "9">2042 sentences in the training corpus failed to parse and were discarded from the training for both hierarchical and syntactic models to 1 http://www.statmt.org/wmt09/ # Model % BLEU Using parse tree 1 2 3 4 Hierarchical Syntax rules Joint hier.</S>
			<S sid ="138" ssid = "10">+ syntax rules Soft syntax rules 15.9 14.9 16.1 16.7 Using chunk tags 5 6 Hierarchical Soft syntax 16.3 16.8 Table 2: German–English results for hierarchical and syntactic models, in %BLEU ensure that train on identical amounts of data.</S>
			<S sid ="139" ssid = "11">Similarly, 991 out of 1026 sentences were parsable in the test set.</S>
			<S sid ="140" ssid = "12">To compare like-for-like, the baseline translates the same 991 sentences, but evaluated over 1026 sentences.</S>
			<S sid ="141" ssid = "13">(In the experiments with chunk tags below, all 1026 sentences are used).</S>
			<S sid ="142" ssid = "14">We use as a baseline the vanilla hierarchical model which obtained a BLEU score of 15.9% (see Table 2, line 1).</S>
			<S sid ="143" ssid = "15">7.1 Syntactic translation.</S>
			<S sid ="144" ssid = "16">Using the na¨ıve translation model constrained with syntactic non-terminals significantly decreases translation quality, Table 2, line 2.</S>
			<S sid ="145" ssid = "17">We then ran hierarchical concurrently with the syntactic models, line 3, but see little improvement over the hierarchical baseline.</S>
			<S sid ="146" ssid = "18">However, we see a gain of 0.8% BLEU when using the soft syntax model.</S>
			<S sid ="147" ssid = "19">7.2 Reachability.</S>
			<S sid ="148" ssid = "20">The increased performance using the soft syntax model can be partially explained by studying the effect of changes to the extraction and decoding algorithms has to the capacity of the translation pipeline.</S>
			<S sid ="149" ssid = "21">We run some analysis in which we trained the phrase models with a corpus of one sentence and attempt to decode the same sentence.</S>
			<S sid ="150" ssid = "22">Pruning and recombination were disabled during decoding to negate the effect of language model context and model scores.</S>
			<S sid ="151" ssid = "23">The first thousand sentences of the training corpus was analyzed, Table 3.</S>
			<S sid ="152" ssid = "24">The hierarchical model successfully decode over half of the sentences while a translation model constrained by a source syntax parse tree manages only 113 sentences, illustrating the severe degradation in coverage when a naive syntax model is used.</S>
			<S sid ="153" ssid = "25">Decoding with a hierarchical and syntax model jointly (line 3) only decode one extra sentence over the hierarchical model, suggesting that the 2 http://www.ims.unistuttgart.de/tcl/SOFTWARE/BitPar.html expressive power of the hierarchical model almost Table 3: Reachability of 1000 training sentences: can they be translated with the model?</S>
			<S sid ="154" ssid = "26">Figure 2: Source span lengths completely subsumes that of the syntactic model.</S>
			<S sid ="155" ssid = "27">The MERT tuning adjust the weights so that the syntactic model is very rarely applied during joint decoding, suggesting that the tuning stage prefers the broader coverage of the hierarchical model over the precision of the syntactic model.</S>
			<S sid ="156" ssid = "28">However, the soft syntax model slightly increases the reachability of the target sentences, lines 4.</S>
			<S sid ="157" ssid = "29">7.3 Rule Span Width.</S>
			<S sid ="158" ssid = "30">The soft syntactic model contains rules with three non-terminals, as opposed to 2 in the hierarchical model, and consecutive non-terminals in the hope that the rules will have the context and linguistic information to apply over longer spans.</S>
			<S sid ="159" ssid = "31">Therefore, it is surprising that when decoding with a soft syntactic grammar, significantly more words are translated singularly and the use of long spanning rules is reduced, Figure 2.</S>
			<S sid ="160" ssid = "32">However, looking at the usage of the glue rules paints a different picture.</S>
			<S sid ="161" ssid = "33">There is significantly less usage of the glue rules when decoding with the soft syntax model, Figure 3.</S>
			<S sid ="162" ssid = "34">The use of the glue rule indicates a failure of the translation model to explain the translation so the decrease in its usage is evidence of the better explanatory power of the soft syntactic model.</S>
			<S sid ="163" ssid = "35">An example of an input sentence, and the best translation found by the hierarchical and soft syntax model can be seen in Table 4.</S>
			<S sid ="164" ssid = "36">Figure 4 is the Figure 3: Length and count of glue rules used decoding test set Figure 4: Example input parse tree parse tree given to the soft syntax model.</S>
			<S sid ="165" ssid = "37">Input laut Ja´nos Veres wa¨re dies im ersten Quartal 2008 mo¨ glich . Hierarchical output according to Ja´nos Veres this in the first quarter of 2008 would be possible . Soft Syntax according to Ja´nos Veres this would be possible in the first quarter of 2008 . Table 4: Example input and best output found Both output are lexically identical but the output of the hierarchical model needs to be reordered to be grammatically correct.</S>
			<S sid ="166" ssid = "38">Contrast the derivations produced by the hierarchical grammar, Figure 5, with that produced with the soft syntax model, Figure 6.</S>
			<S sid ="167" ssid = "39">The soft syntax derivation makes use of several non-lexicalized to dictate word order, shown below.</S>
			<S sid ="168" ssid = "40">Figure 5: Derivation with Hierarchical model Figure 6: Derivation with soft syntax model The soft syntax derivation include several rules which are partially decorated.</S>
			<S sid ="169" ssid = "41">Crucially, the last rule in the list above reorders the PP phrase and the non-syntactic phrase X to generate the grammatically correct output.</S>
			<S sid ="170" ssid = "42">The other non- lexicalized rules monotonically concatenate the output.</S>
			<S sid ="171" ssid = "43">This can be performed by the glue rule, but nevertheless, the use of empirically backed rules allows the decoder to better compare hypotheses.</S>
			<S sid ="172" ssid = "44">The derivation also rely less on the glue rules than the hierarchical model (shown in solid rectangles).</S>
			<S sid ="173" ssid = "45">Reducing the maximum number of non- terminals per rule reduces translation quality but increasing it has little effect on the soft syntax model, Table 5.</S>
			<S sid ="174" ssid = "46">This seems to indicate that non- terminals are useful as context when applying rules up to a certain extent.</S>
			<S sid ="175" ssid = "47">7.4 English to German.</S>
			<S sid ="176" ssid = "48">We experimented with the reverse language direction to see if the soft syntax model still increased # non-terms % BLEU 2 16.5 3 16.8 5 16.8 Table 5: Effect on %BLEU of varying number of non-terminals # Model % BLEU 1 2 Hierarchical Soft syntax 10.2 10.6 Table 6: English–German results in %BLEU translation quality.</S>
			<S sid ="177" ssid = "49">The results were positive but less pronounced, Table 6.</S>
			<S sid ="178" ssid = "50">7.5 Using Chunk Tags.</S>
			<S sid ="179" ssid = "51">Parse trees of the source language provide useful information that we have exploited to create a better translation model.</S>
			<S sid ="180" ssid = "52">However, parsers are an expensive resource as they frequently need manually annotated training treebanks.</S>
			<S sid ="181" ssid = "53">Parse accuracy is also problematic and particularly brittle when given sentences not in the same domain as the training corpus.</S>
			<S sid ="182" ssid = "54">This also causes some sentences to be unparseable.</S>
			<S sid ="183" ssid = "55">For example, our original test corpus of 1026 sentences contained 35 unparsable sentences.</S>
			<S sid ="184" ssid = "56">Thus, high quality parsers are unavailable for many source languages of interest.</S>
			<S sid ="185" ssid = "57">Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al., 2008).</S>
			<S sid ="186" ssid = "58">The soft syntax translation model is not dependent on the linguistic information being in a tree structure, only that the labels identify contiguous spans.</S>
			<S sid ="187" ssid = "59">Chunk taggers (Abney, 1991) does just that.</S>
			<S sid ="188" ssid = "60">They offer higher accuracy than syntactic parser, are not so brittle to out-of-domain data and identify chunk phrases similar to parser-based syntactic phrases that may be useful in guiding reordering.</S>
			<S sid ="189" ssid = "61">We apply the soft syntax approach as in the previous sections but replacing the use of parse constituents with chunk phrases.</S>
			<S sid ="190" ssid = "62">Figure 7: Chunked sentence 7.6 Experiments with Chunk Tags.</S>
			<S sid ="191" ssid = "63">We use the same data as described earlier in this chapter to train, tune and test our approach.</S>
			<S sid ="192" ssid = "64">The Treetagger chunker (Schmidt and Schulte im Walde, 2000) was used to tag the source (German) side of the corpus.</S>
			<S sid ="193" ssid = "65">The chunker successfully processed all sentences in the training and test dataset so no sentences were excluded.</S>
			<S sid ="194" ssid = "66">The increase training data, as well as the ability to translate all sentences in the test set, explains the higher hierarchical baseline than the previous experiments with parser data.</S>
			<S sid ="195" ssid = "67">We use the noun, verb and prepositional chunks, as well as part-of-speech tags, emitted by the chunker.</S>
			<S sid ="196" ssid = "68">Results are shown in Table 2, line 5 &amp; 6.</S>
			<S sid ="197" ssid = "69">Using chunk tags, we see a modest gain of 0.5% BLEU.</S>
			<S sid ="198" ssid = "70">The same example sentence in Table 4 is shown with chunk tags in Figure 7.</S>
			<S sid ="199" ssid = "71">The soft syntax model with chunk tags produced the derivation tree shown in Figure 8.</S>
			<S sid ="200" ssid = "72">The derivation make use of an unlexicalized rule local reordering.</S>
			<S sid ="201" ssid = "73">In this example, it uses the same number of glue rule as the hierarchical derivation but the output is grammatically correct.</S>
			<S sid ="202" ssid = "74">Figure 8: Translated chunked sentence However, overall, the number of glue rules used shows the same reduction that we saw using soft syntax in the earlier section, as can be seen in Figure 9.</S>
			<S sid ="203" ssid = "75">Again, the soft syntax model, this time using chunk tags, is able to reduce the use of the glue rule with empirically informed rules.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "8">
			<S sid ="204" ssid = "1">We show in this paper that combining the generality of the hierarchical approach with the specificity of syntactic approach can improve transla Figure 9: Chunk - Length and count of glue rules used decoding test set tion.</S>
			<S sid ="205" ssid = "2">A reason for the improvement is the better long-range reordering made possible by the increase capacity of the translation model.</S>
			<S sid ="206" ssid = "3">Future work in this direction includes using tree-to-tree approaches, automatically created constituency labels, and back-off methods between decorated and undecorated rules.</S>
	</SECTION>
	<SECTION title="Acknowledgement. " number = "9">
			<S sid ="207" ssid = "1">This work was supported in part by the EuroMatrixPlus project funded by the European Commission (7th Framework Programme) and in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.</S>
			<S sid ="208" ssid = "2">HR0011 06-C-0022.</S>
	</SECTION>
</PAPER>
