<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Choosing the right parameters for a word sense disambiguation task is critical to the success of the experiments.</S>
		<S sid ="2" ssid = "2">We explore this idea for prepositions, an often overlooked word class.</S>
		<S sid ="3" ssid = "3">We examine the parameters that must be considered in preposition disambiguation, namely context, features, and granularity.</S>
		<S sid ="4" ssid = "4">Doing so delivers an increased performance that significantly improves over two state-of- the-art systems, and shows potential for improving other word sense disambiguation tasks.</S>
		<S sid ="5" ssid = "5">We report accuracies of 91.8% and 84.8% for coarse and fine-grained preposition sense disambiguation, respectively.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Ambiguity is one of the central topics in NLP.</S>
			<S sid ="7" ssid = "7">A substantial amount of work has been devoted to disambiguating prepositional attachment, words, and names.</S>
			<S sid ="8" ssid = "8">Prepositions, as with most other word types, are ambiguous.</S>
			<S sid ="9" ssid = "9">For example, the word in can assume both temporal (“in May”) and spatial (“in the US”) meanings, as well as others, less easily classifiable (“in that vein”).</S>
			<S sid ="10" ssid = "10">Prepositions typically have more senses than nouns or verbs (Litkowski and Hargraves, 2005), making them difficult to disambiguate.</S>
			<S sid ="11" ssid = "11">Preposition sense disambiguation (PSD) has many potential uses.</S>
			<S sid ="12" ssid = "12">For example, due to the relational nature of prepositions, disambiguating their senses can help with all-word sense disambiguation.</S>
			<S sid ="13" ssid = "13">In machine translation, different senses of the same English preposition often correspond to different translations in the foreign language.</S>
			<S sid ="14" ssid = "14">Thus, disambiguating prepositions correctly may help improve translation quality.1 Coarse-grained PSD can also be valuable for information extraction, where the sense acts as a label.</S>
			<S sid ="15" ssid = "15">In a recent study, Hwang et al.</S>
			<S sid ="16" ssid = "16">(2010) identified preposition related features, among them the coarse-grained PP labels used here, as the most informative feature in identifying caused-motion constructions.</S>
			<S sid ="17" ssid = "17">Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing, one of the most frequent sources of parse errors.</S>
			<S sid ="18" ssid = "18">Several papers have successfully addressed PSD with a variety of different approaches (Rudzicz and Mokhov, 2003; O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009).</S>
			<S sid ="19" ssid = "19">However, while it is often possible to increase accuracy by using a different classifier and/or more features, adding more features creates two problems: a) it can lead to overfitting, and b) while possibly improving accuracy, it is not always clear where this improvement comes from and which features are actually informative.</S>
			<S sid ="20" ssid = "20">While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions to guide engineering decisions.</S>
			<S sid ="21" ssid = "21">We go beyond simply improving accuracy to analyze various parameters in order to determine which ones are actually informative.</S>
			<S sid ="22" ssid = "22">We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454–462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.</S>
			<S sid ="23" ssid = "23">Using the resulting parameters in a Maximum Entropy classifier, we are able to improve significantly over existing results.</S>
			<S sid ="24" ssid = "24">The general outline we present can potentially be extended to other word classes and improve WSD in general.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="25" ssid = "1">Rudzicz and Mokhov (2003) use syntactic and lexical features from the governor and the preposition itself in coarse-grained PP classification with decision heuristics.</S>
			<S sid ="26" ssid = "2">They reach an average F- measure of 89% for four classes.</S>
			<S sid ="27" ssid = "3">This shows that using a very small context can be effective.</S>
			<S sid ="28" ssid = "4">However, they did not include the object of the preposition and used only lexical features for classification.</S>
			<S sid ="29" ssid = "5">Their results vary widely for the different classes.</S>
			<S sid ="30" ssid = "6">O’Hara and Wiebe (2003) made use of a window size of five words and features from the Penn Treebank (PTB) (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepositions.</S>
			<S sid ="31" ssid = "7">They show that using high level features, such as semantic roles, significantly aid disambiguation.</S>
			<S sid ="32" ssid = "8">They caution that using collocations and neighboring words indiscriminately may yield high accuracy, but has the risk of overfit- ting.</S>
			<S sid ="33" ssid = "9">O’Hara and Wiebe (2009) show comparisons of various semantic repositories as labels for PSD approaches.</S>
			<S sid ="34" ssid = "10">They also provide some results for PTB-based coarse-grained senses, using a five- word window for lexical and hypernym features in a decision tree classifier.</S>
			<S sid ="35" ssid = "11">SemEval 2007 (Litkowski and Hargraves, 2007) included a task for fine-grained PSD (more than 290 senses).</S>
			<S sid ="36" ssid = "12">The best participating system, that of Ye and Baldwin (2007), extracted part-of- speech and WordNet (Fellbaum, 1998) features using a word window of seven words in a Maximum Entropy classifier.</S>
			<S sid ="37" ssid = "13">Tratz and Hovy (2009) present a higher-performing system using a set of 20 positions that are syntactically related to the preposition instead of a fixed window size.</S>
			<S sid ="38" ssid = "14">Though using a variety of different extraction methods, contexts, and feature words, none of these approaches explores the optimal configurations for PSD.</S>
	</SECTION>
	<SECTION title="Theoretical Background. " number = "3">
			<S sid ="39" ssid = "1">The following parameters are applicable to other word classes as well.</S>
			<S sid ="40" ssid = "2">We will demonstrate their effectiveness for prepositions.</S>
			<S sid ="41" ssid = "3">Analyzing the syntactic elements of prepositional phrases, one discovers three recurring elements that exhibit syntactic dependencies and define a prepositional phrase.</S>
			<S sid ="42" ssid = "4">The first one is the governing word (usually a noun, verb, or adjective)2, the preposition itself, and the object of the preposition.</S>
			<S sid ="43" ssid = "5">Prepositional phrases can be fronted (“In May, prices dropped by 5%”), so that the governor (in this case the verb “drop”) occurs later in the sentence.</S>
			<S sid ="44" ssid = "6">Similarly, the object can be fronted (consider “a dessert to die for”).</S>
			<S sid ="45" ssid = "7">In the simplest version, we can do classification based only on the preposition and the governor or object alone.3 Furthermore, directly neighboring words can influence the preposition, mostly two- word prepositions such as “out of” or “because of”.</S>
			<S sid ="46" ssid = "8">To extract the words discussed above, one can either employ a fixed window size, (which has to be large enough to capture the words), or select them based on heuristics or parsing information.</S>
			<S sid ="47" ssid = "9">The governor and object can be hard to extract if they are fronted, since they do not occur in their unusual positions relative to the preposition.</S>
			<S sid ="48" ssid = "10">While syntactically related words improve over fixed-window-size approaches (Tratz and Hovy, 2009), it is not clear which words contribute most.</S>
			<S sid ="49" ssid = "11">There should be an optimal context, i.e., the smallest set of words that achieves the best accuracy.</S>
			<S sid ="50" ssid = "12">It has to be large enough to capture all relevant information, but small enough to avoid noise words.4 We surmise that earlier approaches were not utilizing that optimal context, but rather include a lot of noise.</S>
			<S sid ="51" ssid = "13">Depending on the task, different levels of sense granularity may be used.</S>
			<S sid ="52" ssid = "14">Fewer senses increase the likelihood of correct classification, but may in 2 We will refer to the governing word, irrespective of class, as governor.</S>
			<S sid ="53" ssid = "15">3 Basing classification on the preposition alone is not feasible, because of the very polysemy we try to resolve.</S>
			<S sid ="54" ssid = "16">4 It is not obvious how much information a sister-PP can provide, or the subject of the superordinate clause.</S>
			<S sid ="55" ssid = "17">correctly conflate prepositions.</S>
			<S sid ="56" ssid = "18">A finer granularity can help distinguish nuances and better fit the different contexts.</S>
			<S sid ="57" ssid = "19">However, it might suffer from sparse data.</S>
			<S sid ="58" ssid = "20">18000 16995 16000 14000 12000 PTB class distrib</S>
	</SECTION>
	<SECTION title="Experimental Setup. " number = "4">
			<S sid ="59" ssid = "1">We explore the different context types (fixed window size vs. selective), the influence of the words in that context, and the preprocessing method (heuristics vs. parsing) on both coarse and fine 10000 8000 6000 4000 2000 0 10332 5414 1781 1071 280 44 grained disambiguation.</S>
			<S sid ="60" ssid = "2">We use a most-frequent- sense baseline.</S>
			<S sid ="61" ssid = "3">In addition, we compare to the state-of-the-art systems for both types of granularity (O’Hara and Wiebe, 2009; Tratz and Hovy, 2009).</S>
			<S sid ="62" ssid = "4">Their results show what has been achieved so far in terms of accuracy, and serve as a second measure for comparison beyond the baseline.</S>
			<S sid ="63" ssid = "5">4.1 Model.</S>
			<S sid ="64" ssid = "6">We use the MALLET implementation (McCallum, 2002) of a Maximum Entropy classifier (Berger et al., 1996) to construct our models.</S>
			<S sid ="65" ssid = "7">This classifier was also used by two state-of-the-art systems (Ye and Baldwin, 2007; Tratz and Hovy, 2009).</S>
			<S sid ="66" ssid = "8">For fine-grained PSD, we train a separate model for each preposition due to the high number of possible classes for each individual preposition.</S>
			<S sid ="67" ssid = "9">For coarse-grained PSD, we use a single model for all prepositions, because they all share the same classes.</S>
			<S sid ="68" ssid = "10">4.2 Data.</S>
			<S sid ="69" ssid = "11">We use two different data sets from existing resources for coarse and fine-grained PSD to make our results as comparable to previous work as possible.</S>
			<S sid ="70" ssid = "12">For the coarse-grained disambiguation, we use data from the POS tagged version of the Wall Street Journal (WSJ) section of the Penn Tree- Bank.</S>
			<S sid ="71" ssid = "13">A subset of the prepositional phrases in this corpus is labelled with a set of seven classes: beneficial (BNF), direction (DIR), extent (EXT), location (LOC), manner (MNR), purpose (PRP), and temporal (TMP).</S>
			<S sid ="72" ssid = "14">We extract only those prepositions that head a PP labelled with such a class (N = 35, 917).</S>
			<S sid ="73" ssid = "15">The distribution of classes is highly skewed (cf.</S>
			<S sid ="74" ssid = "16">Figure 1).</S>
			<S sid ="75" ssid = "17">We compare the LOC TMP DIR MNR PRP EXT BNF c l a s s e s Figure 1: Distribution of Class Labels in the WSJ Section of the Penn TreeBank.</S>
			<S sid ="76" ssid = "18">results of this task to the findings of O’Hara and Wiebe (2009).</S>
			<S sid ="77" ssid = "19">For the fine-grained task, we use data from the SemEval 2007 workshop (Litkowski and Har- graves, 2007), separate XML files for the 34 most frequent English prepositions, comprising 16, 557 training and 8096 test sentences, each instance containing one example of the respective preposition.</S>
			<S sid ="78" ssid = "20">Each preposition has between two and 25 senses (9.76 on average) as defined by The Preposition Project (Litkowski and Hargraves, 2005).</S>
			<S sid ="79" ssid = "21">We compare our results directly to the findings from Tratz and Hovy (2009).</S>
			<S sid ="80" ssid = "22">As in the original workshop task, we train and test on separate sets.</S>
	</SECTION>
	<SECTION title="Results. " number = "5">
			<S sid ="81" ssid = "1">In this section we show experimental results for the influence of word extraction method (parsing vs. POS-based heuristics), context, and feature selection on accuracy.</S>
			<S sid ="82" ssid = "2">Each section compares the results for both coarse and fine-grained granularity.</S>
			<S sid ="83" ssid = "3">Accuracy for the coarse-grained task is in all experiments higher than for the fine grained one.</S>
			<S sid ="84" ssid = "4">5.1 Word Extraction.</S>
			<S sid ="85" ssid = "5">In order to analyze the impact of the extracPtaiogen 1 method, we compare parsing versus POS-based heuristics for word extraction.</S>
			<S sid ="86" ssid = "6">Both O’Hara and Wiebe (2009) and Tratz and Hovy (2009) use constituency parsers to preprocess the data.</S>
			<S sid ="87" ssid = "7">However, parsing accuracy varies, and the problem of PP attachment ambiguity increases the likelihood of wrong extractions.</S>
			<S sid ="88" ssid = "8">This is especially troublesome in the present case, where we focus on prepositions.5 We use the MALT parser (Nivre et al., 2007), a state-of-the- art dependency parser, to extract the governor and object.</S>
			<S sid ="89" ssid = "9">The alternative is a POS-based heuristics approach.</S>
			<S sid ="90" ssid = "10">The only preprocessing step needed is POS tagging of the data, for which we used the system of Shen et al.</S>
			<S sid ="91" ssid = "11">(2007).</S>
			<S sid ="92" ssid = "12">We then use simple heuristics to locate the prepositions and their related words.</S>
			<S sid ="93" ssid = "13">In order to determine the governor in the absence of constituent phrases, we consider the possible governing noun, verb, and adjective.</S>
			<S sid ="94" ssid = "14">The object of the preposition is extracted as first noun phrase head to the right.</S>
			<S sid ="95" ssid = "15">This approach is faster than parsing, but has problems with long- range dependencies and fronting of the PP (e.g., the PP appearing earlier in the sentence than its governor).</S>
			<S sid ="96" ssid = "16">C o n t e x t co ars e fi n e2 wor d win do w 9 1 . 6 8 0 . 43 wor d win do w 9 2 . 0 8 1 . 44 wor d win do w 9 1 . 6 7 9 . 85 wor d win do w 9 1 . 0 7 8 . 7 Go ver nor, pre p 8 0 . 7 7 8 . 9 Pre p, obj ect 9 4 . 2 5 6 . 9 Go ver nor, pre p, obj ect 9 4 . 0 8 4 . 8 Table 2: Accuracies (%) for Different Context Types and Sizes The results show that the approach using both governor and object is the most accurate one.</S>
			<S sid ="97" ssid = "17">Of the fixed-window-size approaches, three words to either side works best.</S>
			<S sid ="98" ssid = "18">This does not necessarily reflect a general property of that window size, but can be explained by the fact that most governors and objects occur within this window size.7 This contextword sedleicstitoannce can vary from corpus to corpus, so win dow size would have to be determined individually for each task.</S>
			<S sid ="99" ssid = "19">The difference between using governor and preposition versus preposition and object between coarse and fine-grained classification might reflect the annotation process: while Table 1: Accuracies (%) for Word-Extraction Using MALT Parser or Heuristics.</S>
			<S sid ="100" ssid = "20">Interestingly, the extraction method does not significantly affect the final score for fine-grained PSD (see Table 1).</S>
			<S sid ="101" ssid = "21">The high score achieved when using the MALT parse for coarse-grained PSD can be explained by the fact that the parser was originally trained on that data set.</S>
			<S sid ="102" ssid = "22">The good results we see when using heuristics-based extraction only, however, means we can achieve high- accuracy PSD even without parsing.</S>
			<S sid ="103" ssid = "23">5.2 Context.</S>
			<S sid ="104" ssid = "24">We compare the effects of fixed window size ver Litkowski and Hargraves (2007) selected examples based on a search for governors8, most annotators in the PTB may have based their decision of the PP label on the object that occurs in it.</S>
			<S sid ="105" ssid = "25">We conclude that syntactically related words present a better context for classification than fixed window sizes.</S>
			<S sid ="106" ssid = "26">5.3 Featu res Having established the context we want to use, we now turn to the details of extracting the feature words from that context.9 Using higher-level features instead of lexical ones helps accounting for sparse training data (given an infinite amount of data, we would not need to take any higher-level sus syntactically related words as context.</S>
			<S sid ="107" ssid = "27">Table 2 shows the results for the different types and sizes of contexts.6 5 Rudzicz and Mokhov (2003) actually motivate their work as a means to achieve better PP attachment resolution.</S>
			<S sid ="108" ssid = "28">6 See also (Yarowsky and Florian, 2002) for experiments on the effect of varying window size for WSD.</S>
			<S sid ="109" ssid = "29">7 Based on such statistics, O’Hara and Wiebe (2003) ac-.</S>
			<S sid ="110" ssid = "30">tually set their window size to 5.</S>
			<S sid ="111" ssid = "31">8 Personal communication..</S>
			<S sid ="112" ssid = "32">9 As one reviewer pointed out, these two dimensions are highly interrelated and influence each other.</S>
			<S sid ="113" ssid = "33">To examine the effects, we keep one dimension constant while varying the other.</S>
			<S sid ="114" ssid = "34">Page 1 features into account, since every case would be covered).</S>
			<S sid ="115" ssid = "35">Compare O’Hara and Wiebe (2009).</S>
			<S sid ="116" ssid = "36">Following the prepocessing, we use a set of rules to select the feature words, and then generate feature values from them using a variety of feature-generating functions.10 The word- selection rules are listed below.</S>
			<S sid ="117" ssid = "37">Word-Selection Rules • Governor from the MALT parse • Object from the MALT parse • Heuristically determined object of the preposition • First verb to the left of the preposition • First verb/noun/adjective to the left of the preposition • Union of (First verb to the left, First verb/noun/adjective to the left) • First word to the left The feature-generating functions, many of which utilize WordNet (Fellbaum, 1998), are listed below.</S>
			<S sid ="118" ssid = "38">To conserve space, curly braces are used to represent multiple functions in a single line.</S>
			<S sid ="119" ssid = "39">The name of each feature is the combination of the word-selection rule and the output from the feature-generating function.</S>
			<S sid ="120" ssid = "40">WordNet-based Features • {Hypernyms, Synonyms} for {1st, all} sense(s) of the word • All terms in the definitions (‘glosses’) of the word • Lexicographer file names for the word • Lists of all link types (e.g., meronym links) associated with the word • Part-of-speech indicators for the existence of NN/VB/JJ/RB entries for the word • All sentence frames for the word • All {part, member, substance}-of holonyms for the word • All sentence frames for the word Other Features • Indicator that the word-finding rule found a word 10 Some words may be selected by multiple word-selection.</S>
			<S sid ="121" ssid = "41">• Capitalization indicator • {Lemma, surface form} of the word • Part-of-speech tag for the word • General POS tag for the word (e.g. NNS → NN, VBZ → VB) • The {first, last} {two, three} letters of each word • Indicators for suffix types (e.g., de- adjectival, de-nominal [non]agentive, de-verbal [non]agentive) • Indicators for a wide variety of other affixes including those related to degree, number, order, etc.</S>
			<S sid ="122" ssid = "42">(e.g., ultra-, poly-, post-) • Roget’s Thesaurus divisions for the word To establish the impact of each feature word on the outcome, we use leave-one-out and only-one evaluation.11 The results can be found in Table 3.</S>
			<S sid ="123" ssid = "43">A word that does not perform well as the only attribute may still be important in conjunction with others.</S>
			<S sid ="124" ssid = "44">Conversely, leaving out a word may not hurt performance, despite being a good single at tribute.</S>
			<S sid ="125" ssid = "45">word selection coars e f i n e W o r d LO O Onl y LO O Onl y MAL T gov erno r 92 .1 80 .1 84 .3 78 .9 MAL T obje ct 93 .4 94 .2 84 .9 56 .3 Heu ristic s VB to left 92 .0 77 .9 85 .0 62 .1 Heu r. NN/ VB/ ADJ to left 92 .1 78 .7 84 .3 78 .5 Heu r. Gov erno r Unio n 92 .1 78 .4 84 .5 81 .0 Heu ristic s wor d to left 92 .0 78 .8 84 .4 77 .2 Heu ristic s obje ct 91 .9 93 .0 84 .9 56 .8 non e 91 .8 – 84 .8 –Table 3: Accuracies (%) for LeaveOne- Out (LOO) and Only-One Word Extraction-Rule Evaluation.</S>
			<S sid ="126" ssid = "46">none includes all words and serves for comparison.</S>
			<S sid ="127" ssid = "47">Important words reduce accuracy for LOO, but rank high when used as only rule.</S>
			<S sid ="128" ssid = "48">Independent of the extraction method (MALT parser or POS-based heuristics), the governor is the most informative word.</S>
			<S sid ="129" ssid = "49">Combining several heuristics to locate the governor is the best single feature for fine grained classification.</S>
			<S sid ="130" ssid = "50">The rule looking only for a governing verb fails to account rules.</S>
			<S sid ="131" ssid = "51">For example, the governor of the preposition may be identified by the Governor from MALT parse rule, first noun/verb/adjective to left, and the first word to the left rule.</S>
			<S sid ="132" ssid = "52">11 Since the feature words are not independent of one.</S>
			<S sid ="133" ssid = "53">another, neither of the two measures is decisive on its own.</S>
			<S sid ="134" ssid = "54">full both f i n e coar se f i n e coar se P r e p Tot al Ac c Tot al Ac c P r e p Tot al Ac c To tal Ac c abo ard – – 6 10 0.0 like 1 2 5 9 0.</S>
			<S sid ="135" ssid = "55">4 5 3 4 7.</S>
			<S sid ="136" ssid = "56">2 abo ut 3 6 4 9 4.</S>
			<S sid ="137" ssid = "57">0 5 8 0.</S>
			<S sid ="138" ssid = "58">0 nea r – – 7 4 9 3.</S>
			<S sid ="139" ssid = "59">2 abo ve 2 3 6 9.</S>
			<S sid ="140" ssid = "60">6 7 8 6 5.</S>
			<S sid ="141" ssid = "61">4 nea rest – – 1 0 . 0 acr oss 1 5 1 9 6.</S>
			<S sid ="142" ssid = "62">7 8 7 7 9.</S>
			<S sid ="143" ssid = "63">3 nex t – – 7 7 1.</S>
			<S sid ="144" ssid = "64">4 afte r 5 3 7 9.</S>
			<S sid ="145" ssid = "65">2 8 4 1 9 2.</S>
			<S sid ="146" ssid = "66">5 of 14 78 8 7.</S>
			<S sid ="147" ssid = "67">9 7 1 6 4.</S>
			<S sid ="148" ssid = "68">8 aga inst 9 2 9 2.</S>
			<S sid ="149" ssid = "69">4 1 6 4 3.</S>
			<S sid ="150" ssid = "70">8 off 7 6 8 4.</S>
			<S sid ="151" ssid = "71">2 2 8 7 5.</S>
			<S sid ="152" ssid = "72">0 alo ng 1 7 3 9 6.</S>
			<S sid ="153" ssid = "73">0 4 5 7 1.</S>
			<S sid ="154" ssid = "74">1 on 4 4 1 8 1.</S>
			<S sid ="155" ssid = "75">4 2 2 8 7 9 0.</S>
			<S sid ="156" ssid = "76">8 alo ngsi de – – 5 8 0.</S>
			<S sid ="157" ssid = "77">0 ont o 5 8 9 1.</S>
			<S sid ="158" ssid = "78">4 1 5 5 3.</S>
			<S sid ="159" ssid = "79">3 ami d – – 5 8 7 0.</S>
			<S sid ="160" ssid = "80">7 out – – 9 0 6 8.</S>
			<S sid ="161" ssid = "81">9 am ong 5 0 8 0.</S>
			<S sid ="162" ssid = "82">0 3 5 8 9 3.</S>
			<S sid ="163" ssid = "83">9 out side – – 6 2 9 0.</S>
			<S sid ="164" ssid = "84">3 am ong st – – 1 0 . 0 ove r 9 8 7 9.</S>
			<S sid ="165" ssid = "85">6 4 1 7 8 9.</S>
			<S sid ="166" ssid = "86">4 aro und 1 5 5 6 9.</S>
			<S sid ="167" ssid = "87">0 1 0 7 8 6.</S>
			<S sid ="168" ssid = "88">0 pas t – – 6 8 3.</S>
			<S sid ="169" ssid = "89">3 as 8 4 10 0.0 2 3 2 8 4.</S>
			<S sid ="170" ssid = "90">5 per – – 3 10 0.0 astr ide – – 2 5 0.</S>
			<S sid ="171" ssid = "91">0 rou nd 8 2 6 5.</S>
			<S sid ="172" ssid = "92">9 – – at 3 6 7 8 6.</S>
			<S sid ="173" ssid = "93">4 30 78 9 2.</S>
			<S sid ="174" ssid = "94">0 sinc e – – 4 4 9 9 4.</S>
			<S sid ="175" ssid = "95">4 ato p – – 5 10 0.0 tha n – – 2 0 . 0 bec aus e – – 4 2 0 9 1.</S>
			<S sid ="176" ssid = "96">7 thro ugh 2 0 8 4 8.</S>
			<S sid ="177" ssid = "97">1 3 6 4 6 9.</S>
			<S sid ="178" ssid = "98">0 bef ore 2 0 9 0.</S>
			<S sid ="179" ssid = "99">0 3 8 4 8 3.</S>
			<S sid ="180" ssid = "100">3 thro ugh out – – 6 2 9 3.</S>
			<S sid ="181" ssid = "101">5 beh ind 6 8 7 7.</S>
			<S sid ="182" ssid = "102">9 6 5 8 7.</S>
			<S sid ="183" ssid = "103">7 till – – 3 10 0.0 bel ow – – 9 4 7 1.</S>
			<S sid ="184" ssid = "104">3 to 5 7 2 8 9.</S>
			<S sid ="185" ssid = "105">7 3 1 6 6 9 7.</S>
			<S sid ="186" ssid = "106">5 ben eat h 2 8 7 8.</S>
			<S sid ="187" ssid = "107">6 1 1 7 2.</S>
			<S sid ="188" ssid = "108">7 tow ard – – 5 5 6 5.</S>
			<S sid ="189" ssid = "109">5 besi de 2 9 10 0.0 4 10 0.0 tow ard s 1 0 2 9 7.</S>
			<S sid ="190" ssid = "110">1 2 10 0.0 besi des – – 1 0 . 0 und er – – 6 0 4 9 1.</S>
			<S sid ="191" ssid = "111">4 bet wee n 1 0 2 9 4.</S>
			<S sid ="192" ssid = "112">1 9 8 8 4.</S>
			<S sid ="193" ssid = "113">7 und ern eat h – – 2 5 0.</S>
			<S sid ="194" ssid = "114">0 bey ond – – 4 5 6 4.</S>
			<S sid ="195" ssid = "115">4 until – – 2 0 8 9 4.</S>
			<S sid ="196" ssid = "116">2 by 2 4 8 8 8.</S>
			<S sid ="197" ssid = "117">3 13 41 8 7.</S>
			<S sid ="198" ssid = "118">5 up – – 2 0 7 5.</S>
			<S sid ="199" ssid = "119">0 dow n 1 5 3 8 1.</S>
			<S sid ="200" ssid = "120">7 1 6 5 6.</S>
			<S sid ="201" ssid = "121">2 upo n – – 2 3 7 3.</S>
			<S sid ="202" ssid = "122">9 duri ng 3 9 8 7.</S>
			<S sid ="203" ssid = "123">2 5 4 7 9 2.</S>
			<S sid ="204" ssid = "124">1 via – – 2 2 4 0.</S>
			<S sid ="205" ssid = "125">9 exc ept – – 1 0 . 0 whe ther – – 1 10 0.0 for 4 7 8 8 2.</S>
			<S sid ="206" ssid = "126">4 14 55 8 4.</S>
			<S sid ="207" ssid = "127">5 whil e – – 3 3 3.</S>
			<S sid ="208" ssid = "128">3 fro m 5 7 8 8 5.</S>
			<S sid ="209" ssid = "129">5 17 12 9 0.</S>
			<S sid ="210" ssid = "130">5 with 5 7 8 8 4.</S>
			<S sid ="211" ssid = "131">4 2 7 2 6 9.</S>
			<S sid ="212" ssid = "132">5 in 6 8 8 7 7.</S>
			<S sid ="213" ssid = "133">0 157 06 9 5.</S>
			<S sid ="214" ssid = "134">0 with in – – 2 1 3 9 6.</S>
			<S sid ="215" ssid = "135">2 insi de 3 8 7 3.</S>
			<S sid ="216" ssid = "136">7 2 4 9 1.</S>
			<S sid ="217" ssid = "137">7 with out – – 6 9 6 3.</S>
			<S sid ="218" ssid = "138">8 into 2 9 7 8 6.</S>
			<S sid ="219" ssid = "139">2 4 1 5 8 0.</S>
			<S sid ="220" ssid = "140">0 Ov eral l 80 96 8 4.</S>
			<S sid ="221" ssid = "141">8 35 91 7 9 1.</S>
			<S sid ="222" ssid = "142">8 Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics.</S>
			<S sid ="223" ssid = "143">Sorted by preposition.</S>
			<S sid ="224" ssid = "144">for noun governors, which consequently leads to a slight improvement when left out.</S>
			<S sid ="225" ssid = "145">Curiously, the word directly to the left is a better single feature than the object (for fine-grained classification).</S>
			<S sid ="226" ssid = "146">Leaving either of them out in creases accuracy, which implies that their information can be covered by other words.</S>
			<S sid ="227" ssid = "147">Page 1 coarse both 2009 Most Frequent Sense O&apos;Hara/Wieb e 2009 10-fold CV Cla ss pre c re c f 1 pre c re c f 1 pre c re c f 1 L O C T M P DI R M N R P R P E X T B N F 7 1.</S>
			<S sid ="228" ssid = "148">8 7 7.</S>
			<S sid ="229" ssid = "149">5 9 1.</S>
			<S sid ="230" ssid = "150">6 6 9.</S>
			<S sid ="231" ssid = "151">9 7 8.</S>
			<S sid ="232" ssid = "152">2 0 . 0 0 . 0 9 7.</S>
			<S sid ="233" ssid = "153">4 3 9.</S>
			<S sid ="234" ssid = "154">4 9 4.</S>
			<S sid ="235" ssid = "155">2 4 3.</S>
			<S sid ="236" ssid = "156">2 4 8.</S>
			<S sid ="237" ssid = "157">8 0 . 0 0 . 0 8 2.</S>
			<S sid ="238" ssid = "158">6 5 2.</S>
			<S sid ="239" ssid = "159">3 9 2.</S>
			<S sid ="240" ssid = "160">8 5 3.</S>
			<S sid ="241" ssid = "161">4 6 0.</S>
			<S sid ="242" ssid = "162">1 0 . 0 0 . 0 9 0.</S>
			<S sid ="243" ssid = "163">8 8 4.</S>
			<S sid ="244" ssid = "164">5 9 5.</S>
			<S sid ="245" ssid = "165">6 8 2.</S>
			<S sid ="246" ssid = "166">6 7 9.</S>
			<S sid ="247" ssid = "167">3 8 1.</S>
			<S sid ="248" ssid = "168">7 – 9 3.</S>
			<S sid ="249" ssid = "169">2 8 5.</S>
			<S sid ="250" ssid = "170">2 9 6.</S>
			<S sid ="251" ssid = "171">5 5 5.</S>
			<S sid ="252" ssid = "172">8 7 0.</S>
			<S sid ="253" ssid = "173">1 8 4.</S>
			<S sid ="254" ssid = "174">6 – 9 2.</S>
			<S sid ="255" ssid = "175">0 8 4.</S>
			<S sid ="256" ssid = "176">8 9 6.</S>
			<S sid ="257" ssid = "177">1 6 6.</S>
			<S sid ="258" ssid = "178">1 7 4.</S>
			<S sid ="259" ssid = "179">4 8 2.</S>
			<S sid ="260" ssid = "180">9 – 9 4.</S>
			<S sid ="261" ssid = "181">7 9 4.</S>
			<S sid ="262" ssid = "182">6 9 4.</S>
			<S sid ="263" ssid = "183">6 8 3.</S>
			<S sid ="264" ssid = "184">3 9 0.</S>
			<S sid ="265" ssid = "185">6 8 7.</S>
			<S sid ="266" ssid = "186">5 7 5.</S>
			<S sid ="267" ssid = "187">0 9 6.</S>
			<S sid ="268" ssid = "188">4 9 4.</S>
			<S sid ="269" ssid = "189">6 9 4.</S>
			<S sid ="270" ssid = "190">5 7 5.</S>
			<S sid ="271" ssid = "191">0 8 3.</S>
			<S sid ="272" ssid = "192">8 8 2.</S>
			<S sid ="273" ssid = "193">1 3 4.</S>
			<S sid ="274" ssid = "194">1 9 5.</S>
			<S sid ="275" ssid = "195">6 9 4.</S>
			<S sid ="276" ssid = "196">6 9 4.</S>
			<S sid ="277" ssid = "197">5 7 8.</S>
			<S sid ="278" ssid = "198">9 8 7.</S>
			<S sid ="279" ssid = "199">1 8 4.</S>
			<S sid ="280" ssid = "200">7 4 6.</S>
			<S sid ="281" ssid = "201">9 Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification.</S>
			<S sid ="282" ssid = "202">Comparison to O’Hara and Wiebe (2009).</S>
			<S sid ="283" ssid = "203">Classes ordered by frequency 5.4 Comparison with Related Work.</S>
			<S sid ="284" ssid = "204">To situate our experimental results within the body of work on PSD, we compare them to both a most-frequent-sense baseline and existing work for both granularities (see Table 6).</S>
			<S sid ="285" ssid = "205">The results use a syntactically selective context of preposition, governor, object, and word to the left as determined by combined extraction information (POS tagging and parsing).</S>
			<S sid ="286" ssid = "206">strictly comparable, yet they provide a sense of the general task difficulty.</S>
			<S sid ="287" ssid = "207">See Table 5.</S>
			<S sid ="288" ssid = "208">We note that both systems perform better than the most- frequent-sense baseline.</S>
			<S sid ="289" ssid = "209">DIR is reliably classified using the baseline, while EXT and BNF are never selected for any preposition.</S>
			<S sid ="290" ssid = "210">Our method adds considerably to the scores for most classes.</S>
			<S sid ="291" ssid = "211">The low score for BNF is mainly due to the low number of instances in the data, which is why it was accuracieexs cluded by O’Hara and Wiebe (2009).</S>
			<S sid ="292" ssid = "212">Table 6: Accuracies (%) for Different Classifications.</S>
			<S sid ="293" ssid = "213">Comparison with O’Hara and Wiebe (2009)*, and Tratz and Hovy (2009)**.</S>
			<S sid ="294" ssid = "214">Our system easily exceeds the baseline for both coarse and fine-grained PSD (see Table 6).</S>
			<S sid ="295" ssid = "215">Comparison with related work shows that we achieve an improvement of 6.5% over Tratz and Hovy (2009), which is significant at p &lt; .0001, and of 4.5% over O’Hara and Wiebe (2009), which is significant at p &lt; .0001.</S>
			<S sid ="296" ssid = "216">A detailed overview over all prepositions for frequencies and accuracies of both coarse and fine-grained PSD can be found in Table 4.</S>
			<S sid ="297" ssid = "217">In addition to overall accuracy, O’Hara and Wiebe (2009) also measure precision, recall and F-measure for the different classes.</S>
			<S sid ="298" ssid = "218">They omitted BNF because it is so infrequent.</S>
			<S sid ="299" ssid = "219">Due to different training data and models, the two systems are not</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="300" ssid = "1">To get maximal accuracy in disambiguating prepositions—and also other word classes—one needs to consider context, features, and granularity.</S>
			<S sid ="301" ssid = "2">We presented an evaluation of these parameters for preposition sense disambiguation (PSD).</S>
			<S sid ="302" ssid = "3">We find that selective context is better than fixed window size.</S>
			<S sid ="303" ssid = "4">Within the context for prepositions, the governor (head of the NP or VP governing the preposition), the object of the preposition (i.e., head of the NP to the right), and the word directly to the left of the preposition have the highest influence.12 This corroborates the linguistic intuition that close mutual constraints hold between the elements of the PP.</S>
			<S sid ="304" ssid = "5">Each word syntactically and semantically restricts the choice of the other elements.</S>
			<S sid ="305" ssid = "6">Combining different extraction mePtahgoed1s (POS-based heuristics and dependency parsing) works better than either one in isolation, though high accuracy can be achieved just using heuristics.</S>
			<S sid ="306" ssid = "7">The impact of context and features varies somewhat for different granularities.</S>
			<S sid ="307" ssid = "8">12 These will likely differ for other word classes..</S>
			<S sid ="308" ssid = "9">Not surprisingly, we see higher scores for coarser granularity than for the more fine-grained one.</S>
			<S sid ="309" ssid = "10">We measured success in accuracy, precision, recall, and F-measure, and compared our results to a most-frequent-sense baseline and existing work.</S>
			<S sid ="310" ssid = "11">We were able to improve over state-of-the-art systems in both coarse and fine-grained PSD, achieving accuracies of 91.8% and 84.8% respectively.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="311" ssid = "12">The authors would like to thank Steve DeNeefe, Victoria Fossum, and Zornitsa Kozareva for comments and suggestions.</S>
			<S sid ="312" ssid = "13">StephenTratz is supported by a National Defense Science and Engineering fellowship.</S>
	</SECTION>
</PAPER>
