<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data.</S>
		<S sid ="2" ssid = "2">However, these solutions are impractical in complex structured prediction problems such as statistical machine translation.</S>
		<S sid ="3" ssid = "3">We present an online gradient-based algorithm for relative margin maximization, which bounds the spread of the projected data while maximizing the margin.</S>
		<S sid ="4" ssid = "4">We evaluate our optimizer on ChineseEnglish and ArabicEnglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.22 BLEU and 1.74.3 TER on average over state-of-the-art optimizers with the large feature set.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features.</S>
			<S sid ="6" ssid = "6">Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012).</S>
			<S sid ="7" ssid = "7">While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses.</S>
			<S sid ="8" ssid = "8">In every SMT system, and in machine learning in general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translations for previously unseen sentences.</S>
			<S sid ="9" ssid = "9">However, as the dimension of the feature space increases, generalization becomes increasingly difficult.</S>
			<S sid ="10" ssid = "10">Since only a small portion of all (sparse) features may be observed in a relatively small fixed set of instances during tuning, we are prone to overfit the training data.</S>
			<S sid ="11" ssid = "11">An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bi- text (Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012), which is usually substantially larger than the tuning set, but this is complementary to our goal here of better generalization given a fixed size tuning set.</S>
			<S sid ="12" ssid = "12">In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective.</S>
			<S sid ="13" ssid = "13">We focus on large-margin methods such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA.</S>
			<S sid ="14" ssid = "14">Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one.</S>
			<S sid ="15" ssid = "15">This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006).</S>
			<S sid ="16" ssid = "16">Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (CesaBianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidence- weighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b).</S>
			<S sid ="17" ssid = "17">The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 49 2013.</S>
			<S sid ="18" ssid = "18">Qc 2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the weight vector w. Unfortunately, not all advances in machine learning are easy to apply to structured prediction problems such as SMT; the latter often involve latent variables and surrogate references, resulting in loss functions that have not been well explored in machine learning (Mcallester and Keshet, 2011; Gimpel and Smith, 2012).</S>
			<S sid ="19" ssid = "19">Although Shivaswamy and Jebara extended RMM to handle sequential structured prediction (Shivaswamy and Jebara, even where previously MERT was shown to be advantageous (§5).</S>
			<S sid ="20" ssid = "20">Finally, we discuss the spread and other key issues of RM (§6), and conclude with discussion of future work (§7).</S>
	</SECTION>
	<SECTION title="Learning in SMT. " number = "2">
			<S sid ="21" ssid = "1">Given an input sentence in the source language x ∈ X , we want to produce a translation y ∈ Y(x) using a linear model parameterized by a weight vector w: 2009a), their batch approach to quadratic optimization, using existing off-the-shelf QP solvers, does not provide a practical solution: as Taskar et (y∗, d∗) = arg max (y,d)∈Y(x),D(x) w f (x, y, d) al.</S>
			<S sid ="22" ssid = "2">(2006) observe, “off-the-shelf QP solvers tend to scale poorly with problem and training sample size” for structured prediction problems..</S>
			<S sid ="23" ssid = "3">This motivates an online gradient-based optimization approach—an approach that is particularly attractive because its simple update is well suited for efficiently processing structured objects with sparse features (Crammer et al., 2012).</S>
			<S sid ="24" ssid = "4">The contributions of this paper include (1) introduction of a loss function for structured RMM in the SMT setting, with surrogate reference translations and latent variables; (2) an online gradient- based solver, RM, with a closed-form parameter update to optimize the relative margin loss; and (3) an efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010).1 In addition, (4) as our solution is not dependent on any specific QP solver, it can be easily incorporated into practically any gradient- based learning algorithm.</S>
			<S sid ="25" ssid = "5">After background discussion on learning inSMT (§2), we introduce a novel online learning al gorithm for relative margin maximization suitable for SMT (§3).</S>
			<S sid ="26" ssid = "6">First, we introduce RMM (§3.1) and propose a latent structured relative margin objective which incorporates cost-augmented hypothesis selection and latent variables.</S>
			<S sid ="27" ssid = "7">Then, we derive a simple closed-form online update necessary to create a large margin solution while simultaneously bounding the spread of the projection ofthe data (§3.2).</S>
			<S sid ="28" ssid = "8">ChineseEnglish translation exper iments show that our algorithm, RM, significantly outperforms strong state-of-the-art optimizers, in both a basic feature setting and high-dimensional(sparse) feature space (§4).</S>
			<S sid ="29" ssid = "9">Additional Arabic English experiments further validate these results, where w f (x, y, d) is the weighted feature scoring function, hereafter s(x, y, d), and Y(x) is the space of possible translations of x. While many derivations d ∈ D(x) can produce a given transla tion, we are only able to observe y; thus we model d as a latent variable.</S>
			<S sid ="30" ssid = "10">Although our models are actually defined over derivations, they are always paired with translations, so our feature function f (x, y, d) is defined over derivation–translation pairs.2 The learning goal is then to estimate w. The instability of MERT in larger feature sets (Foster and Kuhn, 2009; Hopkins and May, 2011), has motivated many alternative tuning methods for SMT.</S>
			<S sid ="31" ssid = "11">These include strategies based on batch log-linear models (Tillmann and Zhang, 2006; Blunsom et al., 2008), as well as the introduction of online linear models (Liang et al., 2006a; Arun and Koehn, 2007).</S>
			<S sid ="32" ssid = "12">Recent batch optimizers, PRO and RAMPION, and Batch-MIRA (Cherry and Foster, 2012), have been partly motivated by existing MT infrastructures, as they iterate between decoding the entire tuning set and optimizing the parameters.</S>
			<S sid ="33" ssid = "13">PRO considers tuning a classification problem and employs a binary classifier to rank pairs of outputs.</S>
			<S sid ="34" ssid = "14">RAMPION aims to address the disconnect between MT and machine learning by optimizing a structured ramp loss with a concave-convex procedure.</S>
			<S sid ="35" ssid = "15">2.1 Large-Margin Learning.</S>
			<S sid ="36" ssid = "16">Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).</S>
			<S sid ="37" ssid = "17">The usual presentation of MIRA’s optimization problem is given as a quadratic program: 1 https://github.com/veidel/cdec 2 We may omit d in some equations.</S>
			<S sid ="38" ssid = "18">for clarity.</S>
			<S sid ="39" ssid = "19">wt+1 = arg min w ||w − wt||2 + C ξi (1) s.t. s(xi, yi, d) − s(xi, y1, d) ≥ ∆i(y1) − ξi where y1 is the single most violated constraint, the cost ∆i(y) is computed using an external measure of quality, such as 1BLEU(yi, y), and a slack variable ξi is introduced to allow for non-separable instances.</S>
			<S sid ="40" ssid = "20">C acts as a regularization parameter, trading off between margin maximization and constraint violations.</S>
			<S sid ="41" ssid = "21">While solving the optimization problem relies on computing the margin between the correct output yi, and y1, in SMT our decoder is often inca pable of producing the reference translation, i.e. yi ∈/ Y(xi).</S>
			<S sid ="42" ssid = "22">We must instead resort to selecting a surrogate reference, y+ ∈ Y(xi).</S>
			<S sid ="43" ssid = "23">This issue has recently received considerable attention (Liang et al., 2006a; Eidelman, 2012; Chiang, 2012), with preference given to surrogate references obtained through cost-diminished hypothesis selection.</S>
			<S sid ="44" ssid = "24">Thus, y+ is selected based on a combination of model score and error metric from the k-best list produced by our current model.</S>
			<S sid ="45" ssid = "25">A similar selection is made for the cost-augmented hypothesis y− ∈ Y(xi): (b) Figure 1: (a) RM and large margin solution comparison and (b) the spread of the projections given by each.</S>
			<S sid ="46" ssid = "26">RM and large margin solutions are shown with a darker dotted line and a darker solid line, respectively.</S>
	</SECTION>
	<SECTION title="The Relative Margin Machine in SMT. " number = "3">
			<S sid ="47" ssid = "1">3.1 Relative Margin Machine.</S>
			<S sid ="48" ssid = "2">The margin, the distance between the correct hypothesis and incorrect one, is defined bys(x , y+, d+) and s(x , y−, d−).</S>
			<S sid ="49" ssid = "3">It is maxi i i (y+, d+) ← arg max (y,d)∈Y(xi ),D(xi ) s(xi, y, d) − ∆i(y) mized by minimizing the norm in SVM, or analogously, the proximity constraint in MIRA: 1 2 (y−, d−) ← arg max (y,d)∈Y(xi ),D(xi ) s(xi, y, d) + ∆i(y) arg minw 2 ||w − wt|| . However, theoretical re sults supporting large-margin learning, such as the In this setting, the optimization problem becomes: 1 VC-dimension (Vapnik, 1995) or the Rademacher bound (Bartlett and Mendelson, 2003) consider wt+1 = arg min w ||w − wt||2 + C ξi (2) measures of complexity, in addition to the empir ical performance, when describing future predic s.t. δs(xi, y+, y−) ≥ ∆i(y−) − ∆i(y+) − ξi where δs(xi, y+, y−)=s(xi, y+, d+)-s(xi, y−, d−) This leads to a variant of the structured ramp loss to be optimized: = tive ability.</S>
			<S sid ="50" ssid = "4">The measures of complexity usually take the form of some value on the radius of the data, such as the ratio of the radius of the data to the margin (Shivaswamy and Jebara, 2009a).</S>
			<S sid ="51" ssid = "5">As radius is a way of measuring spread in any projection direction, here we will specifically be in − max (y+ ,d+ )∈Y(xi ),D(xi ) + max (y−,d−)∈Y(xi ),D(xi ) s(xi, y+, d+) − ∆i(y+) s(xi, y−, d−) + ∆i(y−) (3) terested in the the spread of the data as measured after the projection defined by the learned model w. More formally, the spread is the distance between y+, and the worst candidate The passive-aggressive update (Crammer et al., (yw , dw ) ← arg min(y,d) (xi ),D (xi ) s(xi, y, d),2006), which is used to solve this problem, up dates w on each round such that the score of the correct hypothesis y+ is greater than the score of the incorrect y− by a margin at least as large as the cost incurred by predicting the incorrect hypothesis, while keeping the change to w small.</S>
			<S sid ="52" ssid = "6">after projecting both onto the line defined by the weight vector w. For each y1, this projection is conveniently given by s(xi, y1, d), thus the spread is calculated as δs(xi, y+, yw ).</S>
			<S sid ="53" ssid = "7">RMM was introduced as a generalization over SVM that incorporates both the margin constraint and information regarding the spread of the data.</S>
			<S sid ="54" ssid = "8">The relative margin is the ratio of the absolute, or maximum margin, to the spread of the projected data.</S>
			<S sid ="55" ssid = "9">Thus, the RMM learns a large margin solution relative to the spread of the data, or in other words, creates a max margin while simultaneously bounding the spread of the projected data.</S>
			<S sid ="56" ssid = "10">As a concrete example, consider the plot shown in Figure 1(a), with hypotheses represented by two-dimensional feature vectors.</S>
			<S sid ="57" ssid = "11">The point marked with a circle in the upper right represents f (xi, y+), while all other squares represent alter native incorrect hypotheses f (xi, y1).</S>
			<S sid ="58" ssid = "12">The large margin decision boundary is shown with a darker solid line, while the relative margin solution is shown with a darker dotted line.</S>
			<S sid ="59" ssid = "13">The lighter lines parallel to each define the margins, with the square at the intersection being f (xi, y−).</S>
			<S sid ="60" ssid = "14">The bottom portion of Figure 1(b) presents an alternative view of each solution, showing the projections of the cient optimization procedure that does not require batch training or an off-the-shelf QP solver.</S>
			<S sid ="61" ssid = "15">3.2 RM Algorithm.</S>
			<S sid ="62" ssid = "16">We address the above-mentioned limitations by introducing a novel online learning algorithm for relative margin maximization, RM.</S>
			<S sid ="63" ssid = "17">The relative margin solution is obtained by maximizing the same margin as Equation (2), but now with respect to the distance between y+, and the worst candidate yw . Thus, the relative margin dictates trading-off between a large margin as before, and a small spread of the projection, in other words, bounding the distance between y+ and yw . The additional computation required, namely, obtaining yw , is efficient to perform, and has likely already happened while obtaining the k-best derivations necessary for the margin update.</S>
			<S sid ="64" ssid = "18">The online latent structured soft relative margin optimization problem is then: hypotheses given the learned model of each.</S>
			<S sid ="65" ssid = "19">Notice that with a large margin solution, although the wt+1 = arg min w ||w − wt||2 + C ξi + Dτi distance between y+ and y− is greater, the points are highly spread, extending far to the left of the decision boundary.</S>
			<S sid ="66" ssid = "20">In contrast, with a relative margin, although we have a smaller absolute margin, the spread is smaller, all points being within a smaller distance E of the decision boundary.</S>
			<S sid ="67" ssid = "21">The higher the spread of the projection, the higher the variance of the projected points, and the greater the likelihood that we will mislabel a new instance, since the high variance projections may cross the learned decision boundary.</S>
			<S sid ="68" ssid = "22">In higher dimensions, accounting for the spread becomes even more crucial, as will be discussed in Section 6.3 Although RMM is theoretically well-founded and improves practical performance over large s.t.: δs(xi, y+, y−) ≥ ∆i(y−) − ∆i(y+) − ξi − B − τi ≤ δs(xi, y+, yw ) ≤ B + τi (4) where additional bounding constraints are added to the usual margin constraints in order to contain the spread by bounding the difference in projections.</S>
			<S sid ="69" ssid = "23">B is an additional parameter; it controls the spread, trading off between margin maximization and spread minimization.</S>
			<S sid ="70" ssid = "24">Notice that when B → ∞, the bounding constraints disappear, andwe are left with the original problem in Equa tion (2).</S>
			<S sid ="71" ssid = "25">D, which plays an analogous role to C , allows penalized violations of the bounding constraints.</S>
			<S sid ="72" ssid = "26">The dual of Equation (4) can be derived as: margin learning in the settings where it was introduced, it is unsuitable for most complex structured max = α,β,β∗ y∈Y (xi ) I αy − B y∈Y (xi ) βy − B ∗ y∈Y (xi ) prediction in NLP.</S>
			<S sid ="73" ssid = "27">Nonetheless, since structured RMM is a generalization of Structured SVM, − 1 α ω (y+ , y) − 2 y i y∈Y (xi ) βy ωi (y+ , y) y∈Y (xi ) which shares its underlying objective with MIRA, our intuition is that SMT should be able to benefit + β∗ω (y+ , y), y i y∈Y (xi ) as well.</S>
			<S sid ="74" ssid = "28">But to take advantage of the second-order information RMM utilizes for increased general y ∈Y (xj ) αy ωj (y , y ) − y ∈Y (xj ) βy ωj (y , y )izability in SMT, we need a computationally effi 3 The motivation of confidence-weighted estimation (Dredze and Crammer, 2008) and AROW (Crammer et al., 2009a) is related in spirit.</S>
			<S sid ="75" ssid = "29">They use second-order + y ∈Y (xj ) y ωj (y \ , y ) (5) information in the form of a distribution over weights to change the maximum margin solution.</S>
			<S sid ="76" ssid = "30">where the α Lagrange multiplier corresponds to the standard margin constraint, while β and β∗ each correspond to a bounding constraint, and ωi(y+, y1) corresponds to the difference of Algorithm 2 RM update with α, β, β∗ 1: procedure OPTIMIZE(w, S1 , S2 , S3 , C, B)f (xi, y+, d+) and f (xi, y1, d1).</S>
			<S sid ="77" ssid = "31">The weight up 2: while w changes do i i i 3: if 1 &gt; 1 then date can then be obtained from the dual variables: 4: UPDATEMARGIN(w, S1 , C ) 5: end if 2 y ω (y+ , y) 6: if Si &gt; 1 then αy ωi (y+ , y) − βy ωi (y+ , y) + β∗ i 7: UPDATEUPPERBOUND(w, S2 , B) (6) 8: end if 9: if S3 &gt; 1 then iThe dual in Equation (5) can be optimized us 10: UPDATELOWERBOUND(w, S3 , B) ing a cutting plane algorithm, an effective method for solving a relaxed optimization problem in the dual, used in Structured SVM, MIRA, and 11: end if 12: end while 13: end procedure 14: procedure UPDATEMARGIN(w, S1 , C ) RMM (Tsochantaridis et al., 2004; Chiang, 2012; 15: αy ← 0 for all y ∈ 1 16: αy+ ← C Shivaswamy and Jebara, 2009a).</S>
			<S sid ="78" ssid = "32">The cutting plane presented in Alg.</S>
			<S sid ="79" ssid = "33">1 decomposes the overall problem into subproblems which are solved independently by creating working sets Sj , which correspond to the largest violations of either the margin constraint, or bounding constraints, and iteratively satisfying the constraints in each set.</S>
			<S sid ="80" ssid = "34">The cutting plane in Alg.</S>
			<S sid ="81" ssid = "35">1 makes use of the i 17: for n ← 1...M axI ter do 18: Select two constraints y, y from S1 i 19: ∆i (y )−∆i (y)−δs(xi , y, y ) ||ω(y,y )||2 20: γα ← max(−αy , min(αy , γα )) 21: αy ← αy + γα ; αy ← αy − γα 22: w ← w + γα (ω(y, y )) 23: end for 24: end procedure 25: procedure UPDATEUPPERBOUND(w, S2 , B) 26: βy ← 0 for all y ∈ S2the closed-form gradient-based updates we de 27: for n ← 1...M axI ter do rived for RM presented in Alg.</S>
			<S sid ="82" ssid = "36">2.</S>
			<S sid ="83" ssid = "37">The updates 28: Select one constraint y from S2 amount to performing a subgradient descent step to update w in accordance with the constraints.</S>
			<S sid ="84" ssid = "38">29: γβ ← max(0, 30: βy ← βy + γβ B−δs(xi ,y+ ,y) ||ω(y+ ,y)||2 Since the constraint matrix of the dual program is not strictly decomposable across constraint types, we are in effect solving an approximation of the 31: w ← w − γβ (ω(y+ , y)) 32: end for 33: end procedure 34: procedure UPDATELOWERBOUND(w, S3 , B) β∗ 3 35: y ← 0 for all y ∈ Si original problem.</S>
			<S sid ="85" ssid = "39">36: for n ← 1...M axI ter do 37: Select one constraint y from S3 Algorithm 1 RM Cutting Plane Algorithm 38: γβ∗ ← max(0, −B−δs(xi ,y ,y) ) (adapted from (Shivaswamy and Jebara, 2009a)) 39: β∗ ∗ β∗ ||ω(y+ ,y)||2 Require: ith training example (xi , yi ), weight w, margin reg.</S>
			<S sid ="86" ssid = "40">C , bound B, bound reg.</S>
			<S sid ="87" ssid = "41">D, , B 1: S1 ← y+ , S2 ← y+ , S3 ← y+ y ← βy + γ 40: w ← w + γβ∗ (ω(y+ 41: end for 42: end procedure , y)) i i i 2: repeat 3: H (y) := ∆i (y) − ∆i (y+ ) − δs(xi , y+ , y) 4: y1 ← arg maxy∈Y (xi ) H (y) 5: y2 ← arg maxy∈Y (xi ) G(y) := δs(xi , y 6: y3 ← arg miny∈Y (xi ) −G(y) 7: ξ ← max {0, maxy∈Si H (y)} 8: V1 ← H (y1 ) − ξ − 9: V2 ← G(y2 ) − B − B 10: V3 ← −G(y3 ) − B − B 11: j ← arg maxj ∈{1,2,3} Vj 12: if Vj &gt; 0 then 13: Sj ← Sj ∪ {yj } , y) each set, if there is one, and perform the corre sponding parameter updates in Alg.</S>
			<S sid ="88" ssid = "42">2.</S>
			<S sid ="89" ssid = "43">We refer to the resulting passive aggressive algorithm as RM-PA, and the cutting plane version as RMCP.</S>
			<S sid ="90" ssid = "44">Preliminary experiments showed that RM PA performs on par with RMCP, thus RM-PA is the one used in the empirical evaluation below.</S>
			<S sid ="91" ssid = "45">i i 14: OPTIMIZE( 15: end if w, S1 , S2 , S3 , C, B) r&gt; see Alg.</S>
			<S sid ="92" ssid = "46">2 i i i A graphical depiction of the passive-aggressive RM update is presented in Figure 2.</S>
			<S sid ="93" ssid = "47">The upper.</S>
			<S sid ="94" ssid = "48">16: until S1 , S2 , S3 do not change i i i Alternatively, we could utilize a passive- aggressive updating strategy (Crammer et al., 2006), which would simply bypass the cutting plane and select the most violated constraint for right circle represents y+, while all other squares represent alternative hypotheses y1.</S>
			<S sid ="95" ssid = "49">As in the stan dard MIRA solution, we select the maximum margin constraint violator, y−, shown as the triangle, and update such that the margin is greater than the cost.</S>
			<S sid ="96" ssid = "50">Additionally, we select the maximum bound dist &gt; B Bounding Constraint dist B cost &gt; mar gin margin cost English side of the corpus with additional words from non-NYT and non LAT, randomly selected portions of the Gigaword v4 corpus, using modified KneserNey smoothing (Chen and Goodman, 1996).</S>
			<S sid ="97" ssid = "51">We used cdec (Dyer et al., 2010) as our hierarchical phrase based decoder, and tuned the parameters of the system to optimize BLEU (Pap- ineni et al., 2002) on the NIST MT06 corpus.</S>
			<S sid ="98" ssid = "52">We applied several competitive optimizers as Margin Constraint Model Score Figure 2: RM update with margin and bounding constraints.</S>
			<S sid ="99" ssid = "53">The diagonal dotted line depicts cost–margin equilibrium.</S>
			<S sid ="100" ssid = "54">The vertical gray dotted line depicts the bound B. White arrows indicate updates triggered by constraint violations.</S>
			<S sid ="101" ssid = "55">Squares are data points in the k-best list not selected for update in this round.</S>
			<S sid ="102" ssid = "56">task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).</S>
			<S sid ="103" ssid = "57">The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k best aggregation across iterations.</S>
			<S sid ="104" ssid = "58">RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers.</S>
			<S sid ="105" ssid = "59">MIRA and RM were run with 15 paral Donald et al., 2010).</S>
			<S sid ="106" ssid = "60">All optimizers were implemented in cdec and use the same system config ArEn MT05 1056 36k 33k uration, thus the only indepen dent variable is the MT08 1360 51k 45k 4-gram LM 24M 600M – Table 1: Corpus statistics ing constraint violator, yw , shown as the upside- down triangle, and update so the distance from y+ is no greater than B.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
			<S sid ="107" ssid = "1">4.1 Setup.</S>
			<S sid ="108" ssid = "2">To evaluate the advantage of explicitly accounting for the spread of the data, we conducted several experiments on two ChineseEnglish translation test sets, using two different feature sets in each.</S>
			<S sid ="109" ssid = "3">For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al., 2005).</S>
			<S sid ="110" ssid = "4">The data statistics are summarized in the top half of Table 1.</S>
			<S sid ="111" ssid = "5">The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003).</S>
			<S sid ="112" ssid = "6">We trained a 4-gram LM on the optimizer itself.</S>
			<S sid ="113" ssid = "7">We set C to 0.01, and M axI ter to 100.</S>
			<S sid ="114" ssid = "8">We selected the bound step size D, based on performance on a held-out dev set, to be 0.01 for the basic feature set and 0.1 for the sparse feature set.</S>
			<S sid ="115" ssid = "9">The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost ∆i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.</S>
			<S sid ="116" ssid = "10">All results are averaged over 3 runs.</S>
			<S sid ="117" ssid = "11">4.2 Feature.</S>
			<S sid ="118" ssid = "12">Sets We experimented with a small (basic) feature set, and a large (sparse) feature set.</S>
			<S sid ="119" ssid = "13">For the small feature set, we use 14 features, including a language model, 5 translation model features, penalties for unknown words, the glue rule, and rule arity.</S>
			<S sid ="120" ssid = "14">For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.</S>
			<S sid ="121" ssid = "15">We explored alternative values for B, as well as scaling it by the current candidate’s cost, and found that the optimizer is fairly insensitive to these changes, resulting in only minor differences in BLEU.</S>
			<S sid ="122" ssid = "16">Optimizer Zh Ar MIRA 35k 37k PRO 95k 115k RAMPION 22k 24k RM 30k 32k Active+Inactive 3.4M 4.9M Table 2: Active sparse feature templates abe et al., 2007; Simianer et al., 2012).</S>
			<S sid ="123" ssid = "17">Non-lexical features include structural distortion, which captures the dependence between reordering and the size of a filler, and rule shape, which bins grammar rules by their sequence of terminals and nonterminals (Chiang et al., 2008).</S>
			<S sid ="124" ssid = "18">Lexical features on rules include rule ID, which fires on a specific grammar rule.</S>
			<S sid ="125" ssid = "19">We also introduce context-dependent lexical features for the 300 most frequent aligned word pairs (f ,e) in the training corpus, which fire on triples (f ,e,f+1) and (f ,e,f−1), capturing when we see f aligned to e, with f+1 and f−1 occurring to the right or left of f , respectively.</S>
			<S sid ="126" ssid = "20">All other words fall into the default (unk) feature bin.</S>
			<S sid ="127" ssid = "21">In addition, we have insertion and deletion features for the 150 most frequently unaligned target and source words.</S>
			<S sid ="128" ssid = "22">These feature templates resulted in a total of 3.4 million possible features, of which only a fraction were active for the respective tuning set and optimizer, as shown in Table 2.</S>
			<S sid ="129" ssid = "23">4.3 Results.</S>
			<S sid ="130" ssid = "24">As can be seen from the results in Table 3, our RM method was the best performer in all ChineseEnglish tests according to all measures – up to 1.9 BLEU and 6.6 TER over MIRA – even though we only optimized for BLEU.5 Surprisingly, it seems that MIRA did not benefit as much from the sparse features as RM.</S>
			<S sid ="131" ssid = "25">The results are especially notable for the basic feature setting – up to 1.2 BLEU and 4.6 TER improvement over MERT – since MERT.</S>
			<S sid ="132" ssid = "26">has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al., 2008).</S>
			<S sid ="133" ssid = "27">For the tuning set, the decoder performance was consistently the lowest with RM, compared to the 5 In the small feature set RAMPION yielded similar best BLEU scores, but worse TER.</S>
			<S sid ="134" ssid = "28">In preliminary experiments with a smaller trigram LM, our RM method consistently yielded the highest scores in all ChineseEnglish tests – up to 1.6 BLEU and 6.4 TER from MIRA, the second best performer.</S>
			<S sid ="135" ssid = "29">other optimizers.</S>
			<S sid ="136" ssid = "30">We believe this is due to the RM bounding constraint being more resistant to overfitting the training data, and thus allowing for improved generalization.</S>
			<S sid ="137" ssid = "31">Conversely, while PRO had the second lowest tuning scores, it seemed to display signs of underfitting in the basic and large feature settings.</S>
	</SECTION>
	<SECTION title="Additional Experiments. " number = "5">
			<S sid ="138" ssid = "1">In order to explore the applicability of our approach to a wider range of languages, we also evaluated its performance on ArabicEnglish translation.</S>
			<S sid ="139" ssid = "2">All experimental details were the same as above, except those noted below.</S>
			<S sid ="140" ssid = "3">For training, we used the non-UN portion of the NIST training corpora, which was segmented using an HMM segmenter (Lee et al., 2003).</S>
			<S sid ="141" ssid = "4">Dataset statistics are given in the bottom part of Table 1.</S>
			<S sid ="142" ssid = "5">The sparse feature templates resulted here in a total of 4.9 million possible features, of which again only a fraction were active, as shown in Table 2.</S>
			<S sid ="143" ssid = "6">As can be seen in Table 4, in the smaller feature set, RM and MERT were the best performers, with the exception that on MT08, MIRA yielded somewhat better (+0.7) BLEU but a somewhat worse (-0.9) TER score than RM.</S>
			<S sid ="144" ssid = "7">On the large feature set, RM is again the best performer, except, perhaps, a tied BLEU score with MIRA on MT08, but with a clear 1.8 TER gain.</S>
			<S sid ="145" ssid = "8">In both ArabicEnglish feature sets, MIRA seems to take the second place, while RAMPION lags behind, unlike in ChineseEnglish (§4).6 Interestingly, RM achieved substantially higher BLEU precision scores in all tests for both language pairs.</S>
			<S sid ="146" ssid = "9">However, this was also usually coupled had a higher brevity penalty (BP) than MIRA, with the BP increasing slightly when moving to the sparse setting.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "6">
			<S sid ="147" ssid = "1">The trend of the results, summarized as RM gain over other optimizers averaged over all test sets, is presented in Table 5.</S>
			<S sid ="148" ssid = "2">RM shows clear advantage in both basic and sparse feature sets, over all other state-of-the-art optimizers.</S>
			<S sid ="149" ssid = "3">The RM gains are notably higher in the large feature set, which we take 6 In our preliminary experiments with the smaller trigram LM, MERT did better on MT05 in the smaller feature set, and MIRA had a small advantage in two cases.</S>
			<S sid ="150" ssid = "4">RAMPION performed similarly to RM on the smaller feature set.</S>
			<S sid ="151" ssid = "5">RM’s loss was only up to 0.8 BLEU (0.7 TER) from MERT or MIRA, while its gains were up to 1.7 BLEU and 2.1 TER over MIRA.</S>
			<S sid ="152" ssid = "6">O pt i m iz er S m all (b as ic) fe at ur e se t La rg e (s pa rs e) fe at ur e set T u n e M T 0 3 M T 0 5 T u n e M T 0 3 M T 0 5 ↑ B LE U ↑ B LE U ↓T ER ↑ B LE U ↓T ER ↑ B LE U ↑ B LE U ↓T ER ↑ B LE U ↓T ER M E R T 3 5 . 4 3 5 . 8 60.8 3 2 . 4 63.9 M I R A 3 5 . 5 3 5 . 8 61.1 3 2 . 1 64.6 3 6 . 6 3 5 . 9 60.6 3 2 . 1 64.1 P R O 3 4 . 1 3 6 . 0 60.2 3 1 .</S>
	</SECTION>
	<SECTION title="63.4" number = "7">
			<S sid ="153" ssid = "1">3 5 . 7 3 4 .</S>
	</SECTION>
	<SECTION title="56.1" number = "8">
			<S sid ="154" ssid = "1">3 1 . 4 59.1 R A M PI O N 3 5 . 1 3 6 . 5 58.6 3 3 . 0 61.3 3 6 . 7 3 6 .</S>
	</SECTION>
	<SECTION title="57.7" number = "9">
			<S sid ="155" ssid = "1">3 3 . 3 60.6 R M 3 1 . 3 3 6 . 5 56.4 3 3 . 6 59.3 3 3 . 2 3 7 . 5 54.6 3 4 . 0 57.5 Table 3: Performance on ZhEn with basic (left) and sparse (right) feature sets on MT03 and MT05.</S>
			<S sid ="156" ssid = "2">O pt i m iz er S m all (b as ic) fe at ur e se t La rg e (s pa rs e) fe at ur e set T u n e M T 0 5 M T 0 8 T u n e M T 0 5 M T 0 8 ↑ B LE U ↑ B LE U ↓T ER ↑ B LE U ↓T ER ↑ B LE U ↑ B LE U ↓T ER ↑ B LE U ↓T ER M E R T 4 3 . 8 5 3 . 3 40.2 4 1 . 0 50.7 M I R A 4 3 . 0 5 2 . 8 40.8 4 1 . 3 50.6 4 4 . 4 5 3 . 4 40.1 4 1 . 8 50.2 P R O 4 1 . 5 5 1 . 3 41.5 3 9 . 4 51.5 4 6 . 8 5 3 . 2 40.0 4 1 . 4 49.7 R A M PI O N 4 2 . 4 5 2 . 0 40.8 4 0 . 0 50.8 4 4 . 6 5 2 . 9 40.4 4 1 . 0 50.4 R M 3 8 . 5 5 3 . 3 39.8 4 0 . 6 49.7 4 3 . 0 5 5 . 3 37.5 4 1 . 8 48.4 Table 4: Performance on ArEn with basic (left) and sparse (right) feature sets on MT05 and MT08.</S>
			<S sid ="157" ssid = "3">O pt i m iz er S m a l l s e t B L E U T E R L a r g e s e t B L E U T E R M E R T 0 . 4 2.6 M I R A 0 . 5 3.0 1 . 4 4.3 P R O 1 . 4 2.9 2 . 0 1.7 R A M PI O N 0 . 6 1.6 1 . 2 2.8 Table 5: RM gain over other optimizers averaged over all test sets.</S>
			<S sid ="158" ssid = "4">as an indication for the importance of bounding the spread.</S>
			<S sid ="159" ssid = "5">Spread analysis: For RM, the average spread of the projected data in the ChineseEnglish small feature set was 0.9±3.6 for all tuning iterations,and 0.7±2.9 for the iteration with the highest de coder performance.</S>
			<S sid ="160" ssid = "6">In comparison, the spread of the data for MIRA was 5.9±20.5 for the best it eration.</S>
			<S sid ="161" ssid = "7">In the sparse setting, RM had an average spread of 0.9±2.4 for the best iteration, while MIRA had a spread of 14.0±31.1.</S>
			<S sid ="162" ssid = "8">Similarly, on ArabicEnglish, RM had a spread of 0.7±2.4 in the small setting, and 0.82±1.4 in the sparse setting, while MIRA’s spread was 9.4±26.8 and11.4±22.1, for the small and sparse settings, re spectively.</S>
			<S sid ="163" ssid = "9">Notice that the average spread for RM stays about the same when moving to higher dimensions, with the variance decreasing in both cases.</S>
			<S sid ="164" ssid = "10">For MIRA, however, the average spread increases in both cases, with the variance being much higher than RM.</S>
			<S sid ="165" ssid = "11">For instance, observe that the spread of MIRA on Chinese grows from 5.9 to 14.0 in the sparse feature setting.</S>
			<S sid ="166" ssid = "12">While bounding the spread is useful in the low-dimensional setting (0.71.5 BLEU gain with RM over MIRA as shown in Table 3), accounting for the spread is even more crucial with sparse features, where MIRA gains only up to 0.1 BLEU, while RM gains 1 BLEU.</S>
			<S sid ="167" ssid = "13">These results support the claim that our imposed bound B indeed helps decrease the spread, and that, in turn, lower spread yields better generalization performance.</S>
			<S sid ="168" ssid = "14">Error Analysis: The inconclusive advantage of RM over MIRA (in BLEU vs. TER scores) on ArabicEnglish MT08 calls for a closer look.</S>
			<S sid ="169" ssid = "15">Therefore we conducted a coarse error analysis on 15 randomly selected sentences from MERT, RMM and MIRA, with basic and sparse feature settings for the latter two.</S>
			<S sid ="170" ssid = "16">This sample yielded 450 data points for analysis: output of the 5 conditions on 15 sentences scored in 6 violation categories.</S>
			<S sid ="171" ssid = "17">The categories were: function word drop, content word drop, syntactic error (with a reasonable meaning), semantic error (regardless of syntax), word order issues, and function word mis- translation and “hallucination”.</S>
			<S sid ="172" ssid = "18">The purpose of this analysis was to get a qualitative feel for the output of each model, and a better idea as to why we obtained performance improvements.</S>
			<S sid ="173" ssid = "19">RM no ticeably had more word order and excess/wrong function word issues in the basic feature setting than any optimizer.</S>
			<S sid ="174" ssid = "20">However, RM seemed to benefit the most from the sparse features, as its bad word order rate dropped close to MIRA, and its excess/wrong function word rate dropped below that of MIRA with sparse features (MIRA’s rate actually doubled from its basic feature set).</S>
			<S sid ="175" ssid = "21">We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al.</S>
			<S sid ="176" ssid = "22">(2008).</S>
			<S sid ="177" ssid = "23">This correlates with our observation that RM’s overall BLEU score is negatively impacted by the BP, as the BLEU precision scores are noticeably higher.</S>
			<S sid ="178" ssid = "24">K-best: RM is potentially more sensitive to the size and order of the k-best list.</S>
			<S sid ="179" ssid = "25">While MIRA is only concerned with the margin between y+ and y−, RM also accounts for the distance between y+ and yw . It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selec tion, such as bold updating, local updating (Liang et al., 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact.</S>
			<S sid ="180" ssid = "26">Also, we only explored several settings of B, and there remains a continuum of RM solutions that trade off between margin and spread in different ways.</S>
			<S sid ="181" ssid = "27">Active features: Perhaps contrary to expectation, we did not see evidence of a correlation between the number of active features and optimizer performance.</S>
			<S sid ="182" ssid = "28">RAMPION, with the fewest features, is the closest performer to RM in Chinese, while MIRA, with a greater number, is the closest on Arabic.</S>
			<S sid ="183" ssid = "29">We also notice that while PRO had the lowest BLEU scores in Chinese, it was competitive in Arabic with the highest number of features.</S>
			<S sid ="184" ssid = "30">7 Conclusions and Future Work.</S>
			<S sid ="185" ssid = "31">We have introduced RM, a novel online margin- based algorithm designed for optimizing high- dimensional feature spaces, which introduces constraints into a large-margin optimizer that bound the spread of the projection of the data while maximizing the margin.</S>
			<S sid ="186" ssid = "32">The closed-form online update for our relative margin solution accounts for surrogate references and latent variables.</S>
			<S sid ="187" ssid = "33">Experimentation in statistical MT yielded significant improvements over several other state- of-the-art optimizers, especially in a high- dimensional feature space (up to 2 BLEU and 4.3 comparable performance according to two scoring methods in two language pairs, with two test sets each, in small and large feature settings.</S>
			<S sid ="188" ssid = "34">Moreover, across conditions, RM always yielded the best combined TERBLEU score.7 These improvements are achieved using standard, relatively small tuning sets, contrasted with improvements involving sparse features obtained using much larger tuning sets, on the order of hundreds of thousands of sentences (Liang et al., 2006a; Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012).</S>
			<S sid ="189" ssid = "35">Since our approach is complementary to scaling up the tuning data, in future work we intend to combine these two methods.</S>
			<S sid ="190" ssid = "36">In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al.</S>
			<S sid ="191" ssid = "37">(2008).</S>
			<S sid ="192" ssid = "38">Finally, although motivated by statistical machine translation, RM is a gradient-based method that can easily be applied to other problems.</S>
			<S sid ="193" ssid = "39">We plan to investigate its utility elsewhere in NLP (e.g. for parsing) as well as in other domains involving high-dimensional structured prediction.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="194" ssid = "40">We would like to thank Pannaga Shivaswamy for valuable discussions, and the anonymous reviewers for their comments.</S>
			<S sid ="195" ssid = "41">Vladimir Eidelman is supported by a National Defense Science and Engineering Graduate Fellowship.</S>
			<S sid ="196" ssid = "42">This work was also supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract HR001112-C-0015.</S>
	</SECTION>
</PAPER>
