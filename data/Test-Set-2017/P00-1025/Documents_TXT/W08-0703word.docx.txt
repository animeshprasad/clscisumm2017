A Bayesian  model of natural language  phonology:
generating alternations from underlying forms



David Ellis 
de@cs.brown.edu 
Brown University 
Providence, RI 02912








Abstract

A stochastic approach to learning phonology. 
The model presented captures 7-15% more 
phonologically plausible underlying forms 
than a simple majority solution, because it 
prefers “pure” alternations.  It could be use- 
ful in cases where an approximate solution is 
needed, or as a seed for more complex mod- 
els.  A similar process could be involved in 
some stages of child language acquisition; in 
particular, early learning of phonotactics.


1   Introduction

Sound changes in natural language, such as stem 
variation in inflected forms,  can be described as 
phonological processes.  These are governed by a 
constraint hierarchy as in Optimality Theory (OT), 
or by a set of ordered rules.   Both rely on a sin- 
gle lexical representation of each morpheme (i.e., its 
underlying form), and context-sensitive transforma- 
tions to surface forms. Phonological changes often 
affect segments near morpheme boundaries, but can 
also apply over an entire prosodic word, as in vowel 
harmony.
  It does not seem straightforward to incorporate 
context into a Bayesian model of phonology, al- 
though a clever solution may yet be found.   A 
standard way of incorporating conditioning envi- 
ronments is to treat them as factors in a Gibbs 
model (Liang and Klein, 2007), but such models 
require an explicit calculation of the partition func- 
tion. Unless the rule contexts possess some kind of 
locality, we don’t know how to compute this par- 
tition function efficiently.  Some context could be


captured by generating underlying phonemes from an 
n-gram model, or by annotating surface forms with 
neighborhood features. However, the effects of 
autosegmental phonology and other long-range de- 
pendencies (like vowel harmony) cannot be easily 
Bayesianized.


1.1   Related Work

In the last decade, finite-state approaches to phonol- 
ogy (Gildea and Jurafsky, 1996; Beesley and Kart- 
tunen, 2000) have effectively brought theoretical lin- 
guistic work on rewrite rules into the computational 
realm.   A finite-state approximation of optimality 
theory (Karttunen, 1998) was later refined into a 
compact treatment of gradient constraints (Gerde- 
mann and van Noord, 2000).
  Recent work on Bayesian models of morpholog- 
ical segmentation (Johnson et al., 2007) could be 
combined with phonological rule induction (Gold- 
water and Johnson,  2004) in a  variety of ways, 
some of which will be explored in our discussion 
of future work. Also, the Hierarchical Bayes Com- 
piler (Daume III, 2007) could be used to generate a 
model similar to the one presented here, but less con- 
strained1  which makes correspondingly more ran- 
dom, less accurate predictions.


1.2   Dataset

As we describe the model and its implementation in 
this and subsequent sections, we will refer to a sam-

  1 Recent updates to HBC, inspired by discussions with the 
author, have addressed some of these limitations.



12
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 12–19, 
Columbus, Ohio, USA June 2008. Qc 2008 Association for Computational Linguistics


ple dataset (in Figure 1), consisting of a paradigm2 
of verb stems and person/number suffixes.   The 
head of each row or column is an /underlying/ form, 
which in 3rd person singular is a phonologically null 
segment (represented as /ø/). In [surface] forms, the 
realization of each morpheme is affected by phono- 
logical processes. For example, in the combination 
of /tieta¨/ + /vat/, the result is [tieta¨+va¨t], where the
3rd person plural /a/ becomes [a¨] due to vowel har- 
mony.

1.3   Bayesian Approach

As a baseline model, we select the most frequently 
occurring allophone as the underlying form.   Our 
goal is to outperform this baseline using a Bayesian 
model. In other words, what patterns in phonologi- 
cal processes can be inferred with such a statistical 
model? This simple framework begins learning with 
the assumption that the underlying forms are faithful 
to the surface (i.e., without considering markedness 
or phonotactics).
  We model the generation of surface forms from 
underlying ones on the segmental (character) level. 
Input is an inflectional paradigm, with tokens of the 
form stem+suffix. Morphology is limited to a 
single suffix (no agglutination), and is already iden- 
tified. Each character of an underlying stem or suf- 
fix (ui ) generates surface characters (sij ) in an entire 
row or column of the input.
  To capture the phonology of a variety of lan- 
guages with a single model, we need constraints 
from linguistically plausible priors (universal gram- 
mar).  We prefer that underlying characters be pre- 
served in surface forms, especially when there is no 
alternation. It is also reasonable that there be fewer 
underlying forms (phonemes) than surface forms 
(phones, phonetic inventory), to account for allo- 
phones.  We expect to be able to capture a signifi- 
cant subset of phonological processes using a simple 
model (only faithfulness constraints).

1.4   Pure Generators

Our model has an advantage over the baseline in its 
preference for “purity” in underlying forms.  Each 
underlying segment should generate as few distinct

  2 The paradigm format lends itself to analysis of word types, 
but if supplemented with surface counts, can also handle tokens.


surface segments as possible:  if it generates non- 
alternating (identical) segments, it will be less likely 
to generate an alternation in addition.  This means 
that when two segments alternate, the underlying 
form should be the one that appears less frequently 
in other contexts, irrespective of the majority within 
the alternation.
  In the first stem of our Finnish verb conjugation 
(Figure 1), we see a [t,d] alternation (a case of con- 
sonant gradation), as well as unalternating [t]. If we 
isolate three of the surface forms where /tieta¨/ is in- 
flected (1st person singular, and 3rd person singular 
and plural), and consider only the dental segments in 
the stem of each, we have two underlying segments. 
Here, we use question marks to indicate unknown 
underlying segments.
/??/ [dt] [tt] [tt]

In this subset of the data, the reasonable candidate 
underlying forms are /t/ and /d/. These two compete 
to explain the observed data (surface forms). The na- 
ture of the prior probability distribution determines 
whether the majority is hypothesized for each under- 
lying form, so /t/ produces both alternating and unal- 
ternating surface segments, or /d/ is hypothesized as 
the source of the alternation (and /t/ remains “pure”). 
In a Bayesian setting, we impose a sparse prior over 
underlying forms conditioned on the surface forms 
they generate.
If u2 is hypothesized to be /t/, the posterior prob-
ability of u1 being /t/ goes down:

P (u1 = /t/|u2  = /t/) < P (u1 = /t/)

The probability of u1 being the competitor, /d/, cor- 
respondingly increases:

P (u1 = /d/|u2  = /t/) > P (u1 = /d/)

  Even though the majority in this case would be /t/, 
the favored candidate for the alternating form was
/d/.  This happened because of how we defined the 
model’s prior, in combination with the evidence that
/t/ (assigned to u2) generated the sequence of [t]. So
selection bias prefers /d/ as the source of an ambigu- 
ous segment, leaving /t/ to always generate itself.
  A similar effect can occur if there are both unal- 
ternating [t]’s and [d]’s on the surface, in addition to 
the [t,d] alternation. The candidate (/t/ or /d/) that is


❛
S











Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.




generating fewer unalternating segments is preferred 
to explain the alternation. For example, if there were
1000 cases of [t], 500 [d] and 500 [t,d], we would ex- 
pect the following hypotheses: /t/ → [t], /d/ → [d] 
and /d/ → [t, d].  This is because one of the two
candidates must be responsible for both unalternat- 
ing and alternating segments, but we prefer to have 
as much “‘purity” as possible, to minimize ambigu- 
ity.
  With this solution, we still have 1000 pure /t/ → 
[t], and only the 500 /d/ → [d] are now indistinct 
from /d/ → [t, d].  If we had selected /t/ as the
source of the alternation, there would be only 500 
remaining “pure” (/d/) segments, and 1500 ambigu- 
ous /t/.  Our Bayesian model should prefer the less


prior over underlying form encourages sparse solu- 
tions, so βu  < 1 for all u.  The prior over surface 
form given underlying encourages identity mapping,
/x/ → [x], so αxx   > 1, and discourages different 
segments, /x/ → [y], so αxy  < 1 for all x /= y.




                  β 
α
φ


θ


ambiguous (“purer”) solution, given an appropriate                          nc
prior.

2   Model

We will use boldface to indicate vectors, and sub- 
scripts to identify an element from a vector or ma- 
trix.	The variable N(u)  is a vector of observed 
counts with the current underlying form hypothe-
ses.  The notation we use for a vector u with one



u



 s mnu
nu


Figure 2: Bayesian network: α and β are vectors of hy-


element i removed is u−i,  so we can exclude the
counts associated with a particular underlying form



perparameters, and θi


(for i ∈   {1, . . . , nc}) and φ are



by indicating that in the parenthesized variable (i.e., 
N(u−4) is all the counts except those associated with 
the fourth underlying form). Ni (u) is the number of 
times character i is used as an underlying form, and 
Nij (u) is the number of times character i generated 
surface character j.
  The priors over surface s and underlying u seg- 
ments in Figure 2 are captured by Dirichlet priors 
α and β, which generate the multinomial distribu- 
tions θ and φ,  respectively (see Figure 3).   The


distributions. u is a vector of underlying forms, generated 
from φ, and si  (for i ∈  nu) is a set of observed surface
forms generated from the hidden variable ui according to
θi

  Phones and phonemes are drawn from a set of 
characters (e.g., IPA, unicode) C used to represent 
them.  φi  is the probability of a character (Ci  for
i ∈ nc) being an underlying form, irrespective of
current alignments or its position in the paradigm.
θij  is the conditional probability of a surface char-


θc	|	α 	∼	DIR(α), c = 1, . . . , nc
φ	|	β 	∼	DIR(β)
ui	|	φi	∼	MU LTI(φi), i = 1, . . . , nu
sij	|	ui , θui	∼	MU LTI(θ ui ), i = 1, . . . , nu,
j = 1, . . . , mi

Figure 3: Model parameters: nc is # different segments,
nu  is # underlying segments


acter (skn  = Cj  for j ∈ nc, n ∈ mk ) given the 
underlying character it is generated from (uk  = Ci
for i ∈ nc, k ∈ nu), which is determined by its po-
sition in the paradigm.
  In our Finnish example (Figure 1), if k = 1, we 
are looking at the first underlying character, which 
is /t/ (from /tieta¨/), so assuming our character set is 
the Finnish alphabet, of which ‘t’ is the 20th char-


put in the paradigm.  Rather than reestimate θ and 
φ at each iteration before sampling from u, we can 
marginalize these intermediate probability distribu- 
tions in order to ease implementation and speed con- 
vergence.
  Our search procedure tries to sample from the 
posterior probability, according to Bayes’ rule.
posterior ∝ likelihood ∗ prior
P (u, s|β, α) ∝ P (u|β)P (s, u|α)
Each of these probabilities is drawn from a Dirichlet 
distribution, which is defined in terms of the multi- 
variate Beta function, C . The prior β added to un- 
derlying counts N(u) forms the posterior Dirichlet
corresponding to P (u|β).   In P (s|u, α),  each αi
vector is supplemented by the observed counts of
(si).


acter, u1 = C20 = t. It generates the first character 
of each inflected form (1st, 2nd, 3rd person, singu-


(underlying, surface) pairs N


(β + N (u))



lar and plural) of that stem, so m1  = 6, and since 
there is no alternation s1n = t (for n ∈ {1, . . . , 6}).



             C P (u, s|β, α) =



C (β)


Given the phonologically plausible (gold) underly-


nc    C (αc +  



i:ui =c


N (si))


ing forms, the probability of /t/ is φ20 = 7/41.
On the other hand, k = 33 identifies the 3rd per-



c=1


C (α)


son singular /ø/, which inflects each of the seven 
stems, so m33   = 7.   Since we need our alpha- 
bet to identify a null character,  we’ll give it in- 
dex zero (i.e., u33  = C0  = ø).   For each of the 
(underlying, surface) alignments in this alternation 
(caused by vowel gemination), we can identify the 
probability in θ.  For 3rd person singular [tieta¨+a¨], 
where s33,1 = C28 = a¨, the conditional probability 
θ0,28 = 1/7.
The prior hyperparameters can be understood as
follows. As βi  gets smaller, an underlying form uk


The collapsed update procedure consists of re-
sampling each underlying form, u, incorporating the 
prior hyperparameters α, β and counts N over the 
rest of the dataset.  The relevant counts for a can- 
didate k being the underlying form ui  are Nk (u−i)
and Nksij (u−i) for j ∈ mi. P (ui  = k|u−i, α, β) is
proportional to the probability of generating ui  = k, 
given the other u−i  and all sij  (for j ∈ mi), given
s−i and u−i.


Nc(u−i ) + βc


is less likely to be Ci.  As αij  gets smaller, an un-


P (ui  = c|u−i, α, β) ∝


n − 1 + β•


derlying uk  = Ci  is less likely to generate a surface
segment skn  = Cj  ∀n ∈ mk .  In our experiments,


C (α +   i =    i:u


=c N (s! ) + N (si))



we will vary αi=j  (prior over identity map from un-


C (α +   i  =i:u


=c N (s! ))


derlying to surface) and αi =j .
  Our implementation of this model uses Gibbs 
sampling (c.f., (Bishop, 2006), pp 542-8), an algo- 
rithm that produces samples from the posterior dis- 
tribution. Each sample is an assignment of the hid- 
den variables, u (i.e., a set of hypothesized underly- 
ing forms). Our sampler initializes u from a uniform 
distribution over segments in the training data, and 
resamples underlying forms in a fixed order, as in-
  

Suppose we were updating this sampler running 
on the Finnish verb inflections. If we had all seg- 
ments as in Figure 1, but wanted to resample u31 (1st 
person singular /n/), we would consider the counts 
N excluding that form (i.e., under u−31).  The prior 
for /n/, β14, is fixed, and there are no other occur- 
rences, so N14(u−31)  = 0.  Another potential un- 
derlying form, like /t/, would have higher uncondi- 
tioned posterior probability, because of the counts


(7, in this case) added to its prior from β. Then, we 
have to multiply by the probability of each generated
surface segment (all are [n], so 7 ∗ P ([n]|c, α) for a
given hypothesis u31 = c).
We select a given character c ∈ C for u31 from a
distribution at random. Depending on the prior, /n/
will be the most likely choice, but other values are


3.2	Convergence

Testing convergence, we run again on the sample 
data (Figure 1), using αij   = 0.1 when i /= j and
10 when i = j and β = 0.1, starting from different 
initializations, we get the same solution.

6
x 10


still possible with smaller probability.  The counts 
used for the next resampling, N (u−31), are affected 
by this choice, because the new identity of u31 has 
contributed to the posterior distribution.  After un- 
bounded iterations, Gibbs sampling is guaranteed to 
converge and produce samples from the true poste- 
rior (Geman and Geman, 1984).

3   Evaluation

This model provides a language agnostic solution to 
a subset of phonological problems.   We will first 
examine performance on the sample Finnish data 
(from Figure 1), and then look more closely at the is- 
sue of convergence. Finally, we present results from 
larger corpora 3 .

3.1   Finnish

Output from a trial run on Finnish verbs (from Fig- 
ure 1) follows, with hyperparameters αij {100  ⇐⇒ 
i = j, 0.05 ⇐⇒ i /= j} and βi  = {0.1}.
  In the paradigm (a sample after 1000 iterations), 
each [sur+face] form is followed by its hypothesized
/under/ + /lying/ morphemes. 
[tieda¨+n] : /tieda¨/ + /n/
[tieda¨+t] : /tieda¨/ + /t/
[tieta¨+a¨] : /tieda¨/ + /a¨/ 
[tieda¨+mme] : /tieda¨/ + /mme/ 
[tieda¨+tte] : /tieda¨/ + /tte/ 
[tieta¨+va¨t] : /tieda¨/ + /va¨t/ 
[aiøo+n] : /aiøo/ + /n/
...
[pelka¨a¨+va¨t] : /pelka¨a¨/ + /vat/
  With strong enough priors (faithfulness con- 
straints), our sampler often selects the most com- 
mon surface form aligned with an underlying seg- 
ment.  Although [vat] is more common than [va¨t], 
we choose the latter as the purer underlying form. 
So /a/ is always [a], but /a¨/ can be either [a¨] or [a].

  3 2.8 million word types from Morphochallenge2007 (Ku- 
rimo et al., 2007)


3.05

3.04

3.03

3.02

3.01

3

2.99

2.98

2.97
0 	10 	20 	30 	40 	50 	60 	70 	80 	90 	100
Iteration

Figure 4: Posterior likelihood at each of the first 100 iter- 
ations, from 4 runs (with different random seeds) on 10% 
of the Morphochallenge dataset (αi/=j  = 0.001, αi=j  =
100, β = 0.1), indicating convergence within the first 15
iterations.

  To confirm that the sampler has converged, we 
output and plot trace statistics at each iteration, in- 
cluding marginal probability, log likelihood, and 
changes in underlying forms (i.e., variables resam- 
pled). If the sampler has converged, there should no 
longer be a trend (consistent slope) in any of these 
statistics (as in Figure 4).
  Examining the posterior probability of each se- 
lected underlying form reveals interesting patterns 
that also help explain the variation. In the above run, 
the ambiguous segments (with surface alternations) 
were drawn from the distributions (with improbable 
segments elided) in Figure 5.
  We expect this model to maximize the probabil- 
ity of either the “majority” solution or a solution 
demonstrating selection bias.   We compare likeli- 
hood of the posterior sample with that of a “phono- 
logically plausible” solution (in which underlying 
forms are determined by referring to formal lin- 
guistic accounts of phonological derivation) and a 
“majority solution” (see Figure 6 for a log-log plot, 
where lower is more likely).
  The posterior sample has optimal likelihood with 
each parameter setting, as expected.  The majority 
parse is selected with αi=j   = 0.5 With lower val- 
ues of αi=j , the “phonologically plausible” parse is


u4=/d/ s4=[d,d,t,d,d,t]
P (ui  = c)	≈
d	0.99968
         t 0.00014 
u8=/k/ s8 =[ø,ø,k,ø,ø,k] 
(same behavior as u12)
P (ui  = c)	≈
ø	0.642
k	0.124
u33=/e/ s33=[a¨,o,e,u,ø,e,ø]
P 
(ui  
= 
c)
≈
a
¨
,
o
,
u
0.0
02
9
ø
0.2
15
a
0.0
00
3
e
0.2
97

Figure 5: Resampling probabilities for alternations, after
1000 iterations.



posterior  sample 
majority solution 
phonologically plausible


4.26
10




4.25
10




M
ajo
rit
y
Ba
ye
sia
n
ty
p
e
s
5
0
.
8
4
6
9
.
5
3
tok
en
s
6
5
.
2
3
7
2
.
1
1

Figure 7: Accuracy of underlying segment hypotheses.


notated morphemes.
  For example, the word ajavalle is listed in the cor- 
pus as follows:
ajavalle	aja:ajaa|V	va:PCP1	lle:ALL	The
word is segmented into a verb stem, ‘aja’ (drive), 
a present participle marker ‘va’, and the allative suf- 
fix (“for”). Each surface realization of a given mor- 
pheme is identified by the same tag (e.g., PCP1). 
However, in this corpus, insertion and deletion are 
not explicitly marked (as they were in the paradigm, 
by ø).   Rather than introduce another component 
to determine which segments in the form were 
dropped, we ignore these cases.
  The sampling algorithm proceeds as described in 
section 2. To run on tokens (as opposed to types), we 
incorporate another input file that contains counts 
from the original text (ajavalle appeared 8 times). 
The counts of each morpheme’s surface forms then 
reflect the number of times that form appeared in any 
word in the corpus.






4.24
10
−2 	−1
10 	10
alpha


Figure 6: Parse likelihood


3.
3.
1    
T
y
p
e 
o
r 
T
o
k
e
n
0
In Finnish verb conjugation, 
3rd person (esp. sin-
gular) forms have high 
frequency and tend to be un- 
marked (i.e., closer to 
underlying).  In types, un- 
marked is a minority (one 
third), but incorporat- ing 
token frequency shifts that 
balance, benefiting


more likely than the majority.  However, the sam-
pler does not converge to this solution, because in 
this [t,d] alternation, the “phonologically plausible” 
solution identifies /t/, but neither selection bias nor 
majority rules would lead to that with the given data.

3.3    Morphologically segmented corpora

In our search for appropriate data for additional, 
larger-scale  experiments,  we  found  several  vi- 
able alternatives.   The correct morphological seg- 
mentations for Finnish data used in Morphochal- 
lenge2007 (Kurimo et al., 2007) provide a rich and 
varied set of words, and are readily analyzable by 
our sampler.  Rather than associating each surface 
form with a position in the paradigm, we use the an-


the “majority learner.” Among noun inflections, un- 
marked has higher frequency in speech, but marked 
tokens may still dominate in text. We might expect 
that it is easier to learn from tokens than types, in 
part because more data is often helpful.
Testing on  half  of  the  Morphochallenge 2007
Finnish data (1M word types,  5M morph types,
17.5M word tokens, 48M morph tokens), we ran 
both our Bayesian model and a majority solver on 
the morphological analyses, and compared against 
phonologically plausible (gold) underlying forms. 
Results are reported in Figure 7.
  The Bayesian estimate consistently outperformed 
the majority solution, and cases where the two differ 
could often be ascribed to the preference for “pure”


analyses.

4   Conclusion

We have described a model where surface forms 
are generated from underlying representations seg- 
ment by segment. Taking this approach allowed us 
to investigate the properties of a Bayesian statistical 
learner, and how these can be useful in the context 
of sound systems, a basic component of language. 
Experiments with our implementation of a collapsed 
sampler have produced results largely confirming 
our hypotheses.
  Without context, we can often learn about 60 to 80 
percent of the mapping from underlying phonemes 
to surface phones. Especially with lower values of 
αi=j , closer to 0, our model does prefer pure alter- 
nations.  Gibbs sampling tends to select the major- 
ity underlying form, particularly with αi=j relatively 
high, closer to 1. So, a sparser prior leads us further 
from the baseline, and often closer to a phonologi- 
cally plausible solution.

4.1   Directions

In future research, we hope to integrate morpholog- 
ical analysis into this sort of a treatment of phonol- 
ogy.  This is a natural approach for children learn- 
ing their first language.  They intuitively discover 
phonotactics, and how it affects the prosodic shape 
of each word, as they learn meaningful units and 
compose them together.  It is clear that many lay- 
ers of linguistic information interact in the early 
stages of child language acquisition (Demuth and 
Ellis, 2005 in press), so they should also interact 
in our models.   As discussed above, the present 
model should be applicable to analysis of language- 
learners’ speech errors, and this connection should 
be explored in greater depth.
  It might be interesting to predispose the sampler 
to select underlying forms from open syllables. That 
is,  set α  to increase the probability of matching 
one of the surface segments if its context (feature 
annotations) includes a vocalic segment or a word 
boundary immediately following.  The probability 
of phonological processes like assimilation could be 
similarly modeled, with the prior higher for choos- 
ing a segment that appears on the surface in a con- 
trastive context (where it shares few features with


neighboring segments).
  If we define a MaxEnt distribution over Optimal- 
ity Theoretic constraints, we might use that to in- 
form our selection of underlying forms.  In (Gold- 
water and Johnson, 2003), the learning algorithm 
was given a set of candidate surface forms asso- 
ciated with an underlying form, and tried to opti- 
mize the constraint weights. In addition to the con- 
straint weights, we must also optimize the underly- 
ing form, since our goal is to take as input only ob- 
servable data. Sampling from this type of complex 
distribution is quite difficult, but some approaches 
(e.g., (Murray et al., 2006)) may help reduce the in- 
tractability.


References

Kenneth R. Beesley and Lauri Karttunen. 2000.  Finite- 
state non-concatenative morphotactics. In Lauri Kart- 
tunen, Jason Eisner, and Alain The´riault, editors, SIG- 
PHON2000, August 6 2000. Proceedings of the Fifth 
Workshop of the ACL Special Interest Group in Com- 
putational Phonology., pages 1–12.
Christopher M. Bishop.  2006.  Pattern Recognition and
Machine Learning (Information Science and  Statis- 
tics). Springer, August.
Hal Daume III. 2007. Hbc: Hierarchical bayes compiler.
Katherine Demuth and David Ellis, 2005 (in press). Re-
visiting the acquisition of Sesotho noun class prefixes. 
Lawrence Erlbaum.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, gibbs distributions, and the bayesian restora- 
tion of images.  IEEE Trans. Pattern  Anal. Machine 
Intell., 6(6):721–741, Nov.
Dale Gerdemann and Gertjan van Noord. 2000. Approx-
  imation and exactness in finite state optimality theory. 
Daniel Gildea and Daniel Jurafsky. 1996. Learning bias 
and phonological-rule induction. Computational Lin-
guistics, 22(4):497–530.
Sharon Goldwater and Mark Johnson. 2003. Learning ot
  constraint rankings using a maximum entropy model. 
Sharon Goldwater and Mark Johnson.  2004.  Priors in
Bayesian learning of phonological rules. In Proceed- 
ings of the Seventh Meeting of the ACL Special Inter- 
est Group in Computational Phonology, pages 35–42, 
Barcelona, Spain, July. Association for Computational 
Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for spec- 
ifying compositional nonparametric Bayesian models. 
In B. Scho¨ lkopf, J. Platt, and T. Hoffman, editors, Ad- 
vances in Neural Information Processing Systems 19, 
pages 641–648. MIT Press, Cambridge, MA.


Lauri Karttunen. 1998. The proper treatment of optimal- 
ity in computational phonology.  In Lauri Karttunen, 
editor, FSMNLP’98: International Workshop on Fi- 
nite State Methods in Natural Language Processing, 
pages 1–12. Association for Computational Linguis- 
tics, Somerset, New Jersey.
Mikko  Kurimo,  Mathias  Creutz,  and  Ville  Turunen.
2007.   Overview of morpho challenge in clef 2007. 
In Working Notes for the CLEF 2007 Workshop, Bu- 
dapest, Hungary.
Percy Liang and Dan Klein. 2007. Tutorial 1: Bayesian
nonparametric structured models, June.
Iain Murray, Zoubin Ghahramani, and David MacKay.
2006. MCMC for doubly-intractable distributions. In
UAI. AUAI Press.

