Experiments with the Successor Variety Algorithm Using the  Cutoff and  Entropy Methods

Riyad  Al-Shalabi, Ghassan Kannan, Iyad Hilat,  Ahmad  Ababneh and Ahmad  Al-Zubi
Yarmouk  University, lrbid, Jordan

Abstract

In the present  study a system  have developed that uses the Successor Variety  Stemming  Algoritlnn 
to find   stems for Arabic words.  A corpus of  242 abstracts have obtained from the Saudi Arabian National 
Computer Conference. All of these  abstracts  involve  computer science  and  information systems.  The  study 
have set out to discover  whether the Successor  Variety  Stemming  Algorithm  technique with the Cutoff Method 
can be used for the Arabic Language or not. In addition, the Successor Variety Algorithrn   have compared with 
the  Cutoff and  the  Successor  Variety with Entropy  Method.  Stemming  is typically   used in the hope  of 
improving  the accuracy  of the search   reducing  the size of the index.  The results of  present research show that 
the  Successor  Variety Algorithm  with the Cutoff  Method  is better than Successor Variety Algorithm  with the 
Entropy Method. We have achieved  an 84% level of correctness using the Cutoff  Method,  but a 64% level of 
correctness using the Entropy  Method.  These experiments were carried out using Visual Basic 6.0.

Key words:  Successor, entropy, cutoff, stem, suffixes, prefixes 	



INTRODUCTION

    Word stemming  is an important feature supported by 
present  day indexing  and search systems.  The idea is to 
improve  recall by automatic  handling  of word endings  by 
reducing the  words  to  their  word  roots,  at  the  time  of 
indexing and searching.  Stemming  broadens  our results to 
include  both   word   roots   and  word   derivatives.  It is 
commonly accepted that removal  of word-endings 
(sometimes  called suffix stripping) is a good idea; removal 
of  prefixes   can  be  useful  in  some  subject  domains.   A 
stemming algorithm  is an algorithm  that converts  a word 
to  a  related   form.  One  of  the  simplest  such 
transformations is the conversion of plurals  to singulars. 
One example is Porter's Algorithm.  The Porter Stemmer  is 
a conflation stemmer  developed by Martin  Porter  at the 
University of Cambridge in 1980.   The Porter  stemming 
algorithm (or 'Porter  stemmer') is a process  for removing 
the commoner morphological  and inflexional endings from 
words in English. It is the most effective  and widely used. 
stemmer for English Porter's  Algorithm  works  based  on 
the number  of vowel characters that are followed  by a 
consonant character   in  the  stem.     This  number     (the 
Measure) must  be  greater  than  one  for  the  rule  to  be 
applied.  One of the limitations of this algorithm  is that it 
can only be applied to text in the English  Language.
Frequently, the user specifies  a word  in a query  but
only   a  variant   of  this  word   is  present   in  a  relevant 
document. This problem  can be partially  overcome  with 
the  substitution of  stems  for  the  words.  A  stem  is  the 
portion of a word that is left after the removal of its affixes


(i.e., prefixes and suffixes).  Stems are thought  to be useful 
for improving  retrieval  performance, because they reduce 
variants of the same root word to a common concept. 
Furthermore, stemming  has the  secondary  effect  of 
reducing the  size  of the  indexing  structure  because  the 
number  of  distinct  index  terms  is  reduced.   Many  Web 
search engines do not adopt any stemming algorithm 
whatsoever.    Frakes[ 1           distinguishes    four    types     of 
stemming  strategies:   affix  removal,   table   lookup, 
successor  variety   and  n-grams.   Table   lookup   consists 
simply of looking  for the stem of a word in a table.  Since 
such data is not readily available and might require 
considerable storage  space,  this  type  of  stemming 
algorithm may  not  be  practical.   Successor variety 
stemming is based on the determination of morpheme 
bmmdaries,  uses  knowledge from  structural linguistics 
and  is  more  complex  than  an  affix  removal-stemming
algorithm[2l.
    The goal  of this study  was to experiment with 
alternative  stemming     techniques  using   the  successor 
variety  approach.   We  all  agree   that  a    word   in  any 
language  consists  of a meaningful string   of letters.   An 
index    in any language  consists  of a number  of words  in 
a related domain.  A word may have nonstem  letters in the 
beginning (prefix), in the middle (infix), or at the end 
(postfix) of the word. From the information retrieval  point 
of  view  stemming  is one  technique  to  provide  ways  of 
finding  morphological variants  of search terms. It is used 
to improve  retrieval  effectiveness and to reduce  the size
of indexing files (Fig. 1 ).




Corresponding Author: Dr. Riyad Al-Shalabi, Yarrnouk University, Irbid, Jordan E-mail: shalabi@yu.edu.jo


----Conflation Methods



Inform. Technol. J., 4 (I): 55-
62, 2005

against  the  stemmer described  by Dawsod 
16l  usi





the




Manual




Automatic  
(stemmers
)


Cranfi
eld-I  
test 
collect
ion.  
Katzer  
et al. [i 
sJ 
examin
ed  the
perfor
mance 
of 
stemm
ed 
title-
abstrac
t term 
s 
against  
six 
other   
docum
ent   
represe
ntation
s.   
Karen  
et a/Y 
9l   did  
a 
throug
h   
review   
and   
study  
of   
stemmi
ng  
algorit
hms.
[
2
0
l


Aflx


Successor	Table


n-gram


Harman


used  three  
stemmers-Porter,  
Lovins  and  S






Longest 
match


Removal



Simple removal


Variety	look up


removal-
on three 
databases
-Cranfield  
1400, 
Medlars 
and
CACM   
and   
found  
that  none   
of   them   
sig 
nificantly 
improved 
retrieval 
effectivene
ss in a IR 
system 
called 
IRX that 
ranks its 
output in 
order of 
relevance.
    The  
latest  
natural  
lang uage  
research  
focuses  
on 
building    
systems  
for  the  
Arabic   
language  
with  
high


Fig. 1:  Taxonomy of stemming algorithms[3l

    We set out to test the  Successor Variety Algorithm, 
which determines word and morpheme boundaries based 
on the distribution of phonemes in a large body of 
utterances.   The  successor   variety  of  a   string  is  the 
number of different characters that follow it in words in 
some body of text.
    Successor variety stemmers[4l  are based on work in 
structural ling uistics,  which attempts to determine word 
and  morpheme  bmmdaries  based on the distribution  of 
phonemes  in a large body of utterances[5l_  A   stemming 
algorithm  is  a  computational   procedure  that  seeks  to 
reduce  all  words with  the  same  stem  to a  commod5l . 
Several algorithms have been developed to handle stems 
in E    lish. Dar wish[6l  presents a good technique  with an 
accuracy of92.7%  on 9,606  words. He stripped away the 
prefix   and   suffix.   Beesleyl7l     presents   a   finite-state 
morphological  analyzer  for Arabic,  which  displays  the 
root, pattern and prefixes/suffixes. The analyses are based 
on manually  acquired  lexicons and rules. Although  his 
analyzer is comprehensive  in the types of knowledge it 
presents,   it   has   been   criticized    for   its   extensive 
development   time  and   lack  of   robustness[6l.	Most 
stemmers  currently  in  use  are  iterative  longest  match
stemmers, a kind of affix removal stemmer first developed



-


performance and  results relevant to the user needs. This
gave rise to the idea of our project, to develop a system 
dealing  with Arabic  words. The system has  been built 
based  on the   Successor Variety  Stemming Algorithm, 
which determine the successor variety for a word, then 
uses  this information  to segment the word. We use the 
cutoff and entropy methods in our system to observe the 
effects of the successor variety approach on text in the 
Arabic L ang uage and to discover whether it can be useful 
for special purposes for Arabic language applications.

Successor variety algorithm: The successor variety of a 
string is the number of different characters that fo llow it 
in words in some body of text. The successor variety of 
substrings of a term will decrease as more characters are
added lllltil a segment boundary is reached[2l.
Successor variety  stemmers[4l     are based on work in
str uctural linguistics, which attempted to determine word 
and morpheme bolllldaries based on the distribution of 
phonemes in a large body of text. The stemming  method 
based on this work uses letters in place of phonemically 
transcribed utterances.
When this process is carried out using a large body
of text, Hafer and Weiss[4l  report 2000 terms to be a stable 
number), the successor variety of substrings of a term will 
decrease  as more characters are added until a segment


by Lovins[sJ.    Increasi


interest  in the  
development  of


bolllldary is reached. At 
this point, the successor 
variety


stemming algorithms has appeared for languages such as
the Arabic language.  The most notable of the efforts in
this context are those reported by many authors[5•9-12J_   In 
addition  to Lovins[sJ   , iterative  longest  match stemmers
have been reported by many authors[ 13  16l_
    There have been many experimental evaluations of 
stemmers. Salton and Ymmg[3l  examined the relative 
retrieval performance of fully stemmed terms against terms 
with  only  the  suffix "s"  removed.  Hafer  and Weiss[4l 
tested  their  stemmer  against  other  stemming  methods 
using  the  ADI  collection  and  the  Carolina  Population
Center (CFC) collection consisting  of 75 documents and 
five  queries.  Van  Rijsbergen[ 17l   tested their  stemmer[ 14l


will sharply increase. This information is used to find the
stem. When the successor varieties  for a given word have 
been derived, the information   must be used to segment 
the word. Hafer and Weiss[4l  discuss four ways of doing 
this:

1.		The cutoff  method  (this is the  method  we apply  in 
our work):
•      Some   cutoff   value   (Threshol d)  is   selected   for
successor varieties and a bolllld ary is reached.
•      The problem with this method is that if the threshold
value   selected   is too small, incorrect cuts will be 
made; if it is too large, correct cuts will missed.





2. 	Peak and plateau  method.
• 	A  segment  break  is  made  after  a  character  whose


 Table 1: Successor variety  stem process 	
 Prefix 	Successor variety 	Letters 	


successor variety  exceeds that of the character 
immediately  preceding   it  and  the  character 
immediately following  it.
• 	This method  does not suffer  from the problem  of the
cutoff  method.


R
RE REA READ READA READAB
READABL


3 	E,I, 0
2 	A.D
1 	D
3 	A, I, S
B L E


3. 	Complete  word method
• 	A break  is made  after a segment  if the segment  is a 
complete  word in the corpus.
4.	Entropy  method:  (this is  the other method  we use in 
our experiments)
• 	Takes    advantage   of  the  distribution of  successor
variety letters. The method  works as follows:
• 	Let  I                 D",l be  the  number  of  words  in  a text  body 
beginning  with the i length sequence  of letters a.
• 	Let   ID"''I   the  number   of  words   in  ID",I   with  the 
successor j computed in step one.
• 		The  probability that  a word  has  the  successor j is 
given by:


 READABLE 	(!ll>mk) 	

Peak and plateau method
•	break   at  the  character   whose  successor variety   is 
greater  than  both  its  preceding   and  following 
character READ I ABLE

Complete word method
•	break  is made  if the segment  is a complete  word  in 
the corpus (READ)

Entropy method
•	for i = 2, a = RE,  P"'I = 5
•	forj ='A', P",1l = 4
•	forj= 'D', P",1l = 1


• 	The entropy of  I         Dm I      is: I         Dm, I
IDm I
Hm = :f: _I Dm1 l·log) Dm1 I
IDml	IDml

• 		a cutoff value is selected  and a bormdary is identified 
whenever  the cutoff value reached

    A  set  of  the  above  measures   for  predecessors can 
also be defined similarly.

    In summary, the successor  variety stemming  process 
has 3 parts:
1. 	Determine  the successor varieties  of a word.
2. 	Use this information to segment  the word  using  one 
of the methods  above.
3. 	Select one of the segments  as a stem.
    The  aim  of  Hafer  and  Weiss[4 J       was  to  develop   a 
stemmer that required  little or no human  processing.

Full  English Example
Test Word:  READABLE
Corpus:   ABLE,  APE,  BEATABLE, FIXABLE, READ, 
READABLE,
READING, READS,  RED, ROPE, RIPE
The   successor  variety   stem   process   is  sho\Vll  m
Table 1.

Cutoff method
•	segment  when successor variety>= threshold
•	consider threshold2  RIEIADIABLE


•    Hotf  -1/5 * log2(1/5)- 4/5 * log2(4/5)0.46+0.26
0.72
This  value  is  low  because  "REA  .. " appears  four 
times!

Our  methodology Steps:  The  input  data  consists  of  a 
corpus of words  and a reverse  of each word.

Step  one:  Determine the successor varieties  for a word. 
Take a word  from the corpus and name it  FW ord and call 
the  reverse  RevWord:

1.     (For  Successor) Starting  from  ;1 to  length  of  the
FWord:
a. 	For the rightmost i letters inFWord:
1. 	Cormt  the  number  of letters  in the corpus  that 
follow the first ith right most letters of FW ord.
11. 	Store the successor value from the above step in 
a list called SucList.
2.     (For   Predecessor)  Starting   from  j length  of   the
RevWord:
a. 	For the rightmost}letters inRevWord:
1. 	Cormt  the  number   of  letters  in  the  Reversed 
corpus  that follow the firstjth right most letters 
ofRevWord.
11. 	Store  the Predecessor value from the above step 
in a list calledPrdList.

Step two: Use this information to segment the word using:
1.     Cutoff method
a. 	Segmentation Process  for Successor:





1. 	Set a variable namedSegSuc of type string.
n.    For each character  at position kin the Fword
1.  if the corresponding Successor  value for k is 
greater    than    or    equal    to    the  threshold 
value  (11)   then  add the contents  of SegSuc 
to the list SucSegList and empty  it.
2.  else add the character at position k to SegSuc.

111.     If the length of SegSuc  is greater  than zero, then 
add its contents  to SucSegList and empty it
b. 	Segmentation Process  for Predecessor:
1. 	Set a variable  named SegPre  of type string.
n.    For  each character  at position  f in the Revword
111.     if  the  corresponding  Successor   value  for  f is 
greater than or equal to the threshold value (16) 
then add the contents  of SegPre to the list 
PreSegList and empty it.
IV.     else add the character at position f to SegPre.
v. 	If the length of SegPre  is greater  than zero, then 
add its contents  toPreSegList and empty it.

Entropy method:
1. 	for each word in the corpus we find the following
1. 		I D"'I  The   number   of  words   in  a  text   body 
beginning with  the  i length  sequence  of letters
    a in FWord  is  computed and stored in step one. 
n. 	I D"'I The  number   of  words  in  Dcxi with  the
successor}is computed  in step one.
111.     The  probability that  a  word  has the   successor
j is given  by

The entropy of  I  Dm I is: I  Dm, I
IDml
Hm = :f: _I Dm1 l·log) Dm1 I
,_,	I  Dm I	I  Dm I

    The  entropy  value,  calculated for  each  letter  in the 
word,  store the result in a list called SucEntl    ist
    Repeat  this   step   for   predecessors  and   store   the 
resulting  values  in a list calledPreEntl 	ist.
c. 	Segmentation Process  for Entropy  Successor:
1. 	Set a variable  named SegSucEnt of type string.
11.       For each character  at position min the Fword
1.  if the corresponding Successor  value  form is 
greater    than    or    equal    to    the  threshold 
value    (2. 7)      then   add   the   contents    of


d.     Segmentation Process  for Entropy  Predecessor:
1. 	Set a variable namedSegPreEntoftype string.
11.       For  each character  at position  n in the Revword
1.  if the corresponding Successor value for n is 
greater    than    or    equal    to    the  threshold 
value  (3.3)     then  add  the  contents  of 
SegPreEnt  to   the   list  PreEntSegList  and 
empty  it.
2.  else   add   the   character    at   position    n  to
SegPreEnt.
111.     If the  length  of SegPreEnt is greater  than  zero, 
then   add   its   contents   to  PreEntSegList  and 
empty  it.

Step three:  Select one of the segments  as the stem. 
For each segment  in SucSegList do the following:
1.    If the  segment occurs    less  than  16  times  m 
words  in  the  corpus  then  add  this  segment   to 
the variable  S1.
For each segment  inPreSegList do the following:
1.    If the  segment occurs    less  than  16  times  m 
words  in  the  corpus  then  add  this  segment   to 
the variable  S2.
For each segment  in SucEntSegList do the following:
1.    If the  segment occurs    less  than  16  times  m 
words  in  the  corpus  then  add  this  segment   to 
the variable  S3.
For each segment  inPreEntSegList do the following:
1.    If the  segment  occurs  in less  than  14  times  in 
words  in  the  corpus  then  add  this  segment   to 
the variable  S4.

Step Four:  The first stem is the intersection of S1 and S2 
and  Store  in variable  FirstStem. The entropy   stem is the 
intersection of  S3  and  S4  and  Store  in  variable 
EntropyStem.

Step  Five:  Store  the  value  of FirstStem in FWord  and 
repeat   steps   One-to-   Four   using  the  new  FWord,  the 
resulting  word is called SecondStem.

Step Six: If the length of the SecondS/em is less than the 
length of FirstStem
then take the SecondStem as the stem. 
else select either of them as a stem.

11


SegSucEnt  to   the   list   SucEntSegList  and


Example:  The word    " 4-wWI


is inserted in the text


empty  it.
2.  else   add   the   character    at   position   m   to
SegSucEnt.
111.     If the  length  of SegSucEnt is greater  than  zero, 
then   add   its   contents   to   SucEntSegList and 
empty it


box. "When the button labeled find, in the main form of our 
application, is pressed the program  will perform the 
following steps:

Step one: Determine the successor varieties  for this word:
II	II
FW ord assigned to                                      and reverse  it as





RevWord	" 	t:;"
I.	(For  Successor) Starting  from  ;] to  length  of  the
FWord(8):
    -At  i=l   the  system  searches   the  corpus  and  finds 
that, there are 16letters following  the ith segment  and this 
is the successor variety  we look for.. The Table  2 shows 
the successor variety  for all ith segments:

Table 2: Successor  variety for ith segments 	 
I 	Segment 	Successor 	Letters 	
16 	{••.o}
2 	26 	{, LP,,_p,,U",J ,J ,t ,c_ ,rz , ,.::.,'""'I ,Is
,-ArJr, J,3>    ,t,t,..h,_h,oL>}
3 	6 	{J,'-'"",J,J,rz,l}
4 	WI 	{ }
......
7


2.   else add the character  at position k to SegSuc.
v1. If the length of SegSuc  is greater  than zero, then 
add its contents  to SucSegList and empty it
The  system  will  segment  the  word  into  three  parts,  the
SucSegList looks like this:

 Segment 	Successor 	
16 >= 11
26 >= 11
 	6< 11 	


b. 	Segmentation Process  for Predecessor:
vii.  Set a variable  named SegPre  of type string.
viii. For  each character  at position  f in the Revword
IX.     if  the  corresponding  Successor   value  for  f  is 
greater than or equal to the threshold value (16)


6 	{I}
 	....., 	{c.}


8 	Bl>mk

Store  the  successor value  from  the  above  step  in a list 
called SucList.
2. 	(For   Predecessor)  Starting   from  j Jength  of   the
RevW ord (8):
For the rightmost}letters inRevWord:    Cmmt the 
number  of letters  in the Reversed corpus  that follow 
the firstjth rightmost letters of RevWord.

   -Atj=l the system  searches  the reversed  corpus  and 
finds  that,  there  are 16 letters  following  the jth segment 
and this is the predecessor variety we look for. The Table
3   below   shows   the   predecessor  variety    for   all   jth
segments.

Table 3:  Predecessor variety for all ;th segments 	
I 	Segment 	Predecessor  Letters 	
1 	.0	16 	{ ,-.. ,.,lJ,i'",J, ,J,t,<Y",,_;,,J,t,c,u,l}
2 	17 	{ "''•,u'i'"'J,,J,--il,bt,oL>,J,J,c,rz,""}
3 	yO 	3 	{-.. ,LJ,<Y'}
4 	"""	{I}
6 	{J}
7 	J-L.,O	{I}
8 	""'"'' 	{Blank} 	
    Store the Predecessor value  from the above step in a 
list calledPrdLisl.

Step two: Use the information above to segment  the word 
usmg:
I.	cutoff method
a.   Segmentation Process  for Successor:
IV.   Set a variable Named SegSuc  of type string. v.   
For each character  at position kin the Fword
1.   if the corresponding Successor value for k is 
greater  than  or equal  to the  threshold value 
(II) then  add the contents  of SegSuc  to the 
list SucSegList and empty it.


then   add   the  contents   of  SegPre   to  the  list
PreSegList and empty it.
x. 	else add the character  at position fto SegPre.
x1.  If the length of SegPre  is greater  than zero, then 
add its contents  toPreSegList and empty it.
The  system  will  segment  the word  into three  parts,
thePreSegListlooks like this:


 Segment 	Predecessor 	
.::.	16>-16
I	17>=11
t......y	3<11


2. 	Entropy  method:  for each segment
Find  the  frequency  of each  letter  after  the  ith most 
right segment  in FWORD

When  i=l, find the frequency  of each letter that 
follow  "1". Find the sum of all the letters.,  Here 
the sum is 230.
For each letter:

Table 4: Frequency of each letter after the ith most right segment in FWORD
Letter    Frequency   Entropyx formula 	Entropyx value
.0	4 	-1*(4/230)*logl4/230) 	-0.010166
2 	c 	2 	-1*(2/230)*logl2/230) 	-5.9526e-2
3 	t 	-1*(11230)*loglll230)	-3.411082e-2
4 	2 	-1*(2/230)*logl2/230) 	-5.9526e-2
5 	-1*(11230)*loglll230) 	-3.411082e-2
6 	8 	-1*(8/230)*logl8/230) 	-0.16850
7 	..;.	2 	-1*(2/230)*logl2/230) 	-5.9526e-2
8 	2 	-1*(2/230)*logl2/230) 	-5.9526e-2
9 	t	2 	-1*(2/230)*logl2/230) 	-5.9526e-2
10 	J 	-1*(11230)*loglll230)	-3.411082e-2
11 	J 	189 	-1*(189/230)*logl189/230) 	-0.2327
12 	3 	-1*(3/230)*logl3/230) 	-8.1659e-2
13 	u	5 	-1*(5/230)*logl5/230) 	-0.12007
14 	1 	-1*(11230)*lo&(ll230)	-3.411082e-2
15 	5 	-1*(5/230)*lo&(5/230)	-0.12007
16 	2 	-1*(2/230)*logl2/230) 	-5.9526e-2
Sum 	230 	1.318368 	





• 	When   i=2,  find  the  frequency   of  each  letter  that 
follows "Jl". Find the sum of all the letters.  Here the 
sum is 189.  For each letter.
• 	When  i=3,  find  the  frequency  of  each  letter  that


• 	When   j=4,  find  the  frequency   of  each  letter  that 
follows       t ;11   in reverse corpus.  Find the sum of all 
the letters (sum of all words with this segment).  Here
the sum is 2.


follows "c;ll". Find the sum of all the letters Here the	 	


sum is 9. For each letter
• 	When   i=4,  find  the  frequency   of  each  letter  that


 	Letter 	Frequency 	EntropYx formula  	EntropYx value
2 	-1*(2/2)*logl2/2) 	0


follows  "WI". Find the sum of all the letters.   Here 
the sum is 2. For each letter

 	Letter 	Frequency 	Entrop)'x formula 	Entropy" value 	
2 	-1*(2/2)*logl2/2) 	0


Sum 	2 	0 	

• 	When   j=5,  find  the  frequency   of  each  letter  that 
follows 11         l:i11   in the reverse corpus.  Find the sum of 
all the letters.  Here the sum is 2.


Sum 	2 	0 		 	
 	Letter 	Frequency 	EntropYx formula  	EntropYx value


• 	When  i=5,  find  the  frequency  of  each  letter  that


c 	2 	-1*(2/2)*lo&(212) 	0


follows  "u.uWI". Find the sum of all the letters.  Here 
the sum is 2. For each letter

 	Letter 	Frequency 	Entrop)'x formula 	Entropy" value 	


Sum 	2 	0 	

• 	When   j=6,  find  the  frequency   of  each  letter  that 
follows 11     c.L.&yt:i 11     in the reverse corpus.  Find the sum
of all the letters.  Here the sum is 1.


2 	-1*(2/2)*logl2/2) 	0
Sum 	2 	0 		 	
 	Letter 	Frequency 	Entrop)'x formula  	Entrop)'x value



• 	When  i=6,  find  the  frequency  of  each  letter  that
follows 	LJI "F. ind the sum of all the letters.  Here



Sum


J 	-1*(111)*lo&(ll1)	0
0


the sum is 1. For each letter

 	Letter 	Frequency 	Entrop)'x formula 	Entropy" value 	



• 	When  j=7,  find  the  frequency   of  each  letter  that 
follows 	L.a:tl:i 11     in the reverse  corpus.  Find the
sum of all the letters.  Here the sum is 1.


-1*(111)*loglll1) 	0


Sum 	0 	

• 	When   i=7,  find  the  frequency   of  each  letter  that 
follows 11             WI11 •    Find the sum of all the letters Here
the sum is 1. For each letter

 	Letter 	Frequency 	Entropyx formula 	Entropyx value 	
-1*(111)*log2(111)	0



 	Letter 	Frequency 	Entrop)'x formula  	Entrop)'x value
-1*(111)*loglll1) 	0
Sum 	0 	

• 	When  j=8,  find  the  frequency   of  each  letter  that 
follows  11       l...u:tl:i  11     in the reverse  corpus.  Find the 
sum of all the letters.  Here the sum is 1.


Sum 	0 		 	
 	Letter 	Frequency 	Entrop)'x formula 	Entrop)'x value


• 	When   i=8,  find  the  frequency   of  each  letter  that
follows" .: WI".Find the sum of all the letters.  A
ssume that the sum is 1.

Find  the  frequency  of each letter  after the ith most  right 
segment  inRevWORD
•	When   j=1,  find  the  frequency   of  each  letter  that 
follows 11 ..:::.. 11    in the reverse  corpus. Find the sum of all 
the letters (sum of all words with this segment).  Here 
the sum is 74.
•	When   j=2,  find  the  frequency   of  each  letter  that 
follows 11 t:;" in the reverse corpus.  Find the sum of all
the letters (sum of all words with this segment).  Here 
the sum is 54.
• 	When   j=3,  find  the  frequency   of  each  letter  that
follows 11    yl:i  11   in the reverse  corpus. Find the sum of
all the letters.  Here the sum is 5.


-1*(111)*lo&(ll1)	0
Sum 	0 	

Segmentation Process for Entropy  Successor:
1. 	 Set a variable named SegSucEnt of type string. 
n. 	For each character  at position min the Fword
1.   if the corresponding Successor value for m is 
greater than or equal to the threshold value (2.7) 
then  add  the contents  of SegSucEnt to the  list 
SucEntSegList and empty it.
2.    else    add   the    character    at   position   m   to
SegSucEnt.

The following table shows the SucEntSegList

 	Segment 	Successor entropy 	
4.0219  2.7
 2 	2.41938<2.7 	





Segmentation Process  for Entropy  Predecessor:
1.     Set a variable Named SegPreEnt of type string.
11.    For  each character  at position  n in the Revword
1.    if the corresponding Successor value for n is 
greater  than  or equal  to the threshold value 
(3.3)   then add the contents  of SegPreEnt to
the hstPreEntSegList and empty it.


Step four:

•	The  first  stem  is the intersection of S1 and S2 and
Store in variable FirstStem.  S1 is "	6:' and S2 is "
...,.....wt    ". The intersection is " Y-'Mb.''
•	The  first  stem  is the intersection of S3 and S4 and
Store in variable EntropyStem. S3 is " 4--ub." and S4


2.    else   add   the   character    at   position   n  to
SegPreEnt.
111.  If  the  length  of SegPreEnt is greater  than  zero, 
then   add   its   contents   to   PreEntSegList  and


is " WI". The intersection is "  l.:t..
is the stem.

Step five:


a" nd this


empty 	it 	The 	following 	table 	shows
PreEntSegLis:

Step three:  now, we attempt to select one of the segments 
as the stem:
1.   For each segment inSucSegList do the following: 
If the segment occurs  less than 16 times in words 
in   the   corpus   then   add   this   segment   to  the 
variable  Sl.
The comparison is shown:

 	Segment 	Occurs in 	Comapison 	


Store  the value of FirstStem=" k.."   in FWord  and
repeat  steps One-to- Four using new FWord, the resulted 
word called SecondStem = " y..u  '. The result will be the 
same.

Step  Six:
     The  length  of  the  SecondStem( 4)  is equal  to the 
length of FirstStem
Then we can select any one of them.
The stem produced as " y..wand this is correct.

The implementation:


1
2 	J
3 	.:0
ThevalueifSl is"   ""4-uotll..


230
50
1


230-16
50= 16
1 < 16
    

We have 
implemented this 
algorithm  using  the 
well­ kno\Vll 
programming 
language  Visual 
Basic  version  6.0. 
The  purpose behind  
using  this 
programming 
language  is that  it is 
easy  to use, it 
facilitates the 
construction of an


2.  For each segment inPreSegList do the following: 
If the segment occurs  less than 16 times in words 
in  the  reverse  corpus  then  add  this  segment  to 
the variable  S2.

 	Segment 	Occurs in 	Comapison 	


3 	1 	1 < 16 	
The value of S2 is reverse  of"  t....., " that is "  ..,.....wt "

3. For  each  segment  in SucEntSegList do the 
following:


attractive user interface  for our system and there are many
manuals  that describe  the features  of this language.
     We  focus  mainly  on the  features    dealing  with  the 
Arabic  Language, also  the  ability  to  write  applications 
that use an Arabic database.  For our database, we used 
Microsoft Access  97 for storing  and accessing  the Arabic 
corpus.   It is a good  database  engine  and  easy  to learn 
and use.

  Table 5:   Computed stems for lists of  words that share the same root 	 
Computed stem    Correct or     Computed stem 	Correct or
Test word 	cutoff method 	not correct     entropy method      not correct
._WI	..,....t..	Correct 	..,....t..	Correct


4-o--6..11	.......


Correct 	.......



Correct


the corpus  then add this segment  to the variable  S3.


yt-.11	y!....	Correct 	yl....	Correct
Correct 	...... ...	Correct


".'."."."'"


......


.,...


..:o L......JI_,	yl.... 	Correct 	y!....	Correct


Jl 	189 	189-16


t-It,	..,....t..	Correct


Incorrect


2 	t,....t.... 	1 < 16 	


..........r..,a.Jr 	..,...(p	Correct 	......,,.... 	correct


The value if S3 is "     l,....t.ll.

4.  For   each   segment    in   PreEntSegList  do   the


..........	................
,..,_, 	......  ..


Correct 	.,...._	Correct
Correct 	....- 	Correct
Incorrect 	.,...t..	Incorrect



If the segment  occurs  less than 14 times in words in


".."..",",_"'	...... ..	Incorrect 	t b	Incorrect



the corpus  then add this segment  to the variable  S4.



."..'..'...-..


..,....t..
y..l.......


Correct 	_y!......



Correct




Occurs in 	Comapison



., wr	......



Correct
Correct


..	Correct
.- 	Incorrect


..,....t..	Correct


Incorrect



The value of S4 is reverse of"  t...JI that is "     ...,.....LWt


.,_L...Jt	.,...t..	Incorrect 	.,...t..	Incorrect


Inform. Techno!. J., 4 (I): 55-62, 2005



EXPERIMENTS AND RESULTS

    we describe  experiments done to test the correctness 
of our work.   The results are shown  in Table 5.

CONCLUSIONS

    After   applying   these   algorithms  to  2000   Arabic 
words, we  conclude   that  we  can  apply  the  Successor 
Variety  Algorithm  with the Cutoff Method to the  Arabic 
Language  since we have achieved an 80% level of 
correctness.  On the other hand, we have achieved a 75% 
level of correctness by applying the Entropy Method.
Several    advantages   of    the    Successor   Variety
Algorithm  can be observed; the most  important one is the 
ability  to find a stem without the need to use a dictionary. 
Another   advantage   is that  it       can  be  used  in several 
domains; it is basically (domain independent).

REFERENCES

1.    Frakes, W., 1992.   Stemming Techniques.  Chapter  6 
in Frakes and Baeza-Yates, 1992.
2. 	Fox, C., 1992. Lexical Analysis and Stoplists.  Chapter
5 in Frakes and Baeza-Yates, 1992.
3. 	Salton  G.  and C.S.  Yang, 1980.  Contribution to the
Theory  ofindexing.  American Elsevier,  New  York,
1980.
4. 	Hafer,     M.A.     and     S.F.    Weiss,     1974:    Word 
segmentation	by      letter      successor      varieties. 
Information Storage and Retrieval, 10: 371-385.
5. 	Mustafa  Suleiman and Qasem  Ahmad Al-Radaideh,
2001.  Arabic  word  stemming using  letter  successor 
and Predecessor Variety, ACIT 2001.
6. 	Darwish,  K.,   2002.   Building    a   shallow   Arabic 
morphological analyzer  in one  day.  Proceedings  of 
the   Workshop  on   Computational Approaches to 
Semitic Languages. ACL, Philadelphia, pp: 47-54.
7 .      Beesley, K. andL. Karttunen, 2000.  Finite-state non­ 
concatenative  morphotactics.  Proceedings  of   the 
ACL, Hong  Kong, pp: 191-198.
8. 	Lovins,    J.,   1968.   Development  of   a   stemming
algorithm. 	Mechanical	Translation	and
Computational Linguistics, 11: 22-31.


9. 	Al-Fedaghi, S. and F. Al-A=i, 1989. A new algorithm 
to generate Arabic root-pattern forms. In Proceedings 
of the 11th  National Computer Conference and 
Exhibition, March, Dhahran, Saudi Arabia, pp: 04-07.
10.   Al-Kharashi, I. and M.W. Evens, 1994.  Words,  stems 
and roots  in an Arabic  information retrieval  system. 
J. American Soc. Inform.  Sci. , 45: 548-560.
11. Al-Shalabi,  R.   and  M.W.   Evens,   1998.  A 
computational morphology system for Arabic 
computational approaches to semitic  languages. 
Workshop,	COLING     98,     Montreal,    Canada, 
pp:  58-65.
12.   Khoja,  S., 1999.   Stemming  Arabic  text.  Lancaster,
U.K., Computing  Department, Lancaster University. 
www. comp.lancs.uk/           computing/userslkhoja/ 
stemmer.ps.
13.   Paik, W., 1994.  Chronological Information Extraction 
System  (CIES),  Dagstuhl-Seminar-Report: 79 on 
Summarizing Text for Intelligent Communication, 
Endres-Niggemeyer, B., Hobbs,  J.  and  Jones,  K.S. 
(Eds.), Wadem, Germany:  IBFI
14.   Porter, M.F., 1980.  An algorithm  for suffix stripping.
Program,  14: 130-137.
15.   Salton, G., 1968. Automatic Information Organization 
and Retrieval.  New York, NY McGraw-Hill.
16. Dawson, J.L.,  1974.  Suffix  removal   for  word 
conflation. Bulletin of the  Association for  Literary 
and Linguistic Computing, 2: 33-46.
17.  Van  Rijsbergen, C.J.,  1979.  Information Retrieval.
London,  UK: Butterworths.
18.   Katzer,   J.,  M.J. McGill, J.A. Tessier, W. Frakes and 
P. Das-Gupta,   1982.  A study of  the overlap  among 
document  representations. Information Technology: 
Research and Development, 2: 261-274.
19. Karen,  S.J., S. Walker,  S.E. Robertson, 2000. A 
probabilistic model  of information retrieval:  A 
development and comparative  experiments-Part 2. Inf. 
Process.  Manage., 36: 809-840.
20.  Harman,  D., 1991.   How  effective is  suffixing?  J.
American Soc. Inform.  Sci. , 42: 7-15.





