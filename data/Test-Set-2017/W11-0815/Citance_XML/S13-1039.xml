<PAPER>
	<ABSTRACT>
	<SECTION title="Compositionality of MWEs. " number = "1">
			<S sid ="1" ssid = "1">A multiword expression (MWE) is any combination of words with lexical, syntactic or semantic idiosyncrasy (Sag et al., 2002; Baldwin and Kim, 2009), in that the properties of the MWE are not predictable from the component words.</S>
			<S sid ="2" ssid = "2">For example, with ad hoc, the fact that neither ad nor hoc are standalone English words, makes ad hoc a lexically- idiosyncratic MWE; with shoot the breeze, on the other hand, we have semantic idiosyncrasy, as the meaning of “to chat” in usages such as It was good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze.</S>
			<S sid ="3" ssid = "3">Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/non- compositional distinction (Fazly and Stevenson, 2007).</S>
			<S sid ="4" ssid = "4">There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011).</S>
			<S sid ="5" ssid = "5">Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).</S>
			<S sid ="6" ssid = "6">As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2 ), we would not be interested in documents on rodents.</S>
			<S sid ="7" ssid = "7">These results underline the need for methods for broad-coverage MWE composition- ality prediction.</S>
			<S sid ="8" ssid = "8">In this research, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE.</S>
			<S sid ="9" ssid = "9">We use Panlex to translate the MWE and its components, and compare the translations of the MWE with the translations of its components using string similarity measures.</S>
			<S sid ="10" ssid = "10">The greater the string similarity, the more compositional the MWE is. Whereas past research on MWE compositionality has tended to be tailored to a specific MWE type (McCarthy et al., 2007; Kim and Baldwin, 2007; Fazly et al., 2009), our method is applicable to any MWE type in any language.</S>
			<S sid ="11" ssid = "11">Our experiments 1 The example is taken from http://www..</S>
			<S sid ="12" ssid = "12">thefreedictionary.com</S>
	</SECTION>
	<SECTION title="This definition is from WordNet. " number = "2">
			<S sid ="13" ssid = "1">3.1.</S>
			<S sid ="14" ssid = "2">266 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 266–275, Atlanta, Georgia, June 1314, 2013.</S>
			<S sid ="15" ssid = "3">Qc 2013 Association for Computational Linguistics over two English MWE types demonstrate that our method is competitive with state-of-the-art methods over standard datasets.</S>
			<S sid ="16" ssid = "4">2 Related Work.</S>
			<S sid ="17" ssid = "5">Most previous work on measuring MWE compositionality makes use of lexical, syntactic or semantic properties of the MWE.</S>
			<S sid ="18" ssid = "6">One early study on MWE compositionality was Lin (1999), who claimed that the distribution of non-compositional MWEs (e.g. shoot the breeze) differs significantly from the distribution of expressions formed by substituting one of the components with a semantically similar word (e.g. shoot the wind).</S>
			<S sid ="19" ssid = "7">Unfortunately, the method tends to fall down in cases of high statistical idiosyncrasy (or “institutionalization”): consider frying pan which is compositional but distributionally very different to phrases produced through word- substitution such as sauteing pan or frying plate.</S>
			<S sid ="20" ssid = "8">Some research has investigated the syntactic properties of MWEs, to detect their composition- ality (Fazly et al., 2009; McCarthy et al., 2007).</S>
			<S sid ="21" ssid = "9">The assumption behind these methods is that non- compositional MWEs are more syntactically fixed than compositional MWEs.</S>
			<S sid ="22" ssid = "10">For example, make a decision can be passivised, but shoot the breeze cannot.</S>
			<S sid ="23" ssid = "11">One serious problem with syntax-based methods is their lack of generalization: each type of MWE has its own characteristics, and these characteristics differ from one language to another.</S>
			<S sid ="24" ssid = "12">Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011).</S>
			<S sid ="25" ssid = "13">Much of the recent work on MWEs focuses on their semantic properties, measuring the semantic similarity between the MWE and its components using different resources, such as WordNet (Kim and Baldwin, 2007) or distributional similarity relative to a corpus (e.g. based on Latent Semantic Analysis: Schone and Jurafsky (2001), Bannard et al.</S>
			<S sid ="26" ssid = "14">(2003), Reddy et al.</S>
			<S sid ="27" ssid = "15">(2011)).</S>
			<S sid ="28" ssid = "16">The size of the corpus is important in methods based on distributional similarity.</S>
			<S sid ="29" ssid = "17">Unfortunately, however, large corpora are not available for all languages.</S>
			<S sid ="30" ssid = "18">Reddy et al.</S>
			<S sid ="31" ssid = "19">(2011) hypothesize that the number of common co-occurrences between a given MWE and its component words indicates the de gree of compositionality of that MWE.</S>
			<S sid ="32" ssid = "20">First, the co- occurrences of a given MWE/word are considered as the values of a vector.</S>
			<S sid ="33" ssid = "21">They then measure the Cosine similarity between the vectors of the MWE and its components.</S>
			<S sid ="34" ssid = "22">Bannard et al.</S>
			<S sid ="35" ssid = "23">(2003) presented four methods to measure the compositionality of English verb particle constructions.</S>
			<S sid ="36" ssid = "24">Their best result is based on the previously-discussed method of Lin (1999) for measuring compositionality, but uses a more-general distributional similarity model to identify synonyms.</S>
			<S sid ="37" ssid = "25">Recently, a few studies have investigated using parallel corpora to detect the degree of compositionality (Melamed, 1997; Moiro´ n and Tiedemann, 2006; de Caseli et al., 2010; Salehi et al., 2012).</S>
			<S sid ="38" ssid = "26">The general approach is to word-align the source and target language sentences and analyse alignment patterns for MWEs (e.g. if the MWE is always aligned as a single “phrase”, then it is a strong indicator of non-compositionality).</S>
			<S sid ="39" ssid = "27">de Caseli et al.</S>
			<S sid ="40" ssid = "28">(2010) consider non-compositional MWEs to be those candidates that align to the same target language unit, without decomposition into word alignments.</S>
			<S sid ="41" ssid = "29">Melamed (1997) suggests using mutual information to investigate how well the translation model predicts the distribution of words in the target text given the distribution of words in the source text.</S>
			<S sid ="42" ssid = "30">Moiro´ n and Tiedemann (2006) show that entropy is a good indicator of compositionality, because word alignment models are often confused by non-compositional MWEs.</S>
			<S sid ="43" ssid = "31">However, this assumption does not always hold, especially when dealing with high-frequency non-compositional MWEs.</S>
			<S sid ="44" ssid = "32">Salehi et al.</S>
			<S sid ="45" ssid = "33">(2012) tried to solve this problem with high frequency MWEs by using word alignment in both directions.3 They computed backward and forward entropy to try to remedy the problem with especially high-frequency phrases.</S>
			<S sid ="46" ssid = "34">However, their assumptions were not easily generalisable across languages, e.g., they assume that the relative frequency of a specific type of MWE (light verb constructions) in Persian is much greater than in English.</S>
			<S sid ="47" ssid = "35">Although methods using bilingual corpora are intuitively appealing, they have a number of drawbacks.</S>
			<S sid ="48" ssid = "36">The first and the most important problem 3 The IBM models (Brown et al., 1993), e.g., are not bidirectional, which means that the alignments are affected by the alignment direction.</S>
			<S sid ="49" ssid = "37">is data: they need large-scale parallel bilingual corpora, which are available for relatively few language pairs.</S>
			<S sid ="50" ssid = "38">Second, since they use statistical measures, they are not suitable for measuring the composition- ality of MWEs with low frequency.</S>
			<S sid ="51" ssid = "39">And finally, most experiments have been carried out on English paired with other European languages, and it is not clear whether the results translate across to other language pairs.</S>
	</SECTION>
	<SECTION title="Resources. " number = "3">
			<S sid ="52" ssid = "1">In this research, we use the translations of MWEs and their components to estimate the relative degree of compositionality of a MWE.</S>
			<S sid ="53" ssid = "2">There are several resources available to translate words into various languages such as Babelnet (Navigli and Ponzetto, 2010),4 Wiktionary,5 Panlex (Baldwin et al., 2010) and Google Translate.6 As we are ideally after broad coverage over multiple languages and MWEs/component words in a given language, we exclude Babelnet and Wiktionary from our current research.</S>
			<S sid ="54" ssid = "3">Babelnet covers only six languages at the time of writing this paper, and in Wiktionary, because it is constantly being updated, words and MWEs do not have translations into the same languages.</S>
			<S sid ="55" ssid = "4">This leaves translation resources such as Panlex and Google Translate.</S>
			<S sid ="56" ssid = "5">However, after manually analysing the two resources for a range of MWEs, we decided not to use Google Translate for two reasons: (1) we consider the MWE out of context (i.e., we are working at the type level and do not consider the usage of the MWE in a particular sentence), and Google Translate tends to generate com- positional translations of MWEs out of context; and (2) Google Translate provides only one translation for each component word/MWE.</S>
			<S sid ="57" ssid = "6">This left Panlex.</S>
			<S sid ="58" ssid = "7">Panlex is an online translation database that is freely available.</S>
			<S sid ="59" ssid = "8">It contains lemmatized words and MWEs in a large variety of languages, with lemma- based (and less frequently sense-based) links between them.</S>
			<S sid ="60" ssid = "9">The database covers more than 1353 languages, and is made up of 12M lemmas and expressions.</S>
			<S sid ="61" ssid = "10">The translations are sourced from handmade electronic dictionaries, making it more accu 4 http://lcl.uniroma1.it/babelnet/ 5 http://www.wiktionary.org/ 6 http://translate.google.com/ rate than translation dictionaries generated automatically, e.g. through word alignment.</S>
			<S sid ="62" ssid = "11">Usually there are several direct translations for a word/MWE from one language to another, as in translations which were extracted from electronic dictionaries.</S>
			<S sid ="63" ssid = "12">If there is no direct translation for a word/MWE in the database, we can translate indirectly via one or more pivot languages (indirect translation: Soderland et al.</S>
			<S sid ="64" ssid = "13">(2010)).</S>
			<S sid ="65" ssid = "14">For example, English ivory tower has direct translations in only 13 languages in Panlex, including French (tour d’ivoire) but not Esperanto.</S>
			<S sid ="66" ssid = "15">There is, however, a translation of tour d’ivoire into Esperanto (ebura turo), allowing us to infer an indirect translation between ivory tower and ebura turo.</S>
	</SECTION>
	<SECTION title="Dataset. " number = "4">
			<S sid ="67" ssid = "1">We evaluate our method over two datasets, as described below.</S>
			<S sid ="68" ssid = "2">RE D DY (Reddy et al., 2011): 90 English (binary) noun compounds (NCs), where the overall NC and each component word has been annotated for compositionality on a scale from 0 (non-compositional) to 5 (compositional).</S>
			<S sid ="69" ssid = "3">In order to avoid issues with polysemy, the annotators were presented with each NC in a sentential context.</S>
			<S sid ="70" ssid = "4">The authors tried to achieve a balance of compositional and non- compositional NCs: based on a threshold of 2.5, the dataset consists of 43 (48%) compositional NCs, 46 (51%) NCs with a compositional usage of the first component, and 54 (60%) NCs with a compositional usage of the second component.</S>
			<S sid ="71" ssid = "5">BA N NA R D (Bannard, 2006): 160 English verb particle constructions (VPCs) were annotated for compositionality relative to each of the two component words (the verb and the particle).</S>
			<S sid ="72" ssid = "6">Each annota- tor was asked to annotate each of the verb and particle as yes, no or don’t know.</S>
			<S sid ="73" ssid = "7">Based on the majority annotation, among the 160 VPCs, 122 (76%) are verb-compositional and 76 (48%) are particle- compositional.</S>
			<S sid ="74" ssid = "8">We compute the proportion of yes tags to get the compositionality score.</S>
			<S sid ="75" ssid = "9">This dataset, unlike RED DY, does not include annotations for the compositionality of the whole VPC, and is also less balanced, containing more VPCs which are verb-compositional than verb-non-compositional.</S>
			<S sid ="76" ssid = "10">MWE Components ...</S>
			<S sid ="77" ssid = "11">Translate ...</S>
			<S sid ="78" ssid = "12">Translations Panlex English Persian Translation kick the bucket mord kick zad the – bucket satl mak e a deci sion tasmim gereft make sakht a y e k d e c i s i o n t a s m i m public service khadamaat omumi public omumi service khedmat Compare Score Figure 1: Schematic of our proposed method</S>
	</SECTION>
	<SECTION title="Method. " number = "5">
			<S sid ="79" ssid = "1">To predict the degree of compositionality of an MWE, we require a way to measure the semantic similarity of the MWE with its components.</S>
			<S sid ="80" ssid = "2">Our hypothesis is that compositional MWEs are more likely to be word-for-word translations in a given language than non-compositional MWEs.</S>
			<S sid ="81" ssid = "3">Hence, if we can locate the translations of the components in the translation of the MWE, we can deduce that it is compositional.</S>
			<S sid ="82" ssid = "4">Our second hypothesis is that the more languages we use as the basis for determining translation similarity between the MWE and its component words, the more accurately we will be able to estimate compositionality.</S>
			<S sid ="83" ssid = "5">Thus, rather than using just one translation language, we experiment with as many languages as possible.</S>
			<S sid ="84" ssid = "6">Figure 1 provides a schematic outline of our method.</S>
			<S sid ="85" ssid = "7">The MWE and its components are translated using Panlex.</S>
			<S sid ="86" ssid = "8">Then, we compare the translation of the MWE with the translations of its components.</S>
			<S sid ="87" ssid = "9">In order to locate the translation of each component in the MWE translation, we use string simi Table 1: English MWEs and their components with their translation in Persian.</S>
			<S sid ="88" ssid = "10">Direct matches between the translation of a MWE and its components are shown in bold; partial matches are underlined.</S>
			<S sid ="89" ssid = "11">larity measures.</S>
			<S sid ="90" ssid = "12">The score shown in Figure 1 is derived from a given language.</S>
			<S sid ="91" ssid = "13">In Section 6, we show how to combine scores across multiple languages.</S>
			<S sid ="92" ssid = "14">As an example of our method, consider the English-to-Persian translation of kick the bucket as a non-compositional MWE and make a decision as a semi-compositional MWE (Table 1).7 By locating the translation of decision (tasmim) in the translation of make a decision (tasmim gereftan), we can deduce that it is semi-compositional.</S>
			<S sid ="93" ssid = "15">However, we cannot locate any of the component translations in the translation of kick the bucket.</S>
			<S sid ="94" ssid = "16">Therefore, we conclude that it is non-compositional.</S>
			<S sid ="95" ssid = "17">Note that in this simple example, the match is word-level, but that due to the effects of morphophonology, the more likely situation is that the components don’t match exactly (as we observe in the case of khadamaat and khedmat for the public service example), which motivates our use of string similarity measures which can capture partial matches.</S>
			<S sid ="96" ssid = "18">We consider the following string similarity measures to compare the translations.</S>
			<S sid ="97" ssid = "19">In each case, we normalize the output value to the range [0, 1], where 1 indicates identical strings and 0 indicates completely different strings.</S>
			<S sid ="98" ssid = "20">We will indicate the translation of the MWE in a particular language t as MWE t , and the translation of a given component in 7 Note that the Persian words are transliterated into English for ease of understanding.</S>
			<S sid ="99" ssid = "21">language t as component t . Longest common substring (LCS): The LCS measure finds the longest common substring between two strings.</S>
			<S sid ="100" ssid = "22">For example, the LCS between ABABC and BABCAB is BABC.</S>
			<S sid ="101" ssid = "23">We calculate a normalized similarity value based on the length of the LCS as follows: LongestCommonString (MWE t , component t) min(len(MWE t ), len(component t )) Levenshtein (LEV1): The Levenshtein distance calculates for the number of basic edit operations required to transpose one word into the other.</S>
			<S sid ="102" ssid = "24">Edits consist of single-letter insertions, deletions or substitutions.</S>
			<S sid ="103" ssid = "25">We normalize LEV1 as follows: LEV1 (MWE t, component t ) 1 − max(len(MWE t), len(component t )) Levenshtein with substitution penalty (LEV2): One well-documented feature of Levenshtein distance (Baldwin, 2009) is that substitutions are in fact the combination of an addition and a deletion, and as such can be considered to be two edits.</S>
			<S sid ="104" ssid = "26">Based on this observation, we experiment with a variant of LEV1 with this penalty applied for substitutions.</S>
			<S sid ="105" ssid = "27">Similarly to LEV1, we normalize as follows: LEV2 (MWE t, component t ) Seq1: ATGCATCCCATGAC Seq2: TCTATATCCGT As the example shows, it looks for the longest common string but has an inbuilt mechanism for including gaps in the alignment (with penalty).</S>
			<S sid ="106" ssid = "28">This characteristic of SW might be helpful in our task, because there may be morphophonological variations between the MWE and component translations (as seen above in the public service example).</S>
			<S sid ="107" ssid = "29">We normalize SW similarly to LCS: len(alignedSequence ) min(len(MWE t ), len(component t ))</S>
	</SECTION>
	<SECTION title="Computational. " number = "6">
			<S sid ="108" ssid = "1">Model Given the scores calculated by the aforementioned string similarity measures between the translations for a given component word and the MWE, we need some way of combining scores across component words.9 First, we measure the compositionality of each component within the MWE (s1 and s2): s1 = f1(sim1(w1 , MWE ), ..., simi (w1 , MWE )) s2 = f1(sim1(w2 , MWE ), ..., simi (w2 , MWE )) where sim is a string similarity measure, simi indicates that the calculation is based on translations in language i, and f1 is a score combination function.</S>
			<S sid ="109" ssid = "2">Then, we compute the overall compositionality of 1 − len(MWE t) + len(component t ) the MWE (s3) from s1 and s2 using f2: Smith Waterman (SW) This method is based on the NeedlemanWunsch algorithm,8 and was developed to locally-align two protein sequences (Smith and Waterman, 1981).</S>
			<S sid ="110" ssid = "3">It finds the optimal similar regions by maximizing the number of matches and minimizing the number of gaps necessary to align the two sequences.</S>
			<S sid ="111" ssid = "4">For example, the optimal local sequence for the two sequences below is AT−−ATCC, in which “-” indicates a gap: 8 The NeedlemanWunsch (NW) algorithm, was designed to align two sequences of amino-acids (Needleman and Wunsch, 1970).</S>
			<S sid ="112" ssid = "5">The algorithm looks for the sequence alignment which maximizes the similarity.</S>
			<S sid ="113" ssid = "6">As with the LEV score, NW minimizes edit distance, but also takes into account character-to- character similarity based on the relative distance between char s3 = f2(s1, s2 ) Since we often have multiple translations for a given component word/MWE in Panlex, we exhaustively compute the similarity between each MWE translation and component translation, and use the highest similarity as the result of simi.</S>
			<S sid ="114" ssid = "7">If an instance does not have a direct/indirect translation in Panlex, we assign a default value, which is the mean of the highest and lowest annotation score (2.5 for RED DY and 0.5 for BA N NA R D).</S>
			<S sid ="115" ssid = "8">Note that word order is not an issue in our method, as we calculate the similarity independently for each MWE component.</S>
			<S sid ="116" ssid = "9">In this research, we consider simple functions for f1 such as mean, median, product, min and max.</S>
			<S sid ="117" ssid = "10">f2 acters on the keyboard.</S>
			<S sid ="118" ssid = "11">We exclude this score, because it is highly similar to the LEV scores, and we did not obtain encouraging results using NW in our preliminary experiments.</S>
			<S sid ="119" ssid = "12">9 Note that in all experiments we only combine scores.</S>
			<S sid ="120" ssid = "13">given by the same string similarity measure.</S>
			<S sid ="121" ssid = "14">NC Language Frequency Family Czech 100 Slavic Norwegian 100 Germanic Portuguese 100 Romance Thai 99 Kamthai French 95 Romance Chinese 94 Chinese Dutch 93 Germanic Romanian 91 Romance Hindi 67 Indic Russian 43 Slavic VPC:verb Language Frequency Family Basque 100 Basque Lithuanian 100 Baltic Slovenian 100 Slavic Hebrew 99 Semitic Arabic 98 Semitic Czech 95 Slavic Slovak 92 Slavic Latin 79 Italic Tagalog 74 Austronesian Polish 44 Slavic Table 2: The 10 best languages for RE D DY using LCS.</S>
			<S sid ="122" ssid = "15">was selected to be the same as f1 in all situations, except when we use mean for f1.</S>
			<S sid ="123" ssid = "16">Here, following Reddy et al.</S>
			<S sid ="124" ssid = "17">(2011), we experimented with weighted mean: Table 3: The 10 best languages for the verb component of BA N NA RD using LCS.</S>
			<S sid ="125" ssid = "18">f2(s1, s2 ) = αs1 + (1 − α)s2 Based on 3-fold cross validation, we chose α = 0.7 for RED DY.10 Since we do not have judgements for the compositionality of the full VPC in BA N NA R D (we instead have separate judgements for the verb and particle), we cannot use f2 for this dataset.</S>
			<S sid ="126" ssid = "19">Ban- nard et al.</S>
			<S sid ="127" ssid = "20">(2003) observed that nearly all of the verb-compositional instances were also annotated as particle-compositional by the annotators.</S>
			<S sid ="128" ssid = "21">In line with this observation, we use s1 (based on the verb) as the compositionality score for the full VPC.</S>
	</SECTION>
	<SECTION title="Language Selection. " number = "7">
			<S sid ="129" ssid = "1">Our method is based on the translation of an MWE into many languages.</S>
			<S sid ="130" ssid = "2">In the first stage, we chose 54 languages for which relatively large corpora were available.11 The coverage, or the number of instances which have direct/indirect translations in Panlex, varies from one language to another.</S>
			<S sid ="131" ssid = "3">In preliminary experiments, we noticed that there is a high correlation (about 0.50 for BA N NA R D and 10We considered values of α from 0 to 1, incremented by 0.1.</S>
			<S sid ="132" ssid = "4">Table 4: The 10 best languages for the particle component of BA N NA RD using LCS.</S>
			<S sid ="133" ssid = "5">about 0.80 for RED DY) between the usefulness of a language and its translation coverage on MWEs.</S>
			<S sid ="134" ssid = "6">Therefore, we excluded languages with MWE translation coverage of less than 50%.</S>
			<S sid ="135" ssid = "7">Based on nested 10-fold cross validation in our experiments, we select the 10 most useful languages for each cross- validation training partition, based on the Pearson correlation between the given scores in that language and human judgements.12 The 10 best languages are selected based only on the training set for each fold.</S>
			<S sid ="136" ssid = "8">(The languages selected for each fold will later be used to predict the compositionality of the items in the testing portion for that fold.)</S>
			<S sid ="137" ssid = "9">In Tables 2, 3 11In future work, we intend to look at the distribution of translations of the given MWE and its components in corpora for many languages.</S>
			<S sid ="138" ssid = "10">The present method does not rely on the availability of large corpora.</S>
			<S sid ="139" ssid = "11">12Note that for VPCs, we calculate the compositionality of only the verb part, because we don’t have the human judge ments for the whole VPC.</S>
			<S sid ="140" ssid = "12">f1 sim() N1 N2 NC SW 0.541 0.396 0.637 f1 sim() Verb Particle SW 0.369 0.510 Mean Prod Median Min Max LCS 0.525 0.431 0.649 LEV1 0.405 0.200 0.523 LEV2 0.481 0.263 0.577 SW 0.451 0.287 0.410 LCS 0.430 0.233 0.434 LEV1 0.299 0.128 0.311 LEV2 0.294 0.188 0.364 SW 0.443 0.334 0.544 LCS 0.408 0.365 0.553 LEV1 0.315 0.054 0.376 LEV2 0.404 0.134 0.523 SW 0.420 0.176 0.312 LCS 0.347 0.225 0.307 LEV1 0.362 0.310 0.248 LEV2 0.386 0.345 0.338 SW 0.371 0.408 0.345 LCS 0.406 0.430 0.335 LEV1 0.279 0.362 0.403 LEV2 0.380 0.349 0.406 Mean Prod Median Min Max LCS 0.40 6 0.50 9 LEV 1 0.335 0.454 LEV 2 0.340 0.460 S W 0.315 0.316 LCS 0.33 9 0.29 9 LEV 1 0.322 0.280 LEV 2 0.342 0.284 S W 0.316 0.409 LCS 0.35 2 0.42 3 LEV 1 0.295 0.387 LEV 2 0.309 0.368 S W 0.262 0.210 LCS 0.32 9 0.25 1 LEV 1 0.307 0.278 LEV 2 0.310 0.281 S W 0.141 0.288 LCS 0.26 8 0.29 9 LEV 1 0.145 0.450 LEV 2 0.170 0.398 Table 5: Correlation on RE D DY (NCs).</S>
			<S sid ="141" ssid = "13">N1, N2 and NC, are the first component of the noun compound, its second component, and the noun compound itself, respectively.</S>
			<S sid ="142" ssid = "14">and 4, we show how often each language was selected in the top-10 languages over the combined 100 (10×10) folds of nested 10-fold cross validation, based on LCS.13 The tables show that the selected languages were mostly consistent over the folds.</S>
			<S sid ="143" ssid = "15">The languages are a mixture of Romance, Germanic and languages from other families (based on Voegelin and Voegelin (1977)), with no standout language which performs well in all cases (indeed, no language occurs in all three tables).</S>
			<S sid ="144" ssid = "16">Additionally, there is nothing in common between the verb and the particle top-10 languages.</S>
	</SECTION>
	<SECTION title="Results. " number = "8">
			<S sid ="145" ssid = "1">As mentioned before, we perform nested 10-fold cross-validation to select the 10 best languages on the training data for each fold.</S>
			<S sid ="146" ssid = "2">The selected languages for a given fold are then used to compute s1 13Since our later results show that LCS and SW have higher results, we only show the best languages using LCS.</S>
			<S sid ="147" ssid = "3">These largely coincide with those for SW.</S>
			<S sid ="148" ssid = "4">Table 6: Correlation on BA N NA RD (VPC), based on the best-10 languages for the verb and particle individually and s2 (and s3 for NCs) for each instance in the test set for that fold.</S>
			<S sid ="149" ssid = "5">The scores are compared with human judgements using Pearson’s correlation.</S>
			<S sid ="150" ssid = "6">The results are shown in Tables 5 and 6.</S>
			<S sid ="151" ssid = "7">Among the five functions we experimented with for f1, Mean performs much more consistently than the others.</S>
			<S sid ="152" ssid = "8">Median is less prone to noise, and therefore performs better than Prod, Max and Min, but it is still worse than Mean.</S>
			<S sid ="153" ssid = "9">For the most part, LCS and SW perform better than the other measures.</S>
			<S sid ="154" ssid = "10">There is little to separate these two methods, partly because they both look for a sequence of similar characters, unlike LEV1 and LEV2 which do not consider contiguity of match.</S>
			<S sid ="155" ssid = "11">The results support our hypothesis that using multiple target languages rather than one, results in a more accurate prediction of MWE compositionality.</S>
			<S sid ="156" ssid = "12">Our best result using the 10 selected languages on RED DY is 0.649, as compared to the best single- language correlation of 0.497 for Portuguese.</S>
			<S sid ="157" ssid = "13">On BA N NA R D, the best LCS result for the verb component is 0.406, as compared to the best single language correlation of 0.350 for Lithuanian.</S>
			<S sid ="158" ssid = "14">Reddy et al.</S>
			<S sid ="159" ssid = "15">(2011) reported a correlation of 0.714 on RED DY.</S>
			<S sid ="160" ssid = "16">Our best correlation is 0.649.</S>
			<S sid ="161" ssid = "17">Note that Reddy et al.</S>
			<S sid ="162" ssid = "18">(2011) base their method on identification of MWEs in a corpus, thus requiring MWE- specific identification.</S>
			<S sid ="163" ssid = "19">Given that this has been shown to be difficult for MWE types including English VPCs (McCarthy et al., 2003; Baldwin, 2005), the fact that our method is as competitive as this is highly encouraging, especially when you consider that it can equally be applied to different types of MWEs in other languages.</S>
			<S sid ="164" ssid = "20">Moreover, the computational processing required by methods based on dis- tributional similarity is greater than our method, as it does not require processing a large corpus.</S>
			<S sid ="165" ssid = "21">Finally, we experimented with combining our method (STR IN G SIMM E A N ) with a reimplementation of the method of Reddy et al.</S>
			<S sid ="166" ssid = "22">(2011), based on simple averaging, as detailed in Table 7.</S>
			<S sid ="167" ssid = "23">The results are higher than both component methods and the state- of-the-art for RED DY, demonstrating the complementarity between our proposed method and methods based on distributional similarity.</S>
			<S sid ="168" ssid = "24">In Table 8, we compare our results (STR IN G SIMM E A N ) with those of Bannard et al.</S>
			<S sid ="169" ssid = "25">(2003), who interpreted the dataset as a binary classification task.</S>
			<S sid ="170" ssid = "26">The dataset used in their study is a subset of BA N NA R D, containing 40 VPCs, of which 29 (72%) were verb compositional and 23 (57%) were particle compositional.</S>
			<S sid ="171" ssid = "27">By applying a threshold of 0.5 over the output of our regression model, we binarize the VPCs into the compositional and non-compositional classes.</S>
			<S sid ="172" ssid = "28">According to the results shown in Table 6, LCS is a better similarity measure for this task.</S>
			<S sid ="173" ssid = "29">Our proposed method has higher results than the best results of Bannard et al.</S>
			<S sid ="174" ssid = "30">(2003), in part due to their reliance on VPC identification, and the low recall on the task, as reported in the paper.</S>
			<S sid ="175" ssid = "31">Our proposed method does not rely on a corpus or MWE identification.</S>
	</SECTION>
	<SECTION title="Error Analysis. " number = "9">
			<S sid ="176" ssid = "1">We analyse items in RED DY which have a high difference (more than 2.5) between the human annotation and our scores (using LCS and Mean).</S>
			<S sid ="177" ssid = "2">The words are cutting edge, melting pot, gold mine and ivory tower, which are non-compositional accord ing to RED DY.</S>
			<S sid ="178" ssid = "3">After investigating their translations, we came to the conclusion that the first three MWEs have word-for-word translations in most languages.</S>
			<S sid ="179" ssid = "4">Hence, they disagree with our hypothesis that word- for-word translation is a strong indicator of compositionality.</S>
			<S sid ="180" ssid = "5">The word-for-word translations might be because of the fact that they have both compositional and non-compositional senses, or because they are calques (loan translations).</S>
			<S sid ="181" ssid = "6">However, we have tried to avoid such problems with calques by using translations into several languages.</S>
			<S sid ="182" ssid = "7">For ivory tower (“a state of mind that is discussed as if it were a place”)14 we noticed that we have a direct translation into 13 languages.</S>
			<S sid ="183" ssid = "8">Other languages have indirect translations.</S>
			<S sid ="184" ssid = "9">By checking the direct translations, we noticed that, in French, the MWE is translated to tour and tour d’ivoire.</S>
			<S sid ="185" ssid = "10">A noisy (wrong) translation of tour “tower” resulted in wrong indirect translations for ivory tower and an inflated estimate of compositionality.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number = "10">
			<S sid ="186" ssid = "1">In this study, we proposed a method to predict MWE compositionality based on the translation of the MWE and its component words into multiple languages.</S>
			<S sid ="187" ssid = "2">We used string similarity measures between the translations of the MWE and each of its components to predict the relative degree of composition- ality.</S>
			<S sid ="188" ssid = "3">Among the four similarity measures that we experimented with, LCS and SW were found to be superior to edit distance-based methods.</S>
			<S sid ="189" ssid = "4">Our best results were found to be competitive with state-of-the- art results using vector-based approaches, and were also shown to complement state-of-the-art methods.</S>
			<S sid ="190" ssid = "5">In future work, we are interested in investigating whether alternative ways of combining our proposed method with vector-based models can lead to further enhancements in results.</S>
			<S sid ="191" ssid = "6">These models could be especially effective when comparing translations which are roughly synonymous but not string-wise similar.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="192" ssid = "7">We would like to thank Timothy Baldwin, Su Nam Kim, and the anonymous reviewers for their valuable comments and suggestions.</S>
			<S sid ="193" ssid = "8">14This definition is from Wordnet 3.1.</S>
			<S sid ="194" ssid = "9">si m () ST R IN G SI M M E A N ST R IN G SI M M E A N + Re dd y et al. S W 0 . 6 3 7 0 . 7 3 5 L C S 0 . 6 4 9 0 . 7 4 2 L E V 1 0 . 5 2 3 0 . 7 2 4 L E V 2 0 . 5 7 7 0 . 7 2 6 Table 7: Correlation after combining Reddy et al.’s method and our method with Mean for f1 (ST RIN G SIMMEAN ).</S>
			<S sid ="195" ssid = "10">The correlation using Reddy et al.’s method is 0.714.</S>
			<S sid ="196" ssid = "11">M eth od Pr eci sio n Re cal lF sc or e (β = 1) Ac cu rac y Ba nn ar d et al.</S>
			<S sid ="197" ssid = "12">(2 00 3) 0 . 6 0 8 0.</S>
			<S sid ="198" ssid = "13">66 6 0 . 6 3 6 0 . 6 0 0 ST R IN G SI M M E A N 0 . 8 6 2 0.</S>
			<S sid ="199" ssid = "14">71 8 0 . 7 7 4 0 . 6 9 3 Table 8: Results for the classification task.</S>
			<S sid ="200" ssid = "15">ST RIN G SIMMEAN is our method using Mean for f1 NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.</S>
	</SECTION>
</PAPER>
