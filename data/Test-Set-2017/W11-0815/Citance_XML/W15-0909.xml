<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper, we present the first attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation, in integrating compositionality scores for English noun compounds into the TESLA machine translation evaluation metric.</S>
		<S sid ="2" ssid = "2">The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="3" ssid = "3">While the explicit identification of multiword expressions (“MWEs”: Sag et al.</S>
			<S sid ="4" ssid = "4">(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al.</S>
			<S sid ="5" ssid = "5">(2011)) and machine translation (“MT”: Weller et al.</S>
			<S sid ="6" ssid = "6">(2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).</S>
			<S sid ="7" ssid = "7">For instance, Acosta et al.</S>
			<S sid ="8" ssid = "8">(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.</S>
			<S sid ="9" ssid = "9">This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs.</S>
			<S sid ="10" ssid = "10">The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatches between MT outputs and the reference translations, based on compositionality.</S>
			<S sid ="11" ssid = "11">For example, an MT output of white tower should not be rewarded for partial overlap with ivory tower in the reference translation, as tower here is most naturally interpreted compositionally in the MT output, but non-compositionally in the reference translation.</S>
			<S sid ="12" ssid = "12">On the other hand, a partial mismatch between traffic signal and traffic light should be rewarded, as the usage of traffic is highly compositional in both cases.</S>
			<S sid ="13" ssid = "13">That is, we ask the question: can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs, focusing on compound nouns, and the TESLA machine translation metric (Liu et al., 2010).</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="14" ssid = "1">In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs.</S>
			<S sid ="15" ssid = "2">2.1 Machine Translation Evaluation.</S>
			<S sid ="16" ssid = "3">Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators.</S>
			<S sid ="17" ssid = "4">This scoring can be based on: (1) simple string similarity (Pap- ineni et al., 2002; Snover et al., 2006); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010); or (3) deeper linguistic information such as semantic roles (Gime´nez and Ma`rquez, 2008; Pado´ et al., 2009).In this research, we focus on the TESLA MT eval 54 Proceedings of NAACLHLT 2015, pages 54–59, Denver, Colorado, May 31 – June 5, 2015.</S>
			<S sid ="18" ssid = "5">Qc 2015 Association for Computational Linguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs.</S>
			<S sid ="19" ssid = "6">2.2 Compositionality of MWEs.</S>
			<S sid ="20" ssid = "7">Earlier work on MWE compositionality (Bannard, 2006) approached the task via binary classification (compositional or non-compositional).</S>
			<S sid ="21" ssid = "8">However, there has recently been a shift towards regression analysis of the task, and prediction of a continuous- valued compositionality score (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014).</S>
			<S sid ="22" ssid = "9">This is the (primary) approach we take in this paper, as outlined in Section 3.2.</S>
	</SECTION>
	<SECTION title="Methodology. " number = "3">
			<S sid ="23" ssid = "1">3.1 Using compositionality scores in TESLA.</S>
			<S sid ="24" ssid = "2">In this section, we introduce TESLA and our method for integrating compositionality scores into the method.</S>
			<S sid ="25" ssid = "3">Firstly, TESLA measures the similarity between the unigrams of the two given sentences (MT output and reference translation) based on the following three terms for each pairing of unigrams x and y: 1 if lemma (x) = lemma (y) TESLA uses an integer linear program to find the phrase alignment that maximizes the similarity scores over the three terms (Sms, Slem and Spos) for all n-grams.</S>
			<S sid ="26" ssid = "4">In order to add the compositionality score to TESLA, we first identify MWEs in the MT output and reference translation.</S>
			<S sid ="27" ssid = "5">If an MWE in the reference translation aligns exactly with an MWE in the MT output, the weight remains as 1.</S>
			<S sid ="28" ssid = "6">Otherwise, we replace the computed weight computed for the noun compound with the product of computed weight and the compositionality degree of the MWE.</S>
			<S sid ="29" ssid = "7">This forces the system to be less flexible when encountering less compositional noun compounds.</S>
			<S sid ="30" ssid = "8">For instance, in TESLA, if the reference sentence contains ivory tower and the MT output contains white building, TESLA will align them with a score of 1.</S>
			<S sid ="31" ssid = "9">However, by multiplying this weight with the compositionality score (which should be very low for ivory tower), the alignment will have a much lower weight.</S>
			<S sid ="32" ssid = "10">3.2 Predicting the compositionality of MWEs.</S>
			<S sid ="33" ssid = "11">In order to predict the compositionality of MWEs, we calculate the similarity between the MWE and each of its component words, using the three approaches detailed below.</S>
			<S sid ="34" ssid = "12">We calculate the overall compositionality of the MWE via linear interpolation over the component word scores, as: Sms = 2 otherwis e Slem(x, y) = I (lemma (x) = lemma (y)) Spos(x, y) = I (POS (x) = POS (y)) where: a = I (synset (x) ∩ synset (y)) b = I (POS (x) = POS (y)) lemma returns the lemmatised unigram, POS returns the POS tag of the unigram, synset returns the WordNet synsets associated with the unigram, and I (.)</S>
			<S sid ="35" ssid = "13">is the indicator function.</S>
			<S sid ="36" ssid = "14">The similarity between two n-grams x = x1,2,...,n and y = y1,2,...,n is measured as follows: 0 if ∃i, s(xi, yi) = 0 comp (mwe ) = αcompc(mwe , w1) + (1 − α)compc(mwe , w2) where mwe is, without loss of generality, made up of component words w1 and w2, and compc is the compositionality score between mwe and the indicated component word.</S>
			<S sid ="37" ssid = "15">Based on the findings of Reddy et al.</S>
			<S sid ="38" ssid = "16">(2011), we set α = 0.7.</S>
			<S sid ="39" ssid = "17">Distributional Similarity (DS): the distributional similarity between the MWE and each of its components (Salehi et al., 2014), calculated based on cosine similarity over co-occurrence vectors, in the manner of Schu¨ tze (1997), using the 51st–1050th most frequent words in the corpus as dimensions.</S>
			<S sid ="40" ssid = "18">Context vectors were constructed from En s(x, y) = 1 Zn.</S>
			<S sid ="41" ssid = "19">n i=1 s(xi, yi)) otherwise glish Wikipedia.</S>
			<S sid ="42" ssid = "20">All sentences Contains NC METEOR 0.277 0.273 BLEU 0.216 0.206 TESLA 0.238 0.224 TESLADS 0.238 0.225 TESLASS+DS 0.238 0.225 TESLA0/1 0.238 0.225 Table 1: Kendall’s (τ ) correlation over WMT 2013 (all- en), for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output TESLA0/1 0.308 0.464 Table 2: Pearson’s (r) correlation results over the WMT all-en dataset, and the subset of the dataset that contains noun compounds SS+DS: the arithmetic mean of DS and string similarity (“SS”), based on the findings of Salehi et al.</S>
			<S sid ="43" ssid = "21">(2014).</S>
			<S sid ="44" ssid = "22">SS is calculated for each component using the LCS-based string similarity between the MWE and each of its components in the original language as well as a number of translations (Salehi and Cook, 2013), under the hypothesis that com- positional MWEs are more likely to be word-for- word translations in a given language than non- compositional MWEs.</S>
			<S sid ="45" ssid = "23">Following Salehi and Cook (2013), the translations were sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014).</S>
			<S sid ="46" ssid = "24">In Salehi and Cook (2013), the best translation languages are selected based on the training data.</S>
			<S sid ="47" ssid = "25">Since, we focus on NCs in this paper, we use the translation languages reported in that paper to work best for English noun compounds, namely: Czech, Norwegian, Portuguese, Thai, French, Chinese, Dutch, Romanian, Hindi and Russian.</S>
	</SECTION>
	<SECTION title="Dataset. " number = "4">
			<S sid ="48" ssid = "1">We evaluate our method over the data from WMT2013, which is made up of a total of 3000 transla tions for five to-English language pairs (Bojar et al., 2013).</S>
			<S sid ="49" ssid = "2">As our judgements, we used: (1) the original pairwise preference judgements from WMT 2013 (i.e. which of translation A and B is better?); and (2) continuous-valued adequacy judgements for each MT output, as collected by Graham et al.</S>
			<S sid ="50" ssid = "3">(2014).</S>
			<S sid ="51" ssid = "4">We used the Stanford CoreNLP parser (Klein and Manning, 2003) to identify English noun compounds in the translations.</S>
			<S sid ="52" ssid = "5">Among the 3000 sentences, 579 sentences contain at least one noun compound.</S>
	</SECTION>
	<SECTION title="Results. " number = "5">
			<S sid ="53" ssid = "1">We performed two evaluations, based on the two sets of judgements (pairwise preference or continuous- valued judgement for each MT output).</S>
			<S sid ="54" ssid = "2">In each case, we use three baselines (each applied at the segment level, meaning that individual sentences get a score): (1) METEOR (Banerjee and Lavie, 2005), (2) BLEU (Papineni et al., 2002), and (3) TESLA (without compositionality scores).</S>
			<S sid ="55" ssid = "3">We compare these with TESLA incorporating compositionality scores, based on DS (“TESLADS”) and SS+DS (“TESLASS+DS”).</S>
			<S sid ="56" ssid = "4">We also include results for an exact match method which treats the MWEs as a single token, such that unless the MWE is translated exactly the same as in the reference translation, a score of zero results (“TESLA0/1”).</S>
			<S sid ="57" ssid = "5">We did not experiment with the string similarity approach alone, because of the high number of missing translations in PanLex.</S>
			<S sid ="58" ssid = "6">In the first experiment, we calculate the segment level Kendall’s τ following the method used in the WMT 2013 shared task, as shown in Table 1, including the results over the subset of the data which contains a compound noun in both the reference and the MT output (“contains NC”).</S>
			<S sid ="59" ssid = "7">When comparing TESLA with and without MWE compositionality, we observe a tiny improvement with the inclusion of the compositionality scores (magnified slightly over the NC subset of the data), but not great enough to boost the score to that of METEOR.</S>
			<S sid ="60" ssid = "8">We also observe slightly lower correlations for TESLA0/1 than TESLADS and TESLASS+DS, which consider degrees of compositionality, for fren, deen and es-en (results not shown).</S>
			<S sid ="61" ssid = "9">In the second experiment, we calculate Pearson’s r correlation over the continuous-valued adequacy Language Pair comp P→N N→P ∆ fren DS 17 18 1 SS+DS 14 16 2 0/1 30 29 −1 deen DS 21 24 3 SS+DS 14 18 4 0/1 48 40 −8 es-en DS 12 18 6 SS+DS 11 17 6 0/1 20 25 5 ration of the compositionality judgements (“N→P”).</S>
			<S sid ="62" ssid = "10">Overall, the two compositionality methods per form better than the exact match method, and utilising compositionality has a more positive effect than negative.</S>
			<S sid ="63" ssid = "11">However, the difference between the numbers is, once again, very small, except for the ruen language pair.</S>
			<S sid ="64" ssid = "12">The exact match method (“0/1”) has a bigger impact, both positively and negatively, as a result of the polarisation of n-gram overlap scores for MWEs.</S>
			<S sid ="65" ssid = "13">We also noticed that the N→P sentencescs en D S 2 1 2 3 2 f o r S S + D S a r e a s u b s et o f t h e N → P s e nt e n c e s f o r S S + D S 1 4 1 6 2 D S . M o e r o v e r, t h e N → P s e nt e n c e s f o r D S a r e a s u b 0/1 46 49 3 set of the N→P sentences for 0/1; the same is trueru en D S 3 8 5 1 1 3 f o r t h e P → N s e n t e n c e s . S S + D S 2 9 3 9 1 0 0/1 65 80 15 6 Discussion Table 3: The number of judgements that were ranked correctly by TESLA originally, but incorrectly with the in corporation of compositionality scores (“P→N”) and vice versa (“N→P”), and the absolute improvement with com positionality scores (“∆”) judgements, as shown in Table 2, again over the full dataset and also the subset of data containing compound nouns.</S>
			<S sid ="66" ssid = "14">The improvement here is slightly greater than for our first experiment, but not at a level of statistical significance (Graham and Baldwin, 2014).</S>
			<S sid ="67" ssid = "15">Perhaps surprisingly, the exact compositionality predictions produce a higher correlation than the continuous-valued compositionality predictions, but again, even with the inclusion of the compositionality features, TESLA is outperformed by METEOR.</S>
			<S sid ="68" ssid = "16">The correlation over the subset of the data containing compound nouns is markedly higher than that over the full dataset, but the r values with the inclusion of compositionality values are actually all slightly below those for the basic TESLA.</S>
			<S sid ="69" ssid = "17">As a final analysis, we examine the relative impact on TESLA of the three compositionality methods, in terms of pairings of MT outputs where the ordering is reversed based on the revised TESLA scores.</S>
			<S sid ="70" ssid = "18">Table 3 details, for each language pairing, the number of pairwise judgements that were ranked correctly originally, but incorrectly when the compositional ity score was incorporated (“P→N”); and also thenumber of pairwise judgements that were ranked incorrectly originally, and corrected with the incorpo As shown in the previous section, the incorporation of compositionality scores can improve the quality of MT evaluation based on TESLA.</S>
			<S sid ="71" ssid = "19">However, the improvements are very small and not statistically significant.</S>
			<S sid ="72" ssid = "20">Part of the reason is that we focus exclusively on noun compounds, which are contiguous and relatively easy to translate for MT systems (Koehn and Knight, 2003).</S>
			<S sid ="73" ssid = "21">Having said that, preliminary error analysis would suggest that most MT systems have difficulty translating non-compositional noun compounds, although then again, most noun compounds in the WMT 2013 shared task are highly compositional, limiting the impact of composition- ality scores.</S>
			<S sid ="74" ssid = "22">We speculate that, for the method to have greater impact, we would need to target a larger set of MWEs, including non-contiguous MWEs such as split verb particle constructions (Kim and Baldwin, 2010).</S>
			<S sid ="75" ssid = "23">Further error analysis suggests that incorrect identification of noun compounds in a reference sentence can have a negative impact on MT evaluation.</S>
			<S sid ="76" ssid = "24">For example, year student is mistakenly identified as an MWE in ... a 21-year-old final year student at Temple ....</S>
			<S sid ="77" ssid = "25">Furthermore, when an MWE occurs in a reference translation, but not an MT system’s output, incorporating the compositionality score can sometimes result in an error.</S>
			<S sid ="78" ssid = "26">For instance, in the first example in Table 4, the reference translation contains the compound noun cash flow.</S>
			<S sid ="79" ssid = "27">According to the dataset, the output of MT system 1 is better than that of MT sys Reference This means they are much better for our cash flow.</S>
			<S sid ="80" ssid = "28">MT system 1 That is why they are for our money flow of a much better.</S>
			<S sid ="81" ssid = "29">MT system 2 Therefore, for our cash flow much better.</S>
			<S sid ="82" ssid = "30">Reference ‘I felt like I was in a luxury store,’ he recalls.</S>
			<S sid ="83" ssid = "31">MT system 1 ‘I feel as though I am in a luxury trade,’ recalls soldier.</S>
			<S sid ="84" ssid = "32">MT system 2 ‘I felt like a luxury in the store,’ he recalled the soldier.</S>
			<S sid ="85" ssid = "33">Table 4: Two examples from the all-en dataset.</S>
			<S sid ="86" ssid = "34">Each example shows a reference translation, and the outputs of two machine translation systems.</S>
			<S sid ="87" ssid = "35">In each case, the output of MT system 1 is annotated as the better translation.</S>
			<S sid ="88" ssid = "36">tem 2.</S>
			<S sid ="89" ssid = "37">However, since the former translation does not contain an exact match for cash flow, our method decreases the alignment score by multiplying it by the compositionality score for cash flow.</S>
			<S sid ="90" ssid = "38">As a result, the overall score for the first translation becomes less than that of the second, and our method incorrectly chooses the latter as a better translation.</S>
			<S sid ="91" ssid = "39">Incorrect estimation of compositionality scores can also have a negative effect on MT evaluation.</S>
			<S sid ="92" ssid = "40">In the second example in Table 4, the similarity score between luxury store and luxury trade given by TESLA is 0.75.</S>
			<S sid ="93" ssid = "41">The compositionality score, however, is estimated as 0.22.</S>
			<S sid ="94" ssid = "42">The updated similarity between luxury trade and luxury store is therefore 0.16, which in this case results in our method incorrectly selecting the second sentence as the better translation.</S>
			<S sid ="95" ssid = "43">7 Conclusion.</S>
			<S sid ="96" ssid = "44">This paper described the first attempt at integrating MWE compositionality scores into an automatic MT evaluation metric.</S>
			<S sid ="97" ssid = "45">Our results show a marginal improvement with the incorporation of compositionality scores of noun compounds.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="98" ssid = "46">We thank the anonymous reviewers for their insightful comments and valuable suggestions.</S>
			<S sid ="99" ssid = "47">NICTA is funded by the Australian government as represented by Department of Broadband, Communication and Digital Economy, and the Australian Research Council through the ICT Centre of Excellence programme.</S>
	</SECTION>
</PAPER>
