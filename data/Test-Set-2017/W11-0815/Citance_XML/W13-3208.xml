<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents a comparative study of 5 different types of Word Space Models (WSMs) combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of word expressions.</S>
		<S sid ="2" ssid = "2">Many combinations of WSMs and mea-</S>
	</ABSTRACT>
	<SECTION title="the task" number = "1">
			<S sid ="3" ssid = "3">before.</S>
			<S sid ="4" ssid = "4">The study follows Biemann and Giesbrecht (2011) who attempted to find a list of expressions for which the composition- ality assumption – the meaning of an expression is determined by the meaning of its constituents and their combination – does not hold.</S>
			<S sid ="5" ssid = "5">Our results are very promising and can be appreciated by those interested in WSMs, compositionality, and/or relevant evaluation methods.</S>
			<S sid ="6" ssid = "6">1 Introduction.</S>
			<S sid ="7" ssid = "7">Our understanding of WSM is in agreement with Sahlgren (2006): “The word space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in terms of spatial proximity”.</S>
			<S sid ="8" ssid = "8">There are many types of WSMs built by different algorithms.</S>
			<S sid ="9" ssid = "9">WSMs are based on the Harris distributional hypothesis (Harris, 1954), which assumes that words are similar to the extent to which they share similar linguistic contexts.</S>
			<S sid ="10" ssid = "10">WSM can be viewed as a set of words associated with vectors representing contexts in which the words occur.</S>
			<S sid ="11" ssid = "11">Then, similar vectors imply (semantic) similarity of the words and vice versa.</S>
			<S sid ="12" ssid = "12">Consequently, WSMs provide a means to find words semantically similar to a given word.</S>
			<S sid ="13" ssid = "13">This capability of WSMs is exploited by many Natural Language Processing (NLP) applications as listed e.g. by Turney and Pantel (2010).</S>
			<S sid ="14" ssid = "14">This study follows Biemann and Giesbrecht (2011), who attempted to find a list of non- compositional expressions whose meaning is not fully determined by the meaning of its constituents and their combination.</S>
			<S sid ="15" ssid = "15">The task turned out to be frustratingly hard (Johannsen et al., 2011).</S>
			<S sid ="16" ssid = "16">Biemann’s idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).</S>
			<S sid ="17" ssid = "17">We extend this motivation by stating that WSMs could also benefit from a set of non-compositional expressions.</S>
			<S sid ="18" ssid = "18">Specifically, WSMs could treat semantically non-compositional expressions as single units.</S>
			<S sid ="19" ssid = "19">As an example, consider “kick the bucket”, “hot dog”, or “zebra crossing”.</S>
			<S sid ="20" ssid = "20">Treating such expressions as single units might improve the quality of WSMs since the neighboring words of these expressions should not be related to their constituents (“kick”, “bucket”, “dog” or “zebra”), but instead to the whole expressions.</S>
			<S sid ="21" ssid = "21">Recent works, including that of Lin (1999), Baldwin et al.</S>
			<S sid ="22" ssid = "22">(2003), Biemann and Giesbrecht (2011), Johannsen et al.</S>
			<S sid ="23" ssid = "23">(2011), Reddy et al.</S>
			<S sid ="24" ssid = "24">(2011a), Krcˇma´ˇr et al.</S>
			<S sid ="25" ssid = "25">(2012), and Krcˇma´ˇr et al.</S>
			<S sid ="26" ssid = "26">(2013), show the applicability of WSMs in determining the compositionality of word expressions.</S>
			<S sid ="27" ssid = "27">The proposed methods exploit various types of WSMs combined with various measures for determining the compositionality applied to various datasets.</S>
			<S sid ="28" ssid = "28">First, this leads to non-directly comparable results and second, many combinations of 64 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 64–73, Sofia, Bulgaria, August 9 2013.</S>
			<S sid ="29" ssid = "29">Qc 2013 Association for Computational Linguistics WSMs and measures have never before been applied to the task.</S>
			<S sid ="30" ssid = "30">The main contribution and novelty of our study lies in systematic research of several basic and also advanced WSMs combined with all the so far, to the best of our knowledge, proposed WSM-based measures for determining the semantic compositionality.</S>
			<S sid ="31" ssid = "31">The explored WSMs, described in more detail in Section 2, include the Vector Space Model, noted as log) or √cij (denoted as sqrt).</S>
			<S sid ="32" ssid = "32">The purpose of local weighting is to lower the importance of highly occurring words in the document.</S>
			<S sid ="33" ssid = "33">The global function weights every value in row i of C by the same value calculated for row i. Typically: none (denoted as N o), Inverse Document Frequency (denoted as I df ) or a function referred to as Entropy (Ent).</S>
			<S sid ="34" ssid = "34">I df is calculated as 1 + log(ndocs/df (i)) and Ent Latent Semantic Analysis, Hyperspace Analogue as 1 + { j p(i, j) log p(i, j)}/ log ndocs, where to Language, Correlated Occurrence Analogue to Lexical Semantics, and Random Indexing.</S>
			<S sid ="35" ssid = "35">The measures, including substitutability, endocentricity, compositionality, and neighbors-in-common- based, are described in detail in Section 3.</S>
			<S sid ="36" ssid = "36">Section 4 describes our experiments performed on the manually annotated datasets – Distributional Semantics and Compositionality dataset (DISCO) and the dataset built by Reddy et al.</S>
			<S sid ="37" ssid = "37">(2011a).</S>
			<S sid ="38" ssid = "38">Section 5 summarizes the results and Section 6 concludes the paper.</S>
	</SECTION>
	<SECTION title="Word Space Models. " number = "2">
			<S sid ="39" ssid = "1">The simplest and oldest types of WSMs1 are the Vector Space Model (VSM) and Hyperspace Analogue to Language (HAL).</S>
			<S sid ="40" ssid = "2">More recent and advanced models include Latent Semantic Analysis (LSA), which is based on VSM, and Correlated Occurrence Analogue to Lexical Semantics (COALS), which originates from HAL.</S>
			<S sid ="41" ssid = "3">Random Indexing (RI) is WSM joining the principles of LSA and HAL.</S>
			<S sid ="42" ssid = "4">Many other WSMs have been proposed too.</S>
			<S sid ="43" ssid = "5">Their description is outside the scope of this paper and can be found e.g. in Turney and Pantel (2010) or Jurgens and Stevens (2010).</S>
			<S sid ="44" ssid = "6">VSM is based on the assumption that similar (related) words tend to occur in the same documents.2 VSM stores occurrence counts of all word types in documents a given corpus in a co-occurrence matrix C. The row vectors of the matrix correspond to the word types and the columns to the documents in the corpus.</S>
			<S sid ="45" ssid = "7">The numbers of occurrences cij in C are usually weighted by the product of the local and global weighting functions (Nakov et al., 2001).</S>
			<S sid ="46" ssid = "8">The local function weights cij by the same mathematical function; typically none (further denoted as no), log(cij + 1) (de 1 WSMs are also referred to as distributional models of.</S>
			<S sid ="47" ssid = "9">ndocs is the number of documents in the corpora, df (i) is the number of documents containing word type i, and p(i, j) is the probability of occurrence of word type i in document j. LSA builds on VSM and was introduced by Landauer and Dumais (1997).</S>
			<S sid ="48" ssid = "10">The LSA algorithm works with the same co-occurrence matrix C which can be weighted in the same manner as in VSM.</S>
			<S sid ="49" ssid = "11">The matrix is than transformed by Singular Value Decomposition (SVD) (Deerwester et al., 1990) into C. The purpose of SVD is to project the row vectors and column vectors of C into a lower-dimensional space and thus bring the vectors of word types and vectors of documents, respectively, with similar meanings near to each other.3 The output number of dimensions is a parameter of SVD and typically ranges from 200 to 1000 (Landauer and Dumais, 1997; Rohde et al., 2005).</S>
			<S sid ="50" ssid = "12">HAL was first explored by Lund and Burgess (1996).</S>
			<S sid ="51" ssid = "13">It differs from VSM and LSA in that it only exploits neighboring words as contexts for word types.</S>
			<S sid ="52" ssid = "14">HAL processes the corpus by moving a sliding double-sided window with a size ranging from 1 to 5 around the word type in focus and accumulating the weighted co-occurrences of the preceding and following words into a matrix.</S>
			<S sid ="53" ssid = "15">Typically, the linear weighting function is used to ensure that the occurrences of words which are closer to the word type in focus are more significant.</S>
			<S sid ="54" ssid = "16">The dimensions of the resulting co occurrence matrix are of size |V | and 2|V |, where V denotes the vocabulary consisting of all the word types occurring in the processed corpora.</S>
			<S sid ="55" ssid = "17">Finally, the HAL co-occurrence matrix can be reduced by retaining the most informative columns only.</S>
			<S sid ="56" ssid = "18">The columns with the highest values of entropy (− j pj log pj , where pj denotes the prob semantics, vector space models, or semantic spaces.</S>
			<S sid ="57" ssid = "19">2 VSM was originally developed for the SMART information retrieval system (Salton, 1971).</S>
	</SECTION>
	<SECTION title="In this way, LSA is able to capture higher-order  co-. " number = "3">
			<S sid ="58" ssid = "1">occurrences.</S>
			<S sid ="59" ssid = "2">ability of a word in the investigated column j) can be considered as the most informative.</S>
			<S sid ="60" ssid = "3">The alternatives and their description can be found e.g. in The mathematical formulas are presented below.</S>
			<S sid ="61" ssid = "4">cos(a, b) = i=1 aibi Song et al.</S>
			<S sid ="62" ssid = "5">(2004).</S>
			<S sid ="63" ssid = "6">i=1(ai)2 1 i=1(bi)2 COALS was introduced by Rohde et al.</S>
			<S sid ="64" ssid = "7">(2005).</S>
			<S sid ="65" ssid = "8">euc(a, b) = 1 + i=1 (ai − bi)2 Compared to HAL, COALS also processes a corpus by using a sliding window and linear weight cor(a, b) = n n i=1 (ai − a¯)(bi − ¯b) 2 n ¯ 2 ing, but differs in several aspects: the window size i=1(ai − a¯) n i=1(bi − b) n of COALS is 4 and this value is fixed; COALS where a¯ = i=1 ai , ¯b = i=1 bi does not distinguish between the preceding and n n following words and treats them equally; applying COALS supposes that all but the most frequent m columns reflecting the most common open-class words are discarded; COALS transforms weighted counts in the co-occurrence matrix in a special way (all the word pair correlations are calculated, negative values are set to 0, and non-negative ones are square rooted – corr); and optionally, Singular Value Decomposition (Deerwester et al., 1990) can be applied to the COALS co-occurrence matrix.</S>
			<S sid ="66" ssid = "9">RI is described in Sahlgren (2005) and can be viewed as a mixture of HAL and LSA.</S>
			<S sid ="67" ssid = "10">First, RI assigns random vectors to each word type in the corpus.</S>
			<S sid ="68" ssid = "11">The random vectors, referred to as index vectors, are very sparse, typically with a length of thousands, and contain only several (e.g. 7) SU The substitutability-based Measure is based on the fact that the replacement of non- compositional expressions’ constituents by the words similar to them leads to anti-collocations (Pearce, 2002).</S>
			<S sid ="69" ssid = "12">The compositionality of expressions is calculated as the ratio between the number of occurrences of the expression in a corpora and the sum of occurrences of its alternatives – possibly anti-collocations.</S>
			<S sid ="70" ssid = "13">In a similar way, we can compare pointwise mutual information scores (Lin, 1999).</S>
			<S sid ="71" ssid = "14">As an example, consider the possible occurrences of “hot dog” and “warm dog” in the corpora.</S>
			<S sid ="72" ssid = "15">Formally, adopted from Krcˇma´ˇr et al.</S>
			<S sid ="73" ssid = "16">(2012), we calculate the compositionality score csu for an examined expression as follows: nonzero values from the {-1,1} set.</S>
			<S sid ="74" ssid = "17">Second, RI H W (ah, m) ∗ M W (h, am) processes the corpus by exploiting a sliding window like HAL and COALS.</S>
			<S sid ="75" ssid = "18">However, RI does not csu = i=1 i j=1 j , W (h, m) accumulate the weighted co-occurrence counts of neighboring words to the vector of the word type in focus.</S>
			<S sid ="76" ssid = "19">Instead, RI accumulates the index vectors of the co-occurring words.</S>
			<S sid ="77" ssid = "20">For accounting the word order, the permutation variant of RI was also developed (Sahlgren et al., 2008).</S>
			<S sid ="78" ssid = "21">This variant permutes the index vectors of neighboring words of the word type in focus according to the word order.</S>
			<S sid ="79" ssid = "22">3 Compositionality Measures.</S>
			<S sid ="80" ssid = "23">We experimented with four basically different compositionality measures (further referred to as Measures) (Krcˇma´ˇr et al., 2013).</S>
			<S sid ="81" ssid = "24">Each Measure employs a function to measure similarity of WSM vectors.</S>
			<S sid ="82" ssid = "25">We experimented with the following ones: cosine (cos), Euclidian (inverse to Euclidian distance) (euc), and Pearson correlation (cor).where (h, m) denotes the number of corpora oc currences of the examined expression consisting of a head and a modifying word, ah and am denote i j i-th and j-th most similar word4 in a certain WSM to the head and modifying word of the expression, respectively.</S>
			<S sid ="83" ssid = "26">W stands for a weighting function; following Krcˇma´ˇr et al.</S>
			<S sid ="84" ssid = "27">(2012), we experimented with no (no) and logarithm (log) weighting.</S>
			<S sid ="85" ssid = "28">The∗ symbol stands for one of the two operators: ad dition (plus) and multiplication (mult).</S>
			<S sid ="86" ssid = "29">EN The endocentricity-based Measure, also referred to as component or constituent-based, compares the WSM vectors of the examined expressions and their constituents.</S>
			<S sid ="87" ssid = "30">The vectors expected to be different from each other are e.g. the vector representing the expression “hot dog” and the vector representing the word “dog”.</S>
			<S sid ="88" ssid = "31">Formally, the 4 When exploiting POS tags, we constrained the similar words to be of the same POS category in our experiments.</S>
			<S sid ="89" ssid = "32">compositionality score cen can be calculated as follows: cen = f (xh, xm) , where xh and xm denote the similarity (sim) or inverse rank distance (–dist) between the examined expression and its head and modifying constituent, respectively, with regards to a certain WSM.</S>
			<S sid ="90" ssid = "33">Function f stands for a combination of its parameters: 0.5xh + 0.5xm (avg), 0xh + 1xm (mOnly), 1xh + 0xm (hOnly), min(xh, xm) (min), and max(xh, xm) (max).</S>
			<S sid ="91" ssid = "34">CO The compositionality-based Measure compares the true co-occurrence vector of the examined expression and the vector obtained from the vectors corresponding to the constituents of the expression using some compositionality function (Reddy et al., 2011a).</S>
			<S sid ="92" ssid = "35">Commonly used compo sitionality functions are vector addition (⊕) and pointwise vector multiplication (⊗) (Mitchell andLapata, 2008).</S>
			<S sid ="93" ssid = "36">The vectors expected to be dif ferent from each other are e.g. “hot dog” and “hot”⊕“dog”.</S>
			<S sid ="94" ssid = "37">Formally, cco = s(ve, vh ∗ vm) , where ve, vh, and vm stand for vectors of an examined expression, its head and modifying constituents, respectively.</S>
			<S sid ="95" ssid = "38">∗ stands for a vector opera tion.</S>
			<S sid ="96" ssid = "39">NE The neighbors-in-common-based Measure is based on overlap of the most similar words to the examined expression and to its constituents (McCarthy et al., 2003).</S>
			<S sid ="97" ssid = "40">As an example, consider that “hot dog” is similar to “food” or “chips” and “dog” is similar to “cat” or “bark”.</S>
			<S sid ="98" ssid = "41">On the other hand, the list of neighbors of a semantically com- positional expression such as “black dog” is supposed to overlap with at least one of the lists of neighbors of both the expression constituents.</S>
			<S sid ="99" ssid = "42">For Datasets We experimented with the DISCO (Biemann and Giesbrecht, 2011) and Reddy (Reddy et al., 2011a) human annotated datasets, built for the task of automatic determining of semantic compositionality.</S>
			<S sid ="100" ssid = "43">The DISCO and Reddy datasets consist of manually scored expressions of adjective-noun (AN), verb-object (VO), and subject-verb (SV) types and the noun-noun (NN) type, respectively.</S>
			<S sid ="101" ssid = "44">The DISCO dataset consists of 349 expressions divided into training, validation, and test data (TestD); the Reddy dataset consists of one set containing 90 expressions.</S>
			<S sid ="102" ssid = "45">Since the DISCO validation data are of low size (35), we concatenated them with the training data (TrValD).</S>
			<S sid ="103" ssid = "46">To TrValD and TestD we added the Reddy dataset, which we had divided stratifically ahead of time.</S>
			<S sid ="104" ssid = "47">Numbers of expressions of all the different types are summarized in Table 1.</S>
			<S sid ="105" ssid = "48">da ta se t AN VO S V A N V O S V N N Tr Va lD 17 5 68 68 39 45 Te st D 17 4 77 62 35 45 Table 1: Numbers of expressions of all the different types from the DISCO and Reddy datasets.</S>
			<S sid ="106" ssid = "49">WSM construction Since the DISCO and Reddy data were extracted from the ukWaC corpus (Baroni et al., 2009), we also build our WSMs from the same corpus.</S>
			<S sid ="107" ssid = "50">We use our own modification of the S-Space package (Jurgens and Stevens, 2010).</S>
			<S sid ="108" ssid = "51">The modification lies in treating multiword expressions and handling stopwords.</S>
			<S sid ="109" ssid = "52">Specifically, we extended the package with the capability of building WSM vectors for the examined expressions in such a way that the WSM vectors previously built for words are preserved.</S>
			<S sid ="110" ssid = "53">This differentiates our approach e.g. from Baldwin et al.</S>
			<S sid ="111" ssid = "54">(2003), who label the expressions in the corpus ahead of 5 mally, time and treat them as single words.</S>
			<S sid ="112" ssid = "55">As for treat cne = oh + om , ing stopwords, we map trigrams containing deter miners as the middle word into bigrams without where oh and om stand for the number of same the determiner s. The intuition is to extract better words occurring in the list of the most similar words to the examined expression and to its head and modifying constituent, respectively.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
			<S sid ="113" ssid = "1">We evaluated the ability of various combinations of WSMs and Measures to rank expressions as the human annotators had done ahead of time.</S>
			<S sid ="114" ssid = "2">co-occurrence statistics for VO expressions often containing an intervening determiner.</S>
			<S sid ="115" ssid = "3">As an example, compare the occurrences of “reinvent (de 5 Since many single word occurrences disappear, the WSM vectors for words change.</S>
			<S sid ="116" ssid = "4">The more expressions are treated as single words, the more WSM changes.</S>
			<S sid ="117" ssid = "5">Consequently, we believe that this approach cannot be used for building a list of all expressions occurring in an examined corpus ordered by their compositionality score.</S>
			<S sid ="118" ssid = "6">terminer) wheel” and “reinvent wheel” in ukWaC being 623 and 27, respectively.</S>
			<S sid ="119" ssid = "7">We experimented with lemmas (noT ) or with lemmas concatenated with their part of speech (POS) tags (yesT ).</S>
			<S sid ="120" ssid = "8">We labeled the following strings in ukWaC as stopwords: low-frequency words (lemmas with frequency &lt; 50), strings containing two adjacent non-letter characters (thus omitting sequences of various symbols), and closed-class words.</S>
			<S sid ="121" ssid = "9">For our experiments, we built WSMs using various parameters examined in previous works (see Section 2) and parameters which are implied from our own experience with WSMs.</S>
			<S sid ="122" ssid = "10">Figure 1 summarizes all the parameters we used for building WSMs.</S>
			<S sid ="123" ssid = "11">Measure settings We examined various Measure settings (see Section 3), summarized in Table 2.</S>
			<S sid ="124" ssid = "12">For all the vector comparisons, we used the cos similarity.</S>
			<S sid ="125" ssid = "13">Only for HAL we also examined euc and for COALS cor, since these are the recommended similarity functions for these particular WSMs (Lund and Burgess, 1996; Rohde et al., 2005).</S>
			<S sid ="126" ssid = "14">M et.</S>
			<S sid ="127" ssid = "15">pa r. po ssi bl e va lu es all si m. co s, eu c if H A L, co r if C O A L S S U H 0, 1,.</S>
			<S sid ="128" ssid = "16">.., 20 ,3 0,.</S>
			<S sid ="129" ssid = "17">.., 10 0 S U M 0, 1,.</S>
			<S sid ="130" ssid = "18">.., 20 ,3 0,.</S>
			<S sid ="131" ssid = "19">.., 10 0 S U W no , lo g S U E N ∗ x pl us , m ult si m, – di st E N f av g, m O nly , h O nly , mi n, m ax C O N E ∗ N ⊕, ⊗ 10 ,2 0,.</S>
			<S sid ="132" ssid = "20">.., 50 ,1 00 ,2 00 ,...</S>
			<S sid ="133" ssid = "21">,5 00 ,1 00 0 Table 2: All the parameters of Measures for determining semantic compositionality described in Section 3 used in our experiments.</S>
			<S sid ="134" ssid = "22">Experimental setup Following Biemann and Giesbrecht (2011), Reddy et al.</S>
			<S sid ="135" ssid = "23">(2011a), Krcˇma´ˇr et al.</S>
			<S sid ="136" ssid = "24">(2012), and Krcˇma´ˇr et al.</S>
			<S sid ="137" ssid = "25">(2013), we use the Spearman correlation (ρ) for the evaluation of all the combinations of WSMs and Measures (Setups).</S>
			<S sid ="138" ssid = "26">Since the distribution of scores assigned to Reddy’s NN dataset might not have corresponded to the distribution of DISCO scores, we decided not to map them to the same scale.</S>
			<S sid ="139" ssid = "27">Thus, we do not create a single list consisting of all the examined expressions.</S>
			<S sid ="140" ssid = "28">Instead, we order our Setups accord ing to the weighted average of Spearman correlations calculated across all the expression types.</S>
			<S sid ="141" ssid = "29">The weights are directly proportional to the frequencies of the particular expression types.</S>
			<S sid ="142" ssid = "30">Thus, the Setup score (wAvg) is calculated as follows: wAvg = |AN |ρAN + |V O|ρV O + |SV |ρSV + |N N |ρN N . |AN | + |V O| + |SV | + |N N | Having the evaluation testbed, we tried to find the optimal parameter settings for all WSMs combined with all Measures with the help of TrValD.</S>
			<S sid ="143" ssid = "31">Then, we applied the found Setups to TestD.</S>
			<S sid ="144" ssid = "32">Notes Because several expressions or their constituents concatenated with their POS tags did not occur sufficiently often (for expressions: ≥ 0, for constituents: ≥ 50) in ukWaC, we removed them from the experiments; we removed “number crunching”, “pecking order”, and “sacred cow” from TrValD and “leading edge”, “broken link”, “spinning jenny”, and “sitting duck” from TestD.</S>
			<S sid ="145" ssid = "33">5 Results The Setups achieving the highest wAvg when applied to TrValD are depicted in Table 3.</S>
			<S sid ="146" ssid = "34">The same Setups and their results when applied to TestD are depicted in Table 4.</S>
			<S sid ="147" ssid = "35">The values of Spearman correlations in TestD confirm many of the observations from TrValD6: Almost all the combinations of WSMs and Measures achieve correlation values which are statistically significant.</S>
			<S sid ="148" ssid = "36">This is best illustrated by the ρ(AN − V O − SV ) column in Table 4, where a lot of correlation values are statistically (p &lt; 0.05) or highly statistically (p &lt; 0.001) significant, with regards to the number of expressions (172).</S>
			<S sid ="149" ssid = "37">The results suggest that for every expression type, the task of determining compositionality is of varying difficulty.</S>
			<S sid ="150" ssid = "38">While determining the compositionality of the NN expression type seems to be the simplest (the highest correlations observed), determining the compositionality of the SV expression type seems to be hard since the majority of values in the ρSV column are not statistically significant; taking into account the number of SV expressions in TestD – 35, the statistically significant value of ρ at the p &lt; 0.05 level is 0.34.</S>
			<S sid ="151" ssid = "39">The correlation values differ with regards to the expression type.</S>
			<S sid ="152" ssid = "40">Certain WSMs combined with 6 A test of statistical difference between two values of the Spearman correlation is adopted from Papoulis (1990).</S>
			<S sid ="153" ssid = "41">Figure 1: All the parameters of WSMs described in Section 2 used in all our experiments.</S>
			<S sid ="154" ssid = "42">Semicolon denotes OR.</S>
			<S sid ="155" ssid = "43">All the examined combinations of parameters are implied from reading the diagram from left to right.</S>
			<S sid ="156" ssid = "44">certain Measures, although achieving high correlations upon certain expression types, fail to correlate with the rest of the expression types.</S>
			<S sid ="157" ssid = "45">Compare e.g. the correlation values of VSM and LSA combined with the SU Measure upon the AN and SV types with the correlation values upon the VO and NN types.</S>
			<S sid ="158" ssid = "46">The results, as expected, illustrate that employing more advanced alternatives of basic WSMs is more appropriate.</S>
			<S sid ="159" ssid = "47">Specifically, LSA outperforms VSM and COALS outperforms HAL in 21 and 23 correlation values out of 24, respectively.</S>
			<S sid ="160" ssid = "48">Concerning RI, the values of correlations seem to be close to the values of VSM and HAL.</S>
			<S sid ="161" ssid = "49">An interesting observation showing the appropriateness of using wAvg(of ρ) as a good evaluation score is supported by a comparison of the wAvg(of ρ) and ρ(AN −V O−SV ) columns.</S>
			<S sid ="162" ssid = "50">The columns suggest that some Setups might only be able to order the expressions of the same type and might not be able to order the expressions of different types among each other.</S>
			<S sid ="163" ssid = "51">As an example, compare the value of ρ = 0.42 in wAvg(of ρ)with ρ = 0.28 in ρ(AN−V O−SV ) in the row corresponding to COALS combined with SU.</S>
			<S sid ="164" ssid = "52">Con sider also that all the values of correlations are higher or equal to the value in ρ(AN −V O−SV ).</S>
			<S sid ="165" ssid = "53">As for the parameters learned from applying all the combinations of differently set WSM algorithms and Measures to TrValD, their diversity is well illustrated in Tables 5 and 6.</S>
			<S sid ="166" ssid = "54">Due to this diversity, we cannot recommend any particular settings except for one.</S>
			<S sid ="167" ssid = "55">All our SU Measures benefit from weighting numbers of expression occurrences by logarithm.</S>
			<S sid ="168" ssid = "56">The correlation values in TestD are slightly lower – probably due to overfitting – than the ones observed in TrValD.</S>
			<S sid ="169" ssid = "57">HAL combined with the Measures using euc similarity was not as successful as when combined with cos.7 For comparison, the results of Reddy et al.</S>
			<S sid ="170" ssid = "58">(2011b) and Chakraborty et al.</S>
			<S sid ="171" ssid = "59">(2011) as the results of the best performing Setups based on WSMs and association measures, respectively, applied to the DISCO data, are presented (Biemann and Giesbrecht, 2011).</S>
			<S sid ="172" ssid = "60">The correlation values of our Setups based on LSA and COALS, respectively, are mostly higher.</S>
			<S sid ="173" ssid = "61">However, the improvements are not statistically significant.</S>
			<S sid ="174" ssid = "62">Also, the recent results achieved by Krcˇma´ˇr et al.</S>
			<S sid ="175" ssid = "63">(2012) employing COALS and Krcˇma´ˇr et al.</S>
			<S sid ="176" ssid = "64">(2013) employ 7 However, using HAL combined with euc, we observed significant negative correlations which deserve further exploration.</S>
			<S sid ="177" ssid = "65">ing LSA are depicted.</S>
			<S sid ="178" ssid = "66">Discussion As described above, we observed different values of correlations for different expression types.</S>
			<S sid ="179" ssid = "67">This motivates us to think about other classes of expressions different from types; Measures could be e.g. varyingly successful with regards to different occurrence frequency classes of expressions (Evert, 2005).</S>
			<S sid ="180" ssid = "68">However, with such small datasets, as shown e.g. by the fact that the majority of our results are statistically indistinguishable, we cannot carry out any deeper investigations.</S>
			<S sid ="181" ssid = "69">A large dataset would provide a more reliable comparison.</S>
			<S sid ="182" ssid = "70">Ideally, this would consist of all the candidate expressions occurring in some smaller corpus.</S>
			<S sid ="183" ssid = "71">Also, we would prefer the annotated dataset not to be biased towards non-compositional expressions and to be provided with an inner-annotator agreement (Pecina, 2008); which is unfortunately not the case of the DISCO dataset.</S>
			<S sid ="184" ssid = "72">6 Conclusion.</S>
			<S sid ="185" ssid = "73">Our study suggests that different WSMs combined with different Measures perform reasonably well in the task of determining the semantic compositionality of word expressions of different types.</S>
			<S sid ="186" ssid = "74">Especially, LSA and COALS perform well in our experiments since their results are better than those of their basic variants (VSM and HAL, respectively) and, although not statistically significantly, they outperform the best results of the previously proposed approaches (Table 4).</S>
			<S sid ="187" ssid = "75">Importantly, our results demonstrate (Section 5) that the datasets used for the experiments are small for: first, a statistical learning of optimal parameters of both WSM algorithms and Measures; second, a thorough (different types) and reliable (statistically significant) comparison of our and the previously proposed approaches.</S>
			<S sid ="188" ssid = "76">Therefore, we plan to build a larger manually- annotated dataset.</S>
			<S sid ="189" ssid = "77">Finally, we plan to extract a list of semantically non-compositional expressions from a given corpus and experiment with using it in NLP applications.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="190" ssid = "78">We thank to V´ıt Suchomel for providing the ukWaC corpus and the anonymous reviewers for their helpful comments and suggestions.</S>
			<S sid ="191" ssid = "79">This work was supported by the European Regional Development Fund (ERDF), project NTIS – New Technologies for the Information Society, European Centre of Excellence, CZ.1.05/1.1.00/02.0090; by Advanced Computing and Information Systems (grant no.</S>
			<S sid ="192" ssid = "80">SGS 2013029); and by the Czech Science Foundation (grant no.</S>
			<S sid ="193" ssid = "81">P103/12/G084).</S>
			<S sid ="194" ssid = "82">Also, the access to the CERITSC computing facilities provided under the programme Center CERIT Scientific Cloud, part of the Operational Program Research and Development for Innovations, reg.</S>
			<S sid ="195" ssid = "83">no. CZ.1.05/3.2.00/08.0144 is highly appreciated.</S>
	</SECTION>
</PAPER>
