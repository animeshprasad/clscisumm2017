A Bayesian model for joint word alignment and part-of-speech transfer


Robert O¨ stling
Department of Modern Languages, University  of Helsinki 
Department of Linguistics, Stockholm University 
robert.ostling@helsinki.fi, robert@ling.su.se




Abstract

Current methods for word alignment require considerable amounts of parallel text to deliver ac- 
curate results, a requirement which is met only for a small minority  of the world’s approximately
7,000 languages. We show that by jointly performing word alignment and annotation transfer in 
a novel Bayesian model, alignment accuracy can be improved for language pairs where annota- 
tions are available for only one of the languages—a finding which could facilitate the study and 
processing of a vast number  of low-resource languages. We also present an evaluation where 
our method is used to perform single-source and multi-source  part-of-speech transfer with 22 
translations of the same text in four different languages. This allows us to quantify the consid- 
erable variation in accuracy depending on the specific source text(s) used, even with different 
translations into the same language.

1   Introduction

Word alignment is the problem of identifying translationally equivalent words across the languages of 
a parallel  text. It has found widespread use for enabling applications such as statistical machine trans- 
lation (Brown et al., 1993; Koehn et al., 2003), annotation transfer (Yarowsky et al., 2001), word sense 
disambiguation (Diab and Resnik, 2002) and lexicon extraction (Wu and Xia, 1994).
  Although  many types of algorithms have been explored, the main line of research through the last 
couple of decades has been based on the generative IBM models introduced by Brown et al. (1993). 
What these models have in common is that they are unsupervised, asymmetric models, assuming one of 
the languages in a bitext (the source language) generates the corresponding text in the other language 
(the target language), word by word.
  Most often, a variant of the Expectation-Maximization  algorithm (Dempster et al., 1977)  has been 
used for inference in these models, but recently  there has been some work using Bayesian alignment 
models using Gibbs sampling for inference (DeNero et al., 2008; Mermer and Sarac¸lar, 2011; Gal and 
Blunsom, 2013). The incorporation of Bayesian priors into these models has been shown to improve 
accuracy, since they provide a flexible  way of biasing the model towards empirical observations about 
language, most importantly  that a given word type tends to have a very limited  number of translations.
  While the basic word alignment models use only lexical co-occurrence and word order, lexical data 
tends to be sparse and a number  of authors have explored the usefulness of other information  sources. 
Toutanova et al. (2002) showed that Part of Speech (PoS) tags can be integrated into the IBM models to 
improve word alignment accuracy, and others have reported similar results for dependency (Cherry and 
Lin, 2003; Wang and Zong, 2013) and phrase-structure (Yamada and Knight,  2001) parse trees, and for 
lemmatized texts (Bojar and Prokopov, 2006).
  In addition to the studies just mentioned that showed how various types of linguistic  annotation can be 
used to guide word alignment,  there has been research showing that the reverse also holds: word-aligned 
parallel texts can be used to transfer annotations and models from languages where those resources exist 
to languages where they do not. Pioneering work by Yarowsky et al. (2001) explored tasks such as PoS

This work is licenced  under a Creative  Commons  Attribution 4.0 International License.  License  details: http://
creativecommons.org/licenses/by/4.0/


620

Proceedings of COLING 2016, the 26th International  Conference on Computational Linguistics:  Technical Papers, 
pages 620–629, Osaka, Japan, December 11-17 2016.


tagging, shallow parsing and lemmatization, which was followed by e.g. dependency parsing (Hwa et 
al., 2005).
  The present work  combines these previous lines of work by exploring joint models of word alignment 
and annotation transfer (of PoS tags), within a Bayesian framework.  The source code of our implemen- 
tation is available at http://www.ling.su.se/spacos.

2	Methods

This section first discusses Bayesian word alignment  using IBM-based  models along with extensions to 
these, and finally describes our model of joint PoS transfer and word alignment.

2.1	Bayesian IBM models

The IBM 1 alignment model can be extended with sparse Dirichlet priors, and efficient  inference is 
possible using Gibbs sampling (Mermer  and Sarac¸lar, 2011; Mermer et al., 2013; Gal and Blunsom,
2013) or Variational  Bayesian techniques (Riley and Gildea, 2012).
  IBM model 1 assumes each target word  tj  = f of a sentence is generated by one source word saj   = e 
through the alignment variable aj , and that all words are generated independently  and do not depend on 
the sentence positions i and j.  The probability of a target sentence t (of length J ) and an alignment  a 
given a source sentence s (of length I ) then becomes

J
P (t, a|s) ∝ p(J |I ) TI P (tj |sa  ) 	(1)
j=1

One drawback of this model (apart from the extreme independence assumptions addressed by later
IBM models) is that there is no penalty for having very flat distributions P (f |e) for target words con-
ditioned on a source word,  a fact that causes the so-called garbage  collection  effect where rare source
words are incorrectly linked to a large number of target words. By using priors on the translation distribu- 
tions that discourage such solutions, it is possible to improve alignment accuracy. Mermer and Sarac¸lar 
(2011) introduced the use of Dirichlet priors for this task. If the Dirichlet  parameter α is 1, this reduces 
to the uniform distribution, but it turns out that by using much smaller values of α, below about 10−2 
(Riley and Gildea, 2012, Figure 1), the model better reflects the empirical observation that words tend to 
have very few possible translations.

2.2   Inference
For the standard IBM models, the EM algorithm is normally  used for inference. In the Bayesian version 
with Dirichlet priors, we mentioned above that two main options have been investigated:  Variational 
Bayes and Gibbs sampling. While both methods have been shown to improve word alignment accuracy 
for IBM model 1, the computational complexity of Gibbs sampling is lower with more complex models 
(O¨ stling and Tiedemann, 2016, Section 3.2). For this reason, Gibbs sampling is used in the present work 
and will be discussed in further detail.
  Gibbs sampling (Gelfand and Smith, 1991) is a specific instance of the more general Markov  Chain 
Monte Carlo algorithm, which is used to draw samples from a model  M  which defines  a probability 
function pM (x) over the variable space x. This is done by constructing a Markov  chain with pM  as its 
stationary distribution  and performing a sufficiently  long random walk in it. A Gibbs sampler achieves
this by specifying for each variable xi  in x a sampling distribution  P (xi  = a|x−i) for xi  conditioned
on x−i, which denotes all variables in x except xi.  For IBM model 1, this gives the following sam-
pling equation, which we also use, and for more complex models extend with additional factors given
Equation (4):



P (aj  = i) =


na−j
 


,si ,tj


+ αtj



(2)




Here, na


f
 
(
n
a
−
j
 
,
s
i
 
,
f
 
+
 
α
f
 
)
,e,f  is a 
count vector 
representing 
the number 
of times 
each source 
type e is 
aligned to 
each


target type f under the alignment a, not counting the alignment at position j. In the end, we are interested



in computing the expectations E [δaj ,i] under the alignment model, where δ is the Kronecker delta. Given 
a series of samples of a(t) for t ∈ 1 . . . T , we approximate this using



1
E [δaj ,i] ≈ T


T
 

t=1



P (aj  = i|a(t) , s, t) 	(3)



The initial alignments a(0)  are sampled from a uniform distribution,  and in order to reduce initialization 
bias we average the marginals from eight independently initialized samplers. For details on the tradeoffs 
involved in choosing the number of samplers and sampling iterations, we refer to Table 2 of O¨ stling and 
Tiedemann (2016).

2.3   Word order and fertility

Even with good prior parameters, IBM model 1 is a poor model of word alignment  because it ignores two 
important characteristics of parallel text: word order and morpheme counts. While different distortion 
models have been used to model word order, we use the HMM-based model of Vogel et al. (1996), which 
has been demonstrated to deliver better performance than either no distortion model (like IBM model 1) 
or models based on absolute sentence positions (IBM models 2 and 3). This introduces  a distribution
P (aj  − aj−1|I ) of the “jump” aj  − aj−1  in the source sentence when moving  one step in the target
sentence.
As a way of modeling the relative number of morphemes in a word for a pair of languages, the fertility

of a source word e is defined as the number of target words it is aligned to in a particular context. This is 
modeled using a distribution  P (φi  = k|si  = e), where the fertility φi  at position i is conditioned on the
word e at that position. This is particularly important when the languages have large differences in word 
formation  strategies and the general level of morphological complexity.
  Zhao and Gildea (2010) explored a model with a word order and fertility model  as described  above, but 
based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations. 
An important conclusion from their work is that a simple HMM with fertility model is competitive with 
the more complex IBM model 4, and we follow them in using this model  as our baseline.  Our full 
baseline model is given by
P  s, t, a,θ, ψ, π, α, β, γ)


 K  J (k)


   E 	F 	


TI TI
∝ 
k=1 j=1


θs(k)
a(k)
j


,t(k)  · 


TI TI

e=1 f =1


αf −1
e,f 	


 K  J (k) +1
TI  TI


   Imax
TI



mmax
TI




βI ,m −1



(4)


· 	ψa(k)


(k)  · 


ψm	



k=1



j=1


j −aj−1



I =Imin m=mmin


 K  I (k)


     E


nmax 	\


TI TI
· 
k=1 i=1


i  ,φi 


TI TI

e=1 n=0


γn −1
e,n



where K is the number of parallel  sentences, θ ∼ Dir  α) are the lexical translation parameters, ψ ∼ 
Dir  β) are the categorical distribution  parameters for the word order model P  aj  − aj−1  = m|I ), and 
πe ∼ Dir  γ) for the fertility model P (φi  = k|si  = e).

2.4   PoS-guided word alignment

This work follows Toutanova et al. (2002) in adding another factor to the model, akin to the lexical
translation probability P (f |e) but using the PoS tags of the respective words, P (T t |T s).  The main
f	e
difference is that in their work PoS tags for both source and target languages were assumed, whereas
here only one of the languages is assumed to be PoS-annotated. For the other language, the PoS tags are 
sampled using the method described below.


Algorithm 1 Alternating alignment-annotation.
[> Align a single sentence pair s, t. The extension to multiple sentences is straightforward.
function AAA(s, t)
[> initialize  alignments using the baseline HMM + fertility model

[> the forward direction uses alignment vector a 
a ← Baseline(s, t)
[> the backward direction uses alignment vector b 
b ← Baseline(t, s)
while sampling do
M ← estimate bigram HMM model using a, b, s, t, T s
[> set the target sentence tags T t using the Viterbi algorithm
T t ← arg maxT P (T |M )
[> sample alignment variables a, b
for all j ← 1 . . . J do


aj  ∼ P (aj  = i|a−j , s, t, T
end for
for all i ← 1 . . . I do


s, T t)


bi  ∼ P (bi  = j|b−i, s, t, T
end for
end while


s, T t)


[> return expected values for PoS tags alignments,  as in Equation (3)
return E [a], E [b], E [T t]
end function


2.5   PoS transfer

We focus on applying  PoS transfer as a way of obtaining better word alignment accuracy, rather than 
improving  PoS transfer as such. These are largely complementary goals, as our evaluation in Section 3 
shows that small changes in PoS tagging accuracy do not seem to influence alignment accuracy. For this 
reason, and because of our focus on low-resource languages precludes using data-intensive approaches 
like that of Das and Petrov (2011),  we choose the simple method of Yarowsky  and Ngai (2001)  as a 
starting point for the PoS transfer part of our model. The basis of this method is to use heuristics  to 
estimate a robust first-order  HMM tagger from (noisy) projected tags, and to re-tag the data using this 
tagger. Furthermore we extended the tagger using the affix-tree method of Schmid (1994) for rare words, 
in order to be able to handle morphologically  complex languages better.
  While it would have been preferable for reasons of theoretical elegance to use a simpler  PoS transfer 
model, matching the alignment model, such attempts by O¨ stling et al. (2015) resulted in very modest 
improvements for their sign language data set, and their model gave no improvement at all for our data
sets.

2.6   Alternating alignment-annotation (AAA)

Algorithm 1 summarizes our method, which can be viewed  as a modified  Gibbs sampler of the latent 
alignment variables a and b (in the forward  and backward alignment  direction)  as well as the target-side 
PoS tags T t.  While the PoS transfer part is not stochastic,1  it operates on samples of the alignment 
variables a and b and can be seamlessly integrated into the sampler.

3   Evaluation

The empirical evaluation aims at investigating whether the alternating alignment-annotation (AAA) al- 
gorithm improves word alignment  and/or PoS transfer accuracy, compared to the corresponding PoS-

  1 We also tried sampling T t using marginal distributions computed by the forward-backward algorithm, but found no effect 
on the accuracy of the algorithm.


Corpus 	Sentences	|S|	|P |

E
ng
lis
h-
Fr
en
ch
1 
13
0 
5
8
8
4 
03
8
17 
43
8
Ro
m
an
ia
n-
En
gli
sh
4
8
 
6
4
1
5 
03
4
5 
0
3
4
E
ng
lis
h-
In
uk
tit
ut
3
3
3
 
1
8
5
2
9
3
1 
9
7
2
E
ng
lis
h-
Hi
nd
i
3
 
5
5
6
1 
40
9
1 
4
0
9
E
ng
lis
h-
S
w
ed
is
h
6
9
2
 
6
6
2
3 
34
0
4 
5
7
7

Table 1: Total corpus sizes (in sentences) and number of (S)ure and (P)ossible alignment links in their 
respective evaluation sets.


unaware Bayesian IBM model with an HMM word order model and fertility.

3.1   Data

In order  to assess the general usefulness of the method presented, a number of parallel corpora represent- 
ing a diverse  set of languages and genres are used: the English-French  Hansards corpus in the version 
presented by Mihalcea  and Pedersen (2003), the Romanian-English, English-Inuktitut and English-Hindi 
corpora from Martin et al. (2005), as well as parts of the Swedish-English Europarl corpus (Koehn, 2005) 
with the evaluation set of Holmqvist and Ahrenberg (2011). In addition,  a set of translations of the New 
Testament will be used to investigate the quality of the transfered PoS tags. Some properties of these 
corpora are summarized in Table 1.
  Silver-standard PoS annotations were provided for English, French and German by the Stanford Tag- 
ger (Toutanova et al., 2003) and for Swedish by Stagger (O¨ stling, 2013). The native tagsets were mapped 
to the Universal PoS Tagset of Petrov et al. (2012).

3.2   Experimental  setup

The main intended use case is a fairly short parallel text, with two very different  languages of which 
only one has an accurate PoS tagger available.  This excludes the possibility  of extensive per-language 
tuning (unlike some of the previous results cited), and in this evaluation the different language pairs use 
identical  parameters to the largest possible extent.2   We fixed the hyperparameters in Equation (4) to 
α = 10−5, β = γ = 1.
  The experiments use eight individually initialized samplers, each of which used a pipelined approach 
where initially a lexical-only  model equivalent to that of Mermer  and Sarac¸lar (2011) was used, then a 
word order term using the HMM model was added, then the fertility term, and finally (when applicable) 
the PoS translation probability. No burn-in period was used during sampling, since the initial value of 
the last pipeline step is already quite good.
  Since the model is asymmetric,  the alignments are run in both directions and symmetrized.  A soft 
variant of the intersection heuristic is used, where the final set of links L is defined  as L  = {(i, j)  | 
P (aj  = i)P (bi  = j) > t}. for a threshold value t, in these evaluations fixed to 0.25. This gives a fairly
conservative  set of links, favoring precision before recall. Note however that the model does not use 
NULL words, so this conservatism is not as severe as in models with NULL words.3   Heuristics  based on 
the union on the contrary tend to over-generate links under these conditions.

3.3   Results

The systems used as baselines in the evaluation are mainly  from the Workshop on Parallel Text shared 
tasks (Mihalcea  and Pedersen, 2003; Martin et al., 2005), where most of the data sets used were intro-

  2 The main exception is that some of the language pairs (Romanian-English  and English-Hindi), following previous work, 
use a poor man’s stemming  trick where only the first four letters of each token is used. The only other exception is that the 
English-French evaluation did not use the fertility parameter, since it showed no further improvement beyond the plain HMM 
model.
     3 Later experiments with a related model (O¨ stling and Tiedemann, 2016, compare their Table 2 with our Table 2) show 
that for some language pairs in particular, using NULL with standard symmetrization  heuristics gives considerably worse AER 
scores.


duced: ISI2 (Fraser and Marcu, 2005), JHU (Schafer and Dra´bek, 2005), UMIACS2 (Lopez and Resnik,
2005) and XRCE (Dejean et al., 2003). The Swedish-English figures, LIU, are from Holmqvist and 
Ahrenberg (2011). Finally, we have run GIZA++ (Och and Ney, 2003) on the corpora as an additional 
baseline.4   Results from previous work using more data than a bitext plus PoS tags for one of the lan- 
guages are not included,  although some such systems have obtained better results on some of the corpora 
used, using e.g. semi-supervised discriminative training (Liu et al., 2010).
  Table 2 summarizes  the main results of the evaluation.   In all cases,  the alternating alignment- 
annotation  method surpasses the baseline model that does not use PoS tags. The model is generally 
competitive compared to previous work, in particular for the smaller corpora and where the languages 
are substantially  different. The improvement compared to the non-Bayesian  baselines is particularly 
good for the English-Inuktitut  corpus, which could be due to the fact that the morpheme/word ratio of 
Inuktitut is very high, resulting in very many low-frequency words that tend to function  as garbage col- 
lectors in non-Bayesian models. The situation is similar for the English-Hindi  corpus, although in this 
case the cause for the many rare words is rather the short bitext than the languages themselves.
  It is interesting to compare the corresponding AAA and Supervised figures in Table 2, where the only 
difference is that AAA uses a supervised  PoS tag on one language (English)  and annotation transfer to 
the other, whereas Supervised  uses supervised  PoS tags on both languages.  The overall  accuracy figures 
are nearly identical,  even though the accuracy of the transfered tags is lower. This indicates that the 
word alignment algorithm is not very sensitive to PoS tagging accuracy, so that the relatively  simple PoS 
transfer method used is sufficient for the purpose of increasing word alignment accuracy.
  There  are multiple translations of the New Testament into each of English,  French, German and 
Swedish, which can be exploited for multi-source transfer. In our model, multi-source transfer can be 
done trivially by averaging the expectations returned by Algorithm 1. Table 3 shows that this overall has 
a large positive effect on PoS accuracy, with an average error reduction of one fourth compared to the 
median single-source result, and one tenth compared to the best (out of 22) single-source result. Using 
many translations in each language  allows  us to see how widely the accuracy varies, even when using the 
same source (or target) language. This is due to many factors, including  the large time span (hundreds of 
years) between the different translations. In contrast, the multi-source  results are, as could be expected, 
much more robust. This is an encouraging result, given that the New Testament is perhaps the most 
widely translated text of significant length, and offers a great possibility  to transfer linguistic  annotations 
to languages where little other data is available.

4   Conclusions and future work

We have presented a model for joint word alignment and PoS annotation transfer, and shown empirically 
that it leads to improved word alignment accuracy, in particular for low-resource  languages.  Using 
automatically  transfered PoS tags led to improvements  that were as big as the improvements  seen when 
using PoS tags from supervised taggers on both sides of a bitext.
  In addition, we took the opportunity to perform an evaluation investigating what kind of variation can 
be expected depending on which  translation(s)  are used as source texts in PoS annotation transfer, and 
found that this variation  can be great, even among translations into the same language.  Using multi- 
source transfer reduces this variation considerably and typically  gives better accuracy than even the best 
single-source transfer among many.
  In this study, only PoS annotations were considered, but there are other types of annotation  such as 
parse trees, named entities  and word senses which potentially could be transfered jointly with word 
alignment. This is left to future work,  as are improvements  to the baseline alignment model.

Acknowledgments

Thanks to the anonymous reviewers, Mats Wire´n, Jo¨ rg Tiedemann and Joakim Nivre for advice.

  4 In order to provide a competitive but fair baseline, the same general approach was used with GIZA++ as with the new sys- 
tem presented, using default parameters and no language-specific tuning.  The specific alignment pipeline used was 13 h5 33 410 , 
and the symmetrization  that provides the best alignment on the test set is chosen.  This gives GIZA++ some advantage, but 
ensures that any claimed improvements by our algorithm over GIZA++ are not simply due to symmetrization.








Model	|A|	|A ∩ S|	|A ∩ P |	P	R 	F 	AER
English-French (|S| = 4038, |P | = 17438. 1 130 588 sentences)

B
as
eli
ne
53
59
37
17
5
1
3
4
9
5
.
8
92
.1
93
.9
5
.
8
A
A
A
55
05
37
51
5
2
5
4
9
5
.
4
92
.9
94
.1
5
.
6
Su
pe
rvi
se
d
55
42
37
78
5
2
6
3
9
5
.
0
93
.6
94
.3
5
.
6
GI
Z
A
++
48
31
35
31
4
7
1
5
9
7
.
6
87
.4
92
.2
7
.
0
X
R
C
E



9
0
.
1
93
.8
91
.9
8
.
5
Romanian-English 
(|S| = |P | = 6201. 
48 641 sentences)
B
as
eli
ne
33
74
30
70
3
0
7
0
9
1
.
0
61
.0
73
.0
27
.0
A
A
A
34
47
31
20
3
1
2
0
9
0
.
5
62
.0
73
.6
26
.4
GI
Z
A
++
37
30
31
61
3
1
6
1
8
4
.
7
62
.8
72
.1
27
.9
IS
I2



8
7
.
9
63
.1
73
.5
26
.6
R
A
C
AI



7
6
.
8
71
.2
73
.9
26
.1
English-Inuktitut (|S| 
= 293, |P | = 1972. 
333 185 sentences)
B
as
eli
ne
5
9
8
2
6
7
5
5
9
9
3
.
5
91
.1
92
.3
7
.
3
A
A
A
6
3
0
2
7
3
5
9
5
9
4
.
4
93
.2
93
.8
6
.
0
GI
Z
A
++
3
4
2
1
7
0
3
0
6
8
9
.
5
58
.0
70
.4
25
.0
J
H
U



9
6
.
7
76
.8
85
.6
9
.
5
J
H
U



8
4
.
4
92
.2
88
.1
14
.3
English-Hindi (|S| 
= |P | = 1409. 3 
556 sentences)
B
as
eli
ne
7
1
2
6
0
6
6
0
6
8
5
.
1
43
.0
57
.1
42
.9
A
A
A
8
1
7
6
7
7
6
7
7
8
2
.
9
48
.0
60
.8
39
.2
GI
Z
A
++
9
8
4
6
1
5
6
1
5
6
2
.
5
43
.6
51
.4
48
.6
U
MI
A
C
S
2



4
3
.
7
56
.1
49
.1
50
.9
English-Swedish (|S| = 
3340, |P | = 4577. 692 
662 sentences)
B
as
eli
ne
31
83
27
42
2
9
3
3
9
2
.
1
82
.1
86
.8
13
.0
A
A
A
31
25
27
74
2
9
6
1
9
4
.
8
83
.1
88
.5
11
.3
Su
pe
rvi
se
d
32
62
28
23
3
0
3
4
9
3
.
0
84
.5
88
.6
11
.3
GI
Z
A
++
34
36
28
90
3
1
3
6
9
1
.
3
86
.5
88
.8
11
.1
LI
U



8
5
.
3
–
–
12
.6

Table 2: Results from the empirical evaluation, including the Bayesian model without PoS tags (Base- 
line), the alternating alignment-annotation algorithm (AAA), the corresponding method but with super- 
vised PoS taggers for both languages (Supervised), and comparable previous results on the same data.
The number of alignment links |A|, of which |A ∩ S| are considered (S)ure, and |A ∩ P | (P)ossible, are
reported. For convenience, precision (P ), recall (R), F1 score (F ) and Alignment  Error Rate (AER) are
also given.






Target			Source texts			Multi 
deu (8)	eng (5)		fra (5)	swe (4)	All (22)

de
u1
d
e
u
2
 
d
e
u
3
 
d
e
u
4
 
d
e
u
5
 
d
e
u
6
 
d
e
u
7
 
d
e
u
8
 
e
n
g
1
 
e
n
g
2
 
e
n
g
3
 
e
n
g
4
 
e
n
g
5
 
f
r
a
1
 
f
r
a
2
 
f
r
a
3
 
f
r
a
4
 
f
r
a
5
 
s
w
e
1
 
s
w
e
2
 
s
w
e
3
 
s
w
e
4











74
.2	76.2	79.4
79
.2	81.8	84.2
80
.1	81.7	83.7
79
.5	81.4	84.3
80
.3	81.5	84.1
80
.2	82.9	83.5
80
.3	83.5	84.5
80
.5	83.1	83.5
80
.3	83.5	83.8
80
.5	83.2	83.8
80
.4	81.2	81.8
76
.3	77.5	79.4
81
.3	82.1	82.5
81
.8	82.3	82.7
79
.0	80.1	80.9
79
.3	80.8	81.4
79
.8	80.6	81.7
80
.6	81.1	82.4
80
.2	80.7	81.7
79
.4	81.3	81.9
79
.9	81.8	82.3
80
.0	81.4	82.3







80
.1	81.3	81.5
80
.7	80.9	81.1
79
.9	81.2	81.9
80
.0	81.1	81.2
80
.1	81.2	81.4
82
.8	84.0	85.4
80
.9	81.7	82.4
83
.4	85.4	86.4
82
.7	84.8	85.4
80
.1	81.1	81.4
79
.5	80.5	80.7
81
.1	81.9	82.0
81
.0	81.8	81.8
81
.3	81.6	82.2
80
.0	80.7	81.3
81
.1	81.2	81.9
81
.0	81.4	82.0
76
.2	77.0	77.8
80
.9	81.4	82.0
80
.6	81.0	81.6
80
.3	80.7	81.3
80
.7	81.0	81.8







80
.2	81.0	81.9
76
.9	77.9	79.4
81
.1	82.5	82.8
81
.8	82.7	83.3
75
.0	83.7	85.1
78
.3	83.5	85.2
77
.1	84.4	85.9
75
.3	83.2	85.4
76
.5	84.9	85.9
80
.7	85.0	86.0
76
.6	85.6	86.3
76
.3	84.8	85.7
76
.6	81.5	81.7
79
.8	85.8	86.2
80
.7	85.7	86.3
78
.8	85.8	86.5
80
.3	86.0	86.7
78
.0	83.8	84.5
77
.0	83.6	84.7
77
.6	84.3	85.1
77
.4	84.1	84.6
77
.2	84.0	84.8
75
.0	81.0	85.1
78
.3	80.7	85.2
77
.1	81.7	85.9
75
.3	81.6	85.4
76
.5	81.5	85.9
79
.4	81.0	86.0
76
.6	81.7	86.3
76
.3	81.6	85.7
74
.2	76.7	81.7
79
.2	81.8	86.2
80
.1	81.6	86.3
78
.8	81.3	86.5
80
.3	81.5	86.7
78
.0	82.5	84.5
77
.0	83.0	84.7
77
.6	82.7	85.1
77
.4	82.7	84.6
77
.2	82.9	84.8
80
.2	81.2	85.4
76
.3	78.3	82.4
81
.1	82.5	86.4
81
.8	82.7	85.4
8
6
.
8
8
5
.
8
8
8
.
2
8
7
.
3
8
6
.
8
8
5
.
4
8
6
.
4
8
6
.
5
8
3
.
8
8
5
.
5
8
5
.
4
8
5
.
1
8
4
.
7
8
5
.
8
8
6
.
0
8
5
.
3
8
5
.
3
8
6
.
1
9
0
.
3
8
5
.
7
9
0
.
6
9
0
.
7
A
vg
.
79
.6	81.6	82.9
80
.5	81.7	82.4
80
.2	80.9	81.5
77
.7	84.4	85.4
77
.9	81.5	85.3
8
6
.
5

Table 3: PoS transfer accuracy (in percent) using single-source (first five columns) and multi-source (rightmost column) transfer in the New Testament corpus. 
Rows are target texts and columns are source languages. For each language (with number of translations), the worst/median/best results are given for the different 
translations. The All columns summarize the results over all the source texts from the preceding columns.  Finally, Multi is the result of multi-source transfer 
using the sums of tag marginal distributions.  The best result on each row is bold-faced.


References

Ondej Bojar and Magdalena Prokopov. 2006. Czech-English word alignment. In LREC 2006, pages 1236–1239, 
Genova, Italy. ELRA.

Peter F. Brown,  Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of 
statistical machine translation: Parameter estimation.  Computational  Linguistics, 19(2):263–311, June.

Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. In ACL 2003, pages 88–95, 
Sapporo, Japan, July. Association for Computational Linguistics.

Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In ACL 2011, HLT ’11, pages 600–609, Stroudsburg, PA, USA. Association for Computational Linguistics.

Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji Yamada. 2003. Reducing parameter space for word align- 
ment. In HLT-NAACL-PARALLEL ’03, pages 23–26, Stroudsburg, PA, USA. Association for Computational 
Linguistics.

A. P. Dempster,  N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, 39(1):1–38.

John DeNero, Alexandre Bouchard-Coˆ te´, and Dan Klein. 2008. Sampling alignment structure under a Bayesian 
translation model. In EMNLP 2008, pages 314–323, Honolulu, Hawaii, October. Association for Computational 
Linguistics.

Mona Diab and Philip Resnik. 2002. An unsupervised method for word  sense tagging  using parallel  corpora.  In
ACL 2002, pages 255–262, Stroudsburg, PA, USA. Association for Computational Linguistics.

Alexander  Fraser and Daniel Marcu.  2005.  ISI’s participation in the Romanian-English alignment task.  In 
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 91–94, Stroudsburg, PA, USA. 
Association for Computational Linguistics.

Yarin Gal and Phil Blunsom. 2013. A systematic Bayesian treatment of the IBM alignment models. In NAACL
2013, Stroudsburg, PA, USA. Association for Computational Linguistics.

Alan E. Gelfand and Adrian F. M. Smith. 1991. Gibbs sampling for marginal posterior expectations. Technical 
report, Department of Statistics, Stanford University.

Maria Holmqvist and Lars Ahrenberg. 2011. A gold standard for English-Swedish word alignment. In Proceed- 
ings of the 18th Nordic Conference of Computational Linguistics (NODALIDA  2011), number 11 in NEALT 
Proceedings Series, pages 106–113.

Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping  parsers via 
syntactic projection  across parallel texts. Natural  Language Engineering,  11(3):311–325,  September.

Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical  phrase-based translation.   In Proceedings 
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on 
Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Stroudsburg, PA, USA. Association for 
Computational Linguistics.

Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In The Tenth Machine
Translation Summit., Phuket, Thailand.

Yang Liu, Qun Liu, and Shouxun Lin. 2010. Discriminative word alignment by linear modeling. Computational
Linguistics, 36(3):303–339, September.

Adam Lopez and Philip Resnik. 2005. Improved HMM alignment models for languages with scarce resources. In 
Proceedings of the ACL Workshop on Building and Using Parallel Texts, ParaText ’05, pages 83–86, Strouds- 
burg, PA, USA. Association for Computational Linguistics.

Joel Martin, Rada Mihalcea,  and Ted Pedersen. 2005. Word alignment for languages with scarce resources.  In 
Proceedings of the ACL Workshop on Building and Using Parallel Texts, ParaText ’05, pages 65–74, Strouds- 
burg, PA, USA. Association for Computational Linguistics.

Cos¸kun Mermer and Murat Sarac¸lar. 2011. Bayesian word alignment for statistical machine translation.  In ACL
2011, pages 182–187, Stroudsburg, PA, USA. Association for Computational Linguistics.


Cos¸kun Mermer, Murat Sarac¸lar,  and Ruhi Sarikaya. 2013.  Improving statistical machine translation using
Bayesian word alignment and Gibbs sampling.  IEEE Transactions on Audio, Speech, and Language Processing,
21(5):1090–1101, May.

Rada Mihalcea and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Proceedings of the HLT- 
NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond - 
Volume 3, HLT-NAACL-PARALLEL ’03, pages 1–10, Stroudsburg, PA, USA. Association for Computational 
Linguistics.

Franz Josef Och and Hermann Ney.  2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51, March.

Robert O¨ stling and Jo¨ rg Tiedemann.  2016. Efficient word alignment with Markov Chain Monte Carlo. Prague
Bulletin of Mathematical Linguistics, 106:125–146, October.

Robert O¨ stling, Carl Bo¨ rstell, and Lars Wallin. 2015. Enriching the Swedish Sign Language Corpus with part of 
speech tags using joint Bayesian word alignment and annotation transfer. In Proceedings of the 20th Nordic 
Conference on Computational Linguistics (NODALIDA  2015), volume 23 of NEALT Proceedings Series, pages
263–268, Vilnius, Lithuania, May.

Robert O¨ stling. 2013. Stagger: An open-source part of speech tagger for Swedish.  North European Journal of
Language Technology, 3:1–18.

Slav Petrov, Dipanjan Das, and Ryan McDonald.  2012. A universal part-of-speech tagset. In LREC 2012, Istanbul, 
Turkey, may. European Language Resources Association  (ELRA).

Darcey Riley and Daniel Gildea. 2012. Improving the IBM alignment models using variational Bayes. In ACL
2012, pages 306–310, Stroudsburg, PA, USA. Association for Computational Linguistics.

Charles Schafer and Elliott Franco Dra´bek.  2005. Models for Inuktitut-English word alignment. In Proceedings 
of the ACL Workshop on Building  and Using Parallel  Texts, ParaText ’05, pages 79–82, Stroudsburg, PA, USA. 
Association for Computational Linguistics.

Helmut Schmid. 1994. Probabilistic  part-of-speech tagging using decision trees. In Proceedings of the Interna- 
tional Conference on New Methods in Language Processing, Manchester, UK.

Kristina Toutanova, H. Tolga Ilhan, and Christopher Manning.  2002. Extensions to HMM-based statistical word 
alignment models. In EMNLP  2002, pages 87–94.

Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech 
tagging with a cyclic dependency network.  In NAACL  2003, pages 173–180, Stroudsburg,  PA, USA. Association 
for Computational Linguistics.

Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical transla- 
tion. In COLING 1996, pages 836–841, Stroudsburg, PA, USA. Association for Computational Linguistics.

Zhiguo Wang and Chengqing Zong. 2013. Large-scale word alignment using soft dependency cohesion con- 
straints. Transactions of the Association for Computational Linguistics, 1:291–300.

Dekai Wu and Xuanyin Xia. 1994. Learning an English-Chinese lexicon from a parallel corpus.  In Proceedings 
of the First Conference of the Association for Machine Translation in the Americas, pages 206–213.

Kenji Yamada and Kevin Knight.  2001. A syntax-based statistical translation model. In Proceedings of 39th 
Annual Meeting of the Association for Computational Linguistics, pages 523–530, Toulouse, France, July. As- 
sociation for Computational Linguistics.

David Yarowsky and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projec- 
tion across aligned corpora. In NAACL  2001, pages 1–8, Stroudsburg, PA, USA. Association for Computational 
Linguistics.

David Yarowsky,  Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via 
robust projection  across aligned corpora. In HLT 2001, pages 1–8, Stroudsburg, PA, USA. Association for 
Computational Linguistics.

Shaojun Zhao and Daniel Gildea. 2010. A fast fertility Hidden Markov Model for word alignment using MCMC.
In EMNLP  2010, pages 596–605, Cambridge,  MA, USA, October. Association for Computational Linguistics.



