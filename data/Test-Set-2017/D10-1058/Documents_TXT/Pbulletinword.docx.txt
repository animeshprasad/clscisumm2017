The Prague  Bulletin of Mathematical Linguistics
NUMBER  106   OCTOBER 2016    125–146

Eﬃcient  Word Alignment  with  Markov Chain Monte Carlo


Robert Östling, Jörg Tiedemann

Department of  Modern Languages, University of  Helsinki



Abstract

   We present efmaral, a new system for efficient and accurate word alignment using a Bayesian 
model  with  Markov Chain  Monte Carlo (MCMC) inference. Through careful  selection  of data 
structures and  model  architecture we are  able  to surpass the  fast_align system,  commonly 
used  for performance-critical word alignment, both in computational efficiency and alignment 
accuracy.  Our evaluation shows  that a phrase-based statistical machine translation (SMT) sys- 
tem produces translations of higher quality when  using  word alignments from efmaral than 
from fast_align, and  that  translation quality is on par with  what  is obtained using  giza++,  a 
tool requiring orders of magnitude more processing time.  More generally we hope to convince 
the reader that Monte Carlo sampling, rather than being viewed as a slow method of last resort, 
should actually be the method of choice for the SMT practitioner and others  interested in word 
alignment.


1. Introduction

   Word  alignment is an essential step in several  applications, perhaps most promi- 
nently  phrase-based statistical machine translation (Koehn  et al., 2003) and  annota- 
tion transfer (e.g. Yarowsky et al., 2001). The problem is this: given a pair of transla- 
tionally equivalent sentences, identify which word(s) in one language corresponds to 
which word(s) in the other language. A number of off-the-shelf tools exist to solve this 
problem, but they tend  to be slow, inaccurate, or both.  We introduce efmaral, a new 
open-source tool1  for word alignment based on partially collapsed Gibbs sampling in 
a Bayesian model.

1 The source  code and documentation can be found at https://github.com/robertostling/efmaral


©  2016 PBML. Distributed under CC BY-NC-ND.                      Corresponding author: robert.ostling@helsinki.fi 
Cite  as: Robert Östling, Jörg Tiedemann.  Eﬃcient Word  Alignment with Markov Chain Monte Carlo. The  Prague 
Bulletin of Mathematical Linguistics No. 106, 2016, pp. 125–146.  doi: 10.1515/pralin-2016-0013.


PBML 106	OCTOBER 2016


2. Background

   In order  to understand the present work,  we first need  to formalize the problem 
and introduce the family of models used  (Section 2.1), describe  their Bayesian exten- 
sion (Section 2.2), the Markov Chain  Monte  Carlo algorithm used  for inference (Sec- 
tion 2.3) and its particular application to our problem (Section 2.4).

2.1. The IBM models

   The IBM models (Brown  et al., 1993) are asymmetric generative models that  de- 
scribe how a source language sentence generates a target language sentence though a set 
of latent alignment variables. Since the task at hand is to align the words in the source 
and target  language sentences, the words in both sentences are given, and we are left 
with inferring the values  of the alignment variables.
   Formally, we denote the k:th sentence pair ⟨s(k) , t(k) ⟩ with the source sentence s(k) 
containing words s(k) (for each word index i ∈ 1 . . . I(k) ) and the target  sentence t(k) 
containing words t(k) (for j ∈ 1 . . . J(k) ).
Each sentence pair ⟨s(    , t	⟩ is associated with an alignment variable a 	, where


a(k)


(k)


(k)


j	=  i indicates that  target  word tj 	was  generated by source  word si     .  This
implies  an n-to-1 mapping between source  and  target  words, since each target  word
is aligned to exactly one source  word, while each source  word can be aligned to zero 
or more target  words.
Sentences  are assumed to be generated independently, so the probability of gen-
erating a set of parallel sentences ⟨s, t⟩ is
K
P(t|s, a) = ∏ P(t(k) |s(k) , a(k) )	(1)
k=1

For simplicity of notation, we will drop the sentence index  (k) in the following dis-
cussion  and let ⟨s, t⟩ instead denote a single sentence pair, without loss of generality
due to the independence assumption between sentences.
   A source word type e is associated with a lexical distribution, modeled by a categor- 
ical distribution with parameter vector θe . In the simplest of the IBM models (model
1), the probability of generating a target  sentence t is defined as the probability of
independently generating each of the J target  words independently from the lexical 
distributions of their respective aligned source  words.



J
P(t|s, a) ∝ ∏ θs
j=1




,tj




(2)


IBM model  1 assumes a uniform distribution for P(a), which  effectively  means
that the word order  of the sentences are considered irrelevant. This is clearly not true

126


R. Östling, J. Tiedemann                                     Eﬃcient Word Alignment with MCMC (125–146)


in real translated sentences, and  in fact aj and  aj+1 tend  to be strongly correlated. 
Most  research on word alignment has  assumed some  version of a word order model 
to capture this  dependency.  Perhaps the  simplest version is used  in IBM model  2 
and the fast_align model  (Dyer et al., 2013), which  are based  on the observation that
j/J ≈  aj /I, in other  words that  sentences tend  to have  the same  order  of words in
both languages. This is however a very rough approximation, and Vogel et al. (1996)
instead proposed to directly model  P(aj+1  − aj  = x|I), which  describes the length
x of the “jump” in the source  sentence when  moving one word forward in the target
sentence, conditioned on the source  sentence length  I.
   Although the IBM models allow n-to-1 alignments, not all values  of n are equally 
likely.   In general, high  values  of n are unlikely, and  a large  proportion of transla- 
tions are in fact 1-to-1.  The value  of n depends both on the particular languages in- 
volved (a highly  synthetic language like Finnish  translated into English  would yield 
higher values  than a French to English translation) and on the specific word type.  For 
instance, the German Katze ‘cat’ would typically be translated into a single  English 
word, whereas Unabhängigkeitserklärung would normally be translated into two (inde- 
pendence declaration) or three  words (declaration of independence). This can be modeled


by defining the fertility ϕ(i) = ∑J


δaj =i of a source  token  si , and  
introducing a


distribution for P (ϕ(i) = n|si  = e) for each source  word type e.
A large number of models based  on the same general assumptions have been ex-
plored (Brown et al., 1993; Toutanova et al., 2002; Och and  Ney, 2003), and  the inter- 
ested reader may want  to consult  Tiedemann (2011) for a more thorough review  than 
we are able to provide in this work.

2.2. Bayesian IBM models

   The IBM models make no a priori assumptions about  the categorical distributions 
that  define  the model,  and  most authors have  used  maximum-likelihood estimation 
through the Expectation-Maximization algorithm (Dempster et al., 1977) or some ap- 
proximation to it. However, when  translating natural languages the lexical distribu- 
tions should be very sparse, reflecting the fact that a given source word tends to have a 
rather small number of target words as allowable translations, while the vast majority 
of target  words are unimaginable as translations.
   These constraints have recently been modeled with sparse and symmetric Dirichlet 
priors (Mermer and Saraçlar, 2011; Mermer et al., 2013; Riley and Gildea, 2012) which, 
beyond capturing the range of lexical distributions we consider likely, also turn out to 
be mathematically very convenient as the Dirichlet distribution is a conjugate prior to 
the categorical distribution. The d-dimensional Dirichlet  distribution is defined over 
the space of d-dimensional categorical distributions, and  is parameterized by the d-
dimensional vector  α > 0.  If X ∼ Dir(α), the probability density function of X is



127


PBML 106	OCTOBER 2016




given by




P(X = x) 
= 1 ∏ 
xαi −1





(3)


Z 	i 
i=1

where  the normalization constant Z is given by the multinomial beta function

∏d	(	)


B(α) =   	i=1 Γ αi 	



(4)


i=1 αi )

A symmetric Dirichlet  distribution has αi = αj for all i, j, with  the interpretation in 
our case that no particular translation is preferred a priori  for any source  word type,
as this has to be estimated from the data.  By also setting  α ≪ 1 we favor sparse  lexical
distributions where  most probabilities are close to zero.
   While it is possible  to treat  α as a latent  variable to be inferred, good  results can 
be obtained by using  a fixed value  roughly in the range  of 10−6  to 10−2  (Riley and 
Gildea,  2012). Another direction of research has explored hierarchical distributions 
such as the Pitman-Yor process  (Pitman and Yor, 1997) instead of the Dirichlet  distri- 
bution for the translation distribution priors (Gal and  Blunsom,  2013; Östling,  2015). 
Such distributions offer even greater flexibility  in specifying prior  constraints on the 
categorical distributions, but  at the cost of less efficient inference. Since the gain in 
accuracy  has turned out to be limited and  computational efficiency is an important 
concern  to us, we will not further consider hierarchical priors in this work.

2.3. Markov Chain Monte Carlo

   Several  different methods have  been  used  for inference in IBM alignment mod- 
els.  Starting with  Brown  et al. (1993), maximum-likelihood estimation through the 
Expectation-Maximization (EM) algorithm has been a popular choice.  This method 
is generally efficient for simple  models without word order  or fertility  distributions, 
but computing the expectations becomes  intractable for more  complex  models such 
as IBM model  4 so approximative hill-climbing methods are used  instead.
   Another disadvantage of using  plain  EM inference with  the IBM models is that 
it is unable to incorporate priors on the model  parameters, and  as was  pointed out 
in the previous section this deprives us of a powerful tool to steer the model  towards 
more realistic solutions. Riley and Gildea (2012) presented a method to extend the EM 
algorithm to IBM models with  Dirichlet  priors,  through Variational Bayes inference. 
Unfortunately, their method inherits the complexity issues of earlier EM approaches.
   The inference approach chosen by most authors working on Bayesian IBM models 
(Mermer and Saraçlar, 2011; Gal and Blunsom,  2013; Östling,  2015) is Gibbs sampling 
(Gelfand  and Smith, 1991), a special case of the Markov Chain  Monte Carlo (MCMC) 
method which  we will briefly summarize here.

128


R. Östling, J. Tiedemann                                     Eﬃcient Word Alignment with MCMC (125–146)


   Given a probability function pM (x) of some model M on parameter vector x, MCMC 
provides us with  the means  to draw samples from  pM .  This is done  by construct- 
ing a Markov chain  with  values  of x as states,  such  that  its stationary distribution 
is  identical to pM .   In practice,  this  means  deriving expressions for the  transition
probabilities P(x ′ |x) of going  from  state  x to state  x ′ .  Since the number of states  is

enormous or infinite  in typical  applications, it is essential that  there  is some  way  of 
sampling efficiently  from  P(x ′ |x).   With  Gibbs  sampling, this  is done  by sampling
one variable from the parameter vector x at a time, conditioned on all other variables:
P(xi |x1 , x2 , . . . , xi−1 , xi+1 , . . . , xm ) which we will write as P(xi |x(−i) ) to indicate con-
ditioning on all elements of x except  at index  i. All positions i are then  sampled in
some arbitrary but fixed order.  By choosing suitable distributions for the model,  the 
goal  in designing a Gibbs  sampler is to make  sure  that  this  distribution is easy  to 
sample from.

2.4. Gibbs  sampling for Bayesian IBM models

   The Bayesian  version of IBM model  1 defines  the following probability over  the 
parameter vector, which consists of the alignment vector a and the lexical distribution 
vectors  θe for each e in the source  target  vocabulary:


 K    J(k)


  ( E 	F 	)


P(a, θ) = P(s, t, a, θ, α) ∝ ∏ ∏ θ (k)


(k)  ·


∏ ∏ θαf −1


(5)




since s, t and α are constant.



k=1 j=1


s (k) ,tj a
j



e=1 f=1


e,f


A straightforward Gibbs sampler can be derived by observing that


P(x)


P(x(−i) , xi )





which  means  that


P(xi |x(−i) ) =


P(x(−i) ) =


P(x(−i) )


P(a(−j) , aj = i, θ)


P(aj  = i|a(−j) , θ) =	(


)	∝ θs   ,t


(6)




and


P  
a(−j) , 
θ


aj  j


P(θ(−e) , θe = x|a)


∏F 	xαf +ce,f −1


P(θe = x|a, θ(−e) ) =


P(θ(−e) |a)


=   	f=1     f 	
B(αe  + ce )


(7)


where  ce,f  is the number of times  that  word e is aligned to word f given  a, s and  t. 
Equation (7) is a consequence of the fact that the Dirichlet  distribution is a conjugate 
prior  to the categorical distribution, so that if
x ∼ Dir(α)
z ∼ Cat(x)


129


PBML 106	OCTOBER 2016

then given a sequence z of |z| samples from Cat(x) we have
x|z ∼ Dir(α + c(z)) 	(8)



where



|
z
|
c(z)m 
= ∑ 
δz =m


i=1

is the number of samples in z that are equal to m. This can be easily shown from the 
definition of the Dirichlet  distribution using  Bayes’ theorem:
P(x|α, z) ∝ P(z|α, x)P(α, x) 	(9)


d
∏
∝
i=1
d



xαi −1


|z|
∏

i=1



xzi



(10)


= ∏ xαi −1 ∏ xc(z)m	(11)
i=1 	m 
d
= ∏ xαi +c(z)−1 	(12)
i=1

which  is the (unnormalized) Dirichlet  distribution with parameter α + c(z).
   Equation (6) and Equation (7) can be used  for sampling with standard algorithms 
for categorical and  Dirichlet  distributions, respectively, and  together they  define  an 
explicit Gibbs sampler for the Bayesian IBM model  1. While simple,  this sampler suf- 
fers from poor  mixing  (Östling,  2015, section  3.3) and  is not a competitive algorithm 
for word alignment. However, much  better  performance can be achieved by using  a 
collapsed sampler where  the parameters θe  are integrated out so that we only have to
derive  a sampling equation for the alignment variables P(aj  = i|a(−j) ).
First we use Equation (5) to derive  an expression for P(a|s, t, α), from which  the
final sampler can be computed as

P(a(−j) , aj = i|s, t, α)


P(aj  = i|a(−j) , s, t, α) =	(


)	(13)


P  a(−j) |s, t, α

Since the elements of a are exchangeable, a sufficient statistic for a is the count vector
c(·) where  each element

K    J(k)


c(a, e, f)e,f = ∑ ∑ δ (k)



(k)


(14)



k=1 j=1


s (k) =e∧tj       =f
a
j



130


R. Östling, J. Tiedemann                                     Eﬃcient Word Alignment with MCMC (125–146)


represents the number of times that source word type e is aligned to target  word type 
f under the alignment a.  Next, we marginalize over each of the lexical distributions 
θe .


P(a|s, t, α) = ∏ ∫


P(a


|θ  , s, t, α)P(θ |s, t, α) dθ



(15)



e=1  ∆


{j|saj =e}   e	e
	e


Substituting from Equation (5) into the integral we have



P(a|s, t, α) = 	1


∫ 	F
∏	∏ θc(a,s,t)e,f +αf −1 dθe



(16)


B(α)



e=1



∆ f=1


e,f


where the innermost product can be recognized as an unnormalized Dir(α+c(a, s, t))
distribution which has normalization factor B(α + c(a, s, t)), so that the final expres-
sion becomes

E 	(	)
P(a|s, t, α) = ∏ B α + c(a, s, t)
B α


e=1
E
= ∏


(∑F	) ∏
f=1 	f
F


(αf + c(a, s, t)e,f )




(18)


e=1 Γ (∑ =1 (αf + c(a, s, t)e,f )) ∏


Γ (αf )


f	f

Combining Equation (13) with Equation (18) gives us an expression where  almost  all 
of the terms  are cancelled out, except  when  si  = e and  tj = f for which  c(a, s, t)e,f 
and c(a(−j) , s, t)e,f  differ by 1. We are left with a remarkably simple  sampling distri- 
bution:


P(aj  = i|a(−j) , s, t, α) =


αt	(−j)


, s, t)si ,tj



(19)


∑F
f=1


(αf + c(a(−j) , s, t)s  ,f )



   By repeatedly sampling each aj in turn  from Equation (19) we are guaranteed to, 
in the limit,  obtain  an unbiased sample from  P(a) under the model.   What  we are
really  interested in, however, is to estimate the marginal distributions P(aj   = i) as
closely as possible  while  using  as little computation as possible,  given  a sequence of
correlated samples a(t)  for time t ∈ 1 . . . T . Given a sequence of samples a(t)  we can
then approximate the marginal distributions



P(aj  = i) =  E
P(a)


∞
[δaj =i ] =


δa(t)	≈


T
T	δa(t)
1



(20)


t=

In practice  a(0) will be initialized either  from a uniform distribution or by using  the 
output of a simpler model, and the samples will gradually become more independent 
of a(0) as t increases. Since a(0) is likely to lie in a low-probability region of the model,

131


PBML 106                                                                                                   OCTOBER 2016


so do the initial samples, and  it is common to use a burn-in period and  disregard all 
a(t) for t < t0 . To further ameliorate the problem of initialization bias, it is possible  to 
run several  independently initialized samplers and average their results. Combining 
these methods the marginal distribution approximation becomes

1	N	T


P(aj  = i) ≈ N(T − t



+ 1)


∑ ∑

n=1 t=t0


δa(n,t)


(21)



where  N is the number of independent samplers and  t0 is the length  of the burn- 
in period. Finally,  a better  estimate can be obtained by applying the Rao-Blackwell 
theorem (Blackwell,  1947; Gelfand and  Smith,  1991), which  allows  us to re-use  the
computations of P(aj  = i|a(−j) ) during sampling and  averaging these  distributions


rather than δa(t)


. The final 
approximation then 
becomes



1	N	T


P(aj  = i) ≈ N(T − t



+ 1)


∑ ∑

n=1 t=t0


P(a(n,t)  = i|a


(n,t)(−j) )


(22)



3. Methods

   We now  turn  to the particular models and  algorithms implemented in efmaral, 
presenting our  Bayesian  HMM  model  with  fertility,  the Gibbs sampler used  as well 
as the details  on how to make it computationally  efficient.

3.1. Alignment model

   Our goal in this work  is to find a word alignment algorithm that is both accurate 
and efficient. Previous studies have shown that good word order  and fertility models 
are essential to high accuracy  (Brown et al., 1993; Och and Ney, 2003), along with rea- 
sonable priors on the parameters (Mermer and Saraçlar, 2011; Östling,  2015). As was 
discussed in Section 2.3, MCMC algorithms and  in particular collapsed Gibbs sam- 
pling are particularly suitable for inference in this class of models, as long as the con- 
vergence of the Markov chain are sufficiently fast. Even within this class of algorithms 
there  are some  trade-offs between accuracy  and  computational efficiency.  In partic- 
ular,  hierarchical priors have  been  shown to somewhat improve accuracy  (Östling,
2015, p. 65), but  in spite  of improved sampling algorithms (Blunsom  et al., 2009) it 
is still considerably more costly to sample from models with  hierarchical priors than 
with Dirichlet  priors.
   For these reasons, we use a HMM model for word order based on Vogel et al. (1996) 
as well as a simple  fertility  model,  and  the complete probability of an alignment is 
essentially the same as Equation (5) with  extra factors added for the word order  and

132


R. Östling, J. Tiedemann	Eﬃcient Word Alignment with MCMC (125–146)


fertility  model:




P(s, t, a,θ, ψ, π, α, β, γ)


 K    J(k)


  ( E 	F 	)


∏ ∏
∝ 
k=1 j=1


θ (k) (k)
a
j


,t(k)  ·


∏ ∏

e=1 f=1


αf −1 e,f


 K    J(k) +1
∏ ∏
· 




ψa(k)



(k)  ·


( mmax
∏


)

βm −1 m



(23)


k=1


j=1


j   −aj−1


m=mmin


 K    I(k)


  ( E


nmax	)


∏ ∏
· 
k=1 i=1


i  ,ϕ(i,a(k) ) 


∏ ∏

e=1 n=0


γn −1 e,n






where  ψ  ∼  Dir(β) are  the  categorical distribution parameters for the  word order 
model  P(aj − aj−1 = m), and πe ∼ Dir(γ) for the fertility model  P(ϕ(i, a)|si  = e). In
our experiments we fix α = 0.001, ψ = 0.5 and  γ = 1, but these  parameters are not
very critical as long as 0 < α ≪ 1.
The IBM models naturally allow unaligned source language words, but in order  to
also allow target  words to not be aligned we use the extension of Och and Ney (2003) 
to the HMM alignment model,  where  each source  word si  (from sentence s of length 
I) is assumed to have a special null word si+I . The null word generates lexical items 
from the distribution θnull , and the word order  model  is modified so that




   P(aj  = i + I|aj−1  = i ′ ) = pnull δi=i ′		(24) 
P(aj  = i + I|aj−1  = i ′ + I) = pnull δi=i ′		(25) 
P(aj  = i|aj−1  = i ′ + I) = ψi−i ′	(26)





where  pnull is the prior  probability of a null  word alignment (fixed to 0.2 in our ex- 
periments).

133


PBML 106	OCTOBER 2016


   We collapse  the sampler over θ and  ψ in the same manner as was shown in Sec- 
tion 2.4 and obtain  the following approximate2  sampling distribution:



P(aj  = i|a(−j) , s, t, α, β, γ) ∝


αtj   + c(a


(−j)


, s, t)si ,tj


∑F
f=1


(αf + c(a(−j) , s, t)s  ,f )
(−j)


βi−aj−1   + c ′ (a


)i−aj−1


· ∑mmax
m=mmin


(βm + c ′ (a(−j) )m )
(−j)



(27)


βaj+1−i   + c ′ (a


)aj−1 −i


· ∑mmax
m=mmin


(βm + c ′ (a(−j) )m )


πsi ,ϕ(i,a(−j) )+1
πsi ,ϕ(i,a(−j) )

While collapsing over  the θ is essential for acceptable mixing  in the Markov chain, 
this is not the case for π. Instead, we alternate between sampling from Equation (27) 
and
πe ∼ Dir(γ + c ′′ (a)e )) 	(28)
where c ′′ (a)e is the count vector over the fertility distribution for source word e given 
alignments a.  The advantage of this is that  the last product of Equation (27) can be 
precomputed, saving  computation in the inner  loop  in exchange for the (relatively 
minor)  expense of also sampling from Equation (28).

3.2. Computational efficiency

From Equation (27) it is clear that the computational complexity of sampling sen-
tence k is O(I(k) J(k) ), since every alignment variable a(k) for each j ∈ 1 . . . J(k)  needs 
to evaluate the expression in 27 once for each i ∈  1 . . . I(k) , and  each evaluation re-
quires  constant time assuming that  the sums  are cached.  Since sentence lengths are
approximately proportional across languages, I(k) ≈ λJ(k) for some  constant λ, this
gives a total complexity of O(∑ I2 ) per iteration of sampling a.  Note  that  the com-
plexity  does not change  as we go from Equation (19) for the simple  IBM model  1 to
Equation (27) for the more complex  model  with word order  and fertility.
In contrast, the corresponding Expectation-Maximization (EM) algorithm for IBM
alignment models has O(∑ I2 ) complexity in the E-step  only for models with  sim-
ple or no word order  model.   The HMM-based model  of Vogel et al. (1996) can still 
be implemented relatively efficiently  using  dynamic programming, but  complexity
increases to O(∑ I3 ).  For models with  fertility  computing the expectations instead
becomes  intractable, and  previous authors have  solved  this by using  approximative

    2 The approximation consists  of ignoring the dependence between the two draws from the word order 
jump  distribution (second  and third  factors).

134


R. Östling, J. Tiedemann                                     Eﬃcient Word Alignment with MCMC (125–146)


greedy optimization techniques (Brown  et al., 1993) or local Gibbs  sampling (Zhao 
and  Gildea,  2010).  The main  advantage of EM over  a collapsed Gibbs  sampler is 
that the former  is trivial  to parallelize, which  makes  well-implemented parallel EM-
based  implementations of simple  alignment models with  O(∑ I2 ) complexity, such
as fast_align (Dyer et al., 2013), a strong baseline  performance-wise.

Algorithm  1 Inner  loop of our sampler for IBM model  1
function  sample(a(k)(−j) )
▷ Initialize  cumulative probability
s ← 0
for all i ∈ 1 . . . I(k)  do
▷ Load denominator reciprocal (small array  random access)
D−1 ← dk,i
▷ Load numerator index (sequential access)
L ← lk,i,j
▷ Load numerator (large array  random access)
N ← uL
▷ Compute unnormalized probability (one multiplication)
pˆ ← D−1 U
▷ Accumulate probabilities (one addition)
s ← s + pˆ
▷ Store cumulative probability (sequential access)
pi  ← s
end for
▷ Sample from a uniform distribution on the unit interval
r ∼ Uniform(0, 1,)
r ← r · pI
▷ Find the lowest  i such that pi  > r
i ← 1
while pi  ≤ r do
i ← i + 1
end while
a(k)
←
end function

   If a collapsed Gibbs sampler is to be a viable option for performance-critical appli- 
cations, we must  pay attention to details.  In particular, we propose utilizing the fixed 
order  of computations in order  to avoid  expensive lookups. Recall that variables a(k)
are sampled in order,  for k = 1 . . . K, j = 1 . . . J(k) . Now,  for each pair  ⟨k, j⟩ we need

135


PBML 106	OCTOBER 2016




to compute




αtj   + c(a




(−j)




, s, t)si ,tj


∑F
f=1


(αf + c(a(−j) , s, t)s  ,f )



which,  if the numerator sum  and  the reciprocal of the denominator sum  are cached 
in memory, involves two table lookups and  one multiplication. Since multiplication 
is fast and the denominator reciprocal is stored in a relatively small dense  array,  most 
attention has to be paid  to the numerator lookup, which  apart  from the constant αtj


is a sparse  matrix  with  non-zero counts  c(a(−j) , s, t)s  ,t


only  where  si  and  tj  are


i  j
aligned. The standard solution would therefore be to use a hash table with ⟨si , tj ⟩ as
keys to ensure memory efficiency and  constant-time lookup. However, most  counts
are in fact guaranteed to always be zero, as only words from the same parallel sentence 
pair can be aligned. We are therefore able to construct a count  vector u and an index
(−j)


table l such that ulk,i,j   = c(a


, s, t)si ,tj +αtj . At the expense 
of some extra memory


usage  we are able to achieve  the lookup with  only two operations, one of which  is a
cache-efficient  sequential memory access.   With  this  method, the inner  loop  of the 
sampler for IBM model  1 thus  contains only  six operations, outlined in algorithm
1. Adding the HMM word order  model,  two more sequential memory loads and two 
multiplications are needed, and adding the fertility model requires one more memory 
load and a multiplication.

4. Related work

   In this section  we relate  our work  mainly to the literature on Bayesian  models of 
word alignment, as well  as computationally efficient methods for this  problem.  A 
comprehensive survey of word alignment methods is beyond the scope of this article, 
for this we refer the reader to Tiedemann (2011).
   Much of research into word alignment has been based  on the pioneering work  of 
Brown et al. (1993), and we have already introduced part of their family of IBM align- 
ment  models in Section 2.1. Their most advanced models still perform competitively 
after nearly  two decades, but due  to their complexity (with  exact inference being in- 
tractable) many  have suggested simpler alternatives, typically by keeping the lexical 
translation model intact and introducing computationally convenient word order and 
fertility  models so that inference with the Expectation-Maximization (EM) algorithm 
remains tractable. Notable examples include the simple  HMM-based model  of Vogel 
et al. (1996) and  the even simpler reparametrized IBM model  2 of Dyer et al. (2013). 
Neither of these include a model for word fertility, but Toutanova et al. (2002) showed 
that a simplified fertility  model  (which only counts  alignments from consecutive tar- 
get words) can be added to the HMM  model  without increasing the complexity of 
inference, and more recently  this has also been achieved for a general fertility  model 
(Quirk,  2013).

136


R. Östling, J. Tiedemann                                     Eﬃcient Word Alignment with MCMC (125–146)


The EM algorithm requires computing the expected values  of the alignments,
E [δaj =i ], given the current values  of the model  parameters. The authors cited above 
all dealt  with  this fact by analytically deriving expressions for exact computation of 
these expectations in their models. Zhao and Gildea (2010) instead chose to use Gibbs 
sampling to approximate these expectations, which allowed them to perform efficient 
inference with  EM for a HMM  model  with  fertility.   Riley and  Gildea  (2012) later 
showed how Variational Bayesian techniques can be used to incorporate priors on the 
parameters of the IBM models, with  only minor  modifications to the expressions for 
the alignment expectations.
   Recently,  several  authors have  disposed with  EM altogether, relying entirely on 
Gibbs sampling for inference in IBM-based  models with  Bayesian  priors of varying 
complexity (Mermer and Saraçlar,  2011; Mermer et al., 2013; Gal and Blunsom,  2013; 
Östling,  2015). Of these,  Gal and  Blunsom  (2013) and  to some  extent  Östling  (2015) 
prioritize maximizing alignment accuracy,  which  is obtained by using  complex  hier- 
archical  models. Mermer et al. (2013) use Dirichlet  priors with  IBM models 1 and  2 
to obtain  efficient samplers, which they implement in an approximate fashion  (where 
dependencies between variables are ignored during sampling) in order  to facilitate 
parallelization. This article follows  previous work  by the first author (Östling,  2015), 
which however was focused on alignment of short parallel text for applications in lan- 
guage  typology and  transfer learning, rather than  efficient large-scale alignment for 
use with statistical machine translation systems.

5. Results

   In this section  we first investigate the effect of different parameter settings in ef- 
maral, then  we proceed with  a comparison to two other  influential word alignment 
systems with respect  to the performance of statistical machine translation (SMT) sys- 
tems using  the alignments. Since computational efficiency is an important objective 
with efmaral, we report runtime for all experiments.
The following three  systems are used  in our comparison:

giza++:    The standard pipeline of IBM models with standard settings of 5 iterations 
of IBM 1, 5 iterations of the HMM model,  and  5 iterations of IBM model  3 and
4 with Viterbi alignments of the final model  (Och and Ney, 2003). Class depen- 
dencies  in the  final distortion model  use  automatically created word clusters 
using  the mkcls tool, 50 per language.
fast_align:  An log-linear reparameterization of IBM model  2 using  efficient infer- 
ence procedures and  parameter estimations (Dyer et al., 2013). We use the op- 
tions that favor monotonic alignment points  including the optimization proce- 
dures that estimate how close they should be to the monotonic diagonal.
efmaral:   Our implementation of the MCMC alignment approach proposed in this 
article.

137


PBML 106	OCTOBER 2016


   Since these tools all use asymmetric models, we ran each aligner in both directions 
and  applied the  grow-diag-final-and (Section  5.1) or grow-diag-final (Section  5.2) 
symmetrization heuristic (Och and  Ney, 2003, p. 33). This method assumes a set of 
binary alignments, so for efmaral we produce these by choosing the single most prob- 
able value for each aj : arg maxi P(aj  = i). In this way the results are more easily com- 
parable to other systems, although some information is lost before the symmetrization 
step  and  methods have  been explored that  avoid  this (Matusov et al., 2004; Östling,
2015, pp. 46–47).

5.1. Alignment quality experiments

   As discussed in Section  2.4, there  are  two  ways  of trading off computing time 
for approximation accuracy:   increasing the  number of independent samplers, and 
increasing the number of sampling iterations.  Here  we explore  the effects of these 
trade-offs on alignment accuracy.
   Following Och and Ney (2003), most subsequent research has compared the results 
of automatic word alignment to hand-annotated data  consisting of two sets of links:
S, containing sure tuples ⟨i, j⟩ where  the human judgment is that  si  and  tj must  be 
aligned, and  S ⊆  P, containing possible tuples ⟨i, j⟩ where  si  and  tj may  be linked.
Given a set A of alignments to be evaluated, they define  the measures precision (p),
recall (r), and alignment error  rate (AER) as follows:
|A ∩ P|
p =	(29)
|A|
r = |A ∩ S|	(30)
|P|
|A ∩ S| + |A ∩ P|


AER = 1 −


(31)
|A| + |S|



While popular, the AER measure is biased  towards precision rather than  recall and 
correlates poorly  with  machine translation performance.  Fraser  and  Marcu  (2007) 
instead suggest to use the F-measure, which  favors  a balance  between precision and 
recall as defined in Equation (29) and Equation (30):



( α
Fα  =	p +


1 − α )−1
r



(32)



In our experiments, we report both AER and F0.5 .
In order  to evaluate alignment quality we are limited to language pairs  with  an-
notated alignment data.   For this reason,  we use the corpora and  test sets from  the 
WPT 2003 and  2005 shared tasks (Mihalcea  and  Pedersen, 2003; Martin  et al., 2005). 
In addition, we also use the Swedish-English part  of the Europarl corpus version 7

138


R. Östling, J. Tiedemann	Eﬃcient Word Alignment with MCMC (125–146)



Table 1. Data sets used for  our alignment quality experiments.  The total number of 
sentences in the respective corpora are given along with the number of sentences and 
gold-standard (S)ure and (P)ossible alignment links in the corresponding test set.

Co
rp
us
Se
nt
en
ce
s
Tr
ai
ni
n
g
Se
nt
en
ce
s
|
S
|
Tes
t
|
P
|
En
gli
sh-
Fr
en
ch
1,
1
3
0,
5
8
8
4
4
7
4,
03
8
17,
43
8
En
gli
sh-
Ro
m
an
ia
n
4
8
,
6
4
1
2
0
0
5,
03
4
5,
0
3
4
En
gli
sh
-
In
uk
tit
ut
3
3
3
,
1
8
5
75
2
9
3
1,
9
7
2
En
gli
sh
-
Hi
nd
i
3
,
5
5
6
90
1,
40
9
1,
4
0
9
En
gli
sh
-
S
we
di
sh
6
9
2
,
6
6
2
1
9
2
3,
34
0
4,
5
7
7

(Koehn,  2005) with  test set from Holmqvist and  Ahrenberg (2011). The data  sets are 
presented in Table 1, where  it can be noted they differ both in size and in annotation 
style.  In particular, the English-Romanian and  English-Hindi data  only have one set 
of gold-standard links, so that S = P, the English-French and  English-Inuktitut data
have |S| ≪ |P|, while the English-Swedish data lies somewhere in between.
Table 2: Results  of our alignment quality experiments. All timing
and  accuracy  figures  use  means  from  five independently  initial- 
ized  runs.   Note  that  lower  is better  for AER, higher is better  for 
F0.5 .  All experiments are  run  on  a system  with  two  Intel  Xeon 
E5645 CPUs running at 2.4 GHz,  in total  12 physical (24 virtual) 
cores.

                        Quality	Time (seconds) 
Configuration 	AER 	F0.5	   CPU 	Wall
English-French

fa
st
_a
li
gn
1x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
1
5.
3	86.2
8
.
2
	92.3
4
,
1
2
4	243
7
4
1
	
2
7
0
4x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
16
x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
8
.
1
	92.2
8
.
1
	92.1
2,
7
0
0	809
10,
55
7	2,945
1x 
ite
rat
io
ns, 
1 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
4 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
8 
sa
m
pl
er
s
9
.
1
	91.4
7
.
8
	92.6
7
.
6
	92.9
4
7
0
	
2
4
8
1
,
3
2
4	298
2
,
4
5
6	330
Continued on next page







139


PBML 106	OCTOBER 2016


Configuration 	AER 	F0.5	CPU 	Wall
English-Hindi

fa
st
_a
li
gn
1x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
6
7.
3	32.7
4
8.
3	51.7
2
7
	
2
1
0
7
	12
4x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
16
x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
4
9.
0	51.0
5
1.
0	49.0
4
1
6
	
4
6
1
,
6
6
4	183
1x 
ite
rat
io
ns, 
1 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
4 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
8 
sa
m
pl
er
s
4
9.
4	50.6
4
7.
5	52.5
4
6.
7	53.3
8
1
	
1
0
1
4
6
	13
2
3
8
	17
English-Inuktitut

fa
st
_a
li
gn
1x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
2
8.
7	78.1
2
2.
3	81.5
7
5
2
	48
1
6
0
	62
4x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
16
x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
1
9.
7	83.7
1
7.
3	86.0
5
6
0
	
1
9
9
2
,
1
7
6	747
1x 
ite
rat
io
ns, 
1 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
4 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
8 
sa
m
pl
er
s
2
3.
8	80.1
1
9.
6	84.1
1
8.
4	85.3
9
8
	
5
6
2
5
9
	64
5
1
5
	72
English-Romanian

fa
st
_a
li
gn
1x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
3
2.
5	67.5
2
8.
7	71.3
2
6
6
	17
1
6
7
	47
4x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
16
x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
2
9.
0	71.0
2
9.
5	70.5
6
4
8
	
1
7
3
2
,
5
8
0	682
1x 
ite
rat
io
ns, 
1 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
4 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
8 
sa
m
pl
er
s
2
9.
8	70.2
2
8.
2	71.8
2
7.
9	72.1
9
7
	
4
3
3
2
0
	53
6
5
6
	59
English-Swedish

fa
st
_a
li
gn
1x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
2
0.
5	79.8
1
3.
1	87.0
12,
29
8	671
1,
6
0
6	589
4x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
16
x 
ite
rat
io
ns, 
2 
sa
m
pl
er
s
1
1.
4	88.6
1
0.
6	89.4
5,
9
8
9	1,830
23,
09
9	6,519
1x 
ite
rat
io
ns, 
1 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
4 
sa
m
pl
er
s
1x 
ite
rat
io
ns, 
8 
sa
m
pl
er
s
1
3.
8	86.3
1
3.
2	86.8
1
1.
7	88.3
1,
0
0
5	538
2,
6
8
1	626
6,
1
4
7	839



   Table 2 shows  the result  of varying the number of samplers and  iterations for all 
the  language pairs  under consideration. As a baseline  for each  language pair,  we 
use fast_align as well as the default efmaral configuration of two independent sam-
√ 	


plers,  running x = ⌊100/


K⌋ sampling iterations 
where  K is the number of 
parallel


sentences in the data  (with  the additional constraint that  4 ≤  x ≤  250).  Following
140


R. Östling, J. Tiedemann	Eﬃcient Word Alignment with MCMC (125–146)


the practice  set by Brown  et al. (1993), each model  is initialized with  the output of a
simpler model.  For the full HMM+fertility model,  we run  ⌊x/4⌋ sampling iterations
of IBM model  1 initialized with uniformly random alignments, use the last sample to
initialize the fertility-less HMM model  that we also run for ⌊x/4⌋ iterations. Finally, x
samples are drawn from the full model  and  the final alignments are estimated from
these using  Equation (22).
   The experiments described in Table 2 were carried out on a system  with dual Intel 
Xeon E5645 CPUs, with  a total of 24 virtual cores available. Even though this setup 
strongly favors fast_align’s parallel implementation, efmaral is faster for the largest 
corpus (where speed  matters most) in terms  of both wall time and CPU time, and for 
all but the smallest corpora in CPU time.  This trend will also be seen in Section 5.2, 
where  even larger parallel corpora are used  for our machine translation experiments.
   As expected, increasing the number of independently initialized samplers consis- 
tently results in better alignments, in line with research on model averaging for a wide 
range  of machine learning models. When  it comes to increasing the number of sam- 
pling iterations the result is less clear: for some pairs this seems even more important 
than the number of independent samplers, whereas for other pairs the quality metrics 
actually change  for the worse.  Recall that the samplers are initialized with  a sample 
from the fertility-less HMM model,  and that the correlation to this sample decreases 
as the number of samples from the HMM model  with  fertility  increases. Decreasing 
quality therefore indicates that for that particular language pair and annotation style, 
the fertility model  performs worse than the mix between the fertility and fertility-less 
models obtained by using  a small  number of samples.  When  interpreting these  re- 
sults, it is also important to keep in mind  that the quality metrics are computed using 
discretized and  symmetrized alignments, which  are related in a quite  complex  way 
to the probability estimates of the underlying model.
   From a practical point  of view, one should also consider that additional indepen- 
dent samplers can be run in parallel, unlike additional sampling iterations which have 
a serial dependence.  For this reason  and because  of the consistent improvements 
demonstrated in Table 2, increasing the number of samplers should be the preferred 
method for improving alignment quality at the cost of memory and CPU time.

5.2. Machine  translation experiments

   In order  to test the effect of word alignment in a downstream task, we conducted 
some  experiments with  generic  phrase-based machine translation.  Our  models are 
based  on the Moses  pipeline (Koehn  et al., 2007) with  data  coming  from  the Work- 
shop on Statistical Machine Translation. In our setup  we use the news translation task 
from 2013 with translation models for English to Czech, German, Spanish, French and 
Russian  and vice versa.  Parallel  training data comes from Europarl version 7 (Koehn,
2005) (for all language pairs except Russian-English) and the News  Commentary cor- 
pus  version 11. For language modeling, we use the monolingual data  sets from Eu-

141


PBML 106	OCTOBER 2016



Table 3. Data used for  training SMT  models (all  counts in millions). Parallel data sets 
refer to the bitexts aligned to English and their token counts include both languages.


La
ng
ua
ge
Mo
nol
ing
ual	Parallel

Se
nte
nc
es 	Tokens
Se
nte
nc
es 	Tokens
Cz
ec
h
G
e
r
m
a
n
 
E
n
g
l
i
s
h
 
S
p
a
n
i
s
h
 
F
r
e
n
c
h
 
R
u
s
s
i
a
n
8
.
4
	
1
4
5
2
3
.
1
	
4
2
5
1
7
.
3
	
4
1
1
6
.
5
	
1
9
0
6
.
4
	
1
7
3
1
0
.
0
	
1
7
8
0
.
8
	
4
1
2
.
1
	
1
1
4
–
	
–
2
.
0
	
1
0
9
2
.
0
	
1
1
4
0
.
2
	
1
0

roparl and  News  Commentary as well as the shuffled  news  texts from 2012. We did 
not use any of the larger news data sets from more recent years to avoid possible  over- 
laps with the 2013 test set. We apply a pipeline of pre-processing tools from the Moses 
package to prepare all data  sets including punctuation normalization, tokenization, 
lowercasing and corpus cleaning (for parallel corpora). Statistics of the final data sets 
are listed in Table 3.
   All language models use order  five with modified Kneser-Ney smoothing and are 
estimated using  KenLM (Heafield et al., 2013).  Word  alignments are symmetrized 
using  the grow-diag-final heuristics and we use standard settings to extract phrases 
and  to estimate translation probabilities and  lexical weights. For reordering we use 
the default distance-based distortion penalty and parameters are tuned using  MERT 
(Och, 2003) with 200-best lists.
   Table 4 shows  the performance of our SMT models given alignments from the dif- 
ferent word alignment systems. The left-hand part of the table contains results when 
using  full word forms  for the  word alignment systems, whereas the  results in the 
right-hand part  were  obtained by removing any letters  after the four first from each 
word, as a form of approximate stemming since all the languages in our evaluation 
are predominantly suffixing.  Though seemingly very drastic,  this method improves 
accuracy  in most cases since data sparsity is a major problem for word alignment.
   Next  we turn  to the computational cost of the experiments just described, these 
are found in Table 5. In almost all cases, efmaral runs faster by a comfortable margin. 
The only exception is for the smallest dataset, Russian-English, where fast_align uses 
slightly  less wall time (but still much  more  CPU time).  This trend is also present in 
the alignment quality experiments in Section 5.1 with mostly  smaller  corpora, where 
efmaral is only faster for the largest  corpus.3



3 Due to different computing environments, only four CPU cores were available per aligner  in the SMT
experiments, versus 24 cores in the alignment quality experiments.

142


R. Östling, J. Tiedemann	Eﬃcient Word Alignment with MCMC (125–146)



Table 4. Results from our SMT  evaluation. The BLEU  scores are the maximum over the
Moses parameters explored for  the given word alignment conﬁguration.


Tr
an
sla
tio
n 
pa
ir
B
L
E
U
 
s
c
o
r
e
N
o
 
s
t
e
m
m
i
n
g
 
	
4
-
p
r
e
f
i
x
 
s
t
e
m
m
i
n
g

Cz
ec
h-
En
gli
sh
En
gli
sh
-
Cz
ec
h
Ge
rm
an
-
En
gli
sh
En
gli
sh
-
Ge
rm
an
Sp
an
ish
-
En
gli
sh
En
gli
sh
-
Sp
an
ish
Fr
en
ch
-
En
gli
sh
En
gli
sh
-
Fr
en
ch
Ru
ssi
an
-
En
gli
sh
En
gli
sh
-
Ru
ssi
an
ef
m
ar
al 
giz
a+
+ 	fast_align
23.
43	23.29	22.77
16.
22	15.97	15.69
23.
60	23.86	22.84
17.
83	17.69	17.50
28.
50	28.43	28.25
27.
39	27.51	27.08
28.
50	28.45	28.06
27.
73	27.57	27.22
20.
74	20.14	19.55
15.
89	15.55	15.07
ef
m
ar
al 
giz
a+
+ 	fast_align
23.
58	23.57	23.44
16.
11	15.96	15.88
23.
54	23.80	23.08
17.
77	17.70	17.65
28.
57	28.69	28.20
27.
49	27.49	27.08
28.
69	28.67	28.33
27.
66	27.71	27.16
20.
96	20.65	20.38
16.
17	16.13	15.77


Table 5. Timings from the word alignments for  our SMT  evaluation. The values are 
averaged over both alignment directions. For these experiments we used systems with
8-core Intel E5-2670 processors running at 2.6 GHz.

Time (seconds)

Tr
an
sla
tio
n 
pa
ir

St
e
m
Wall 	CPU
ef
m
ar
al
Wall 	CPU
g
i
z
a
+
+
Wall 	CPU
fast_
align
Cz
ec
h-
En
gli
sh
no
3
0
3
4
6
2
13,
08
9
13,
08
3
4
6
5
1,7
59
Cz
ec
h-
En
gli
sh
ye
s
2
3
3
3
6
1
12,
03
5
12,
03
3
3
1
1
1,2
00
Ge
rm
an
-
En
gli
sh
no
5
1
1
7
6
6
42,
07
7
41,
75
4
1,1
51
4,4
07
Ge
rm
an
-
En
gli
sh
ye
s
3
7
7
5
5
8
43,
04
8
43,
02
3
8
1
3
3,1
15
Sp
an
ish
-
En
gli
sh
no
5
0
0
7
8
2
39,
04
7
39,
00
3
1,0
34
3,9
40
Sp
an
ish
-
En
gli
sh
ye
s
3
4
6
5
2
5
38,
89
6
38,
86
6
7
5
8
2,9
11
Fr
en
ch
-
En
gli
sh
no
6
9
6
1,0
88
41,
69
8
41,
64
6
1,6
81
6,4
23
Fr
en
ch
-
En
gli
sh
ye
s
3
8
3
5
8
3
40,
98
6
40,
90
7
8
0
5
3,1
01
Ru
ssi
an
-
En
gli
sh
no
1
2
2
2
0
6
3
5
8
3
3
5
8
1
1
0
7
3
8
2
Ru
ssi
an
-
En
gli
sh
ye
s
8
7
1
5
1
3
1
4
8
3
1
4
3
7
8
2
9
2





143


PBML 106	OCTOBER 2016


6. Concluding remarks

   We hope  that  the reader at this point  is convinced that  Bayesian  alignment mod- 
els with  Markov Chain  Monte  Carlo  inference should be the  method of choice  for 
researchers who need  to align large parallel corpora. To facilitate  a practical shift to- 
wards this direction, we have released the efmaral tool which  the evaluations in this 
article show to be both accurate, computationally efficient, and useful as a component 
of practical machine translation systems.

Acknowledgments

   Computational resources for this  project  were  provided by CSC, the  Finnish  IT 
Center  for Science.4

Bibliography

Blackwell, David.  Conditional Expectation and Unbiased Sequential Estimation. The Annals of
Mathematical Statistics, 18(1):105–110, 03 1947. doi:  10.1214/aoms/1177730497. URL http:
//dx.doi.org/10.1214/aoms/1177730497.
Blunsom, Phil, Trevor Cohn, Sharon  Goldwater, and Mark Johnson.  A Note on the Implemen- 
tation  of Hierarchical Dirichlet  Processes.  In Proceedings of the ACL-IJCNLP 2009 Conference 
Short Papers, ACLShort  ’09, pages  337–340, Stroudsburg, PA, USA, 2009. Association for 
Computational Linguistics. URL http://dl.acm.org/citation.cfm?id=1667583.1667688.
Brown,  Peter  F., Vincent  J. Della Pietra,  Stephen A. Della Pietra,  and  Robert  L. Mercer.   The 
Mathematics of Statistical  Machine  Translation: Parameter Estimation. Computational Lin- 
guistics, 19(2):263–311, June 1993. ISSN 0891-2017. URL http://dl.acm.org/citation.cfm? 
id=972470.972474.
Dempster, A. P., N. M. Laird, and D. B. Rubin.  Maximum Likelihood from Incomplete Data via 
the EM Algorithm. Journal of the Royal Statistical Society, 39(1):1–38, 1977.
Dyer,  Chris,  Victor Chahuneau, and  Noah  A. Smith.   A Simple,  Fast, and  Effective Reparam- 
eterization of IBM Model  2.   In Proceedings of the 2013 Conference of the North  American 
Chapter of the Association for Computational Linguistics: Human Language Technologies, pages
644–648, Atlanta, Georgia,  June  2013. Association for  Computational  Linguistics.   URL
http://www.aclweb.org/anthology/N13- 1073.
Fraser,  Alexander and  Daniel  Marcu.   Measuring Word  Alignment Quality for Statistical  Ma- 
chine Translation. Computational Linguistics, 33(3):293–303, Sept. 2007. ISSN 0891-2017. doi:
10.1162/coli.2007.33.3.293. URL http://dx.doi.org/10.1162/coli.2007.33.3.293.
Gal, Yarin and Phil Blunsom. A Systematic Bayesian Treatment of the IBM Alignment Models.
In Proceedings of the 2013 Conference of the North American Chapter of the Association for Compu- 
tational Linguistics: Human Language Technologies, Stroudsburg, PA, USA, 2013. Association 
for Computational Linguistics.

4 https://www.csc.fi/

144


R. Östling, J. Tiedemann	Eﬃcient Word Alignment with MCMC (125–146)


Gelfand, Alan E. and Adrian F. M. Smith.  Gibbs Sampling for Marginal Posterior Expectations.
Technical  report, Department of Statistics, Stanford University, 1991.
Heafield, Kenneth, Ivan Pouzyrevsky, Jonathan H. Clark, and  Philipp Koehn.  Scalable Modi- 
fied Kneser-Ney Language Model Estimation. In Proceedings of ACL, pages  690–696, 2013.
Holmqvist, Maria  and  Lars Ahrenberg.  A Gold  Standard for English-Swedish Word  Align- 
ment.   In Proceedings of the 18th Nordic Conference of Computational Linguistics (NODALIDA
2011), number 11 in NEALT Proceedings Series, pages  106–113, 2011.
Koehn,  Philipp. Europarl: A Parallel  Corpus for Statistical  Machine  Translation. In The Tenth
Machine Translation Summit.,  Phuket, Thailand, 2005.
Koehn,  Philipp, Franz  Josef Och, and  Daniel  Marcu.   Statistical  Phrase-based Translation.  In 
Proceedings of the 2003 Conference of the North American Chapter of the Association for Com- 
putational Linguistics on Human Language Technology - Volume 1, NAACL  ’03, pages  48–54, 
Stroudsburg, PA, USA, 2003. Association for Computational  Linguistics.   doi:   10.3115/
1073445.1073462. URL http://dx.doi.org/10.3115/1073445.1073462.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola 
Bertoldi,  Brooke Cowan, Wade  Shen, Christine Moran,  Richard  Zens, Christopher J. Dyer, 
Ondřej  Bojar, Alexandra Constantin, and  Evan  Herbst.  Moses:  Open  Source  Toolkit  for 
Statistical  Machine  Translation. In Proceedings of ACL, pages  177–180, 2007.
Martin,  Joel, Rada  Mihalcea,  and  Ted Pedersen.  Word  Alignment for Languages with  Scarce
Resources. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, ParaText
’05, pages  65–74, Stroudsburg, PA, USA, 2005. Association for Computational Linguistics. 
URL http://dl.acm.org/citation.cfm?id=1654449.1654460.
Matusov, Evgeny,  Richard  Zens, and Hermann Ney.  Symmetric Word  Alignments for Statisti- 
cal Machine  Translation. In Proceedings of the 20th International Conference on Computational 
Linguistics, COLING  ’04, Stroudsburg, PA, USA, 2004. Association for Computational Lin- 
guistics.  URL http://dx.doi.org/10.3115/1220355.1220387.
Mermer, Coşkun and Murat Saraçlar.  Bayesian Word  Alignment for Statistical  Machine  Trans- 
lation.  In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: 
Human Language Technologies: short papers - Volume 2, HLT ’11, pages  182–187, Stroudsburg, 
PA, USA, 2011. Association for Computational Linguistics. ISBN 978-1-932432-88-6. URL 
http://dl.acm.org/citation.cfm?id=2002736.2002775.
Mermer, Coşkun, Murat Saraçlar, and Ruhi Sarikaya.  Improving Statistical Machine Translation 
Using Bayesian  Word  Alignment and  Gibbs Sampling. IEEE Transactions on Audio, Speech, 
and Language Processing, 21(5):1090–1101, May 2013.  ISSN 1558-7916. doi:  10.1109/TASL.
2013.2244087.
Mihalcea,  Rada and Ted Pedersen. An Evaluation Exercise for Word Alignment. In Proceedings 
of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine 
Translation and Beyond - Volume 3, HLT-NAACL-PARALLEL ’03, pages  1–10, Stroudsburg, 
PA, USA, 2003. Association for Computational Linguistics. doi:  10.3115/1118905.1118906. 
URL http://dx.doi.org/10.3115/1118905.1118906.
Och, Franz Josef. Minimum Error Rate Training in Statistical  Machine  Translation. In Proceed- 
ings of ACL, pages  160–167, 2003.

145


PBML 106	OCTOBER 2016


Och, Franz Josef and Hermann Ney.  A Systematic Comparison of Various Statistical Alignment
Models.   Computational Linguistics, 29(1):19–51, Mar. 2003.  ISSN 0891-2017.  doi:  10.1162/
089120103321337421. URL http://dx.doi.org/10.1162/089120103321337421.
Östling,  Robert.    Bayesian Models for Multilingual  Word Alignment.    PhD  thesis,  Stockholm
University, 2015. URL http://urn.kb.se/resolve?urn=urn:nbn:se:su:diva- 115541.  ISBN
978-91-7649-151-5.
Pitman, Jim and  Marc  Yor.   The two-parameter Poisson-Dirichlet distribution derived from 
a stable  subordinator.  The Annals  of Probability, 25(2):855–900, 1997.  doi:  10.1214/aop/
1024404422.
Quirk, Chris.  Exact Maximum Inference for the Fertility Hidden Markov Model.  In Proceedings 
of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August
2013, Sofia, Bulgaria, Volume 2: Short Papers, pages  7–11, 2013.  URL http://aclweb.org/
anthology/P/P13/P13- 2002.pdf.
Riley, Darcey  and  Daniel  Gildea.    Improving the  IBM Alignment Models  Using  Variational 
Bayes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: 
Short Papers - Volume 2, ACL ’12, pages 306–310, Stroudsburg, PA, USA, 2012. Association for 
Computational Linguistics. URL http://dl.acm.org/citation.cfm?id=2390665.2390736.
Tiedemann, Jörg.   Bitext Alignment.    Synthesis Lectures  on Human Language Technologies.
Morgan & Claypool Publishers, 2011.
Toutanova, Kristina, H. Tolga Ilhan, and Christopher Manning. Extensions to HMM-based Sta- 
tistical Word Alignment Models.  In 2002 Conference on Empirical Methods in Natural Language 
Processing (EMNLP 2002), pages 87–94, 2002. URL http://ilpubs.stanford.edu:8090/557/.
Vogel, Stephan, Hermann Ney, and Christoph Tillmann. HMM-Based Word Alignment in Sta- 
tistical  Translation.  In Proceedings of the 16th Conference on Computational Linguistics - Vol- 
ume 2, COLING ’96, pages  836–841, Stroudsburg, PA, USA, 1996. Association for Computa- 
tional Linguistics. doi: 10.3115/993268.993313.  URL http://dx.doi.org/10.3115/993268.
993313.
Yarowsky, David,  Grace Ngai, and Richard  Wicentowski. Inducing Multilingual Text Analysis 
Tools via Robust  Projection  Across Aligned Corpora. In Proceedings of the First International 
Conference on Human  Language Technology Research, HLT ’01, pages  1–8, Stroudsburg, PA, 
USA, 2001. Association for Computational Linguistics. doi: 10.3115/1072133.1072187.  URL 
http://dx.doi.org/10.3115/1072133.1072187.
Zhao, Shaojun  and Daniel Gildea.  A Fast Fertility  Hidden Markov Model for Word Alignment 
Using MCMC.  In Proceedings of the 2010 Conference on Empirical Methods in Natural Language 
Processing, pages  596–605, Cambridge, MA, USA, October  2010. Association for Computa- 
tional Linguistics. URL http://www.aclweb.org/anthology/D10- 1058.


Address for correspondence: 
Robert Östling 
robert.ostling@helsinki.fi 
PL 24 (Unionsgatan 40, A316)
00014 Helsingfors universitet, Finland


146

