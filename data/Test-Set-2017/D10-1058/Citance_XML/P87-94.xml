<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present an approximative IBM Model 4 for word alignment.</S>
		<S sid ="2" ssid = "2">Diﬀerent with the most widely-used word aligner GIZA++, which implements all the 5 IBM models and HMM model in the framework of Expectation Maximum (EM), we adopt a full Bayesian inference which integrates over all possible parameter values, rather than estimating a single parameter value.</S>
		<S sid ="3" ssid = "3">Empirical results show promising improvements in alignment quality as well as in BLEU score for the translation performance over baselines.</S>
		<S sid ="4" ssid = "4">Keywords : Bayesian inference; Word alignment; Statistical machine translation.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Word alignment can be deﬁned as a procedure for detecting the corresponding words in a bilingual sentence pair.</S>
			<S sid ="6" ssid = "6">One of the notorious criticisms of word alignment is the inconsistence between the word alignment model to the phrase based translation model.</S>
			<S sid ="7" ssid = "7">In this paper, we have no intention to avoid mentioning this inherent weakness of word alignment, but we would say, as far as we know, word alignment is a fundamental component for most of the SMT systems.</S>
			<S sid ="8" ssid = "8">Phrase or the other higher level translation knowledge is extracted based on the word alignment, which is called two-stage approach.</S>
			<S sid ="9" ssid = "9">And even for approaches of so-called direct phrase alignment, they can rarely abandon word alignment thoroughly.</S>
			<S sid ="10" ssid = "10">Because of the computation complexity of phrase alignment, word alignment is usually used to constrain the inference.1 DeNero proposes a relative pure joint phrase model but still uses the word alignment as initialization and smoothing, which shows the least dependency on word alignment.2 Neubig uses Bayesian methods and Inversion Transduction Grammar for joint phrase alignment,3 and the base distribution for the Dirichlet Process 5 prior is constructed by the word alignment model.</S>
			<S sid ="11" ssid = "11">Therefore, word alignment is well worth concern.</S>
			<S sid ="12" ssid = "12">Our hope is to induce a better word alignment by ∗11-1, NojiHigashi, Kusatsu, Shiga, Japan.</S>
			<S sid ="13" ssid = "13">87 88 Z. Li, H. Ikeda utilizing the state-of-the-art learning technology, and establish a better baseline for the word level alignment models.</S>
			<S sid ="14" ssid = "14">Bayesian inference, the approach we adopt in this paper, has been broadly applied to various learning of latent structure.</S>
			<S sid ="15" ssid = "15">Goldwater points out that two theoretical factors contribute to the superiority of Bayesian inference.7 First, integrating over parameter values leads to greater robustness in decision.</S>
			<S sid ="16" ssid = "16">One of the problems that trouble EM algorithm is over-ﬁtting.</S>
			<S sid ="17" ssid = "17">Moore discusses details of how a ”Garbage collector” is generated.11 He also suggests a number of heuristic solutions, but Bayesian inference can oﬀer a more principled solution.</S>
			<S sid ="18" ssid = "18">The second factor is that the integration permits the use of priors favoring sparse distributions, which proved to be more consistent with nature of natural language.</S>
			<S sid ="19" ssid = "19">Another practical advantage is that the implementation can be much easier than EM.12 In the following sections, we will have a review for IBM Model 4 in Section 2, and reformulate it into a simpler and Bayesian form in Section 3.</S>
			<S sid ="20" ssid = "20">Section 4 gives the Bayesian inference, and Section 5 reports results of experiment.</S>
			<S sid ="21" ssid = "21">Section 6 compares related research, and Section 7 concludes.</S>
	</SECTION>
	<SECTION title="IBM Model 4. " number = "2">
			<S sid ="22" ssid = "1">Model 4 is a fertility-based alignment model, and can be viewed as the outstanding representative of all the IBM translation models.</S>
			<S sid ="23" ssid = "2">The model can be expressed as P (F, A|E; n, t, d) = Pϕ (ϕI |E; n)Pτ (τ I |ϕI , E; t)Pπ (πI |ϕI , τ I , E; d) (1) 0 0 0 I I 0 0 0 I ϕi 1 I ϕi = n0 (ϕ0 | ∑ ϕi ) ∏ n(ϕi |ei ) ∏ ∏ t(τik |ei ) ϕ0 ! ∏ ∏ pik (πik ) where i=1 I i=1 ( ∑I i=0 k=1 ) ∑I i=1 k=1 n0 (ϕ0 | ∑ ϕi ) = i=1 i=1 ϕi ϕ0 p0 i=1 ϕi −2ϕ0 p1 ϕ0 (2) { d1 (j − cρi |A(eρi ), B(τi1 )) if k = 1 pik (πik ) = d&gt;1 (j − π ik−1 |B(τik )) if k &gt; 1 (3) Pϕ , Pτ and Pπ denote fertility model, lexical model and distortion model respectively, and their parameters can be described as n, t, and d. More details can be found in Brown et al.4</S>
	</SECTION>
	<SECTION title="Bayesian Model. " number = "3">
			<S sid ="24" ssid = "1">Our Bayesian model almost repeats the same generative scenarios shown in the previous section, but puts an appropriate prior for the parameters in the model.</S>
			<S sid ="25" ssid = "2">That is, parameter will be treated as variable, which makes a signiﬁcant diﬀerence to the traditional MLE or MAP approaches.</S>
			<S sid ="26" ssid = "3">In our proposed Bayesian setting, the fertility A Fully Bayesian Inference for Word Alignment 89 ϕ and translation f for each target word e, both of which follow a Multinominal distribution, will be treated as a random variable with a prior, and Dirichlet distribution is a natural choice for them, since it is conjugate to the Multinominal distribution.</S>
			<S sid ="27" ssid = "4">Since we can not specify the dimensions of the above distributions in advance, one solution is to take advantage of the nonparametric prior.</S>
			<S sid ="28" ssid = "5">Here we use the Dirichlet Process (DP) which can ensure that the resulting distributions concentrate their probability mass on a small number of fertilities or translation candidates while retaining reasonable probability for unseen possibilities.</S>
			<S sid ="29" ssid = "6">ne ∼ D P ( α , P o is s o n ( 1 , ϕ ) ) (4) ϕ|e ∼ n e (5) te ∼ D P ( β , T 0 ( f |e ) ) (6) f |e ∼ te (7) In the above distribution formulas, ne denotes the fertility distribution for e, and hyperparameter α is a concentration parameter which aﬀects the variance of the draws.</S>
			<S sid ="30" ssid = "7">We make P oisson(1, ϕ) as the base distribution for fertility which encodes our prior knowledge about the properties of fertilities.</S>
			<S sid ="31" ssid = "8">Namely, high fertility should be discouraged except that there is enough evidence.</S>
			<S sid ="32" ssid = "9">λ(e) denotes the expected fertility for e, and for simplicity, we assign 1 as the value of expected fertility for all the words.</S>
			<S sid ="33" ssid = "10">te is a translation distribution for e, and β is the concentration parameter.</S>
			<S sid ="34" ssid = "11">As for base distribution T0 , shown as: T0 (f |e) = ∑ p(et|e)p(f t|et)p(f |f t) (8) et,f t where et denotes e’s Part-of-Speech (henceforth POS), and f t denotes f ’s POS.</S>
			<S sid ="35" ssid = "12">p(f t|et) is a POS translation model, p(et|e) is a transition probability from word to POS, and p(f |f t) is a uniform distribution (over word types tagged with f t) for each word f . T0 encodes such a prior knowledge: POS provides clues for the alignment.</S>
			<S sid ="36" ssid = "13">While our Bayesian model still has other free parameters, we still use p0 and p1 as parameters to model fertility for e0 as same as in IBM models, but we ﬁx them to reasonable values in order to focus on learning for the other distributions.</S>
			<S sid ="37" ssid = "14">As for the distortion model, we simply adopt a distance penalty (not including the distortion for words generated by e0 ) shown as follows 1 pπ (A) ∝ ϕ ! J ∏ j=1,aj ̸=0 { πρi ϕρ b|j−prev(j)| (9) prev(j) = if k = 1 i πik−1 if k &gt; 1 (10) 90 Z. Li, H. Ikeda where b is a ﬁxed value less than 1, prev(j) means the position of predecessor for fj . ρi denotes the ﬁrst position to the left of ei for which has a nonzero fertility, and πik is the position of word τik for permutation π.</S>
			<S sid ="38" ssid = "15">The ﬁrst part of our distortion formula models the distortion procedure for words generated by e0 , which uses the same strategy as IBM models that all these words are positioned only after the nonempty positions have been covered.</S>
			<S sid ="39" ssid = "16">Therefore, there are ϕ0 ! ways to order the ϕ0 words.</S>
			<S sid ="40" ssid = "17">Due to the above simpliﬁcation for fertility model, we will see a more convenient inference in following sections.</S>
			<S sid ="41" ssid = "18">Another theoretical reason is that we do not expect a skewed distribution for the above parameters as same as the fertility and lexical models.</S>
			<S sid ="42" ssid = "19">Therefore, it is unnecessary to put a prior for these parameters.</S>
	</SECTION>
	<SECTION title="Bayesian Inference. " number = "4">
			<S sid ="43" ssid = "1">A frequent strategy to infer the posterior distribution is Gibbs sampling.12 For our concerned word alignment, instead of sampling the parameters explicitly, we sample the alignment structure directly with the parameters marginalized out.</S>
			<S sid ="44" ssid = "2">Then the Gibbs sampler is converted into a collapsed Gibbs sampler, and we have P (F, A|E; α, β) = ∫ n,t P (F, A, n, t, d|E; α, β) (11) where n comprises all the ne for each e, and t comprises all the te . d does not need integral since we do not treat this parameter as a random variable, and will be replaced by constant b in the left part of the integral formula.</S>
			<S sid ="45" ssid = "3">Due to the collapsed sampler, we need not sample the parameters explicitly, but directly sample the latent alignment structure in condition of ﬁxed α and β.</S>
			<S sid ="46" ssid = "4">Our collapsed Gibbs sampler works by sampling each component of vector a alternatively.</S>
			<S sid ="47" ssid = "5">The probability for a new component value when the other values are ﬁxed can be written P (aj |aj , F, E; α, β) ∝ (12) Pϕ (aj |aj , F, E; α, β)Pτ (aj |aj , F, E; α, β)Pπ (aj |aj , F, E; α, β) where aj denotes the alignment exclude aj . Pϕ , Pτ and Pπ represent fertility, translation and distortion sub-models respectively.</S>
			<S sid ="48" ssid = "6">The probability of new sample can be calculated according to the three sub-models.</S>
			<S sid ="49" ssid = "7">This calculation is very similar with the procedure that ﬁnds the neighbour alignments in the E-step of EM, but in a way metaphorized as Chinese Restaurant Process instead of using ﬁxed parameters.12 First, we will investigate the translation model.</S>
			<S sid ="50" ssid = "8">Thanks to the exchangeability, we can write C ount(eaj , fj ) + βT0 (fj |eaj ) Pτ (aj |aj , F, E; α, β) ∝ Σf C ount(eaj (13) , f ) + β A Fully Bayesian Inference for Word Alignment 91 where C ount(e, f ) is the number of links between word pair (e, f ) in the other part of this sentence pair and other sentence pairs in the training corpus.</S>
			<S sid ="51" ssid = "9">As for the fertility model, because of the special treatment of the fertility for e0 , two cases should be considered.</S>
			<S sid ="52" ssid = "10">In the ﬁrst case aj ! = 0, C ount(eaj , ϕaj + 1) + αP oisson(1, ϕaj + 1) Pϕ (aj |aj , F, E; α, β) ∝ C ount(eaj , ϕaj ) + αP oisson(1, ϕaj (14) ) where C ount(e, ϕ) is the frequency of cases where word e has a fertility ϕ, and the denominator encodes the fact that the new assignment will cause an instance of word-fertility to be removed from the cache as the new word-fertility is added.</S>
			<S sid ="53" ssid = "11">And in the second case, aj = 0.</S>
			<S sid ="54" ssid = "12">As is described in the previous section, the fertility for empty word is not decided by itself, but decided by the number of words generated by nonempty words, which follows a binominal distribution.</S>
			<S sid ="55" ssid = "13">So we can infer Pϕ (aj = 0|aj , F, E; α, β) ∝ n0 (ϕ0 + 1| ∑I I ϕi ) ∑I = i=1 ϕi − ϕ0 )p1 (15) n0 (ϕ0 | ∑i=1 ϕi ) (ϕ0 + 1)p0 The calculation for the distortion model is more direct since it is unnecessary to consider the cache model.</S>
			<S sid ="56" ssid = "14">Because of the special treatment for distortion of words aligned with the empty word, we also need to take account two cases, as for the ﬁrst case, aj ! = 0 Pπ (aj |aj , F, E; α, β) ∝ b| j−prev(j)|+|next(j)−j|−|next(j)−prev(j)| (16) where the exponent means 3 distortions are changed, and next(j) is subject to j == prev(next(j)).</S>
			<S sid ="57" ssid = "15">In the second case, where aj = 0, we just need to consider the probability of a permutation of ϕ0 words in the remained uncovered positions.</S>
			<S sid ="58" ssid = "16">Notice that, the fertility value changes from ϕ0 to ϕ0 + 1 after this new assignment, then we have Pπ (aj = 0|aj , F, E; α, β) ∝ (ϕ ϕ0 ! = 0 + 1)!</S>
			<S sid ="59" ssid = "17">ϕ0 1 (17) + 1 The ﬁnal probability for the new derivation should combine all the above in- ﬂuence factors, and the production of all the three factors as the ﬁnal probability.</S>
			<S sid ="60" ssid = "18">The algorithm is described in Table 1.</S>
			<S sid ="61" ssid = "19">To accelerate the convergence, we use HMM based Viterbi alignment as an initialization.</S>
			<S sid ="62" ssid = "20">After burn-in iterations, we begin to collect alignment counts from the samples.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="63" ssid = "1">All the corpus we used is ChineseEnglish corpus in patent domain, which is released by NTCIR9.15 We select 350K sentence pairs as training corpus, and 1000 pairs as 92 Z. Li, H. Ikeda Table 1.</S>
			<S sid ="64" ssid = "2">Gibbs sampling for word alignment.</S>
			<S sid ="65" ssid = "3">For each sentence pair (E, F ) in corpus Initialize alignment For each generation For each sentence pair (E, F ) in corpus For each j in [1, |F |] For each i in [0, |E|] calculate p(aj = i|aj , F, E; α, β) Normalize p(aj |aj , F, E; α, β) Sample a new value for aj ; update the cache count If (Current generation ≥ Burn-in) Save alignment for (E, F ) development set.</S>
			<S sid ="66" ssid = "4">We also annotate 300 word aligned sentence pairs to evaluate the quality of word alignment, and select 2000 bilingual pairs as the test set for translation.</S>
			<S sid ="67" ssid = "5">Before running our Bayesian aligner, we should estimate the parameters in T0 . We tagged the training corpus using some POS taggers, and replace each word by its POS to get a POS parallel corpus.</S>
			<S sid ="68" ssid = "6">Then, we ran IBM model 1 on the POS corpus to get the POS translation probabilities.</S>
			<S sid ="69" ssid = "7">Through dividing the number of occurrences of the word-tag pair (e, et) by the number of occurrences of e, we can get p(et|e).</S>
			<S sid ="70" ssid = "8">Suppose word f is tagged with f t at least once in the training corpus, then p(f |f t) is equal to the result of dividing 1 by the number of unique words tagged with f t; otherwise, p(f |f t) is 0.To contrast our approach with GIZA++, we need the Viterbi alignment ex tracted from the multiple samples, and one strategy is to assign each aj as the most frequent value in the collected samples.</S>
			<S sid ="71" ssid = "9">We set 1000 as the number of total iterations and 0 as the burn-in value, and conﬁgure α and β with varying values.</S>
			<S sid ="72" ssid = "10">We run GIZA++ in the standard conﬁguration (Training scheme is abbreviated as 15 H 5 33 43 ).</S>
			<S sid ="73" ssid = "11">Both of the above two approaches need run in two directions and symmetrization.</S>
			<S sid ="74" ssid = "12">Table 2 shows the comparison of AER between GIZA++ (EM) and our Bayesian model.</S>
			<S sid ="75" ssid = "13">When α = 1 and β = 100, our proposed approach can get the best performance, which reveals a satisfying improvement for alignment quality in terms of AER, with a reduction of 3.41% over GIZA++.</S>
			<S sid ="76" ssid = "14">For translation experiments, we use Moses as our decoder,10 and use SRILM to train 4-grams language models on both sides of the bilingual corpus.</S>
			<S sid ="77" ssid = "15">As is shown in Table 3, we can see that the Bayesian approach outperforms EM approach in both directions, which proves the eﬀectiveness of our proposed approach.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "6">
			<S sid ="78" ssid = "1">Our approach is similar with Coskun in spirit to Bayesian inference,9 where it places a prior for the model parameters and adopts a collapsed sampler, but they take Model 1 as the inference object, which we suppose somewhat harsh.</S>
			<S sid ="79" ssid = "2">Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.</S>
			<S sid ="80" ssid = "3">Performance of Word Alignment.</S>
			<S sid ="81" ssid = "4">Met hod A E R EM (GI ZA ++) 1 6 . 1 2 % Bay esia n(α = 0.5, β = 100) 1 3 . 4 3 % Bay esia n(α = 1, β = 100) 1 2 . 7 1 % Bay esia n(α = 1.5, β = 100) 1 3 . 7 4 % Bay esia n(α = 1, β = 50) 1 5 . 0 4 % Bay esia n(α = 1, β = 200) 1 2 . 9 8 % Table 3.</S>
			<S sid ="82" ssid = "5">Performance of Final Translation (BLEU4).</S>
			<S sid ="83" ssid = "6">Met hod C h i n es e E n gl is h E n gl is h C hi n es e EM (GI ZA ++) 0.</S>
			<S sid ="84" ssid = "7">2 7 6 6 0.</S>
			<S sid ="85" ssid = "8">2 9 6 4 Bay esia n(α = 0.5, β = 100) 0.</S>
			<S sid ="86" ssid = "9">2 7 8 7 0.</S>
			<S sid ="87" ssid = "10">2 9 9 3 Bay esia n(α = 1, β = 100) 0 . 2 7 9 8 0 . 3 0 1 1 Bay esia n(α = 1.5, β = 100) 0.</S>
			<S sid ="88" ssid = "11">2 7 8 1 0.</S>
			<S sid ="89" ssid = "12">2 9 8 6 Bay esia n(α = 1, β = 50) 0.</S>
			<S sid ="90" ssid = "13">2 7 7 8 0.</S>
			<S sid ="91" ssid = "14">2 9 7 8 Bay esia n(α = 1, β = 200) 0.</S>
			<S sid ="92" ssid = "15">2 7 9 5 0.</S>
			<S sid ="93" ssid = "16">3 0 0 3 4 but keeps the fertility as a component of modeling.</S>
			<S sid ="94" ssid = "17">But they do not place any prior on the parameters, which can be viewed as a stochastic EM.</S>
			<S sid ="95" ssid = "18">They also assume fertility follows a Poisson distribution, while ours adopts a DP prior and Poisson distribution as the base distribution in the DP prior.</S>
			<S sid ="96" ssid = "19">Darcey et al. use variational Bayes which closely resembles the normal form of EM algorithm to improve the performance of GIZA++, as well as the BLEU score.14</S>
	</SECTION>
	<SECTION title="Conclusions and  Future Work. " number = "7">
			<S sid ="97" ssid = "1">We have described an approximative IBM model 4 for word alignment, and adopt Bayesian inference which currently is a promising replacement for EM and already broadly applied for various tasks in the ﬁeld of NLP.</S>
			<S sid ="98" ssid = "2">Our pilot experiment shows a higher AER for word alignment as well as a modest improved BLEU score for translation.</S>
			<S sid ="99" ssid = "3">Our current research focuses on phrase extraction and reordering from multiple alignment samples generated by our Bayesian inference, and we expect a better performance.</S>
	</SECTION>
</PAPER>
