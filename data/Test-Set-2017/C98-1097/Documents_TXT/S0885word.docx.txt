Available online at www.sciencedirect.com




Computer Speech and Language 26 (2012) 90–104



Enhancing lexical cohesion measure with confidence measures, 
semantic relations and language model interpolation for 
multimedia spoken content topic segmentation�
Camille Guinaudeau a,∗, Guillaume Gravier b, Pascale Sébillot c
a INRIA Rennes, 35000 Rennes, France
b CNRS, IRISA, 35000 Rennes, France
c INSA, IRISA, 35000 Rennes, France
Received 4 February 2011; received in revised form 9 June 2011; accepted 30 June 2011
Available online 18 July 2011



Abstract

  Transcript-based topic segmentation of TV programs faces several difficulties arising from transcription errors, from the presence 
of potentially short segments and from the limited number of word repetitions to enforce lexical cohesion, i.e., lexical relations that 
exist within a text to provide a certain unity. To overcome these problems, we extend a probabilistic measure of lexical cohesion 
based on generalized probabilities with a unigram language model. On the one hand, confidence measures and semantic relations 
are considered as additional sources of information. On the other hand, language model interpolation techniques are investigated for 
better language model estimation. Experimental topic segmentation results are presented on two corpora with distinct characteristics, 
composed respectively of broadcast news and reports on current affairs. Significant improvements are obtained on both corpora, 
demonstrating the effectiveness of the extended lexical cohesion measure for spoken TV contents, as well as its genericity over 
different programs.
© 2011 Elsevier Ltd. All rights reserved.

Keywords: Topic segmentation; Lexical cohesion; Confidence measures; Semantic relations; Language model interpolation; TV broadcasts



1. Introduction

  Structuring video feeds has become a requirement and is a highly challenging issue. Indeed, with the proliferation of 
video-sharing websites and the increasing number of television channels, the quantity of video that users can access has 
become so important that it is necessary to develop methods to structure this material and enable users to navigate. A 
crucial structuring stage is the segmentation of shows into topically homogeneous segments (Wactlar et al., 1996; Allan 
et al., 1998). Since videos available are of different kinds (movies, news, talk shows, etc.) and in order to avoid the use 
of several domain specific methods, such structuring approaches must necessarily be generic enough to treat various 
types of video. To this end, topic segmentation of informative content TV shows can rely on the speech pronounced in


� This paper has been recommended for acceptance by ‘INTERSPEECH guest editors’.
∗  Corresponding author.
E-mail addresses: camille.guinaudeau@inria.fr (C. Guinaudeau), guillaume.gravier@irisa.fr (G. Gravier), pascale.sebillot@irisa.fr (P. Sébillot).

0885-2308/$ – see front matter © 2011 Elsevier Ltd. All rights reserved. 
doi:10.1016/j.csl.2011.06.002



the programs as it is not dependent on the kind of document. In this case, the segmentation is based on the analysis of 
the distribution of words within the speech, a topic change being detected when the vocabulary changes significantly. 
With the improvement of automatic speech recognition (ASR) systems in recent years (Ostendorf et al., 2008), topic 
segmentation of spoken documents can now be performed on automatic transcripts of the speech material. However, 
most of the work in this direction directly apply methods developed for textual topic segmentation to automatic 
transcripts of spoken language without taking into account the specifics of TV programs transcripts (transcription 
errors, short topic segments). These methods are often based on the notion of lexical cohesion which corresponds 
to the lexical relations that exist within a text, mainly enforced by reiterations, i.e., repetitions of the same words 
(Mulbregt et al., 1999; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006). Alternately, discourse markers, 
obtained from a preliminary expert or learning process, can also be used to identify topic boundaries (Beeferman et al.,
1999; Christensen et al., 2005).
  Christensen et al. (2005) have established that transcription errors have little effect on the performance of a supervised 
segmentation algorithm using discourse markers. However, we have observed a large gap of performance between 
manual and automatic transcripts in previous work on topic segmentation of radio broadcasts using an unsupervised 
approach based on lexical cohesion (Huet et al., 2008). This difference is mostly due to the specifics of the material 
on which we focus, i.e., TV shows. Indeed, automatic transcripts of TV shows have certain peculiarities that are 
detrimental to topic segmentation and, in general, to natural language processing. Firstly, the error rate of the ASR 
system used, even if it remains reasonable for news, can be as high as 70% for challenging programs such as talk 
shows or debates. Moreover, TV programs are composed of topic segments that can be very short and contain few 
repetitions of vocabulary, particularly in news where journalists make use of synonyms to avoid reiterations. In our 
corpus, we have measured that a word occurs on average 1.8 times in a topically coherent segment in broadcast news 
and 2.0 times in reports on current affairs (for more details, cf. Section 4.3). In order to overcome difficulties related 
to transcription errors, some studies have suggested to add features specific to spoken documents to the sole concept 
of lexical cohesion. For example, Amaral and Trancoso (2003) exploits speaker detection to locate the anchor speaker 
in news program, relating anchor speaker occurrences with new reports and hence with topic changes. In Stolcke et al. 
(1999), prosody is used in addition to automatic transcription. However, such clues are seldom used in practice because 
their automatic extraction is difficult. Moreover, they imply domain and genre specific knowledge and are, therefore, 
highly dependent on the type of document.
  The aim of this paper is to propose a segmentation method able to deal with peculiarities of professional videos 
(transcription errors, possibly short segments and limited number of repetitions) while remaining generic enough. This 
method is based on the criterion of lexical cohesion which is not dependent on a type of document since it is mainly 
enforced by word repetitions. However lexical cohesion is not efficient when the number of reiterations is low—i.e., 
when synonyms are used or topic segments are very short—and is sensitive to transcription errors, two characteristics of 
our video material. We propose several extensions to a measure of lexical cohesion, based on generalized probabilities 
using a unigram language model, in order to make this criterion more robust to spoken content. On the one hand, 
the measure of the lexical cohesion is modified by an original technique that incorporates two sources of additional 
information: semantic relations, highlighting the semantic proximity between words, and confidence measures. On the 
other hand, we propose to use language model interpolation techniques so as to provide better estimates of the lexical 
cohesion on short segments.
  The paper is organized as follows: we first present the topic segmentation method, based on lexical cohesion, 
developed for the segmentation of textual documents and used as a baseline in this work. In Section 3, extensions 
of the probabilistic lexical cohesion measure to improve robustness to TV program specifics are described. The 
experimental setup is presented in Section 4. Finally, experimental results are extensively discussed in Section 5, 
before the presentation of future work.

2. Topic segmentation

  Within the framework of TV stream structuring, the objective of topic segmentation is to split relevant shows (broad- 
cast news and reports on current affairs in this work) into segments that deal with a single topic. Topic segmentation 
algorithms can be based on the criterion of lexical cohesion. In this case, the segmentation relies on the analysis of 
the distribution of words within the text, a topic change being detected when the vocabulary changes significantly 
(Utiyama and Isahara, 2001; Hearst, 1997).



  In this section, the lexical cohesion criterion and the way it is computed in the context of topic segmentation is first 
described before the presentation of the topic segmentation method of Utiyama and Isahara (2001) which serves as a 
baseline in this work.

2.1. Lexical cohesion

  The notion of lexical cohesion refers to lexical relations that exist within a text to provide a certain unity. Lexical 
cohesion is created by repetitions of the same words, co-references, and the use of sets of semantically related words 
(Halliday and Hasan, 1976). As lexical cohesion is a guide to the organization of the flow of ideas in the text, this 
criterion is widely used in many fields of the natural language processing domain. In discourse analysis, Xingwei (in 
press) studies the relationship between cohesion and coherence in texts while Klebanov et al. (2008) makes a stylistic 
analysis of political speeches. Others studies use lexical cohesion for word sense disambiguation (Manabu and Takeo,
1994), and automatic summarization (Barzilay and Elhadad, 1997; Boguraev and Neff, 2000). Error identification and 
correction applications can also rely on lexical cohesion to detect errors by identifying tokens that are semantically 
unrelated to their context. These methods can be applied on regular text, as in Hirst and Budanitsky (2005), or on 
automatic transcripts (Inkpen and Desilets, 2005).
  In Utiyama and Isahara (2001), the lexical cohesion computation of a segment Si  derives from the ability of a 
language model Δi  whose parameters are estimated from words in Si  to predict the words in the segment. In this 
framework, two important steps are needed: the estimation of the language model Δi  and the computation of the 
generalized probability of words in Si , reflecting the ability of the language model Δi  to predict the words of Si .
Language model. The language model Δi  estimated on Si is a unigram1 language model (Utiyama and Isahara,
2001) which specifies a distribution over all words in the text (or transcript) to be segmented. The calculation of the
language model of a segment Si is formalized, for a Laplace smoothing, as



Δi  =


 
Pi(u) =


Ci(u) + 1
zi



,	∀u ∈ 
VK



,	(1)


with VK the vocabulary of the text, containing K distinct words, and Ci (u) the count of word u in Si corresponding to 
the number of occurrences of u in Si . The probability distribution is smoothed by incrementing the count of each word 
by 1. The normalization term zi ensures that Δi  is a probability mass function and, in the particular case of Eq. (1), 
zi = K + ni with ni the number of word occurrences in Si .
Generalized probability. The language model Δi  is used to compute the generalized probability of the words in Si
as a measure of lexical cohesion according to
ni
ln P [Si; Δi] =       ln P [wi ; Δi],	(2)
j=1

where wi  denotes the jth word in Si . Intuitively the probability favors lexically consistent segments since its value is 
greater when words appear several times within the segment and decreases if many words are different.

2.2. Topic segmentation

  The topic segmentation method used, introduced by Utiyama and Isahara (2001) for textual documents, was chosen 
in this context of transcript-based TV program segmentation for two main reasons. It is currently one of the best 
performing methods that makes no assumption on a particular domain (no discourse markers, no topic models, etc.). 
Moreover, contrary to many methods based on local measures of the lexical cohesion, with the exception of works 
like (Malioutov and Barzilay, 2006) that shares the same philosophy, the global criterion used in Utiyama and Isahara 
(2001) makes it possible to account for the high variability in segment lengths.


 1  Here, the language model serves to represent the vocabulary repetition in our data. Thus, if the use of bigram (or higher order) language model 
is more suitable for a language modeling purpose, in this work a unigram language model allows us to represent the repetitions of simple terms in 
the segments.



  The method consists in searching the segmentation that produces the most consistent segments from a lexical point 
of view, while respecting a prior distribution of segment lengths. Cast in a probabilistic framework, the principle is to
find the most probable segmentation of a sequence of l basic units (words or sentences) W = W l among all possible
segmentations,
Sˆ = argmaxP [W |S]  P [S]. 	(3)
1

Assuming that P [Sm] = n−m, with n the number of words in the text and m the number of segments, and that segments 
are independent, the probability of a text W for a segmentation S = Sm is given by




Sˆ 	argmax
m
1



m

i=1



(ln(P [W bi |Si]) − α 
ln(n)),	(4)


where P [W bi |Si] denotes the generalized probability of the sequence of basic units corresponding to Si as given by
Eq. (2). The parameter α allows for different trade-offs between lexical cohesion and segment lengths.
  In this paper, the basic units used are utterances as given by the partitioning step of the ASR system, thus limiting 
possible topic boundaries to utterance boundaries, and the text W is only composed of lemmatized2 nouns, adjectives 
and non modal verbs.


3. Robust topic segmentation for spoken multimedia contents

  The measure for lexical cohesion based on language model, as defined in Section 2.1, relies only on word repetitions. 
However, this can turn out to be insufficient in the case of automatically transcribed video material. Indeed, two 
occurrences of a word can be erroneously recognized as two different words and therefore not considered for lexical 
cohesion. Moreover, due to potentially small segment lengths and to the use of synonyms, the number of repetitions 
can be very low.
  To adjust the computation of lexical cohesion to spoken documents, we propose several extensions. In the two first 
ones, additional information is incorporated to the lexical cohesion measure while, in the second one, language model 
interpolation techniques are used so as to provide better language model estimates.


3.1. Integration of additional sources of information

  Different kinds of additional information, such as prosody or confidence measures, can be used to improve the 
generalized probability measure of lexical cohesion. In this work, confidence measures and semantic relations are 
employed. On the one hand, confidence measures, associated with each word by the ASR system, correspond to 
(an estimation of) the probability that a word has been correctly transcribed. They are considered so as to reduce 
the contribution of non properly transcribed words in the lexical cohesion calculation. On the other hand, semantic 
relations, which represent the semantic proximity between words, are used to consider the fact that two different words 
can be semantically related and seen as a repetition. Such information can be accounted for either during the language 
model estimation step or during the calculation of the generalized probability.


3.1.1. Using confidence measures
  Confidence measures can be accounted for at the language model level by replacing the count Ci (u) by the sum of 
the confidences over all occurrences of u, i.e.,

C 	i  δ1


i(u) =



j =u


c(wj )


,	(5)




 2  A lemma is an arbitrary canonical form grouping all inflections of a word in a grammatical category, e.g., the infinitive form for verbs and the 
masculine singular form for adjectives.



where c(wi ) ∈ [0, 1] corresponds to the confidence measure for the jth word in Si and δ1 is a parameter used to reduce 
the weight of words whose confidence measure is low. Indeed, the higher δ1 , the lower the impact of words with a low
confidence measure.
  Confidence measures can also be accounted for during the generalized probability computation. In this case, the 
log-likelihood of the occurrence of a word in a segment is multiplied by the confidence measure of the word occurrence,




ln P [Si; Δi] =


ni
c(wi )δ2  ln P [wi ; 
Δi],	(6)


    j	j 
j=1

with δ2 equivalent to δ1 . Eq. (6) allows to reduce the contribution to the lexical cohesion of a word whose confidence 
measure is low. In this case, the language model Δi  can be either estimated from the counts Ci (u), thus limiting the 
use of confidence measures to the probability computation, or from the modified counts Ci(u). Note that in Eq. (6), 
ln P[Si ; Δi ] is not strictly speaking a log probability. However, if δ2 = 1, ln P[Si ; Δi ] can be seen as the joint probability 
for the word wi  to be correctly transcribed and to be represented by the language model Δi .


3.1.2. Using semantic relations
  Many topic segmentation techniques, based on the lexical cohesion criterion, use semantic relations as additional 
information to take into account the semantic links that exist between words. Indeed, the primary goal of semantic 
relations is obviously to ensure that two semantically related words, e.g., “car” and “drive”, contribute to the lexical 
cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic 
relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora 
(Ferret, 2006; Jobbins and Evett, 1998). For example, Ferret (2006) uses a network of lexical co-occurrences built from 
a large corpus to improve a topic segmenter based on lexical reiteration. The algorithm in Jobbins and Evett (1998) 
compares adjacent windows of sentences and determines their lexical similarity, based on repetitions of words and 
collocations, to detect topic boundaries.
  Two main types of semantic relations can be automatically extracted from corpora, namely syntagmatic and 
paradigmatic relations (Manning and Schütze, 1999; Claveau and Sébillot, 2004; Grefenstette, 1994).
  Syntagmatic relations correspond to relations of contiguity that words maintain within a given syntactic context 
(sentence, chunk, fixed length window, etc.), two words being related if they often appear together. A popular criterion 
to measure the degree of syntagmatic proximity between two words u and v is the mutual information (MI) which 
compares the probability of observing the two words u and v together with the probability of observing these two 
words separately (Grefenstette, 1994). Several variants of the mutual information criterion have been proposed. For 
example, in Daille (1996), the mutual information is cubed (thus called MI3 ) to avoid emphasizing rare associations. 
In Church and Hanks (1990), order between words is taken into account.
  Paradigmatic relations link two words having an important common component from a meaning point of view. 
These relations, corresponding to synonyms, hyperonyms, antonyms, etc., are typically calculated by means of context 
vectors for each word, grouping together words that appear in the same contexts. The context vector of a word u 
describes the distribution of words in its vicinity and contains, for each word v, its frequency of occurrence in the 
neighborhood of u, possibly normalized by its average frequency in the neighborhood of any word. The semantic 
proximity between two terms can be defined thanks to the Jaccard index as in Grefenstette (1992) or as the angular 
distance between their respective context vectors.
  In our work, as in Morris and Hirst (1991), Ferret (2006), and Jobbins and Evett (1998), these relations are employed 
to overcome the limited number of vocabulary repetitions. But semantic relations are also expected to limit the impact of 
recognition errors. Indeed, contrary to correctly transcribed words, misrecognized words are unlikely to be semantically 
linked to other words in a topic segment (Inkpen and Desilets, 2005). As a consequence, the contribution of non properly 
transcribed words will be less important than the one of correct words when semantic relations are used.
  As for confidence measures, accounting for semantic relations can be achieved by modifying the counts in the 
language model estimation step. Counts, which normally reflect how many times a word appears in a segment, are 
extended so as to emphasize the probability of a word based on its number of occurrences as well as on the occurrences 
of related words. More formally, the counts Ci in Eq. (2) are amended according to






Ci (u) = Ci(u) +


ni

j=1,wi  =/ u



r(wi , u),	(7)




where r(wi , u) ∈ [0, 1] denotes the semantic proximity between wi  and u, close to 1 for highly related words and null
j	j
for non related words. More details about the computation of r are given in Section 4.2.
  Unlike confidence measures, semantic relations cannot be accounted for during generalized probability computation. 
Indeed, in this case, multiplying the probability for a word to appear in a segment Si by the sum of the relations the 
word maintains with other words in Si does not make sense.
  To conclude this section, it is important to note that, in general, semantic relations are obtained from domain specific 
corpora. They can also be learned on a general purpose corpus but will in any case fail to address all domains. One 
strong point of our technique is that when semantic relations are not adequate for a particular document—e.g., when 
segmenting a document from a domain not in the semantic relations training data—Ci (u) will remain unchanged with
respect to Ci (u) since r(u, v) is null between any two words of the document. Put differently, out of domain relations
will have no impact on topic segmentation, contrarily to latent semantic approaches (Deerwester et al., 1990; Landauer
et al., 1998).



3.2. Combining global and local measurements of lexical cohesion

  In TV programs, and especially in news, topic segments can be very short. An easy, and unfortunately accurate, 
criticism of Eq. (1) is that for small segments, the language model Δi  is poorly estimated. Therefore, in order to deal 
with short segments, the use of a more sophisticated estimation method is needed. To skirt this problem, we propose 
to interpolate the language model at the segment level with a more robust language model estimated on the entire 
transcript. This allows to take into account the whole text to be segmented in order to have a better language model 
estimation for short segments. Two interpolation strategies are studied—interpolation of the probabilities (Jelinek and 
Mercer, 1981) and interpolation of the counts (Bacchiani and Roark, 2003).



3.2.1. Linear interpolation of probabilities
The first interpolation technique is a basic probability interpolation. In this case, the lexical cohesion of a segment
Si , given Si and the text T, is measured according to




ln P [Si; Si, T ]   =


ni
ln(λP [wi ; Δi] + (1 − 
λ)P [wi ; Δt ])


j=1

ni
=



⎛
ln ⎜λ


j


Ci(wi ) + ξ


j



+ (1 − λ)



⎞
Ct (wi )  ⎟
⎟





(8)


j=1


⎝
u∈VT


Ci(u) + ξ



u∈VT


Ct (u) ⎠





where Δi  is the language model estimated on Si and Δt  the one estimated on T. Ct (u) is the count of word u in T and
Ci (u) the count of that word in Si . ξ is a count smoothing bias that corresponds to the Laplace smoothing when ξ = 1.



3.2.2. Count interpolation
  Rather than interpolating probabilities, language model interpolation can be based on the interpolation of counts. 
In this case, the lexical cohesion of a segment Si is defined as






ln P [Si; Si, T ]   =


ni

j=1



ln P [wi ; Δit ]


⎛	⎞
ni 	λ(Cs (wi ) + ξ) + (1 − λ)Ct (wi )



(9)


=	ln ⎜	j	j  ⎟


j=1


⎝
u∈VT


λ(Ci(u) + ξ) + (1 − λ)Ct (u) 
⎠




where Δit  is the interpolated language model of the segment Si and the text T. As for linear interpolation, frequent 
words in T will get a high probability regardless of their frequency in Si while non frequent ones will always get a low 
probability—depending on λ. However, because of the renormalization by the sum of all the counts, this fact might 
be less detrimental than for probability interpolation and the behavior of this interpolation technique more likely to be 
consistent with what is expected.


4. Experimental setup

  Experiments were carried out with our speech recognition system on a comprehensive corpus. Before presenting 
the corpus, a brief description of the ASR system is provided and the selection of semantic relations integrated in our 
system is discussed.


4.1. ASR system and confidence measures

  All TV programs were transcribed using our Irene ASR system, originally developed for broadcast news tran- 
scription. Irene implements a multiple pass strategy, progressively narrowing the set of candidate transcripts—the 
search space—in order to use more complex models. In the final steps, a 4-gram language model over a vocabulary of
65,000 words is used with context-dependent phone models to generate a list of 1000 sentence transcription hypotheses. 
Morphosyntactic tagging, using a tagger specifically designed for ASR transcripts, is used in a post-processing stage 
to generate a final transcription by consensus from a confusion network, combining the acoustic, language model and 
morphosyntactic scores (Huet et al., 2010). Confusion network posterior probabilities are used directly as confidence 
measures.
  Acoustic models were trained on about 250 hours of broadcast news material from the French ESTER 2 data 
(Galliano et al., 2009). The language model probabilities were estimated on 500 million words from French newspapers 
and interpolated with language model probabilities estimated over 2 million words corresponding to the reference 
transcription of radio broadcast news shows. The system exhibits a word error rate (WER) of 16% on the non accented 
news programs of the ESTER 2 evaluation campaign. As far as TV contents are concerned, we estimated word error 
rates ranging from 15% on news programs to more than 70% on talk shows or movies though this was not explicitely 
measured.


4.2. Semantic relations

  Semantic relations were automatically extracted from text corpora and relevant relations were selected so as to avoid 
abusive enforcement of the lexical cohesion.
In this work, the MI3  criterion for syntagmatic relations and the cosine distance between normalized context
vectors for paradigmatic ones were used (cf. Section 3.1.2 for more details on semantic relations automatic extraction 
methods). Relations were extracted from a corpus of articles from French newspapers (about 800 million words) and 
from reference transcripts of radio broadcast news shows (about 2 million words). All the data were lemmatized, 
keeping only nouns, adjectives, and non modal verbs. Semantic proximity scores given either by the MI3 criterion or 
by the angular distance were normalized in [0, 1]. Table 1 shows, for the word “cigarette”, the five related words with 
the highest semantic proximity score, for syntagmatic and for paradigmatic relations.



Table 1
Words with the highest association scores, in decreasing order, for the word “cigarette”, as extracted automatically.

Syntagmatic	Paradigmatic

fumer (to smoke)	cigare (cigar) 
paquet (pack)	gitanea 
allumer (to light)	gauloisea 
contrebande (smuggling)	clope (ciggy)
fabricant (producer)	tabac (tobacco)

a  Brand name.

Table 2
Comparison of the news and reports corpora.

Average � repetitions	Average confidence measure	Average � words per segment	Average � words per file	� topic boundaries

New
s	1.82
0
.
6
2
1
0
6
.
4
2
2
4
3
1
2
0
2
Repo
rts	2.01
0
.
5
7
4
2
4
.
1
2
4
9
5
8
6


  To prevent the creation of too many links between words, a selection step is implemented to choose relevant 
syntagmatic and paradigmatic relations among all the existing ones so as to introduce only the best ones in the 
segmentation algorithm. Two selection strategies were evaluated, either retaining globally the N relations with the 
highest scores (global selection strategy) or retaining for each word the M best relations (word level selection strategy). 
Moreover, common usage words such as “go” or “year” were found to be related to many words, thus jeopardizing 
topic segmentation by abusively enforcing lexical cohesion. Semantic relations for words whose number of related 
words exceeds a given threshold are therefore discarded, the threshold being equal to the average number of relations
associated with a word multiplied by a parameter γ ∈ [0, 1].

4.3. Corpus

  Results are reported on two distinct corpora. The first one, a news corpus, is made up of 57 news programs (≈1/2 
hour each) broadcasted in February and March 2007 on the French television channel France 2, and the second one is a
reports corpus composed of 16 reports on current affairs “Sept à Huit” (≈1 h each) transmitted on the French channel
TF1 between September 2008 and February 2009. In the reports corpus, reports are longer (around 10–15 min) and
possibly on non news topics, while the news corpus follows the classical scheme of rather short reports (usually
2–3 min). Having two distinct corpora makes it possible to study the behavior of topic segmentation on data sets with 
different characteristics. Indeed, in addition to different durations, the average number of topics and the average number 
of segments per show vary greatly between news and reports. Moreover, the number of repetitions is less important in 
news programs than in reports ones, as reported in Table 2, while the transcription error rate is higher on the latter due 
to a larger amount of non professional speakers.
  In each show, headlines and closing remarks were removed, these two particular parts disturbing the segmentation 
algorithm and being easily detectable from audiovisual clues. A reference segmentation was established by considering 
a topic change associated with each report, the start and end boundaries being respectively placed at the beginning of 
the report’s introduction and at the end of the report’s closing remarks. Note that in the news corpus, considering a 
topic change between each report is a choice that can be questioned as, in most cases, the first reports all refer to the 
main news of the day and are therefore dealing with the same broad topic.

5. Results

  The goal of the article is to study to which extent confidence measures, semantic relations and interpolation tech- 
niques can help to make the lexical cohesion criterion more robust for professional video processing. We therefore 
study the influence of each technique and each parameter on the entire data set, reporting results for optimal parameter 
values. Even though the resulting figures are optimistic and do not reflect real-life behavior, their comparison with a 
baseline system clearly demonstrates the impact of each technique on topic segmentation.



Table 3
Integration of confidence measures – news corpus – best F1-measure.

δ1
δ
2


0
0
.
5
1
1
.
5
2
2
.
5
3
3
.
5
4
0	59.7	 60.4 	60.7 	61.0 	61.1 	60.9 	60.9 	59.7 	59.1
0.5
6
0
.
4
6
0
.
7
6
1
.
5
6
1
.
5
6
0
.
9
6
0
.
9
6
0
.
8
5
9
.
5
5
8
.
8
1
6
1
.
1
6
1
.
5
6
1
.
7
6
1
.
1
6
0
.
9
6
0
.
9
6
0
.
6
5
8
.
9
5
8
.
8
1.5
6
1
.
2
6
1
.
6
6
1
.
3
6
0
.
9
6
0
.
5
6
0
.
1
6
0
.
2
5
8
.
9
5
8
.
8
2
6
1
.
3
6
1
.
4
6
1
.
4
6
0
.
8
6
0
.
2
5
9
.
9
5
9
.
9
5
8
.
9
5
8
.
7
2.5
6
1
.
2
6
1
.
5
6
1
.
1
6
0
.
5
6
0
.
1
5
9
.
6
5
9
.
6
5
8
.
6
5
8
.
1
3
6
1
.
6
6
1
.
5
6
0
.
8
6
0
.
4
6
0
.
0
5
9
.
5
5
9
.
4
5
8
.
1
5
7
.
9
3.5
6
0
.
6
6
0
.
5
5
9
.
3
5
8
.
8
5
8
.
8
5
8
.
3
5
8
.
1
5
7
.
8
5
7
.
8
4
6
0
.
9
6
0
.
6
5
9
.
3
5
9
.
0
5
8
.
9
5
8
.
1
5
7
.
8
5
7
.
8
5
7
.
7


  Recall and precision on topic boundaries are considered for evaluation purposes after alignment between reference 
and hypothesized boundaries, with a tolerance on boundary locations of respectively 10 and 30 s for news and reports, 
while different trade-offs between precision and recall are obtained by varying α in Eq. (4). To compare the different 
parameters (δ, γ , etc.), the tables in this section contain the best F1-measure obtained for an optimal value of α, i.e., 
the one leading to the segmentation with average segment length closest to that of the reference segmentation.


5.1. Confidence measures

  Results for the integration of confidence measures (CMs) are presented in Tables 3 and 4 for the news and reports 
corpus respectively. In these tables, the grey line stands for the values obtained when CMs are introduced during the 
language model estimation alone, with δ1  varying from 0 to 4, while the grey column represents their integration 
during the generalized probability computation alone. The dark grey cell therefore corresponds to the baseline method 
(δ1 = δ2 = 0) and bold values refer to the highest scores. Recall and precision curves are reported in Fig. 1.
  We can observe that confidence measures integration leads to an improvement in the topic segmentation quality 
that is statistically significant (t-test). Indeed, the best F1-measure is improved by 2 points for the news corpus and by
5 points for the reports corpus.
  Moreover, results show that the behavior of the integration of confidence measures is different for the two corpora 
depending on whether CMs are integrated during generalized probability or language model computation. For the news 
corpus, results are better when confidence measures are integrated during the generalized probability computation 
alone rather than during the language model computation alone; while for the reports corpus, experiments lead to the 
opposite conclusion. This phenomenon is also observed on the recall/precision curves, where the integration during 
the generalized probability calculation gives better results for the news corpus, while integration during the language 
model computation is more efficient for reports.


Table 4
Integration of confidence measures – reports corpus – best F1-measure.

δ1
δ
2


0
0
.
5
1
1
.
5
2
2
.
5
3
3
.
5
4
0	54.9	 55.0 	56.8 	54.6 	54.7 	55.6 	58.3 	59.9 	59.1
0.5
5
3
.
4
5
6
.
1
5
5
.
3
5
6
.
1
5
4
.
7
5
4
.
7
5
5
.
0
5
7
.
9
5
8
.
2
1
5
5
.
3
5
6
.
5
5
5
.
4
5
5
.
6
5
4
.
7
5
5
.
0
5
5
.
0
5
9
.
1
5
8
.
2
1.5
5
4
.
4
5
5
.
6
5
5
.
6
5
6
.
3
5
4
.
7
5
5
.
0
5
5
.
0
5
6
.
9
5
8
.
6
2
5
4
.
9
5
6
.
0
5
6
.
3
5
5
.
0
5
5
.
3
5
5
.
3
5
7
.
0
5
6
.
1
5
6
.
1
2.5
5
5
.
1
5
5
.
7
5
6
.
3
5
5
.
0
5
5
.
3
5
7
.
0
5
7
.
0
5
6
.
1
5
6
.
5
3
5
5
.
3
5
6
.
8
5
6
.
3
5
5
.
0
5
6
.
7
5
7
.
0
5
8
.
6
5
7
.
7
5
8
.
1
3.5
5
5
.
3
5
6
.
8
5
6
.
3
5
6
.
3
5
8
.
3
5
8
.
6
5
8
.
6
5
7
.
7
5
7
.
7
4
5
4
.
5
5
6
.
8
5
6
.
3
5
9
.
9
5
9
.
5
5
8
.
6
5
7
.
7
5
7
.
7
5
7
.
1





a 80



Baseline    	


b  60



Baseline


75	0-3    	
2-0
70 	1-1

65

60

55

50

45

40
45 	50 	55 	60 	65 	70
Recall
news corpus


55	0-3    	
3-0
50 	3-3

45

40

35

30

25
50 	55 	60 	65 	70 	75 	80 	85 	90
Recall
reports corpus



Fig. 1. Integration of confidence measures – recall/precision curves (figures in the legend correspond to resp. δ1  and δ2 ).


  Another difference between the two corpora is the optimal values of δ1  and δ2 . For the news corpus, the highest 
F1-measure is obtained when δ1  and δ2  are quite small (both equal to 1) while for the reports corpus the optimal 
values for δ1 and δ2 are greater. This difference can be explained by the fact that for high values of δ, c(wi )δ becomes
negligible except for words whose confidence measure is very close to 1. As the proportion of words with a CM less 
than 0.9 is more important in the reports data, the impact of the confidence measures is more perceptible on this data 
set and higher values of δ lead to a greater improvement.
  Similarly, the impact of the transcription quality on the efficiency of the integration of confidence measures is also 
highlighted by the difference between the improvements observed for the two corpora. Fig. 1 shows that confidence 
measures have more impact on the reports corpus than on the news corpus. This difference shows that confidence 
measures are of utmost importance when transcription quality decreases.
  From a more qualitative point of view, we also observed that accounting for confidence measures not only increases 
the number of correct boundaries detected but also improves boundary locations. Indeed, boundary locations are more 
precise when using confidence measures, even if this fact does not show in the recall/precision curves because of the 
tolerated gap on the boundary positions. We also observed that the quality of confidence measures have an impact 
on boundary precision. Indeed, using confidence measures, improved thanks to high-level linguistic features with 
a classifier (Fayolle et al., 2010), resulted in more accurate boundary locations. To conclude, experiments carried 
out on the two corpora confirm the hypothesis that the integration of confidence measures in the lexical cohesion 
computation allows the criterion to be less sensitive to transcription errors. Indeed, amending the calculation method of 
lexical cohesion to include CMs provides a significant improvement in the quality of topic segmentation. Confidence 
measures seem also to lead to an increase in the number of correct boundaries detected but also to the displacement of 
previously recognized borders, moving them closer to reference boundaries.


5.2. Semantic relations

  Various tests, on the choice of the type and number of semantic relations introduced and on the selection strategy 
(global or word level) or the filtering technique have produced a large number of results. The most convincing ones 
are discussed here.
  Tables 5 and 6 summarize the results on the news and reports corpus respectively, with values for syntagmatic 
relations in the leftmost part of the tables and results for paradigmatic ones in the rightmost part. For each kind of 
semantic relations, the two selection strategies are evaluated for different numbers of semantic relations, 2–10 per word 
for the word-level strategy and 5000–90,000 for the global strategy. Finally, results for the filtering technique, used 
to discard semantic relations for words whose number of related words exceeds a given threshold, are presented for
values of parameter γ between 0.2 and 1. In Fig. 2, recall/precision curves for the integration of paradigmatic relations
are presented.
  For the news corpus, semantic relations help the lexical cohesion criterion to be more robust to the limited number of 
vocabulary repetitions due to short segments and/or the use of synonyms or related words. Indeed, the best F1-measure 
is improved in a statistically significant way (t-test) by 1.4 when syntagmatic relations are introduced and by 1.2 for



Table 5
Integration of semantic relations – news corpus – best F1-measures.

γ	Syntagmatic	Paradigmatic


Wor
d 
level



Glob
al



Wor
d 
level



Glob
al


2
3
1
0

5k
20
k
9
0
k

2
3
1
0

5k
20
k
9
0
k
0.2
59.7
59.7
5
9.
7

59.7
59.
7
5
9.
7

59.7
59.7
5
9.
7

59.7
59.
7
5
9.
7
0.4
59.7
59.7
6
0.
8

59.7
59.
8
5
9.
7

59.7
59.7
5
9.
7

59.7
59.
7
6
0.
0
0.5
59.7
59.7
6
0.
1

59.7
60.
3
6
0.
4

59.7
59.7
6
0.
0

59.7
59.
7
6
0.
3
0.6
59.7
59.6
6
0.
5

59.7
60.
0
6
0.
1

59.7
59.7
6
0.
2

59.7
60.
1
6
0.
1
0.8
60.0
59.8
6
0.
9

60.5
60.
6
6
0.
6

59.8
60.3
6
0.
0

59.9
60.
6
6
0.
3
1
58.9
61.1
5
9.
8

60.5
60.
8
6
1.
0

60.9
60.4
6
0.
2

60.0
60.
3
6
0.
3


Table 6
Integration of semantic relations – reports corpus – best F1-measures.

γ	Syntagmatic	Paradigmatic


Wor
d 
level



Glob
al



Wor
d 
level



Glob
al


2
3
1
0

5k
20
k
9
0
k

2
3
1
0

5k
20
k
9
0
k
0.2
54.9
54.9
5
4.
9

54.9
54.
9
5
4.
9

54.9
54.9
5
4.
9

54.9
54.
9
5
4.
9
0.4
54.9
54.9
5
5.
7

54.9
55.
5
5
5.
5

54.9
54.9
5
5.
5

54.9
54.
9
5
4.
9
0.5
54.9
54.9
5
4.
0

54.9
55.
5
5
5.
5

54.9
54.9
5
5.
1

54.9
54.
9
5
5.
1
0.6
54.9
54.9
5
3.
3

54.9
54.
4
5
4.
4

54.9
54.9
5
4.
0

54.9
54.
9
5
4.
4
0.8
56.5
55.7
5
3.
4

55.5
53.
1
5
3.
1

55.8
53.2
5
3.
6

54.9
55.
8
5
4.
8
1
54.7
54.6
5
1.
6

55.1
54.
0
5
4.
0

55.2
53.0
5
3.
0

54.9
52.
3
5
4.
0


paradigmatic relations (Table 5). Moreover, even if the best values of the F1-measure are reached for syntagmatic 
relations, the global improvement is higher with paradigmatic ones. For the reports corpus, the improvement is much 
smaller than for the news one. Indeed, Fig. 2 shows that results for the integration of paradigmatic relations—which are 
comparable to those for syntagmatic relations, cf. Table 6—are almost equivalent to the baseline. Indeed, a significance 
test shows that the improvement observed when using semantic relations is not significant. The difference in results 
for both corpora can be explained by the fact that in the reports corpus the number of reiterations and the segments 
lengths are more important than in the news corpus. Thus, the use of semantic relations is not as effective for this 
corpus. Moreover, in this work, semantic relations were extracted from a corpus composed of news articles and are 
therefore less suited for the reports corpus. However, it is interesting to note that using non appropriate relations does 
not hurt performance.




a  80
75

70

65

60

55

50

45

40



Baseline
Word level - 2    	 Global 
- 20k


b   60
55

50

45

40

35

30

25



Baseline
Wo
rd 
le
v
el 
- 
2    
	 Global - 90k


45 	50 	55 	60 	65 	70
Recall
news corpus


50 	55 	60 	65 	70 	75 	80 	85 	90
Recall
reports corpus



Fig. 2. Integration of paradigmatic relations – recall/precision curves.



Tabl
e 7
Inter
polat
ion – 
best 
F1-
meas
ures.

λ
Cou
nt


Line
ar


news
r
e
p
o
r
t
s

news
r
e
p
o
r
t
s
Base
line
59.7
5
4
.
9

59.7
5
4
.
9
0
3.8
1
4
.
8

12.2
1
0
.
4
0.1
63.7
5
4
.
0

60.8
4
7
.
8
0.2
64.6
5
3
.
4

61.0
4
7
.
3
0.3
64.2
5
2
.
8

61.6
5
0
.
0
0.4
64.2
5
5
.
6

61.7
5
0
.
3
0.5
64.1
5
4
.
3

62.0
5
1
.
1
0.6
64.1
5
2
.
4

61.9
5
1
.
7
0.7
63.7
5
3
.
1

61.9
5
0
.
9
0.8
62.7
5
3
.
3

61.8
5
2
.
7
0.9
61.6
5
4
.
9

60.6
5
3
.
5
1
59.9
5
4
.
9

59.7
5
4
.
9


  Table 6 shows that the integration of semantic relations damages the segmentation quality when a too large number 
of relations is introduced—i.e., when γ is high—degradation which is proportional to the number of relations included. 
This behavior can also be observed for the news corpus but for values of γ greater than 1. Therefore, the filtering 
technique is crucial to avoid considering relations that seem less adapted in the context of topic segmentation because 
they introduce some noise (i.e., they connect words and segments that should not be). The stronger effect of the filtering 
technique on the reports corpus can be explained by the fact that many relations are out of domain for this corpus. 
Therefore, more general relations—associated with words such as “year” and “go”—that introduce noise appear in a 
higher proportion and have more impact on reports than on news.
  Finally, concerning the technique used for the semantic relations selection, no difference was found between the 
global selection strategy and the word level selection strategy for the news corpus. In contrast, for the reports corpus, 
the global selection strategy is better, both for paradigmatic and syntagmatic relations. We believe that the word level 
strategy selects relations which are the most characteristic of the corpus they are learned from, while the global strategy 
is more likely to pick more general relations. Thus, as relations are extracted from a corpus composed of news articles, 
the ones selected by the word level strategy are more suitable for the news corpus but less adapted for the reports one.
  To conclude, the introduction of semantic relations can improve topic segmentation when the number of vocabulary 
repetitions is low. However, it is essential to limit the number of relations considered.


5.3. Interpolation

  Finally, results for language model interpolation are given in Table 7. Values for the count interpolation strategy are 
presented in the leftmost part of the table while results for the linear interpolation of probabilities are located in the 
rightmost one.
  For the two corpora, we can observe that improvements are higher with the interpolation of the counts than with the 
linear interpolation of probabilities. Indeed, the best value for the F1-measure is significantly increased by 4.9 against
2.3 for the news corpus (t-test) and by 0.7 against 0 for the reports corpus. This behavior is explained by the fact that, as 
mentioned in Section 3.2, for the linear interpolation of probabilities, frequent words in the text get a high probability 
regardless of their frequencies in the segment, while non frequent ones always get a low probability. However, because 
of the renormalization in Eq. (9), this fact is less detrimental for count interpolation than for probability interpolation.
  Table 7, as well as the recall/precision curves in Fig. 3, shows that the interpolation techniques are much less effective 
for the reports corpus than for the news corpus. This observation can be easily explained by the fact that segments in 
reports are four times longer than the ones in the news. Thus, the use of interpolation has no effect for the calculation 
of lexical cohesion in the reports segments since the initial language models were already well estimated. This can 
also be seen from the optimal value of parameter λ, representing the weight of the language model of the segment in 
the interpolation, that is greater for reports than for news.





a 80
75
70
65
60
55
50
45
40


 	
Baseline
Count (0.2) Linear 
(0.5)


b 60

55

50

45

40

35

30

25



Baseline
C
ou
nt 
(0
.5
)    
	 Linear (0.9)


45	50	55	60	65	70	75
Recall
news corpus


50	55	60	65	70	75	80	85	90
Recall
reports corpus





5.4. Combination



Fig. 3. Interpolation – 
recall/precision curves.



  Confidence measures, semantic relations and language model interpolation can be used in conjunction. In this part, 
the joint integration of these three elements is evaluated to estimate to what extent their advantages can be combined.
  Confidence + Interpolation. When confidence measures and interpolation techniques are used together, the improve- 
ments observed for both are combined for the news corpus. In this case, the CMs introduced are the ones that exhibit 
the most important amelioration (with δ1  = δ2  = 1) and the interpolation technique is the interpolation of counts with
λ equals to 0.2. Fig. 4 shows that the combination of the two cues leads to a greater improvement, that is statistically
significant (t-test), in the topic segmentation quality than the sole interpolation, especially for high recall values. Thus, 
we can conclude that the problems inherent in TV programs, i.e., transcription errors, potentially short segments and 
limited number of reiterations, are partially handled for the news corpus. For reports, unsurprisingly, comparable 
results are obtained with or without interpolation when confidence measures are used (cf. Fig. 4), interpolation alone 
yielding no improvement on this corpus. However it is important to note that the combination does not damage topic 
segmentation compared to the sole use of CMs.
  Confidence + Semantics. Confidence measures can also be combined with semantic relations, whether syntagmatic 
or paradigmatic. The relations introduced are paradigmatic ones, selected thanks to the word level selection technique (2 
relations per word). For the corpus composed of broadcast news, topic segmentation is slightly, although significantly, 
improved compared to the use of confidence measures or semantic relations alone. For reports, the combination does not 
lead to better topic segmentation, compared with the use of confidence measures alone (cf. Table 8). This observation is 
not surprising as semantic relations do not give real improvements for this corpus as explained in Section 5.2. However, 
as before, the use of two different cues does not damage the topic segmentation quality.
  Semantics + Interpolation (+ Confidence measures). Semantic relations can also be combined with interpolation 
techniques. This combination, possibly associated with the use of confidence measures, does not improve the topic



a 80
75
70
65
60
55
50
45
40



Baseline    	
 Confidence 
Interpolation Conf+Int


b 60
55

50

45

40

35

30

25



Baseline
 C
on
fi
de
nc
e    
	 Interpolation
Conf+Int


45	50	55	60	65	70	75
Recall
news corpus


50	55	60	65	70	75	80	85	90
Recall
reports corpus



Fig. 4. Integration of confidence measures and interpolation – recall/precision curves.


Best 
F1-
meas
ure 
valu
es 
for 
all 
possi
ble 
com
binat
ion.


n
e
w
s
repo
rts
Base
line
5
9
.
7
54
.9
Conf
iden
ce 
meas
ures 
(CM
s)
6
1
.
6
58
.3
Sem
antic
s
6
0
.
9
55
.7
Inter
polat
ion
6
4
.
6
55
.6
CMs 
+ 
inter
polat
ion
6
4
.
4
55
.4
CMs 
+ 
sema
ntics
6
2
.
0
58
.3
Sem
antic
s + 
inter
polat
ion
6
4
.
3
54
.3
Sem
antic
s + 
inter
polat
ion + 
CMs
6
4
.
2
57
.3

segmentation quality, either for the news or the reports corpus. As semantic relations and especially interpolation 
techniques both allow to overcome problems related to small segments, their combination is redundant.
  Finally, Table 8 summarizes all the results obtained with the exploitation of confidence measures, semantic relations 
and language model interpolation, used together or separately. This table gives an indication of the impact of each 
clue on the topic segmentation quality. However, the best F1-measure values presented in the table are extracted from 
the result for a particular value of the α parameter. Therefore, values in the table do not necessarily correspond to the 
overall improvement achieved through the use of one or more clues. For example, the F1-measure obtained for the 
reports corpus when confidence measures and interpolation technique are used is much lower than the one obtained
for the exploitation of CMs alone, while Fig. 4 shows that the results are equivalent. So, it is important to refer also 
to the recall/precision curves presented throughout this article to really understand to which extent the different clues 
can help the lexical cohesion criterion to be more robust to spoken documents specifics.

6. Conclusion and future work

  In this paper, we have improved and extended a probabilistic measure of lexical cohesion to include confidence 
measures and semantic relations, making use of language model interpolation. This results in a lexical cohesion measure 
more robust to TV program specifics while being generic enough to be effective on different kinds of programs. First, it 
has been shown that the use of semantic relations and interpolation techniques improve the topic segmentation quality 
of TV programs divided in short segments and in which lexical repetition rates are low. It was also pointed out that the 
integration of confidence measures has more impact when the transcription quality is low, as for the reports corpus. 
Finally, we demonstrated that these different elements can be used together to combine their advantages. Interestingly, 
it was also found that this combination never leads to a deterioration of topic segmentation.
  These results clearly lead to the integration of other features, such as prosodic ones, to detect lexical stresses or 
speaker changes for example. Moreover, as the integration of confidence measures has a positive impact, we think it 
would be interesting to investigate the application of the topic segmentation method on intermediary outputs of the 
ASR system (such as word graphs or confusion networks) rather than on the final transcripts. Finally, as mentioned 
in Section 4.3, the structure of news programs is clearly hierarchical as, in most cases, the first reports all refer to the 
main news of the day and are therefore dealing with the same broad topic. However, in this work, only linear topic 
segmentation has been considered. Therefore, a long view prospect of this paper is to develop a method that can handle 
a hierarchical topic segmentation of our data.

Acknowledgement

  This work was partially funded by OSEO, French state agency for innovation, in the framework of the Quaero 
project.

References

Allan, J., Carbonell, J., Doddington, G., Yamron, J.Y.Y., et al., 1998. Topic detection and tracking pilot study final report. In: Proceedings of the
DARPA Broadcast News Transcription and Understanding Workshop.



Amaral, R., Trancoso, I., 2003. Topic indexing of TV broadcast news programs. In: Proceedings of the 6th International Workshop on Computational
Processing of the Portuguese Language.
Bacchiani, M., Roark, B., 2003. Unsupervised language model adaptation. In: Proceedings of the 28th International Conference on Acoustics, 
Speech and Signal Processing.
Barzilay, R., Elhadad, M., 1997. Using lexical chains for text summarization. In: Proceedings of the Association for Computational Linguistics
Intelligent Scalable Text Summarization Workshop.
Beeferman, D., Berger, A., Lafferty, J., 1999. Statistical models for text segmentation. Machine Learning 34 (1-3), 177–210.
Boguraev, B.K., Neff, M.S., 2000. Lexical cohesion, discourse segmentation and document summarization. In: Proceedings of the 6th International
Conference on Content-Based Multimedia Information Access.
Christensen, H., Kolluru, B., Gotoh, Y., Renals, S., 2005. Maximum entropy segmentation of broadcast news. In: Proceedings of IEEE International
Conference on Acoustics, Speech and Signal Processing.
Church, K.W., Hanks, P., 1990. Word association norms, mutual information, and lexicography. Computational Linguistics 16 (1), 22–29.
Claveau, V., Sé billot, P., 2004. From effiency to portability: acquisition of semantic relations by semi-supervised machine learning. In: Proceedings 
of the 20th International Conference on Computational Linguistics.
Daille, B., 1996. Study and implementation of combined techniques for automatic extraction of terminology. In: The Balancing Act: Combining
Symbolic and Statistical Approaches to Language. MIT Press, pp. 49–66.
Deerwester, S., Dumais, S., Furnas, G., Landauer, T., Harshman, R., 1990. Indexing by latent semantic analysis. Journal of the American Society 
for Information Science 41 (6), 391–407.
Fayolle, J., Moreau, F., Raymond, C., Gravier, G., Gros, P., 2010. CR F-based combination of contextual features to improve a posteriori word-level 
confidence measures. In: Proceedings of the 11th International Conference on Speech Communication and Technologies.
Ferret, O., 2006. Approches endogène et exogène pour amé liorer la segmentation thématique de documents. Traitement Automatique des Langues
47 (2), 111–135.
Galliano, S., Gravier, G., Chaubard, L., 2009. The ESTER 2 evaluation campaign for the rich transcription of French radio broadcasts. In: Proceedings 
of the 10th Conference of the International Speech Communication Association.
Grefenstette, G., 1992. Sextant: exploring unexplored contexts for semantic extraction from syntactic analysis. In: Proceedings of the 30th Annual
Meeting of the Association for Computational Linguistics.
Grefenstette, G., 1994. Corpus-derived first, second and third-order word affinities. In: Proceedings of the Euralex. 
Halliday, M., Hasan, R., 1976. Cohesion in English. Longman.
Hearst, M.A., 1997. TextTiling: segmenting text into multi-paragraph subtopic passages. Computational Linguistics 23 (1), 33–64.
Hirst, G., Budanitsky, A., 2005. Correcting real-word spelling errors by restoring lexical cohesion. Natural Language Engineering 11, 87–111. 
Huet, S., Gravier, G., Sébillot, P., 2008. Un modèle multi-sources pour la segmentation en sujets de journaux radiophoniques. In: Proceedings of
the 15e Conférence sur le Traitement Automatique des Langues Naturelles.
Huet, S., Gravier, G., Sébillot, P., 2010. Morpho-syntactic post-processing of N-best lists for improved French automatic speech recognition.
Computer Speech and Language 24 (4), 663–684.
Inkpen, D., Desilets, A., 2005. Semantic similarity for detecting recognition errors in automatic speech transcripts. In: Proceedings of the Conference 
on Human Language Technology and Empirical Methods in Natural Language Processing.
Jelinek, F., Mercer, R.L., 1981. Interpolated estimation of Markov source parameters from sparse data. Pattern Recognition in Practice, 381–397. 
Jobbins, A.C., Evett, L.J., 1998. Text segmentation using reiteration and collocation. In: Proceedings of the 17th International Conference on
Computational Linguistics.
Klebanov, B.B., Diermeier, D., Beigman, E., 2008. Lexical cohesion analysis of political speech. Political Analysis 16 (4), 447–463. 
Landauer, T., Foltz, P., Laham, D., 1998. Introduction to latent semantic analysis. Discourse Processes 25, 259–284.
Malioutov, I., Barzilay, R., 2006. Minimum cut model for spoken lecture segmentation. In: Proceedings of the 21st International Conference on
Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics.
Manabu, O., Takeo, H., 1994. Word sense disambiguation and text segmentation based on lexical cohesion. In: Proceedings of the 15th International
Conference on Computational Linguistics.
Manning, C.D., Schütze, H., 1999. Foundations of Statistical Natural Language Processing. MIT Press.
Morris, J., Hirst, G., 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics 17 
(1), 21–48.
Mulbregt, P.V., Carp, I., Gillick, L., Lowe, S., Yamron, J., 1999. Segmentation of automatically transcribed broadcast news text. In: Proceedings of 
the DARPA Broadcast News Workshop.
Ostendorf, M., Favre, B., Grishman, R., Hakkani-Tür, D., Harper, M., Hillard, D., Hirschberg, J.B., Ji, H., Kahn, J.G., Liu, Y., Matusov, E., Ney, H., 
Shriberg, E., Wang, W., Wooters, C., 2008. Speech segmentation and spoken document processing. Signal Processing Magazine 25 (3), 59–69. 
Stolcke, A., Shriberg, E., Hakkani-Tür, D., Tür, G., Rivlin, Z., Sönmez, K., 1999. Combining words and speech prosody for automatic topic
segmentation. In: Proceedings of the DARPA Broadcast News Workshop.
Utiyama, M., Isahara, H., 2001. A statistical model for domain-independent text segmentation. In: Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics.
Wactlar, H.D., Kanade, T., Smith, M.A., Stevens, S.M., 1996. Intelligent access to digital video: Informedia project. Computer 29 (5), 46–52. 
Xingwei, M. The Please provide the year of publication and page range for the reference Xingwei (in press).relationship between cohesion and
coherence, Journal of Foreign Languages, 4.






