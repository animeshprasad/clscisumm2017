<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">An aim of this research is to grasp related words of unknown word.</S>
		<S sid ="2" ssid = "2">Currently, several lexical dictionaries have been developed for semantic retrieval such as WordNet and FrameNet.</S>
		<S sid ="3" ssid = "3">However, more new words are created in every day because of new trends, new paradigm, new technology, etc. And, it is impossible to contain all of these new words.</S>
		<S sid ="4" ssid = "4">The existing methods, which grasp the meaning of unknown word, have a limitation that is not exact.</S>
		<S sid ="5" ssid = "5">To solve this limitation, we have studied the way how to make relations between known words and unknown word.</S>
		<S sid ="6" ssid = "6">As a result, we found a noble method using co-occurrence, WordNet and Bayesian probability.</S>
		<S sid ="7" ssid = "7">The method could find what words are related with unknown word and how much weight other words relate with unknown word.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">This paper deals with a research for real semantic information retrieval with understanding all of words in the document.</S>
			<S sid ="9" ssid = "9">Many lexical dictionaries have been developed to overcome the limitation of current retrieval methods which use a simple keyword matching using probability, but it is impossible to contain words generated everyday by new trends, new items and new technologies.</S>
			<S sid ="10" ssid = "10">So, to guess the meaning of these unknown words, which are not defined in lexical dictionaries, many researchers have studied using several ways such as co-occurrence, TFiDF or learning methods.</S>
			<S sid ="11" ssid = "11">These researches for unknown words show much advanced result but, have a limitation that could not find related words with unknown word.</S>
			<S sid ="12" ssid = "12">This paper addresses research to grasp related words of unknown word for automatic extension of lexical dictionary with getting over the limitation.</S>
			<S sid ="13" ssid = "13">Known words occurring in a document may have only one meaning, but, generally have many meanings and relations between other words.</S>
			<S sid ="14" ssid = "14">Unknown words also can have many meanings and relations with known words which are defined in lexical dictionary.</S>
			<S sid ="15" ssid = "15">However, existing researches have concentrated on methods to define a meaning of unknown word or to find a base class (super-class) of unknown word, which is considered as instance.</S>
			<S sid ="16" ssid = "16">So, in this paper, we have studied what words are related with unknown word and how much weight is related between unknown word and known word.</S>
			<S sid ="17" ssid = "17">To do this, we have used the Word-Net that is lexical dictionary, co- occurrence and Bayesian probability.</S>
			<S sid ="18" ssid = "18">We implemented 07695-30907/08 $25.00 © 2008 IEEE 31 DOI 10.1109/WKDD.2008.151 a system depending on this research.</S>
			<S sid ="19" ssid = "19">The results showed that this research is very valuable and noble.</S>
			<S sid ="20" ssid = "20">This paper organized as follows: Section 2 Step Contents - Creating a group of related nouns introduces our main discourse.</S>
			<S sid ="21" ssid = "21">Section 3 shows experimental result and evaluation.</S>
			<S sid ="22" ssid = "22">Finally, we summarize this study in Section 4.</S>
			<S sid ="23" ssid = "23">Making a Related Noun Group - Calculating Bayesian probability of the group</S>
	</SECTION>
	<SECTION title="Proposed Method. " number = "2">
			<S sid ="24" ssid = "1">When we are reading, we meet UWs (unknown word) often.</S>
			<S sid ="25" ssid = "2">We can guess the meaning or related words easily without referring to a dictionary.</S>
			<S sid ="26" ssid = "3">Because the explanation or the definition of UW occurs in the same sentence using related words.</S>
			<S sid ="27" ssid = "4">In other words, meaning of UW can be found generally through co- occurrence words [5].</S>
			<S sid ="28" ssid = "5">Especially, a noun word has much meaning than other part of speeches, so, plays core role in understanding document [3].</S>
			<S sid ="29" ssid = "6">In this way, we can guess UWs using co-occurred noun words.</S>
			<S sid ="30" ssid = "7">However, a computer has many limitations to grasp the meaning of UW exactly.</S>
			<S sid ="31" ssid = "8">So, we focused on finding related words with UW before grasping the meaning of UW, which is an obstacle in NLP research area.</S>
			<S sid ="32" ssid = "9">We define an UW as word not contained in WordNet v2.0 as English lexical dictionary.</S>
			<S sid ="33" ssid = "10">And we have researched based on following assumptions.</S>
			<S sid ="34" ssid = "11">Assumptions: • The semantically nearest words from UW occur Normalization - Normalizing the Bayesian probability of group 2.1.</S>
			<S sid ="35" ssid = "12">Pre-Processing.</S>
			<S sid ="36" ssid = "13">To extract noun words from sentences, first we need PoS tagging.</S>
			<S sid ="37" ssid = "14">We employed the PoSTagger [1] that is possible to tag PoS quite exactly.</S>
			<S sid ="38" ssid = "15">It just tags the PoS of each word, so, we need to grasp compound noun, which occurs using sequential nouns in a sentence.</S>
			<S sid ="39" ssid = "16">For example, if a noun ‘football’ and a noun ‘player’ occur sequentially, we generate ‘football’, ‘player’, and, ‘football_player’ to match those into WordNet.</S>
			<S sid ="40" ssid = "17">If WordNet contains a compound noun ‘football_player’ then, ‘football’ and ‘player’ will exclude in a part to measure a weight.</S>
			<S sid ="41" ssid = "18">Chapter 2.4 deals with this problem.</S>
			<S sid ="42" ssid = "19">After these steps, a stemming, which is a process to find an original form of inflected word, is added.</S>
			<S sid ="43" ssid = "20">Table 2 shows the result of this pre-processing.</S>
			<S sid ="44" ssid = "21">The sentence in table 2 is a part of a document about soccer player ‘Zidane’ from WIKIPEDIA [2].</S>
			<S sid ="45" ssid = "22">Table 2.</S>
			<S sid ="46" ssid = "23">The result of pre-processing Zinedine Yazid Zidane, popularly nicknamed in the same sentence.</S>
			<S sid ="47" ssid = "24">• UW can have many meanings and many relations with known words those are defined in a dictionary.</S>
			<S sid ="48" ssid = "25">In this chapter, we present our proposed method through several steps.</S>
			<S sid ="49" ssid = "26">Table 1 shows each step and summary.</S>
			<S sid ="50" ssid = "27">And, the following sub-chapters explain the detail contents about table 1.</S>
			<S sid ="51" ssid = "28">Table 1.</S>
			<S sid ="52" ssid = "29">Overall steps of proposed method Step Contents - PoS Tagging Sentence PoS Tagging Extracted Nouns Zizou, is a French football player of Algerian descent, famous for leading France to winning the 1998 World Cup.</S>
			<S sid ="53" ssid = "30">Zinedine/NNP Yazid/NNP Zidane/NNP, popularly/RB nicknamed/VBN Zizou,/NNP is/VBZ a/DT French/JJ football/NN player/NN of/IN Algerian/NNP descent,/NNP famous/JJ for/IN leading/VBG France/NNP to/TO winning/VBG the/DT 1998/CD World/NNP Cup/NNP Zinedine, Yazid, Zidane, Zinedine_Yazid, Yazid_Zidane, Zizou, football, player, football_player, Algerian, descent, Algerian_descent, France, World, Cup, Pre-processing - Extract Nouns from Sentence - Stemming World_Cup UW Extraction - If word which is not contained in WordNet occurs then that is UW.</S>
			<S sid ="54" ssid = "31">2.2.</S>
			<S sid ="55" ssid = "32">UW Extraction.</S>
			<S sid ="56" ssid = "33">Creation of Noun Set Weight Measure of Co-occurrence Word - Make a noun set using noun words occurring with UW in same sentence - To give weight to co-occurrence word using Bayesian probability.</S>
			<S sid ="57" ssid = "34">Noun words extracted in the pre-processing step are mapped into a noun part of WordNet to make a decision which word is UW.</S>
			<S sid ="58" ssid = "35">If there is no agreement word in the WordNet, this word is considered as UW.</S>
			<S sid ="59" ssid = "36">Table 3 is pseudo-code to extract UW.</S>
			<S sid ="60" ssid = "37">UW is a set of UW that will get growing by processing many sentences.</S>
			<S sid ="61" ssid = "38">The UWs extracted from a sentence in Table 2 are presented below of table 3.</S>
			<S sid ="62" ssid = "39">Pseudo Code Tabl e 3.</S>
			<S sid ="63" ssid = "40">Pseudo code to extr act UW Ni : i-th noun extrac ted from docu ment; UW : Unkn own Word Set; WN : Noun part of Word Net; if ( N i ∉ WN ) {</S>
	</SECTION>
	<SECTION title="FIFA. " number = "3">
			<S sid ="64" ssid = "1">attenti on, Brazil, countr y, Cup, Europ e, fame, goal, play maker, 14 World, World_C up, Player, Year, April, football Extracted UW UW = UW + N i ; } Zinedine, Yazid, Zidane, Zinedine_Y azid, Yazid_Zidan e, Zizou, Algerian_de scent 2.4.</S>
			<S sid ="65" ssid = "2">Weight Measure of Co occurrin g Words This step measures the weight of co occurring words with UW using Bayesian probabilit y because frequency is important factor to grasp related degree between UW and known words.</S>
			<S sid ="66" ssid = "3">In the case of compoun d noun, it has 2 more nouns and we generated both each single noun and compoun d noun in chapter 2.1.</S>
			<S sid ="67" ssid = "4">In.</S>
			<S sid ="68" ssid = "5">here, if the WordNet contains the compound 2.3.</S>
			<S sid ="69" ssid = "6">Creation of Noun Set.</S>
			<S sid ="70" ssid = "7">In this step, we make a noun set occurring with UW in the same sentence because co-occurring words have a possibility to relate with UW.</S>
			<S sid ="71" ssid = "8">Table 4 is pseudo-code to create a noun set.</S>
			<S sid ="72" ssid = "9">Table 4.</S>
			<S sid ="73" ssid = "10">Pseudo-code to extract UW UW = {UWi | 1 ≤ i ≤ m} : Unknown word(UWi ) set; noun, all of single noun words contained in compound noun should be excluded from weight-measure.</S>
			<S sid ="74" ssid = "11">For example, if the WordNet contains a compound noun ‘World Cup’ and ‘World Cup’ occurs twice, single noun ‘World’ occurs once then, these are calculated ‘World’ 3 times occurrence, ‘Cup’ twice and ‘World Cup’ twice in the previous step.</S>
			<S sid ="75" ssid = "12">These make an affection of duplication to occurring count of m : Total count of unknown word( UWi ); noun words because only single word ‘World’ occurs once independently.</S>
			<S sid ="76" ssid = "13">Therefore, if a compound noun K i = {kij | 1 ≤ j ≤ ni } : Collection of noun( kij ) set occurring which is consisted of more than 2 words sequentially is with UWi ; detect ed, we find a pure value of Bayes ian proba bility ni : Total count of noun occurring with UWi (total count of of single nouns by extracting Bayesian probability of noun set of UWi ); compound noun.</S>
			<S sid ="77" ssid = "14">Table 6 presents the process to measure the weight of co occurrence words.</S>
			<S sid ="78" ssid = "15">And, table 7 shows the weight of each co occurrence word with Many UWs are extracted and each UW has many co-occurring noun words through processing the whole document using previous steps.</S>
			<S sid ="79" ssid = "16">Table 5 shows the just 3 results of processing the document about ‘Zidane’.</S>
			<S sid ="80" ssid = "17">Table 5.</S>
			<S sid ="81" ssid = "18">Extracted UW and noun set UWs in table 5.</S>
			<S sid ="82" ssid = "19">Table 6.</S>
			<S sid ="83" ssid = "20">Weight-measure of co-occurring words Bayesian = P(B | A)P( A) P(B) oc(UWi ) : Frequency of UWi , oc(kij ) : Frequency of kij i UWi Co-occurred Noun Set ni P(oc(UW ) | oc(k ))P(oc(k )) Bayesian = i ij ij 1 Zidane.</S>
			<S sid ="84" ssid = "21">Algerian, Cup, descent, football, France, June, player, World, World_Cup, attention, Brazil, 19 country, Europe, fame, goal, playmaker, Year, April, P ( o c ( U W i ) ) If, kij = {ki ( j −q ) | 1 ≤ q ≤ r} is compound noun, r : count of single nouns compose d in compou nd noun football_player P(oc(k ) | oc(UW )) = P(oc(k ) | oc(UW )) − P(oc(k ) | oc(UW )) club, Madrid, Real, attention, iq &apos; i iq i ij i Brazil, country, Cup, Europe, q&apos; : independent occurrence of single noun 2 Juventus.</S>
			<S sid ="85" ssid = "22">fame, goal, playmaker, World, 19 championship, Euro, Italy, level, P(oc(kiq&apos; ) | oc(UWi ) Spain, victory, World_Cup P(oc(UW ) | oc(k ))P(oc(k )) P(oc(UW ) | oc(k ))P(oc(k )) = i iq iq − i ij ij P(oc(UWi )) P(oc(UWi )) Table 7.</S>
			<S sid ="86" ssid = "23">Weight of co-occurring words Table 8.</S>
			<S sid ="87" ssid = "24">Related noun group and sum of Bayesian UWi Noun( ) Noun( ) i j i j Alger ian 0.2 April 0.2 UWi G ro u p of R el at e d N o u ns Sum 099514 69(footb all_player) 102838 58(play er) football 0.2 football_player 0.2 June 0.2 player 0.2 Zidane10283858(player) 10284756(playmaker) 08802093(France)-09142657(Europe) 0.6 Zidane World_C up 0.6 attention 0.2 country 0.2 Europe 0.2 goal 0.2 playmak er 0.2 descent 0.2 Brazil 0.2 0 8 8 0 2 0 9 3 ( F r a n c e ) 0 8 0 6 0 6 7 4 ( E u r o p e ) 0 . 4 Juventus 08895440(Madrid)-08894294(Spain) 1.0 France 0.2 fame 0.2 World 0.2 Year 0.2 club 0.6667 Madrid 0.6667FIFA 10284756(playmaker) 10283858(Player) 2.6.</S>
			<S sid ="88" ssid = "25">Normalization.</S>
			<S sid ="89" ssid = "26">0.4 Juventus attention 0.3333 Brazil 0.3333 victory 0.3333 Europe 0.3333 goal 0.3333 playmaker 0.3333 World_Cu p 0.3333 champions hip 0.3333 Italy 0.3333 level 0.3333 Real 0.6667 Spain 0.3333 country 0.3333 Euro 0.3333 fame 0.3333 attentio n 0.2 Brazil 0.2 football 0.2 Europe 0.2 goal 0.2 playma ker 0.2 An aim of this paper is to find related words of UW.</S>
			<S sid ="90" ssid = "27">To do this, we measure d Bayesia n probabil ity of each co occurrin g word and made related noun groups through previous steps.</S>
			<S sid ="91" ssid = "28">Howeve r, that is not probabil ity of relation between UW and group.</S>
			<S sid ="92" ssid = "29">Moreov er, to reflect both probabil ity and relation, it needs normali zation.</S>
			<S sid ="93" ssid = "30">Table 9 and table 10 show way to normali ze and results.</S>
			<S sid ="94" ssid = "31">T a b l e 9 . N o r m a l i z a t i o n o f g r o u p Gi = {Gil | 1 ≤ l ≤ s} : Relate Related Group of UWi (This group is consisted of co occurren ce words with UWi through matching into WordNe t.)</S>
			<S sid ="95" ssid = "32">s : count of groups related with UWi FIFA World_ Cup 0.6 Player 0.2 country 0.2 Year 0.2 fame 0.2 April 0.2 W o r l d 0 . 2 Gil : R e l a t e d w o r d s i n t h e c o o c c u r r e n c e s e t k i s ∑ B a y e s i a n ( G i l | U W i ) N o r m a l i z a t i o n = l = 1 n i ∑ B a y e s i a n ( k i j | U W i ) j = 1 W : Weight of Co-occurring Word Table 10.</S>
			<S sid ="96" ssid = "33">Result of normalization 2.5.</S>
			<S sid ="97" ssid = "34">Making a Related Noun Group.</S>
			<S sid ="98" ssid = "35">The words occurring with UW can have relations with each other.</S>
			<S sid ="99" ssid = "36">These related words can be grasped by matching into WordNet.</S>
			<S sid ="100" ssid = "37">If some words are related, we make a group using these related words because UW has much more related probability with the group of co- occurring words [4, 6].</S>
			<S sid ="101" ssid = "38">The step in this chapter makes a group through co-occurring words, and measures the related probability between UW and the groups using sum of Bayesian probability of each word.</S>
			<S sid ="102" ssid = "39">Table 8 shows the result of group that is consisted of related nouns and Bayesian probability of group.</S>
			<S sid ="103" ssid = "40">i 1,8 For example, if all of evaluators ( e = 10 ) judge the values of every g R _ UWijk ( g = 10 ) to be 1 then the value of ∑ R _ UWijk k =1 is 10.</S>
			<S sid ="104" ssid = "41">So, the relevancy of this experiment is 100(%).</S>
			<S sid ="105" ssid = "42">Through this experiment and evaluation, we found that this research is very meaningful and excellent as 95.88(%).</S>
			<S sid ="106" ssid = "43">N :Normalization In Table X, the UWZidande has many groups and especially two groups (‘football_player’, ‘player’, ‘playmaker’), (‘World_Cup’) show the most related value as 0.167.</S>
			<S sid ="107" ssid = "44">And, the second is the group of ‘France’</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "4">
			<S sid ="108" ssid = "1">An ultimate aim of this research is to extend lexical dictionary through addition of unknown words automatically.</S>
			<S sid ="109" ssid = "2">To accomplish this aim, the meaning, related words and relation such as hyponym, hypernym, synonym, antonym, etc of unknown word have to be grasped.</S>
			<S sid ="110" ssid = "3">In this paper, to grasp related words of unknown word, we applied co-occurrence, Bayesian probability and WordNet as lexical dictionary.</S>
			<S sid ="111" ssid = "4">Through experiment and evaluation, we got semantic and excellent result.</S>
			<S sid ="112" ssid = "5">Based on this research, grasping the meaning of unknown words and relation is future works for auto-extension of lexical dictionary.</S>
			<S sid ="113" ssid = "6">and ‘Europe’.</S>
			<S sid ="114" ssid = "7">In case of UWJuventus , the group of</S>
	</SECTION>
	<SECTION title="Acknowledgment">
</PAPER>
