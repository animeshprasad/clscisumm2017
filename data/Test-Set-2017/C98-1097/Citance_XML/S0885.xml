<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Transcript-based topic segmentation of TV programs faces several difficulties arising from transcription errors, from the presence of potentially short segments and from the limited number of word repetitions to enforce lexical cohesion, i.e., lexical relations that</S>
	</ABSTRACT>
	<SECTION title="lexical cohesion" number = "1">
			<S sid ="2" ssid = "2">based on generalized probabilities with a unigram language model.</S>
			<S sid ="3" ssid = "3">On the one hand, confidence measures and semantic relations are considered as additional sources of information.</S>
			<S sid ="4" ssid = "4">On the other hand, language model interpolation techniques are investigated for better language model estimation.</S>
			<S sid ="5" ssid = "5">Experimental topic segmentation results are presented on two corpora with distinct characteristics, composed respectively of broadcast news and reports on current affairs.</S>
			<S sid ="6" ssid = "6">Significant improvements are obtained on both corpora, demonstrating the effectiveness of the extended lexical cohesion measure for spoken TV contents, as well as its genericity over different programs.</S>
			<S sid ="7" ssid = "7">© 2011 Elsevier Ltd. All rights reserved.</S>
			<S sid ="8" ssid = "8">Keywords: Topic segmentation; Lexical cohesion; Confidence measures; Semantic relations; Language model interpolation; TV broadcasts 1.</S>
			<S sid ="9" ssid = "9">Introduction.</S>
			<S sid ="10" ssid = "10">Structuring video feeds has become a requirement and is a highly challenging issue.</S>
			<S sid ="11" ssid = "11">Indeed, with the proliferation of video-sharing websites and the increasing number of television channels, the quantity of video that users can access has become so important that it is necessary to develop methods to structure this material and enable users to navigate.</S>
			<S sid ="12" ssid = "12">A crucial structuring stage is the segmentation of shows into topically homogeneous segments (Wactlar et al., 1996; Allan et al., 1998).</S>
			<S sid ="13" ssid = "13">Since videos available are of different kinds (movies, news, talk shows, etc.) and in order to avoid the use of several domain specific methods, such structuring approaches must necessarily be generic enough to treat various types of video.</S>
			<S sid ="14" ssid = "14">To this end, topic segmentation of informative content TV shows can rely on the speech pronounced in � This paper has been recommended for acceptance by ‘INTERSPEECH guest editors’.</S>
			<S sid ="15" ssid = "15">∗ Corresponding author.</S>
			<S sid ="16" ssid = "16">Email addresses: camille.guinaudeau@inria.fr (C. Guinaudeau), guillaume.gravier@irisa.fr (G. Gravier), pascale.sebillot@irisa.fr (P. Sébillot).</S>
			<S sid ="17" ssid = "17">08852308/$ – see front matter © 2011 Elsevier Ltd. All rights reserved.</S>
			<S sid ="18" ssid = "18">doi:10.1016/j.csl.2011.06.002 the programs as it is not dependent on the kind of document.</S>
			<S sid ="19" ssid = "19">In this case, the segmentation is based on the analysis of the distribution of words within the speech, a topic change being detected when the vocabulary changes significantly.</S>
			<S sid ="20" ssid = "20">With the improvement of automatic speech recognition (ASR) systems in recent years (Ostendorf et al., 2008), topic segmentation of spoken documents can now be performed on automatic transcripts of the speech material.</S>
			<S sid ="21" ssid = "21">However, most of the work in this direction directly apply methods developed for textual topic segmentation to automatic transcripts of spoken language without taking into account the specifics of TV programs transcripts (transcription errors, short topic segments).</S>
			<S sid ="22" ssid = "22">These methods are often based on the notion of lexical cohesion which corresponds to the lexical relations that exist within a text, mainly enforced by reiterations, i.e., repetitions of the same words (Mulbregt et al., 1999; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006).</S>
			<S sid ="23" ssid = "23">Alternately, discourse markers, obtained from a preliminary expert or learning process, can also be used to identify topic boundaries (Beeferman et al., 1999; Christensen et al., 2005).</S>
			<S sid ="24" ssid = "24">Christensen et al.</S>
			<S sid ="25" ssid = "25">(2005) have established that transcription errors have little effect on the performance of a supervised segmentation algorithm using discourse markers.</S>
			<S sid ="26" ssid = "26">However, we have observed a large gap of performance between manual and automatic transcripts in previous work on topic segmentation of radio broadcasts using an unsupervised approach based on lexical cohesion (Huet et al., 2008).</S>
			<S sid ="27" ssid = "27">This difference is mostly due to the specifics of the material on which we focus, i.e., TV shows.</S>
			<S sid ="28" ssid = "28">Indeed, automatic transcripts of TV shows have certain peculiarities that are detrimental to topic segmentation and, in general, to natural language processing.</S>
			<S sid ="29" ssid = "29">Firstly, the error rate of the ASR system used, even if it remains reasonable for news, can be as high as 70% for challenging programs such as talk shows or debates.</S>
			<S sid ="30" ssid = "30">Moreover, TV programs are composed of topic segments that can be very short and contain few repetitions of vocabulary, particularly in news where journalists make use of synonyms to avoid reiterations.</S>
			<S sid ="31" ssid = "31">In our corpus, we have measured that a word occurs on average 1.8 times in a topically coherent segment in broadcast news and 2.0 times in reports on current affairs (for more details, cf.</S>
			<S sid ="32" ssid = "32">Section 4.3).</S>
			<S sid ="33" ssid = "33">In order to overcome difficulties related to transcription errors, some studies have suggested to add features specific to spoken documents to the sole concept of lexical cohesion.</S>
			<S sid ="34" ssid = "34">For example, Amaral and Trancoso (2003) exploits speaker detection to locate the anchor speaker in news program, relating anchor speaker occurrences with new reports and hence with topic changes.</S>
			<S sid ="35" ssid = "35">In Stolcke et al.</S>
			<S sid ="36" ssid = "36">(1999), prosody is used in addition to automatic transcription.</S>
			<S sid ="37" ssid = "37">However, such clues are seldom used in practice because their automatic extraction is difficult.</S>
			<S sid ="38" ssid = "38">Moreover, they imply domain and genre specific knowledge and are, therefore, highly dependent on the type of document.</S>
			<S sid ="39" ssid = "39">The aim of this paper is to propose a segmentation method able to deal with peculiarities of professional videos (transcription errors, possibly short segments and limited number of repetitions) while remaining generic enough.</S>
			<S sid ="40" ssid = "40">This method is based on the criterion of lexical cohesion which is not dependent on a type of document since it is mainly enforced by word repetitions.</S>
			<S sid ="41" ssid = "41">However lexical cohesion is not efficient when the number of reiterations is low—i.e., when synonyms are used or topic segments are very short—and is sensitive to transcription errors, two characteristics of our video material.</S>
			<S sid ="42" ssid = "42">We propose several extensions to a measure of lexical cohesion, based on generalized probabilities using a unigram language model, in order to make this criterion more robust to spoken content.</S>
			<S sid ="43" ssid = "43">On the one hand, the measure of the lexical cohesion is modified by an original technique that incorporates two sources of additional information: semantic relations, highlighting the semantic proximity between words, and confidence measures.</S>
			<S sid ="44" ssid = "44">On the other hand, we propose to use language model interpolation techniques so as to provide better estimates of the lexical cohesion on short segments.</S>
			<S sid ="45" ssid = "45">The paper is organized as follows: we first present the topic segmentation method, based on lexical cohesion, developed for the segmentation of textual documents and used as a baseline in this work.</S>
			<S sid ="46" ssid = "46">In Section 3, extensions of the probabilistic lexical cohesion measure to improve robustness to TV program specifics are described.</S>
			<S sid ="47" ssid = "47">The experimental setup is presented in Section 4.</S>
			<S sid ="48" ssid = "48">Finally, experimental results are extensively discussed in Section 5, before the presentation of future work.</S>
	</SECTION>
	<SECTION title="Topic segmentation. " number = "2">
			<S sid ="49" ssid = "1">Within the framework of TV stream structuring, the objective of topic segmentation is to split relevant shows (broadcast news and reports on current affairs in this work) into segments that deal with a single topic.</S>
			<S sid ="50" ssid = "2">Topic segmentation algorithms can be based on the criterion of lexical cohesion.</S>
			<S sid ="51" ssid = "3">In this case, the segmentation relies on the analysis of the distribution of words within the text, a topic change being detected when the vocabulary changes significantly (Utiyama and Isahara, 2001; Hearst, 1997).</S>
			<S sid ="52" ssid = "4">In this section, the lexical cohesion criterion and the way it is computed in the context of topic segmentation is first described before the presentation of the topic segmentation method of Utiyama and Isahara (2001) which serves as a baseline in this work.</S>
			<S sid ="53" ssid = "5">2.1.</S>
			<S sid ="54" ssid = "6">Lexical cohesion.</S>
			<S sid ="55" ssid = "7">The notion of lexical cohesion refers to lexical relations that exist within a text to provide a certain unity.</S>
			<S sid ="56" ssid = "8">Lexical cohesion is created by repetitions of the same words, co-references, and the use of sets of semantically related words (Halliday and Hasan, 1976).</S>
			<S sid ="57" ssid = "9">As lexical cohesion is a guide to the organization of the flow of ideas in the text, this criterion is widely used in many fields of the natural language processing domain.</S>
			<S sid ="58" ssid = "10">In discourse analysis, Xingwei (in press) studies the relationship between cohesion and coherence in texts while Klebanov et al.</S>
			<S sid ="59" ssid = "11">(2008) makes a stylistic analysis of political speeches.</S>
			<S sid ="60" ssid = "12">Others studies use lexical cohesion for word sense disambiguation (Manabu and Takeo, 1994), and automatic summarization (Barzilay and Elhadad, 1997; Boguraev and Neff, 2000).</S>
			<S sid ="61" ssid = "13">Error identification and correction applications can also rely on lexical cohesion to detect errors by identifying tokens that are semantically unrelated to their context.</S>
			<S sid ="62" ssid = "14">These methods can be applied on regular text, as in Hirst and Budanitsky (2005), or on automatic transcripts (Inkpen and Desilets, 2005).</S>
			<S sid ="63" ssid = "15">In Utiyama and Isahara (2001), the lexical cohesion computation of a segment Si derives from the ability of a language model Δi whose parameters are estimated from words in Si to predict the words in the segment.</S>
			<S sid ="64" ssid = "16">In this framework, two important steps are needed: the estimation of the language model Δi and the computation of the generalized probability of words in Si , reflecting the ability of the language model Δi to predict the words of Si . Language model.</S>
			<S sid ="65" ssid = "17">The language model Δi estimated on Si is a unigram1 language model (Utiyama and Isahara, 2001) which specifies a distribution over all words in the text (or transcript) to be segmented.</S>
			<S sid ="66" ssid = "18">The calculation of the language model of a segment Si is formalized, for a Laplace smoothing, as Δi = Pi(u) = Ci(u) + 1 zi , ∀u ∈ VK , (1) with VK the vocabulary of the text, containing K distinct words, and Ci (u) the count of word u in Si corresponding to the number of occurrences of u in Si . The probability distribution is smoothed by incrementing the count of each word by 1.</S>
			<S sid ="67" ssid = "19">The normalization term zi ensures that Δi is a probability mass function and, in the particular case of Eq.</S>
			<S sid ="68" ssid = "20">(1), zi = K + ni with ni the number of word occurrences in Si . Generalized probability.</S>
			<S sid ="69" ssid = "21">The language model Δi is used to compute the generalized probability of the words in Si as a measure of lexical cohesion according to ni ln P [Si; Δi] = ln P [wi ; Δi], (2) j=1 where wi denotes the jth word in Si . Intuitively the probability favors lexically consistent segments since its value is greater when words appear several times within the segment and decreases if many words are different.</S>
			<S sid ="70" ssid = "22">2.2.</S>
			<S sid ="71" ssid = "23">Topic segmentation.</S>
			<S sid ="72" ssid = "24">The topic segmentation method used, introduced by Utiyama and Isahara (2001) for textual documents, was chosen in this context of transcript-based TV program segmentation for two main reasons.</S>
			<S sid ="73" ssid = "25">It is currently one of the best performing methods that makes no assumption on a particular domain (no discourse markers, no topic models, etc.).</S>
			<S sid ="74" ssid = "26">Moreover, contrary to many methods based on local measures of the lexical cohesion, with the exception of works like (Malioutov and Barzilay, 2006) that shares the same philosophy, the global criterion used in Utiyama and Isahara (2001) makes it possible to account for the high variability in segment lengths.</S>
			<S sid ="75" ssid = "27">1 Here, the language model serves to represent the vocabulary repetition in our data.</S>
			<S sid ="76" ssid = "28">Thus, if the use of bigram (or higher order) language model is more suitable for a language modeling purpose, in this work a unigram language model allows us to represent the repetitions of simple terms in the segments.</S>
			<S sid ="77" ssid = "29">The method consists in searching the segmentation that produces the most consistent segments from a lexical point of view, while respecting a prior distribution of segment lengths.</S>
			<S sid ="78" ssid = "30">Cast in a probabilistic framework, the principle is to find the most probable segmentation of a sequence of l basic units (words or sentences) W = W l among all possible segmentations, Sˆ = argmaxP [W |S] P [S].</S>
			<S sid ="79" ssid = "31">(3) 1 Assuming that P [Sm] = n−m, with n the number of words in the text and m the number of segments, and that segments are independent, the probability of a text W for a segmentation S = Sm is given by Sˆ argmax m 1 m i=1 (ln(P [W bi |Si]) − α ln(n)), (4) where P [W bi |Si] denotes the generalized probability of the sequence of basic units corresponding to Si as given by Eq.</S>
			<S sid ="80" ssid = "32">(2).</S>
			<S sid ="81" ssid = "33">The parameter α allows for different trade-offs between lexical cohesion and segment lengths.</S>
			<S sid ="82" ssid = "34">In this paper, the basic units used are utterances as given by the partitioning step of the ASR system, thus limiting possible topic boundaries to utterance boundaries, and the text W is only composed of lemmatized2 nouns, adjectives and non modal verbs.</S>
	</SECTION>
	<SECTION title="Robust topic segmentation for spoken multimedia contents. " number = "3">
			<S sid ="83" ssid = "1">The measure for lexical cohesion based on language model, as defined in Section 2.1, relies only on word repetitions.</S>
			<S sid ="84" ssid = "2">However, this can turn out to be insufficient in the case of automatically transcribed video material.</S>
			<S sid ="85" ssid = "3">Indeed, two occurrences of a word can be erroneously recognized as two different words and therefore not considered for lexical cohesion.</S>
			<S sid ="86" ssid = "4">Moreover, due to potentially small segment lengths and to the use of synonyms, the number of repetitions can be very low.</S>
			<S sid ="87" ssid = "5">To adjust the computation of lexical cohesion to spoken documents, we propose several extensions.</S>
			<S sid ="88" ssid = "6">In the two first ones, additional information is incorporated to the lexical cohesion measure while, in the second one, language model interpolation techniques are used so as to provide better language model estimates.</S>
			<S sid ="89" ssid = "7">3.1.</S>
			<S sid ="90" ssid = "8">Integration of additional sources of information.</S>
			<S sid ="91" ssid = "9">Different kinds of additional information, such as prosody or confidence measures, can be used to improve the generalized probability measure of lexical cohesion.</S>
			<S sid ="92" ssid = "10">In this work, confidence measures and semantic relations are employed.</S>
			<S sid ="93" ssid = "11">On the one hand, confidence measures, associated with each word by the ASR system, correspond to (an estimation of) the probability that a word has been correctly transcribed.</S>
			<S sid ="94" ssid = "12">They are considered so as to reduce the contribution of non properly transcribed words in the lexical cohesion calculation.</S>
			<S sid ="95" ssid = "13">On the other hand, semantic relations, which represent the semantic proximity between words, are used to consider the fact that two different words can be semantically related and seen as a repetition.</S>
			<S sid ="96" ssid = "14">Such information can be accounted for either during the language model estimation step or during the calculation of the generalized probability.</S>
			<S sid ="97" ssid = "15">3.1.1.</S>
			<S sid ="98" ssid = "16">Using confidence measures Confidence measures can be accounted for at the language model level by replacing the count Ci (u) by the sum of the confidences over all occurrences of u, i.e., C i δ1 i(u) = j =u c(wj ) , (5) 2 A lemma is an arbitrary canonical form grouping all inflections of a word in a grammatical category, e.g., the infinitive form for verbs and the masculine singular form for adjectives.</S>
			<S sid ="99" ssid = "17">where c(wi ) ∈ [0, 1] corresponds to the confidence measure for the jth word in Si and δ1 is a parameter used to reduce the weight of words whose confidence measure is low.</S>
			<S sid ="100" ssid = "18">Indeed, the higher δ1 , the lower the impact of words with a low confidence measure.</S>
			<S sid ="101" ssid = "19">Confidence measures can also be accounted for during the generalized probability computation.</S>
			<S sid ="102" ssid = "20">In this case, the log-likelihood of the occurrence of a word in a segment is multiplied by the confidence measure of the word occurrence, ln P [Si; Δi] = ni c(wi )δ2 ln P [wi ; Δi], (6) j j j=1 with δ2 equivalent to δ1 . Eq.</S>
			<S sid ="103" ssid = "21">(6) allows to reduce the contribution to the lexical cohesion of a word whose confidence measure is low.</S>
			<S sid ="104" ssid = "22">In this case, the language model Δi can be either estimated from the counts Ci (u), thus limiting the use of confidence measures to the probability computation, or from the modified counts Ci(u).</S>
			<S sid ="105" ssid = "23">Note that in Eq.</S>
			<S sid ="106" ssid = "24">(6), ln P[Si ; Δi ] is not strictly speaking a log probability.</S>
			<S sid ="107" ssid = "25">However, if δ2 = 1, ln P[Si ; Δi ] can be seen as the joint probability for the word wi to be correctly transcribed and to be represented by the language model Δi . 3.1.2.</S>
			<S sid ="108" ssid = "26">Using semantic relations Many topic segmentation techniques, based on the lexical cohesion criterion, use semantic relations as additional information to take into account the semantic links that exist between words.</S>
			<S sid ="109" ssid = "27">Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., “car” and “drive”, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.</S>
			<S sid ="110" ssid = "28">These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).</S>
			<S sid ="111" ssid = "29">For example, Ferret (2006) uses a network of lexical co-occurrences built from a large corpus to improve a topic segmenter based on lexical reiteration.</S>
			<S sid ="112" ssid = "30">The algorithm in Jobbins and Evett (1998) compares adjacent windows of sentences and determines their lexical similarity, based on repetitions of words and collocations, to detect topic boundaries.</S>
			<S sid ="113" ssid = "31">Two main types of semantic relations can be automatically extracted from corpora, namely syntagmatic and paradigmatic relations (Manning and Schütze, 1999; Claveau and Sébillot, 2004; Grefenstette, 1994).</S>
			<S sid ="114" ssid = "32">Syntagmatic relations correspond to relations of contiguity that words maintain within a given syntactic context (sentence, chunk, fixed length window, etc.), two words being related if they often appear together.</S>
			<S sid ="115" ssid = "33">A popular criterion to measure the degree of syntagmatic proximity between two words u and v is the mutual information (MI) which compares the probability of observing the two words u and v together with the probability of observing these two words separately (Grefenstette, 1994).</S>
			<S sid ="116" ssid = "34">Several variants of the mutual information criterion have been proposed.</S>
			<S sid ="117" ssid = "35">For example, in Daille (1996), the mutual information is cubed (thus called MI3 ) to avoid emphasizing rare associations.</S>
			<S sid ="118" ssid = "36">In Church and Hanks (1990), order between words is taken into account.</S>
			<S sid ="119" ssid = "37">Paradigmatic relations link two words having an important common component from a meaning point of view.</S>
			<S sid ="120" ssid = "38">These relations, corresponding to synonyms, hyperonyms, antonyms, etc., are typically calculated by means of context vectors for each word, grouping together words that appear in the same contexts.</S>
			<S sid ="121" ssid = "39">The context vector of a word u describes the distribution of words in its vicinity and contains, for each word v, its frequency of occurrence in the neighborhood of u, possibly normalized by its average frequency in the neighborhood of any word.</S>
			<S sid ="122" ssid = "40">The semantic proximity between two terms can be defined thanks to the Jaccard index as in Grefenstette (1992) or as the angular distance between their respective context vectors.</S>
			<S sid ="123" ssid = "41">In our work, as in Morris and Hirst (1991), Ferret (2006), and Jobbins and Evett (1998), these relations are employed to overcome the limited number of vocabulary repetitions.</S>
			<S sid ="124" ssid = "42">But semantic relations are also expected to limit the impact of recognition errors.</S>
			<S sid ="125" ssid = "43">Indeed, contrary to correctly transcribed words, misrecognized words are unlikely to be semantically linked to other words in a topic segment (Inkpen and Desilets, 2005).</S>
			<S sid ="126" ssid = "44">As a consequence, the contribution of non properly transcribed words will be less important than the one of correct words when semantic relations are used.</S>
			<S sid ="127" ssid = "45">As for confidence measures, accounting for semantic relations can be achieved by modifying the counts in the language model estimation step.</S>
			<S sid ="128" ssid = "46">Counts, which normally reflect how many times a word appears in a segment, are extended so as to emphasize the probability of a word based on its number of occurrences as well as on the occurrences of related words.</S>
			<S sid ="129" ssid = "47">More formally, the counts Ci in Eq.</S>
			<S sid ="130" ssid = "48">(2) are amended according to Ci (u) = Ci(u) + ni j=1,wi =/ u r(wi , u), (7) where r(wi , u) ∈ [0, 1] denotes the semantic proximity between wi and u, close to 1 for highly related words and null j j for non related words.</S>
			<S sid ="131" ssid = "49">More details about the computation of r are given in Section 4.2.</S>
			<S sid ="132" ssid = "50">Unlike confidence measures, semantic relations cannot be accounted for during generalized probability computation.</S>
			<S sid ="133" ssid = "51">Indeed, in this case, multiplying the probability for a word to appear in a segment Si by the sum of the relations the word maintains with other words in Si does not make sense.</S>
			<S sid ="134" ssid = "52">To conclude this section, it is important to note that, in general, semantic relations are obtained from domain specific corpora.</S>
			<S sid ="135" ssid = "53">They can also be learned on a general purpose corpus but will in any case fail to address all domains.</S>
			<S sid ="136" ssid = "54">One strong point of our technique is that when semantic relations are not adequate for a particular document—e.g., when segmenting a document from a domain not in the semantic relations training data—Ci (u) will remain unchanged with respect to Ci (u) since r(u, v) is null between any two words of the document.</S>
			<S sid ="137" ssid = "55">Put differently, out of domain relations will have no impact on topic segmentation, contrarily to latent semantic approaches (Deerwester et al., 1990; Landauer et al., 1998).</S>
			<S sid ="138" ssid = "56">3.2.</S>
			<S sid ="139" ssid = "57">Combining global and local measurements of lexical cohesion.</S>
			<S sid ="140" ssid = "58">In TV programs, and especially in news, topic segments can be very short.</S>
			<S sid ="141" ssid = "59">An easy, and unfortunately accurate, criticism of Eq.</S>
			<S sid ="142" ssid = "60">(1) is that for small segments, the language model Δi is poorly estimated.</S>
			<S sid ="143" ssid = "61">Therefore, in order to deal with short segments, the use of a more sophisticated estimation method is needed.</S>
			<S sid ="144" ssid = "62">To skirt this problem, we propose to interpolate the language model at the segment level with a more robust language model estimated on the entire transcript.</S>
			<S sid ="145" ssid = "63">This allows to take into account the whole text to be segmented in order to have a better language model estimation for short segments.</S>
			<S sid ="146" ssid = "64">Two interpolation strategies are studied—interpolation of the probabilities (Jelinek and Mercer, 1981) and interpolation of the counts (Bacchiani and Roark, 2003).</S>
			<S sid ="147" ssid = "65">3.2.1.</S>
			<S sid ="148" ssid = "66">Linear interpolation of probabilities The first interpolation technique is a basic probability interpolation.</S>
			<S sid ="149" ssid = "67">In this case, the lexical cohesion of a segment Si , given Si and the text T, is measured according to ln P [Si; Si, T ] = ni ln(λP [wi ; Δi] + (1 − λ)P [wi ; Δt ]) j=1 ni = ⎛ ln ⎜λ j Ci(wi ) + ξ j + (1 − λ) ⎞ Ct (wi ) ⎟ ⎟ (8) j=1 ⎝ u∈VT Ci(u) + ξ u∈VT Ct (u) ⎠ where Δi is the language model estimated on Si and Δt the one estimated on T. Ct (u) is the count of word u in T and Ci (u) the count of that word in Si . ξ is a count smoothing bias that corresponds to the Laplace smoothing when ξ = 1.</S>
			<S sid ="150" ssid = "68">3.2.2.</S>
			<S sid ="151" ssid = "69">Count interpolation Rather than interpolating probabilities, language model interpolation can be based on the interpolation of counts.</S>
			<S sid ="152" ssid = "70">In this case, the lexical cohesion of a segment Si is defined as ln P [Si; Si, T ] = ni j=1 ln P [wi ; Δit ] ⎛ ⎞ ni λ(Cs (wi ) + ξ) + (1 − λ)Ct (wi ) (9) = ln ⎜ j j ⎟ j=1 ⎝ u∈VT λ(Ci(u) + ξ) + (1 − λ)Ct (u) ⎠ where Δit is the interpolated language model of the segment Si and the text T. As for linear interpolation, frequent words in T will get a high probability regardless of their frequency in Si while non frequent ones will always get a low probability—depending on λ.</S>
			<S sid ="153" ssid = "71">However, because of the renormalization by the sum of all the counts, this fact might be less detrimental than for probability interpolation and the behavior of this interpolation technique more likely to be consistent with what is expected.</S>
	</SECTION>
	<SECTION title="Experimental setup. " number = "4">
			<S sid ="154" ssid = "1">Experiments were carried out with our speech recognition system on a comprehensive corpus.</S>
			<S sid ="155" ssid = "2">Before presenting the corpus, a brief description of the ASR system is provided and the selection of semantic relations integrated in our system is discussed.</S>
			<S sid ="156" ssid = "3">4.1.</S>
			<S sid ="157" ssid = "4">ASR system and confidence measures.</S>
			<S sid ="158" ssid = "5">All TV programs were transcribed using our Irene ASR system, originally developed for broadcast news transcription.</S>
			<S sid ="159" ssid = "6">Irene implements a multiple pass strategy, progressively narrowing the set of candidate transcripts—the search space—in order to use more complex models.</S>
			<S sid ="160" ssid = "7">In the final steps, a 4-gram language model over a vocabulary of 65,000 words is used with context-dependent phone models to generate a list of 1000 sentence transcription hypotheses.</S>
			<S sid ="161" ssid = "8">Morphosyntactic tagging, using a tagger specifically designed for ASR transcripts, is used in a post-processing stage to generate a final transcription by consensus from a confusion network, combining the acoustic, language model and morphosyntactic scores (Huet et al., 2010).</S>
			<S sid ="162" ssid = "9">Confusion network posterior probabilities are used directly as confidence measures.</S>
			<S sid ="163" ssid = "10">Acoustic models were trained on about 250 hours of broadcast news material from the French ESTER 2 data (Galliano et al., 2009).</S>
			<S sid ="164" ssid = "11">The language model probabilities were estimated on 500 million words from French newspapers and interpolated with language model probabilities estimated over 2 million words corresponding to the reference transcription of radio broadcast news shows.</S>
			<S sid ="165" ssid = "12">The system exhibits a word error rate (WER) of 16% on the non accented news programs of the ESTER 2 evaluation campaign.</S>
			<S sid ="166" ssid = "13">As far as TV contents are concerned, we estimated word error rates ranging from 15% on news programs to more than 70% on talk shows or movies though this was not explicitely measured.</S>
			<S sid ="167" ssid = "14">4.2.</S>
			<S sid ="168" ssid = "15">Semantic relations.</S>
			<S sid ="169" ssid = "16">Semantic relations were automatically extracted from text corpora and relevant relations were selected so as to avoid abusive enforcement of the lexical cohesion.</S>
			<S sid ="170" ssid = "17">In this work, the MI3 criterion for syntagmatic relations and the cosine distance between normalized context vectors for paradigmatic ones were used (cf.</S>
			<S sid ="171" ssid = "18">Section 3.1.2 for more details on semantic relations automatic extraction methods).</S>
			<S sid ="172" ssid = "19">Relations were extracted from a corpus of articles from French newspapers (about 800 million words) and from reference transcripts of radio broadcast news shows (about 2 million words).</S>
			<S sid ="173" ssid = "20">All the data were lemmatized, keeping only nouns, adjectives, and non modal verbs.</S>
			<S sid ="174" ssid = "21">Semantic proximity scores given either by the MI3 criterion or by the angular distance were normalized in [0, 1].</S>
			<S sid ="175" ssid = "22">Table 1 shows, for the word “cigarette”, the five related words with the highest semantic proximity score, for syntagmatic and for paradigmatic relations.</S>
			<S sid ="176" ssid = "23">Table 1 Words with the highest association scores, in decreasing order, for the word “cigarette”, as extracted automatically.</S>
			<S sid ="177" ssid = "24">Syntagmatic Paradigmatic fumer (to smoke) cigare (cigar) paquet (pack) gitanea allumer (to light) gauloisea contrebande (smuggling) clope (ciggy) fabricant (producer) tabac (tobacco) a Brand name.</S>
			<S sid ="178" ssid = "25">Table 2 Comparison of the news and reports corpora.</S>
			<S sid ="179" ssid = "26">Average � repetitions Average confidence measure Average � words per segment Average � words per file � topic boundaries New s 1.82 0 . 6 2 1 0 6 . 4 2 2 4 3 1 2 0 2 Repo rts 2.01 0 . 5 7 4 2 4 . 1 2 4 9 5 8 6 To prevent the creation of too many links between words, a selection step is implemented to choose relevant syntagmatic and paradigmatic relations among all the existing ones so as to introduce only the best ones in the segmentation algorithm.</S>
			<S sid ="180" ssid = "27">Two selection strategies were evaluated, either retaining globally the N relations with the highest scores (global selection strategy) or retaining for each word the M best relations (word level selection strategy).</S>
			<S sid ="181" ssid = "28">Moreover, common usage words such as “go” or “year” were found to be related to many words, thus jeopardizing topic segmentation by abusively enforcing lexical cohesion.</S>
			<S sid ="182" ssid = "29">Semantic relations for words whose number of related words exceeds a given threshold are therefore discarded, the threshold being equal to the average number of relations associated with a word multiplied by a parameter γ ∈ [0, 1].</S>
			<S sid ="183" ssid = "30">4.3.</S>
			<S sid ="184" ssid = "31">Corpus.</S>
			<S sid ="185" ssid = "32">Results are reported on two distinct corpora.</S>
			<S sid ="186" ssid = "33">The first one, a news corpus, is made up of 57 news programs (≈1/2 hour each) broadcasted in February and March 2007 on the French television channel France 2, and the second one is a reports corpus composed of 16 reports on current affairs “Sept à Huit” (≈1 h each) transmitted on the French channel TF1 between September 2008 and February 2009.</S>
			<S sid ="187" ssid = "34">In the reports corpus, reports are longer (around 10–15 min) and possibly on non news topics, while the news corpus follows the classical scheme of rather short reports (usually 2–3 min).</S>
			<S sid ="188" ssid = "35">Having two distinct corpora makes it possible to study the behavior of topic segmentation on data sets with different characteristics.</S>
			<S sid ="189" ssid = "36">Indeed, in addition to different durations, the average number of topics and the average number of segments per show vary greatly between news and reports.</S>
			<S sid ="190" ssid = "37">Moreover, the number of repetitions is less important in news programs than in reports ones, as reported in Table 2, while the transcription error rate is higher on the latter due to a larger amount of non professional speakers.</S>
			<S sid ="191" ssid = "38">In each show, headlines and closing remarks were removed, these two particular parts disturbing the segmentation algorithm and being easily detectable from audiovisual clues.</S>
			<S sid ="192" ssid = "39">A reference segmentation was established by considering a topic change associated with each report, the start and end boundaries being respectively placed at the beginning of the report’s introduction and at the end of the report’s closing remarks.</S>
			<S sid ="193" ssid = "40">Note that in the news corpus, considering a topic change between each report is a choice that can be questioned as, in most cases, the first reports all refer to the main news of the day and are therefore dealing with the same broad topic.</S>
	</SECTION>
	<SECTION title="Results. " number = "5">
			<S sid ="194" ssid = "1">The goal of the article is to study to which extent confidence measures, semantic relations and interpolation techniques can help to make the lexical cohesion criterion more robust for professional video processing.</S>
			<S sid ="195" ssid = "2">We therefore study the influence of each technique and each parameter on the entire data set, reporting results for optimal parameter values.</S>
			<S sid ="196" ssid = "3">Even though the resulting figures are optimistic and do not reflect real-life behavior, their comparison with a baseline system clearly demonstrates the impact of each technique on topic segmentation.</S>
			<S sid ="197" ssid = "4">Table 3 Integration of confidence measures – news corpus – best F1-measure.</S>
			<S sid ="198" ssid = "5">δ1 δ 2 0 0 . 5 1 1 . 5 2 2 . 5 3 3 . 5 4 0 59.7 60.4 60.7 61.0 61.1 60.9 60.9 59.7 59.1 0.5 6 0 . 4 6 0 . 7 6 1 . 5 6 1 . 5 6 0 . 9 6 0 . 9 6 0 . 8 5 9 . 5 5 8 . 8 1 6 1 . 1 6 1 . 5 6 1 . 7 6 1 . 1 6 0 . 9 6 0 . 9 6 0 . 6 5 8 . 9 5 8 . 8 1.5 6 1 . 2 6 1 . 6 6 1 . 3 6 0 . 9 6 0 . 5 6 0 . 1 6 0 . 2 5 8 . 9 5 8 . 8 2 6 1 . 3 6 1 . 4 6 1 . 4 6 0 . 8 6 0 . 2 5 9 . 9 5 9 . 9 5 8 . 9 5 8 . 7 2.5 6 1 . 2 6 1 . 5 6 1 . 1 6 0 . 5 6 0 . 1 5 9 . 6 5 9 . 6 5 8 . 6 5 8 . 1 3 6 1 . 6 6 1 . 5 6 0 . 8 6 0 . 4 6 0 . 0 5 9 . 5 5 9 . 4 5 8 . 1 5 7 . 9 3.5 6 0 . 6 6 0 . 5 5 9 . 3 5 8 . 8 5 8 . 8 5 8 . 3 5 8 . 1 5 7 . 8 5 7 . 8 4 6 0 . 9 6 0 . 6 5 9 . 3 5 9 . 0 5 8 . 9 5 8 . 1 5 7 . 8 5 7 . 8 5 7 . 7 Recall and precision on topic boundaries are considered for evaluation purposes after alignment between reference and hypothesized boundaries, with a tolerance on boundary locations of respectively 10 and 30 s for news and reports, while different trade-offs between precision and recall are obtained by varying α in Eq.</S>
			<S sid ="199" ssid = "6">(4).</S>
			<S sid ="200" ssid = "7">To compare the different parameters (δ, γ , etc.), the tables in this section contain the best F1-measure obtained for an optimal value of α, i.e., the one leading to the segmentation with average segment length closest to that of the reference segmentation.</S>
			<S sid ="201" ssid = "8">5.1.</S>
			<S sid ="202" ssid = "9">Confidence measures.</S>
			<S sid ="203" ssid = "10">Results for the integration of confidence measures (CMs) are presented in Tables 3 and 4 for the news and reports corpus respectively.</S>
			<S sid ="204" ssid = "11">In these tables, the grey line stands for the values obtained when CMs are introduced during the language model estimation alone, with δ1 varying from 0 to 4, while the grey column represents their integration during the generalized probability computation alone.</S>
			<S sid ="205" ssid = "12">The dark grey cell therefore corresponds to the baseline method (δ1 = δ2 = 0) and bold values refer to the highest scores.</S>
			<S sid ="206" ssid = "13">Recall and precision curves are reported in Fig.</S>
			<S sid ="207" ssid = "14">1.</S>
			<S sid ="208" ssid = "15">We can observe that confidence measures integration leads to an improvement in the topic segmentation quality that is statistically significant (t-test).</S>
			<S sid ="209" ssid = "16">Indeed, the best F1-measure is improved by 2 points for the news corpus and by 5 points for the reports corpus.</S>
			<S sid ="210" ssid = "17">Moreover, results show that the behavior of the integration of confidence measures is different for the two corpora depending on whether CMs are integrated during generalized probability or language model computation.</S>
			<S sid ="211" ssid = "18">For the news corpus, results are better when confidence measures are integrated during the generalized probability computation alone rather than during the language model computation alone; while for the reports corpus, experiments lead to the opposite conclusion.</S>
			<S sid ="212" ssid = "19">This phenomenon is also observed on the recall/precision curves, where the integration during the generalized probability calculation gives better results for the news corpus, while integration during the language model computation is more efficient for reports.</S>
			<S sid ="213" ssid = "20">Table 4 Integration of confidence measures – reports corpus – best F1-measure.</S>
			<S sid ="214" ssid = "21">δ1 δ 2 0 0 . 5 1 1 . 5 2 2 . 5 3 3 . 5 4 0 54.9 55.0 56.8 54.6 54.7 55.6 58.3 59.9 59.1 0.5 5 3 . 4 5 6 . 1 5 5 . 3 5 6 . 1 5 4 . 7 5 4 . 7 5 5 . 0 5 7 . 9 5 8 . 2 1 5 5 . 3 5 6 . 5 5 5 . 4 5 5 . 6 5 4 . 7 5 5 . 0 5 5 . 0 5 9 . 1 5 8 . 2 1.5 5 4 . 4 5 5 . 6 5 5 . 6 5 6 . 3 5 4 . 7 5 5 . 0 5 5 . 0 5 6 . 9 5 8 . 6 2 5 4 . 9 5 6 . 0 5 6 . 3 5 5 . 0 5 5 . 3 5 5 . 3 5 7 . 0 5 6 . 1 5 6 . 1 2.5 5 5 . 1 5 5 . 7 5 6 . 3 5 5 . 0 5 5 . 3 5 7 . 0 5 7 . 0 5 6 . 1 5 6 . 5 3 5 5 . 3 5 6 . 8 5 6 . 3 5 5 . 0 5 6 . 7 5 7 . 0 5 8 . 6 5 7 . 7 5 8 . 1 3.5 5 5 . 3 5 6 . 8 5 6 . 3 5 6 . 3 5 8 . 3 5 8 . 6 5 8 . 6 5 7 . 7 5 7 . 7 4 5 4 . 5 5 6 . 8 5 6 . 3 5 9 . 9 5 9 . 5 5 8 . 6 5 7 . 7 5 7 . 7 5 7 . 1 a 80 Baseline b 60 Baseline 75 03 20 70 11 65 60 55 50 45 40 45 50 55 60 65 70 Recall news corpus 55 03 30 50 33 45 40 35 30 25 50 55 60 65 70 75 80 85 90 Recall reports corpus Fig.</S>
			<S sid ="215" ssid = "22">1.</S>
			<S sid ="216" ssid = "23">Integration of confidence measures – recall/precision curves (figures in the legend correspond to resp.</S>
			<S sid ="217" ssid = "24">δ1 and δ2 ).</S>
			<S sid ="218" ssid = "25">Another difference between the two corpora is the optimal values of δ1 and δ2 . For the news corpus, the highest F1-measure is obtained when δ1 and δ2 are quite small (both equal to 1) while for the reports corpus the optimal values for δ1 and δ2 are greater.</S>
			<S sid ="219" ssid = "26">This difference can be explained by the fact that for high values of δ, c(wi )δ becomes negligible except for words whose confidence measure is very close to 1.</S>
			<S sid ="220" ssid = "27">As the proportion of words with a CM less than 0.9 is more important in the reports data, the impact of the confidence measures is more perceptible on this data set and higher values of δ lead to a greater improvement.</S>
			<S sid ="221" ssid = "28">Similarly, the impact of the transcription quality on the efficiency of the integration of confidence measures is also highlighted by the difference between the improvements observed for the two corpora.</S>
			<S sid ="222" ssid = "29">Fig.</S>
			<S sid ="223" ssid = "30">1 shows that confidence measures have more impact on the reports corpus than on the news corpus.</S>
			<S sid ="224" ssid = "31">This difference shows that confidence measures are of utmost importance when transcription quality decreases.</S>
			<S sid ="225" ssid = "32">From a more qualitative point of view, we also observed that accounting for confidence measures not only increases the number of correct boundaries detected but also improves boundary locations.</S>
			<S sid ="226" ssid = "33">Indeed, boundary locations are more precise when using confidence measures, even if this fact does not show in the recall/precision curves because of the tolerated gap on the boundary positions.</S>
			<S sid ="227" ssid = "34">We also observed that the quality of confidence measures have an impact on boundary precision.</S>
			<S sid ="228" ssid = "35">Indeed, using confidence measures, improved thanks to high-level linguistic features with a classifier (Fayolle et al., 2010), resulted in more accurate boundary locations.</S>
			<S sid ="229" ssid = "36">To conclude, experiments carried out on the two corpora confirm the hypothesis that the integration of confidence measures in the lexical cohesion computation allows the criterion to be less sensitive to transcription errors.</S>
			<S sid ="230" ssid = "37">Indeed, amending the calculation method of lexical cohesion to include CMs provides a significant improvement in the quality of topic segmentation.</S>
			<S sid ="231" ssid = "38">Confidence measures seem also to lead to an increase in the number of correct boundaries detected but also to the displacement of previously recognized borders, moving them closer to reference boundaries.</S>
			<S sid ="232" ssid = "39">5.2.</S>
			<S sid ="233" ssid = "40">Semantic relations.</S>
			<S sid ="234" ssid = "41">Various tests, on the choice of the type and number of semantic relations introduced and on the selection strategy (global or word level) or the filtering technique have produced a large number of results.</S>
			<S sid ="235" ssid = "42">The most convincing ones are discussed here.</S>
			<S sid ="236" ssid = "43">Tables 5 and 6 summarize the results on the news and reports corpus respectively, with values for syntagmatic relations in the leftmost part of the tables and results for paradigmatic ones in the rightmost part.</S>
			<S sid ="237" ssid = "44">For each kind of semantic relations, the two selection strategies are evaluated for different numbers of semantic relations, 2–10 per word for the word-level strategy and 5000–90,000 for the global strategy.</S>
			<S sid ="238" ssid = "45">Finally, results for the filtering technique, used to discard semantic relations for words whose number of related words exceeds a given threshold, are presented for values of parameter γ between 0.2 and 1.</S>
			<S sid ="239" ssid = "46">In Fig.</S>
			<S sid ="240" ssid = "47">2, recall/precision curves for the integration of paradigmatic relations are presented.</S>
			<S sid ="241" ssid = "48">For the news corpus, semantic relations help the lexical cohesion criterion to be more robust to the limited number of vocabulary repetitions due to short segments and/or the use of synonyms or related words.</S>
			<S sid ="242" ssid = "49">Indeed, the best F1-measure is improved in a statistically significant way (t-test) by 1.4 when syntagmatic relations are introduced and by 1.2 for Table 5 Integration of semantic relations – news corpus – best F1-measures.</S>
			<S sid ="243" ssid = "50">γ Syntagmatic Paradigmatic Wor d level Glob al Wor d level Glob al 2 3 1 0 5k 20 k 9 0 k 2 3 1 0 5k 20 k 9 0 k 0.2 59.7 59.7 5 9.</S>
			<S sid ="244" ssid = "51">7 59.7 59.</S>
			<S sid ="245" ssid = "52">7 5 9.</S>
			<S sid ="246" ssid = "53">7 59.7 59.7 5 9.</S>
			<S sid ="247" ssid = "54">7 59.7 59.</S>
			<S sid ="248" ssid = "55">7 5 9.</S>
			<S sid ="249" ssid = "56">7 0.4 59.7 59.7 6 0.</S>
			<S sid ="250" ssid = "57">8 59.7 59.</S>
			<S sid ="251" ssid = "58">8 5 9.</S>
			<S sid ="252" ssid = "59">7 59.7 59.7 5 9.</S>
			<S sid ="253" ssid = "60">7 59.7 59.</S>
			<S sid ="254" ssid = "61">7 6 0.</S>
			<S sid ="255" ssid = "62">0 0.5 59.7 59.7 6 0.</S>
			<S sid ="256" ssid = "63">1 59.7 60.</S>
			<S sid ="257" ssid = "64">3 6 0.</S>
			<S sid ="258" ssid = "65">4 59.7 59.7 6 0.</S>
			<S sid ="259" ssid = "66">0 59.7 59.</S>
			<S sid ="260" ssid = "67">7 6 0.</S>
			<S sid ="261" ssid = "68">3 0.6 59.7 59.6 6 0.</S>
			<S sid ="262" ssid = "69">5 59.7 60.</S>
			<S sid ="263" ssid = "70">0 6 0.</S>
			<S sid ="264" ssid = "71">1 59.7 59.7 6 0.</S>
			<S sid ="265" ssid = "72">2 59.7 60.</S>
			<S sid ="266" ssid = "73">1 6 0.</S>
			<S sid ="267" ssid = "74">1 0.8 60.0 59.8 6 0.</S>
			<S sid ="268" ssid = "75">9 60.5 60.</S>
			<S sid ="269" ssid = "76">6 6 0.</S>
			<S sid ="270" ssid = "77">6 59.8 60.3 6 0.</S>
			<S sid ="271" ssid = "78">0 59.9 60.</S>
			<S sid ="272" ssid = "79">6 6 0.</S>
			<S sid ="273" ssid = "80">3 1 58.9 61.1 5 9.</S>
			<S sid ="274" ssid = "81">8 60.5 60.</S>
			<S sid ="275" ssid = "82">8 6 1.</S>
			<S sid ="276" ssid = "83">0 60.9 60.4 6 0.</S>
			<S sid ="277" ssid = "84">2 60.0 60.</S>
			<S sid ="278" ssid = "85">3 6 0.</S>
			<S sid ="279" ssid = "86">3 Table 6 Integration of semantic relations – reports corpus – best F1-measures.</S>
			<S sid ="280" ssid = "87">γ Syntagmatic Paradigmatic Wor d level Glob al Wor d level Glob al 2 3 1 0 5k 20 k 9 0 k 2 3 1 0 5k 20 k 9 0 k 0.2 54.9 54.9 5 4.</S>
			<S sid ="281" ssid = "88">9 54.9 54.</S>
			<S sid ="282" ssid = "89">9 5 4.</S>
			<S sid ="283" ssid = "90">9 54.9 54.9 5 4.</S>
			<S sid ="284" ssid = "91">9 54.9 54.</S>
			<S sid ="285" ssid = "92">9 5 4.</S>
			<S sid ="286" ssid = "93">9 0.4 54.9 54.9 5 5.</S>
			<S sid ="287" ssid = "94">7 54.9 55.</S>
			<S sid ="288" ssid = "95">5 5 5.</S>
			<S sid ="289" ssid = "96">5 54.9 54.9 5 5.</S>
			<S sid ="290" ssid = "97">5 54.9 54.</S>
			<S sid ="291" ssid = "98">9 5 4.</S>
			<S sid ="292" ssid = "99">9 0.5 54.9 54.9 5 4.</S>
			<S sid ="293" ssid = "100">0 54.9 55.</S>
			<S sid ="294" ssid = "101">5 5 5.</S>
			<S sid ="295" ssid = "102">5 54.9 54.9 5 5.</S>
			<S sid ="296" ssid = "103">1 54.9 54.</S>
			<S sid ="297" ssid = "104">9 5 5.</S>
			<S sid ="298" ssid = "105">1 0.6 54.9 54.9 5 3.</S>
			<S sid ="299" ssid = "106">3 54.9 54.</S>
			<S sid ="300" ssid = "107">4 5 4.</S>
			<S sid ="301" ssid = "108">4 54.9 54.9 5 4.</S>
			<S sid ="302" ssid = "109">0 54.9 54.</S>
			<S sid ="303" ssid = "110">9 5 4.</S>
			<S sid ="304" ssid = "111">4 0.8 56.5 55.7 5 3.</S>
			<S sid ="305" ssid = "112">4 55.5 53.</S>
			<S sid ="306" ssid = "113">1 5 3.</S>
			<S sid ="307" ssid = "114">1 55.8 53.2 5 3.</S>
			<S sid ="308" ssid = "115">6 54.9 55.</S>
			<S sid ="309" ssid = "116">8 5 4.</S>
			<S sid ="310" ssid = "117">8 1 54.7 54.6 5 1.</S>
			<S sid ="311" ssid = "118">6 55.1 54.</S>
			<S sid ="312" ssid = "119">0 5 4.</S>
			<S sid ="313" ssid = "120">0 55.2 53.0 5 3.</S>
			<S sid ="314" ssid = "121">0 54.9 52.</S>
			<S sid ="315" ssid = "122">3 5 4.</S>
			<S sid ="316" ssid = "123">0 paradigmatic relations (Table 5).</S>
			<S sid ="317" ssid = "124">Moreover, even if the best values of the F1-measure are reached for syntagmatic relations, the global improvement is higher with paradigmatic ones.</S>
			<S sid ="318" ssid = "125">For the reports corpus, the improvement is much smaller than for the news one.</S>
			<S sid ="319" ssid = "126">Indeed, Fig.</S>
			<S sid ="320" ssid = "127">2 shows that results for the integration of paradigmatic relations—which are comparable to those for syntagmatic relations, cf.</S>
			<S sid ="321" ssid = "128">Table 6—are almost equivalent to the baseline.</S>
			<S sid ="322" ssid = "129">Indeed, a significance test shows that the improvement observed when using semantic relations is not significant.</S>
			<S sid ="323" ssid = "130">The difference in results for both corpora can be explained by the fact that in the reports corpus the number of reiterations and the segments lengths are more important than in the news corpus.</S>
			<S sid ="324" ssid = "131">Thus, the use of semantic relations is not as effective for this corpus.</S>
			<S sid ="325" ssid = "132">Moreover, in this work, semantic relations were extracted from a corpus composed of news articles and are therefore less suited for the reports corpus.</S>
			<S sid ="326" ssid = "133">However, it is interesting to note that using non appropriate relations does not hurt performance.</S>
			<S sid ="327" ssid = "134">a 80 75 70 65 60 55 50 45 40 Baseline Word level2 Global - 20k b 60 55 50 45 40 35 30 25 Baseline Wo rd le v el2 Global - 90k 45 50 55 60 65 70 Recall news corpus 50 55 60 65 70 75 80 85 90 Recall reports corpus Fig.</S>
			<S sid ="328" ssid = "135">2.</S>
			<S sid ="329" ssid = "136">Integration of paradigmatic relations – recall/precision curves.</S>
			<S sid ="330" ssid = "137">Tabl e 7 Inter polat ion – best F1 meas ures.</S>
			<S sid ="331" ssid = "138">λ Cou nt Line ar news r e p o r t s news r e p o r t s Base line 59.7 5 4 . 9 59.7 5 4 . 9 0 3.8 1 4 . 8 12.2 1 0 . 4 0.1 63.7 5 4 . 0 60.8 4 7 . 8 0.2 64.6 5 3 . 4 61.0 4 7 . 3 0.3 64.2 5 2 . 8 61.6 5 0 . 0 0.4 64.2 5 5 . 6 61.7 5 0 . 3 0.5 64.1 5 4 . 3 62.0 5 1 . 1 0.6 64.1 5 2 . 4 61.9 5 1 . 7 0.7 63.7 5 3 . 1 61.9 5 0 . 9 0.8 62.7 5 3 . 3 61.8 5 2 . 7 0.9 61.6 5 4 . 9 60.6 5 3 . 5 1 59.9 5 4 . 9 59.7 5 4 . 9 Table 6 shows that the integration of semantic relations damages the segmentation quality when a too large number of relations is introduced—i.e., when γ is high—degradation which is proportional to the number of relations included.</S>
			<S sid ="332" ssid = "139">This behavior can also be observed for the news corpus but for values of γ greater than 1.</S>
			<S sid ="333" ssid = "140">Therefore, the filtering technique is crucial to avoid considering relations that seem less adapted in the context of topic segmentation because they introduce some noise (i.e., they connect words and segments that should not be).</S>
			<S sid ="334" ssid = "141">The stronger effect of the filtering technique on the reports corpus can be explained by the fact that many relations are out of domain for this corpus.</S>
			<S sid ="335" ssid = "142">Therefore, more general relations—associated with words such as “year” and “go”—that introduce noise appear in a higher proportion and have more impact on reports than on news.</S>
			<S sid ="336" ssid = "143">Finally, concerning the technique used for the semantic relations selection, no difference was found between the global selection strategy and the word level selection strategy for the news corpus.</S>
			<S sid ="337" ssid = "144">In contrast, for the reports corpus, the global selection strategy is better, both for paradigmatic and syntagmatic relations.</S>
			<S sid ="338" ssid = "145">We believe that the word level strategy selects relations which are the most characteristic of the corpus they are learned from, while the global strategy is more likely to pick more general relations.</S>
			<S sid ="339" ssid = "146">Thus, as relations are extracted from a corpus composed of news articles, the ones selected by the word level strategy are more suitable for the news corpus but less adapted for the reports one.</S>
			<S sid ="340" ssid = "147">To conclude, the introduction of semantic relations can improve topic segmentation when the number of vocabulary repetitions is low.</S>
			<S sid ="341" ssid = "148">However, it is essential to limit the number of relations considered.</S>
			<S sid ="342" ssid = "149">5.3.</S>
			<S sid ="343" ssid = "150">Interpolation.</S>
			<S sid ="344" ssid = "151">Finally, results for language model interpolation are given in Table 7.</S>
			<S sid ="345" ssid = "152">Values for the count interpolation strategy are presented in the leftmost part of the table while results for the linear interpolation of probabilities are located in the rightmost one.</S>
			<S sid ="346" ssid = "153">For the two corpora, we can observe that improvements are higher with the interpolation of the counts than with the linear interpolation of probabilities.</S>
			<S sid ="347" ssid = "154">Indeed, the best value for the F1-measure is significantly increased by 4.9 against 2.3 for the news corpus (t-test) and by 0.7 against 0 for the reports corpus.</S>
			<S sid ="348" ssid = "155">This behavior is explained by the fact that, as mentioned in Section 3.2, for the linear interpolation of probabilities, frequent words in the text get a high probability regardless of their frequencies in the segment, while non frequent ones always get a low probability.</S>
			<S sid ="349" ssid = "156">However, because of the renormalization in Eq.</S>
			<S sid ="350" ssid = "157">(9), this fact is less detrimental for count interpolation than for probability interpolation.</S>
			<S sid ="351" ssid = "158">Table 7, as well as the recall/precision curves in Fig.</S>
			<S sid ="352" ssid = "159">3, shows that the interpolation techniques are much less effective for the reports corpus than for the news corpus.</S>
			<S sid ="353" ssid = "160">This observation can be easily explained by the fact that segments in reports are four times longer than the ones in the news.</S>
			<S sid ="354" ssid = "161">Thus, the use of interpolation has no effect for the calculation of lexical cohesion in the reports segments since the initial language models were already well estimated.</S>
			<S sid ="355" ssid = "162">This can also be seen from the optimal value of parameter λ, representing the weight of the language model of the segment in the interpolation, that is greater for reports than for news.</S>
			<S sid ="356" ssid = "163">a 80 75 70 65 60 55 50 45 40 Baseline Count (0.2) Linear (0.5) b 60 55 50 45 40 35 30 25 Baseline C ou nt (0 .5 ) Linear (0.9) 45 50 55 60 65 70 75 Recall news corpus 50 55 60 65 70 75 80 85 90 Recall reports corpus 5.4.</S>
			<S sid ="357" ssid = "164">Combination.</S>
			<S sid ="358" ssid = "165">Fig.</S>
			<S sid ="359" ssid = "166">3.</S>
			<S sid ="360" ssid = "167">Interpolation – recall/precision curves.</S>
			<S sid ="361" ssid = "168">Confidence measures, semantic relations and language model interpolation can be used in conjunction.</S>
			<S sid ="362" ssid = "169">In this part, the joint integration of these three elements is evaluated to estimate to what extent their advantages can be combined.</S>
			<S sid ="363" ssid = "170">Confidence + Interpolation.</S>
			<S sid ="364" ssid = "171">When confidence measures and interpolation techniques are used together, the improvements observed for both are combined for the news corpus.</S>
			<S sid ="365" ssid = "172">In this case, the CMs introduced are the ones that exhibit the most important amelioration (with δ1 = δ2 = 1) and the interpolation technique is the interpolation of counts with λ equals to 0.2.</S>
			<S sid ="366" ssid = "173">Fig.</S>
			<S sid ="367" ssid = "174">4 shows that the combination of the two cues leads to a greater improvement, that is statistically significant (t-test), in the topic segmentation quality than the sole interpolation, especially for high recall values.</S>
			<S sid ="368" ssid = "175">Thus, we can conclude that the problems inherent in TV programs, i.e., transcription errors, potentially short segments and limited number of reiterations, are partially handled for the news corpus.</S>
			<S sid ="369" ssid = "176">For reports, unsurprisingly, comparable results are obtained with or without interpolation when confidence measures are used (cf.</S>
			<S sid ="370" ssid = "177">Fig.</S>
			<S sid ="371" ssid = "178">4), interpolation alone yielding no improvement on this corpus.</S>
			<S sid ="372" ssid = "179">However it is important to note that the combination does not damage topic segmentation compared to the sole use of CMs.</S>
			<S sid ="373" ssid = "180">Confidence + Semantics.</S>
			<S sid ="374" ssid = "181">Confidence measures can also be combined with semantic relations, whether syntagmatic or paradigmatic.</S>
			<S sid ="375" ssid = "182">The relations introduced are paradigmatic ones, selected thanks to the word level selection technique (2 relations per word).</S>
			<S sid ="376" ssid = "183">For the corpus composed of broadcast news, topic segmentation is slightly, although significantly, improved compared to the use of confidence measures or semantic relations alone.</S>
			<S sid ="377" ssid = "184">For reports, the combination does not lead to better topic segmentation, compared with the use of confidence measures alone (cf.</S>
			<S sid ="378" ssid = "185">Table 8).</S>
			<S sid ="379" ssid = "186">This observation is not surprising as semantic relations do not give real improvements for this corpus as explained in Section 5.2.</S>
			<S sid ="380" ssid = "187">However, as before, the use of two different cues does not damage the topic segmentation quality.</S>
			<S sid ="381" ssid = "188">Semantics + Interpolation (+ Confidence measures).</S>
			<S sid ="382" ssid = "189">Semantic relations can also be combined with interpolation techniques.</S>
			<S sid ="383" ssid = "190">This combination, possibly associated with the use of confidence measures, does not improve the topic a 80 75 70 65 60 55 50 45 40 Baseline Confidence Interpolation Conf+Int b 60 55 50 45 40 35 30 25 Baseline C on fi de nc e Interpolation Conf+Int 45 50 55 60 65 70 75 Recall news corpus 50 55 60 65 70 75 80 85 90 Recall reports corpus Fig.</S>
			<S sid ="384" ssid = "191">4.</S>
			<S sid ="385" ssid = "192">Integration of confidence measures and interpolation – recall/precision curves.</S>
			<S sid ="386" ssid = "193">Best F1 meas ure valu es for all possi ble com binat ion.</S>
			<S sid ="387" ssid = "194">n e w s repo rts Base line 5 9 . 7 54 .9 Conf iden ce meas ures (CM s) 6 1 . 6 58 .3 Sem antic s 6 0 . 9 55 .7 Inter polat ion 6 4 . 6 55 .6 CMs + inter polat ion 6 4 . 4 55 .4 CMs + sema ntics 6 2 . 0 58 .3 Sem antic s + inter polat ion 6 4 . 3 54 .3 Sem antic s + inter polat ion + CMs 6 4 . 2 57 .3 segmentation quality, either for the news or the reports corpus.</S>
			<S sid ="388" ssid = "195">As semantic relations and especially interpolation techniques both allow to overcome problems related to small segments, their combination is redundant.</S>
			<S sid ="389" ssid = "196">Finally, Table 8 summarizes all the results obtained with the exploitation of confidence measures, semantic relations and language model interpolation, used together or separately.</S>
			<S sid ="390" ssid = "197">This table gives an indication of the impact of each clue on the topic segmentation quality.</S>
			<S sid ="391" ssid = "198">However, the best F1-measure values presented in the table are extracted from the result for a particular value of the α parameter.</S>
			<S sid ="392" ssid = "199">Therefore, values in the table do not necessarily correspond to the overall improvement achieved through the use of one or more clues.</S>
			<S sid ="393" ssid = "200">For example, the F1-measure obtained for the reports corpus when confidence measures and interpolation technique are used is much lower than the one obtained for the exploitation of CMs alone, while Fig.</S>
			<S sid ="394" ssid = "201">4 shows that the results are equivalent.</S>
			<S sid ="395" ssid = "202">So, it is important to refer also to the recall/precision curves presented throughout this article to really understand to which extent the different clues can help the lexical cohesion criterion to be more robust to spoken documents specifics.</S>
	</SECTION>
	<SECTION title="Conclusion and future work. " number = "6">
			<S sid ="396" ssid = "1">In this paper, we have improved and extended a probabilistic measure of lexical cohesion to include confidence measures and semantic relations, making use of language model interpolation.</S>
			<S sid ="397" ssid = "2">This results in a lexical cohesion measure more robust to TV program specifics while being generic enough to be effective on different kinds of programs.</S>
			<S sid ="398" ssid = "3">First, it has been shown that the use of semantic relations and interpolation techniques improve the topic segmentation quality of TV programs divided in short segments and in which lexical repetition rates are low.</S>
			<S sid ="399" ssid = "4">It was also pointed out that the integration of confidence measures has more impact when the transcription quality is low, as for the reports corpus.</S>
			<S sid ="400" ssid = "5">Finally, we demonstrated that these different elements can be used together to combine their advantages.</S>
			<S sid ="401" ssid = "6">Interestingly, it was also found that this combination never leads to a deterioration of topic segmentation.</S>
			<S sid ="402" ssid = "7">These results clearly lead to the integration of other features, such as prosodic ones, to detect lexical stresses or speaker changes for example.</S>
			<S sid ="403" ssid = "8">Moreover, as the integration of confidence measures has a positive impact, we think it would be interesting to investigate the application of the topic segmentation method on intermediary outputs of the ASR system (such as word graphs or confusion networks) rather than on the final transcripts.</S>
			<S sid ="404" ssid = "9">Finally, as mentioned in Section 4.3, the structure of news programs is clearly hierarchical as, in most cases, the first reports all refer to the main news of the day and are therefore dealing with the same broad topic.</S>
			<S sid ="405" ssid = "10">However, in this work, only linear topic segmentation has been considered.</S>
			<S sid ="406" ssid = "11">Therefore, a long view prospect of this paper is to develop a method that can handle a hierarchical topic segmentation of our data.</S>
	</SECTION>
	<SECTION title="Acknowledgement">
			<S sid ="407" ssid = "12">This work was partially funded by OSEO, French state agency for innovation, in the framework of the Quaero project.</S>
	</SECTION>
</PAPER>
