<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a new discriminative reordeting model for statistical machine translation.</S>
		<S sid ="2" ssid = "2">The model employs a standard data-driven depen­ dency parser to predict reordetings based on syntactic information.</S>
		<S sid ="3" ssid = "3">This is made possi­ ble through the introduction of a reorderiog structure, which is a word alignment structure where the target word order is transposed onto the source sentence as a path.</S>
		<S sid ="4" ssid = "4">The approach is iotegrated io a phrase-based system.</S>
		<S sid ="5" ssid = "5">Exper­ iments show a large iocrease io long distance reorderiogs.</S>
		<S sid ="6" ssid = "6">Both automatic and human evalu­ ations show substantial increases io translation quality on an English to German task.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Handling word order differences between languages is one of the main challenges of statistical machine translation (SMT) today.</S>
			<S sid ="8" ssid = "8">These differences are of­ ten most natorally handled at a syntactic level, since they pertaio to entire syntactic constituents.</S>
			<S sid ="9" ssid = "9">We present a syntactically motivated discrimioa­ tive reordering model.</S>
			<S sid ="10" ssid = "10">The model exploits a reorder­ ing structure, which is a word alignment where the target sentence is unknown.</S>
			<S sid ="11" ssid = "11">This structure allows us to treat the reordering problem as a dependency parsing problem.</S>
			<S sid ="12" ssid = "12">We use a standard data-driven de­ pendency parser to predict reorderings instead of de­ pendencies.</S>
			<S sid ="13" ssid = "13">This is integrated into a phrase-based SMT (PSMT) framework (Koehn et al., 2003).</S>
	</SECTION>
	<SECTION title="Reordering Structure. " number = "2">
			<S sid ="14" ssid = "1">Word alignments are often used to display the rela­ tion between a translation and its source by linking up equivalent words.</S>
			<S sid ="15" ssid = "2">Here we transpose the word alignment information to a representation over a sin­ gle sentence.</S>
			<S sid ="16" ssid = "3">This can be done by representing the corresponding order of the words of the opposite sentences as a path over the words of the current sen­ tence.</S>
			<S sid ="17" ssid = "4">In this work we will focus on transposing the word alignment onto the source sentence by anno­ tating it with the order in which the aligned target words occur.</S>
			<S sid ="18" ssid = "5">This is done in the form of a reorder­ ing structure, which is a word alignment, where the target sentence is unknown.</S>
			<S sid ="19" ssid = "6">The idea of a reorder­ ing structure is similar to the underlying concept of source position target order (Elmiog, 2008) or visit sequence (Ge, 2010), but the extraction algorithm and conceptual representation is different.</S>
			<S sid ="20" ssid = "7">Figure I gives a simple example of how a reorder­ ing structure is created.</S>
			<S sid ="21" ssid = "8">The figure contains a source and a target sentence with a word alignment in be­ tween and the corresponding reordering structure on the source sentence.</S>
			<S sid ="22" ssid = "9">The numbers are merely used to explaio the correlation between links and edges.</S>
			<S sid ="23" ssid = "10">They are not part of the structure.</S>
			<S sid ="24" ssid = "11">The reordering structure is created by following the target words from left to right.</S>
			<S sid ="25" ssid = "12">The first target word is linked to the first source word, and the graph therefore starts by going to the first word.</S>
			<S sid ="26" ssid = "13">Then the second target word links to the third source word, so the graph proceeds to this word, and so on.</S>
			<S sid ="27" ssid = "14">The resulting rep­ resentation consists of the source sentence annotated with a reordering structure which reflects the word order of the corresponding target sentence.</S>
			<S sid ="28" ssid = "15">One requirement for the structure is that all source words partake in an edge.</S>
			<S sid ="29" ssid = "16">The role of null-linked source words in the structure cannot be uniquely de­ termined from a word alignment.</S>
			<S sid ="30" ssid = "17">It can either be inserted after the previous word or before the fol­ lowing word in the structure.</S>
			<S sid ="31" ssid = "18">We employ a syntactic closeness measure to de­ cide between left and right attachment.</S>
			<S sid ="32" ssid = "19">The distance from the null-linked word up to the first common Here he comes Ietzt komm.t H e r e h e c o m e s er Figure 1: Example of a reordering structure and its underlying word alignment.</S>
			<S sid ="33" ssid = "20">Numbers are added for explanatory reasons indicating correlation between links and edges.</S>
			<S sid ="34" ssid = "21">They are not part of the structure.</S>
			<S sid ="35" ssid = "22">ROOT &lt;ROOT&gt; Musharraf &apos;s Last Act ? 0 2 3 4 5 Ftgure 2:Syntactic dependency structure illustrating syn­ tactic closeness.</S>
			<S sid ="36" ssid = "23">node with the left and right neighbor is measured as the nwnber of edges passed on the way.</S>
			<S sid ="37" ssid = "24">If this is the same for bothneighbors, we choose the neighbor with the shortest distance up to the common node.If this is also the same, we attach right.</S>
			<S sid ="38" ssid = "25">As an exam­ ple, if we assume Last in figure 2 is null-linked, we need to decide whether to insert relative to the left word &apos;a or right word Act.</S>
			<S sid ="39" ssid = "26">Common ancestor node is 4 with both neighbors, so distance up from Last is the same.</S>
			<S sid ="40" ssid = "27">We therefore rely on distance up from neighbors, which is 2 passed edges for &apos;s and 0 for Act.Act is therefore syntactically closer, and we at­ tach right Algorithm 1 illustrates the construction of the reordering structure formally.</S>
			<S sid ="41" ssid = "28">The measure is linguistically motivated.</S>
			<S sid ="42" ssid = "29">The common ancestor defines the smallest spanning con­ stituent containing both words.</S>
			<S sid ="43" ssid = "30">The shorter the path up, the smaller the span, and the syntactically closer the words.</S>
			<S sid ="44" ssid = "31">If we simply measured the total path length, we might get fooled by a long path down.</S>
			<S sid ="45" ssid = "32">Anexample is a noun preceded by a preposition and followed by a relative clause.</S>
			<S sid ="46" ssid = "33">Here, the noun itself is the common ancestor with the relative pronO&apos;IUl, i.e. it has a 0 distance up, but the down distance may be long.</S>
			<S sid ="47" ssid = "34">The total path would not classify the noun closest to the relative clause, since the distance to the preceding preposition is 1 up and 0 down.</S>
			<S sid ="48" ssid = "35">The advantage of the reordering structure r sentation is that it is a word alignment representation without explicit reference to the target sentence.</S>
			<S sid ="49" ssid = "36">Re sourcePosition previous = rootPosition; foreach targetPosition t = 0, ..., T do foreach sourcePosition s linlcetfi&apos;o t do add ge from previous to s; I prevtou.ss; end end foreach aourcePosition s E ntdlLinked do if (81) is ayn.tactically closest to 8 then I insert 8 after (s- 1); else I inserts before (s +1); end end Algorithm 1: Algorithm for creating reordering structure from word aligned sentence positions.</S>
			<S sid ="50" ssid = "37">ordering in machine translation can be viewed as a similar challenge, where we want to find the word alignment knowing only the source sentence.</S>
			<S sid ="51" ssid = "38">The reordering structure provides us with a focus on this problem, since it refers only to the source sentence and therefore may be predicted from this.</S>
			<S sid ="52" ssid = "39">The relation between the reordering structure and the word alignment is not reversible.</S>
			<S sid ="53" ssid = "40">Whereas all reordering structures correspond to a unique word alignment, the reverse is not the case.</S>
			<S sid ="54" ssid = "41">Certain word alignments are not representable by a reordering structure.</S>
			<S sid ="55" ssid = "42">In particular, structures where a source word is linked to target words that are separated by target words linking to a different source word cannot be represented without introducing recur­ sion into the structure as exemplified by figure 3.</S>
			<S sid ="56" ssid = "43">As a consequence, the structure becomes ambigu­ ous, since a single word would have more outgoing edges that could be traversed in different orders.</S>
			<S sid ="57" ssid = "44">Crego &amp; Yvon (2009) face similar challenges Er isst nicht . m \5 He does not eat Er isst nicht . Figure 3: Example of a word aligmnent that cannot be represented unrecursively in a reordering structure.</S>
			<S sid ="58" ssid = "45">The language pair is here reversed, since we did not find these structures in the direction we are working with.</S>
			<S sid ="59" ssid = "46">when monotonizing the parallel sentences.</S>
			<S sid ="60" ssid = "47">They handle the problem by making a source word clone for each discontinuous unit it is linked to.</S>
			<S sid ="61" ssid = "48">We do not adopt this approach here, since these structures are not a major concern with PSMT.</S>
			<S sid ="62" ssid = "49">PSMT has two means for handling word order differences be­ tween languages; phrase-internal reordering, where the equivalent words of a phrase pair appear in dif­ ferent orders, and phrase-external reordering, where the phrases are combined in a different order than they appeared in the source sentence.</S>
			<S sid ="63" ssid = "50">Only phrase­ internal reordering can lead to this problematic word alignment in application, since a single source word token cannot participate in more phrases in the same translation.</S>
			<S sid ="64" ssid = "51">Since phrase-internal reordering is very reliable, the main purpose of the reordering model is to guide the phrase-external reordering, which will not produce these link constellations.</S>
	</SECTION>
	<SECTION title="Reordering Structure Modelling. " number = "3">
			<S sid ="65" ssid = "1">In this work, we will pursue the idea that the reorder­ ing structure is conceptually similar to an unlabeled syntactic dependency structure.</S>
			<S sid ="66" ssid = "2">We therefore use the MSTParser (McDonald et al., 2005), a state-of-the­ art data-driven dependency parser, to model the re­ ordering structure.</S>
			<S sid ="67" ssid = "3">The basic idea is that the parser predicts the most favorable word alignment to the target sentence based on the source sentence.</S>
			<S sid ="68" ssid = "4">These predictions are made before translation and passed to the decoder.</S>
			<S sid ="69" ssid = "5">The level of information included in the reordering structure model therefore only depends on what fea­ tures we are able to design for the parser, and is fully independent of the PSMT system.</S>
			<S sid ="70" ssid = "6">The default output of the parser is the most prob­ able reordering structure given its model.</S>
			<S sid ="71" ssid = "7">This is too restrictive for our purpose.</S>
			<S sid ="72" ssid = "8">The model would often not be relevant, if it expected a single word order To position 1 2 3 4 =0 ·...::.:.: . 0&quot;&quot;&apos; p..</S>
			<S sid ="73" ssid = "9">El 0 0 0.</S>
			<S sid ="74" ssid = "10">16 1.</S>
			<S sid ="75" ssid = "11">03 1.</S>
			<S sid ="76" ssid = "12">21 1.</S>
			<S sid ="77" ssid = "13">39 1 0.</S>
			<S sid ="78" ssid = "14">5 0 1.</S>
			<S sid ="79" ssid = "15">0 1 0.</S>
			<S sid ="80" ssid = "16">5 1 2 0.</S>
			<S sid ="81" ssid = "17">9 1 1.</S>
			<S sid ="82" ssid = "18">1 6 1.</S>
			<S sid ="83" ssid = "19">4 8 3 1.</S>
			<S sid ="84" ssid = "20">2 2 1.</S>
			<S sid ="85" ssid = "21">3 4 1.</S>
			<S sid ="86" ssid = "22">1 4 4 0.</S>
			<S sid ="87" ssid = "23">2 3 0.</S>
			<S sid ="88" ssid = "24">1 2 0.</S>
			<S sid ="89" ssid = "25">07 Table 1: illustration of the edge scores that the parser provides for the English sentence in figure 1.</S>
			<S sid ="90" ssid = "26">The highest scoring structure in bold.</S>
			<S sid ="91" ssid = "27">during translation.</S>
			<S sid ="92" ssid = "28">Especially for longer sentences it would be unlikely to get this exact word order.</S>
			<S sid ="93" ssid = "29">One of the characteristics of first-order MST pars­ ing is that the score of each edge is independent of the rest of the structure.</S>
			<S sid ="94" ssid = "30">The parser therefore cal­ culates a matrix of scores for each possible edge in the sentence before searching for the most proba­ ble combined structure.</S>
			<S sid ="95" ssid = "31">We exploit this behavior by emitting the matrix of edge scores instead of the best structure.</S>
			<S sid ="96" ssid = "32">This way, we can provide the decoder with scores for each possible reordering it can produce.</S>
			<S sid ="97" ssid = "33">Table 1 gives an example of such an edge score matrix with the scores that the parser provided for the English sentence in figure 1.</S>
			<S sid ="98" ssid = "34">As an example, an edge from word 2 to word 4 has a cost of 1.48.</S>
			<S sid ="99" ssid = "35">Higher scores are better.</S>
			<S sid ="100" ssid = "36">Position 0 is the root po­ sition, which can only have outgoing edges.</S>
			<S sid ="101" ssid = "37">The bold scores mark the most probable structure, which is the structure represented in figure 1.</S>
	</SECTION>
	<SECTION title="Integration in PSMT. " number = "4">
			<S sid ="102" ssid = "1">As described in the previous section, the decoder re­ cieves an edge score matrix in addition to the source sentence.</S>
			<S sid ="103" ssid = "2">This extra information is only used by a word alignment scoring model.</S>
			<S sid ="104" ssid = "3">This model returns a score each time a phrase is added to a translation Sy ste m L e x i c a l re or de ri ng + T u n e ne ws te st 20 08 De ve lo p m en t ne ws tes t2 00 9 T e s t ne ws te st 20 10 Ba sel in e 1 3 . 6 9 1 3 . 9 8 1 3 . 2 0 1 3 . 7 4 1 4 . 1 8 1 4 . 8 0 Re or de rin g St ru ct ur e + 1 4 . 0 4 1 4 . 4 8 1 3 . 7 6 1 4 . 1 1 1 4 . 6 9 1 4 . 9 3 Or acl e Re or de rin g Str uc tur e + 1 6 . 9 9 1 7 . 1 7 1 6 . 3 4 1 6 . 6 7 1 7 . 6 6 1 8 . 0 6 Table 2: BLEU evaluation for the systems on different data sets.</S>
			<S sid ="105" ssid = "4">hypothesis.</S>
			<S sid ="106" ssid = "5">Since the model returns a single score, it only introduces one additional parameter to sys­ tem optimization.</S>
			<S sid ="107" ssid = "6">The score is calculated as the sum of scores for alignment links of adjacent target word %: wovuld be good positions within the phrase: ;; den Plan zu sehen ware schOn Swa = Ls(ai1, ai) i=l Figure 4: lllustration of the scoring done by the word (1) alignment scoring model when extending the translation hypothesis with a new phrase.</S>
			<S sid ="108" ssid = "7">where i is the target word position in a sentence of length n, ai is the source word position it links to, and s(ai-b ai) is the score for target word positions i- 1 and ibeing aligned to source word positions ail and ai respectively.</S>
			<S sid ="109" ssid = "8">That is, the score of an edge going from to . The scoring process is exemplified in figure 4.</S>
			<S sid ="110" ssid = "9">The left box illustrates the end of a translation hy­ pothesis, and the right box illustrates the new phrase being added to this hypothesis.</S>
			<S sid ="111" ssid = "10">Only the reorder­ ing structure being scored at this stage is shown above the translation.</S>
			<S sid ="112" ssid = "11">Again we indicate the rela­ tionship between the word alignment and the edges using indexes.</S>
			<S sid ="113" ssid = "12">The index 0 shows the final link.</S>
			<S sid ="114" ssid = "13">of the translation hypothesis, which decides where the new phrase links up.</S>
			<S sid ="115" ssid = "14">That is, the score for adding a new phrase is the sum of the score for connecting to the previous phrase (edge 1) and the scores for the phrase-internal edges (edges 2 and 3).</S>
			<S sid ="116" ssid = "15">These phrase­ internal edges can be computed at phrase retrieval to save computation.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="117" ssid = "1">Experiments were conducted from English to Ger­ man, a language pair which exhibits substantial word order challenges.</S>
			<S sid ="118" ssid = "2">5.1 Data.</S>
			<S sid ="119" ssid = "3">We use the EnglishGerman data from the Workshop on Statistical Machine Translation 2011 (WMT11) 1• This consists of 3.4/3.3 million words of parallel news data, 46.0/43.7 million words of parallel Eu­ roparl data, and 309 million monolingual words of europarl and news.</S>
			<S sid ="120" ssid = "4">We only use unique sentences from the monolingual data.</S>
			<S sid ="121" ssid = "5">We use newstest2008 for tuning, newstest2009 for development, and new­ stest2010 for testing.</S>
			<S sid ="122" ssid = "6">5.2 Reordering Structure Model Setup.</S>
			<S sid ="123" ssid = "7">The reordering structure model is created with the MSTParser, a dependency parser based on online discriminative learning.</S>
			<S sid ="124" ssid = "8">Since the reordering struc­ ture will contain many crossing edges, it is neces­ sary to use non-projective parsing.</S>
			<S sid ="125" ssid = "9">There are no algorithmic modification to the parser.</S>
			<S sid ="126" ssid = "10">The only modification we make is that we make it emit the edge score matrix for each sentence that it parses.</S>
			<S sid ="127" ssid = "11">We only train the model on 25,000 sentences from the parallel news data to keep down computational costs.</S>
			<S sid ="128" ssid = "12">The English side of this subcorpus is de­ pendency parsed using an MSTParser trained on the 1 http://www.statmt.orglwmtllltranslation-task.html 2 http://sourceforge.net/projects/mstparser/ Penn Treebank converted to dependency structures, and grow-diag-final-and word alignments from cre­ ating the PSMT system are used to extract reorder­ ing structures in CoNLL format.</S>
			<S sid ="129" ssid = "13">The dependency parse is used to connect null-linked source words as described in section 2, and it provides word form, part-of-speech tag, and dependency relation features for the reordering structure parser.</S>
			<S sid ="130" ssid = "14">We did not do ex­ tensive feature selection for the reordering structure model, but excluding either of the three information levels decreased performance on a translation task.</S>
			<S sid ="131" ssid = "15">5.3 PSMT Setup.</S>
			<S sid ="132" ssid = "16">All our PSMT systems are created with the Moses toolkit (Koehn et al., 2007).</S>
			<S sid ="133" ssid = "17">We use the base­ line system from WMT113 as our baseline with the small modifications that we use truecasing instead of lowercasing and recasing, and allow training sen­ tences of up to 80 words.</S>
			<S sid ="134" ssid = "18">For our reordering exper­ iments, we expand the baseline Moses system with the word alignment scoring model described in sec­ tion 4.</S>
			<S sid ="135" ssid = "19">This is the only change to the baseline sys­ tem.</S>
			<S sid ="136" ssid = "20">The baseline system got the best results with a distortion limit of 10, which we used for all exper­ iments.</S>
			<S sid ="137" ssid = "21">The phrase table and the lexical reordering model is trained on the union of all parallel data with a max phrase length of 7, and the 5-gram language model is trained on the entire monolingual data set.</S>
	</SECTION>
	<SECTION title="Results. " number = "6">
			<S sid ="138" ssid = "1">6.1 Automatic Evaluation.</S>
			<S sid ="139" ssid = "2">Table 2 shows the results from automatic evaluation using the BLEU metric {Papineni et al., 2002).</S>
			<S sid ="140" ssid = "3">We report on the performance of the baseline and the re­ ordering structure system with and without the lexi­ cal reordering model switched on.</S>
			<S sid ="141" ssid = "4">We use bootstrap­ ping&apos; to test the significance of the results (Zhang et al., 2004).</S>
			<S sid ="142" ssid = "5">For all the data sets, the reordering structure system significantly outperforms the cor­ responding baseline system.</S>
			<S sid ="143" ssid = "6">An interesting observation is that adding either the lexical reordering model or the reordering struc­ ture model to the baseline brings an improvement, and adding both improves performance even further.</S>
			<S sid ="144" ssid = "7">3 See a detailed desciption at http://www.statmt.org/.</S>
			<S sid ="145" ssid = "8">wmtlllhaseline.html 4 http://projectile.sv.cmu.edu/research/public/tools/ bootStrap!tutorial.htm A ll e d g es N on m on ot on e e d g e s Tu ne 69 .4 7 1 1 . 8 1 De ve lo p m en t 69 .0 3 1 1 . 8 8 Te st 71 .3 2 1 4 . 5 1 Table 3: Unlabeled attachment scores for the reordering structure model on the data sets.</S>
			<S sid ="146" ssid = "9">This indicates that the two models target different areas of reordering, and therefore they do not even each other out.</S>
			<S sid ="147" ssid = "10">Instead we see a cumulative effect where performance is increased even further.</S>
			<S sid ="148" ssid = "11">The final system represented in table 2 called Or­ acle Reordering Structure gives an indication of the performance that is attainable if the predictions of the reordering structure model are improved.</S>
			<S sid ="149" ssid = "12">Here the gold standard reordering structure was added as a feature, so the parser obtained a 100% unlabeled attachment score on the data sets.</S>
			<S sid ="150" ssid = "13">The idea of this system is to see how much there is to gain if we have a perfect reordering structure model.</S>
			<S sid ="151" ssid = "14">However, the gold standard builds on erroneous automatic word alignments, which means that the &quot;correct&quot; structure may mislead the translation.</S>
			<S sid ="152" ssid = "15">Also this is the oracle best structure, not the oracle best edge score matrix, which is what is actually used by the system.</S>
			<S sid ="153" ssid = "16">Table 3 shows the unlabeled attachment scores for the basic reordering structure model on the data sets.</S>
			<S sid ="154" ssid = "17">The scores are computed based on the most probable parse for each sentence, and they are reported for all edges and for non-monotone edges, i.e. edges going anywhere else than to the right neighboring word.</S>
			<S sid ="155" ssid = "18">These non-monotone edges are the most interesting edges, since they represent the reordering, and the prediction of these is very poor.</S>
			<S sid ="156" ssid = "19">We therefore expect that a fair part of the gain indicated by the Oracle Re­ ordering Structure system is attainable through im­ provement of the reordering structure model.</S>
			<S sid ="157" ssid = "20">6.2 Human Evaluation.</S>
			<S sid ="158" ssid = "21">In addition to the automatic evaluation, we also per­ form a small human evaluation using sentence trans­ lation ranking (CallisonBurch et al., 2010).</S>
			<S sid ="159" ssid = "22">We have two native German speakers rank the transla­ tions from the baseline system and the reordering structure system relative to each other.</S>
			<S sid ="160" ssid = "23">We evaluate on the first 100 sentences of the test corpus (new 7000 ,--- ----;:========:::;­ -·Baseline (+LR) 6000 +---ll- - ----1 ---Baseline (-LR) 5000 -Reordering structure (+LR) --Reordering structure (-LR) 40003000 +--&quot;tt------------- Table 4: Human evaluation comparing the baseline and the reordering structure (RS) system on the first 100 sen &quot;&quot;&apos; 2000 +--_,, _tences of the test set.</S>
			<S sid ="161" ssid = "24">1000 -- --4 ------- stest2010).</S>
			<S sid ="162" ssid = "25">On this subset, the baseline system gets a BLEU score of 10.55, and the reordering structure system gets 10.70.</S>
			<S sid ="163" ssid = "26">The evaluators are presented with the source sen­ tence and the two translations in randomized order.</S>
			<S sid ="164" ssid = "27">They are told to rank the systems from best to worst.</S>
			<S sid ="165" ssid = "28">Ties are allowed.</S>
			<S sid ="166" ssid = "29">The evaluators agreed on their judgements in 67 of the 100 sentences.</S>
			<S sid ="167" ssid = "30">Compared to an expected chance agreement of 1/3, the kappa coefficient is 0.505, which is much in line with find­ ings from WMT10 (CallisonBurch et al., 2010).</S>
			<S sid ="168" ssid = "31">Table 4 shows the results from the human evalua­ tions.</S>
			<S sid ="169" ssid = "32">The translations from the reordering structure system were chosen as better than the baseline sys­ tem more than twice as often as the reverse.</S>
			<S sid ="170" ssid = "33">This indicates that the reorderings introduced by the RS system may improve translation quality more than what the BLEU scores reflect.</S>
			<S sid ="171" ssid = "34">It has previously been reported that BLEU can be insensitive to word order improvements (CallisonBurch et al., 2007).</S>
	</SECTION>
	<SECTION title="Analysis. " number = "7">
			<S sid ="172" ssid = "1">An interesting aspect of the effect of the reordering structure model is the amount of word order differ­ ences it leads to.</S>
			<S sid ="173" ssid = "2">This information can be extracted from the word alignment information produced dur­ ing a translation.</S>
			<S sid ="174" ssid = "3">Figure 5 shows the amount of reordering created by the baseline and reordering structure model systems on the development data.</S>
			<S sid ="175" ssid = "4">The figure shows that the reordering structure sys­ tem introduces a lot more long distance reordering to the translation than the baseline systems.</S>
			<S sid ="176" ssid = "5">With lexical reordering on, it produces more than twice as many reorderings with a distance of 4 words, and more than 4 times as many reorderings with a dis­ tance of 8 words.</S>
			<S sid ="177" ssid = "6">Here we also see a cumulative effect of combin­ ing the reordering structure model with the lexical 2 3 4 5 6 7 8 9 10 Word reordering distance Figure 5: The amount of non-monotone decoding done by the baseline and reordering structure systems on the development data.</S>
			<S sid ="178" ssid = "7">More specifically, the number of tar­ get words representing a given reordering distance.</S>
			<S sid ="179" ssid = "8">LR specifies whether lexical reordering is in use.</S>
			<S sid ="180" ssid = "9">reordering model.</S>
			<S sid ="181" ssid = "10">Together with the baseline sys­ tem, the lexical reordering model does not introduce much long distance reordering, but combined with the reordering structure model the amount of long distance reordering gets boosted, also compared to the reordering structure model by itself.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "8">
			<S sid ="182" ssid = "1">In recent years, the integration of syntactic knowl­ edge into statistical machine translation has received much attention.</S>
			<S sid ="183" ssid = "2">The main motivation for this has been the need for better reordering of the words dur­ ing translation.</S>
			<S sid ="184" ssid = "3">In a framework like synchronous context-free grammars (SCFGs) syntax is incorpo­ rated either on the source side (Liu et al., 2006), the target side (Galley et al., 2004), or both sides (Liu et al., 2009), and reordering is handled through the rules that constitute the building blocks for the trans­ lation.</S>
			<S sid ="185" ssid = "4">Such approaches have proven successful es­ pecially for language pairs which exhibit much non­ local reordering (Zollmann et al., 2008; Birch et al., 2009).</S>
			<S sid ="186" ssid = "5">The hard constraints within the formalisms of these frameworks may however be too restrictive to handle frequently occuring aspects of parallel lan­ guages (Wellington et al., 2006; Sfl)gaard and Kuhn, 2009; Galley and Manning, 2010).</S>
			<S sid ="187" ssid = "6">In order to avoid such hard constraints introduced by the formalism, we place reordering information in the model to motivate certain word orders rather than prohibit others.</S>
			<S sid ="188" ssid = "7">That is, we create a reorder ing model that scores translation in parallel to other scoring models.</S>
			<S sid ="189" ssid = "8">lbis is much in line with Chiang (2010), who place syntactic correspondence infor­ mation in the model as a soft constraint, but their approach is heavily tied to the SCFG framework, whereas our approach is framework independent.</S>
			<S sid ="190" ssid = "9">A lot of work has been done on reordering in PSMT.</S>
			<S sid ="191" ssid = "10">The original approach deterred reordering by applying a distortion penalty for each word that is moved across (Koehn et a!., 2003).</S>
			<S sid ="192" ssid = "11">Another ap­ proach is lexicalized reordering, which conditions the probability of moving a phrase in a certain direc­ tion on the lexical content of the phrase (Tillrnann, 2004; Koehn et a!., 2005).</S>
			<S sid ="193" ssid = "12">A third approach is pre­ translation reordering, which reorders the source words in an attempt to assimilate the word order of the target language prior to translation.</S>
			<S sid ="194" ssid = "13">lbis can be done by supplying the decoder with a single permu­ tation (Xia and McCord, 2004; Collins et a!., 2005; Habash, 2007; Xu et a!., 2009) or multiple weighted permutations (Zhang et a!., 2007; Li et a!., 2007; Elming, 2008; Ge, 2010).</S>
			<S sid ="195" ssid = "14">The present approach relates to the pre-translation reordering approaches in that it tries to predict the target word order from source sentence syntax.</S>
			<S sid ="196" ssid = "15">However, in these previous approaches, the source words are reordered prior to translation.</S>
			<S sid ="197" ssid = "16">lbis is not done in the current approach - instead, we use a decoder-internal model for scor­ ing all generated reorderings.</S>
			<S sid ="198" ssid = "17">The approach utilizes syntactic dependency re­ lations to predict reorderings.</S>
			<S sid ="199" ssid = "18">lbis has previously been suggested to provide a better basis for reorder­ ing in machine translation due to higher inter-lingual phrasal cohesion than phrase structure (Fox, 2002).</S>
			<S sid ="200" ssid = "19">Much previous work has included dependency struc­ ture information in an SMT system.</S>
			<S sid ="201" ssid = "20">Quirk et a!.</S>
			<S sid ="202" ssid = "21">(2005) use a source side dependency structure in their treelet SMT system, which translates from sub­ trees to strings.</S>
			<S sid ="203" ssid = "22">Galley &amp; Manning (2009) use a de­ pendency parser in a phrase-based setup for assign­ ing a dependency structure to the target side during translation.</S>
			<S sid ="204" ssid = "23">lbis allows for the integration of a de­ pendency language model directly into the system.</S>
			<S sid ="205" ssid = "24">Gimpel &amp; Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.</S>
			<S sid ="206" ssid = "25">No syntactic structure is created during decoding in our approach.</S>
			<S sid ="207" ssid = "26">Instead the dependency parser is used for the sole purpose of scoring the word order of the target sentence.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number = "9">
			<S sid ="208" ssid = "1">We have introduced a new syntactically motivated discriminative reordering model.</S>
			<S sid ="209" ssid = "2">The model em­ ploys a standard data-driven dependency parser to predict reorderings.</S>
			<S sid ="210" ssid = "3">lbis is made possible by intro­ ducing a reordering structure.</S>
			<S sid ="211" ssid = "4">Within the framework of PSMT, we obtain substantial increases in trans­ lation quality both measured automatically and by human evaluators on an English to German task.</S>
			<S sid ="212" ssid = "5">In the present work, we did very little feature selection and ouly provided word form, part-of­ speech, and dependency relation information for the parser.</S>
			<S sid ="213" ssid = "6">In the future, we will experiment with ad­ ditional features to improve the reordering structure model.</S>
			<S sid ="214" ssid = "7">In particular, we expect that more syntac­ tic features will be beneficial.</S>
			<S sid ="215" ssid = "8">Also approaches such as second-order and stacked parsing may be helpful, since first-order parsing may be too weak to handle the complexities of the reordering structure.</S>
			<S sid ="216" ssid = "9">We also want to look closer at the features exploited by the standard MTSParser.</S>
			<S sid ="217" ssid = "10">These features are optimized to learn dependency structures, and they may not be optimal for learning the reordering structure.</S>
			<S sid ="218" ssid = "11">One concern with the approach is that the model is trained against a gold standard which was extracted from automatic word alignment.</S>
			<S sid ="219" ssid = "12">lbis means that there will be a lot of noise in the training mate­ rial.</S>
			<S sid ="220" ssid = "13">Also when training against the gold standard, all edges are considered equally important, but this may in fact not be the case for translation.</S>
			<S sid ="221" ssid = "14">Certain reorderings should always apply, while other may be stylistic and optional.</S>
			<S sid ="222" ssid = "15">A better way of training the model might be to train it as part of optimiz­ ing the PSMT system.</S>
			<S sid ="223" ssid = "16">lbis way, the system would be optimizing directly towards improving the word order of the translation.</S>
			<S sid ="224" ssid = "17">Due to the discriminative model&apos;s large number of weights, using a discrimi­ native algorithm to optimize the system (Watanabe eta!., 2007; Chiang eta!., 2008) would be an inter­ esting option.</S>
			<S sid ="225" ssid = "18">lbis could either be done by learn­ ing from ouly the data set used for tuning the PSMT system, or by taking the model trained in the present work as a point of departure and revising the weights in the context of optimizing a PSMT system.</S>
			<S sid ="226" ssid = "19">We ex­ pect to pursue this direction in future work.</S>
	</SECTION>
</PAPER>
