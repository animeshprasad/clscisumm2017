<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Maximum Entropy Principle has been used successfully in various NLP tasks.</S>
		<S sid ="2" ssid = "2">Inthis paper we propose a forward translation model consisting of a set of maximum entropy classifiers: a separate classifier is trained for each (sufficiently frequent) source-side lemma.</S>
		<S sid ="3" ssid = "3">In this way the estimates of translation probabilitiescan be sensitive to a large number of features derived from the source sentence (including non-local features, features making use of sentence syntactic structure,etc.).</S>
		<S sid ="4" ssid = "4">When integrated into English-to-Czech dependency-based translation scenario implemented in the TectoMT framework, the new translation model significantly outperforms the baseline model(MLE) in terms of BLEU.</S>
		<S sid ="5" ssid = "5">The performance is further boosted in a configurationinspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">The principle of maximum entropy states that,given known constraints, the probability distri bution which best represents the current state of knowledge is the one with the largest entropy.Maximum entropy models based on this princi ple have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996),parsing (Charniak, 2000), and named entity recog nition (Bender et al., 2003).</S>
			<S sid ="7" ssid = "7">Maximum entropy models have the following form where fi is a feature function, Ai is its weight, and Z(x) is the normalizing factor Z(x) = � �exp Aifi(x, y) y iIn statistical machine translation (SMT), trans lation model (TM) p(t|s) is the probability that the string t from the target language is the translation of the string s from the source language.</S>
			<S sid ="8" ssid = "8">Typical approach in SMT is to use backward translationmodel p(s|t) according to Bayes’ rule and noisy channel model.</S>
			<S sid ="9" ssid = "9">However, in this paper we deal only with the forward (direct) model.1The idea of using maximum entropy for con structing forward translation models is not new.</S>
			<S sid ="10" ssid = "10">It naturally allows to make use of various featurespotentially important for correct choice of targetlanguage expressions.</S>
			<S sid ="11" ssid = "11">Let us adopt a motivat ing example of such a feature from (Berger et al., 1996) (which contains the first usage of maxenttranslation model we are aware of): “If house ap pears within the next three words (e.g., the phrases in the house and in the red house), then dans might be a more likely [French] translation [of in].” Incorporating non-local features extracted fromthe source sentence into the standard noisychannel model in which only the backward trans lation model is available, is not possible.</S>
			<S sid ="12" ssid = "12">Thisdrawback of the noisy-channel approach is typi cally compensated by using large target-language n-gram models, which can – in a result – play arole similar to that of a more elaborate (more context sensitive) forward translation model.</S>
			<S sid ="13" ssid = "13">How ever, we expect that it would be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side.</S>
			<S sid ="14" ssid = "14">1A backward translation model is used only for pruning training data in this paper.</S>
			<S sid ="15" ssid = "15">1 � p(y|x) = Z(x)exp Aifi(x, y) i 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, Uppsala, Sweden, 1516 July 2010.</S>
			<S sid ="16" ssid = "16">c�2010 Association for Computational Linguistics A deeper discussion on the potential advantagesof maximum entropy approach over the noisy channel approach can be found in (Foster, 2000)and (Och and Ney, 2002), in which another suc cessful applications of maxent translation models are shown.</S>
			<S sid ="17" ssid = "17">Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).</S>
			<S sid ="18" ssid = "18">What makes our approach different from the previously published works is that1.</S>
			<S sid ="19" ssid = "19">we show how the maximum entropy trans lation model can be used in a dependencyframework; we use deep-syntactic dependency trees (as defined in the Prague Depen dency Treebank (Hajiˇc et al., 2006)) as the transfer layer,2.</S>
			<S sid ="20" ssid = "20">we combine the maximum entropy transla tion model with target-language dependency tree model and use tree-modified Viterbisearch for finding the optimal lemmas label ing of the target-tree nodes.</S>
			<S sid ="21" ssid = "21">The rest of the paper is structured as follows.</S>
			<S sid ="22" ssid = "22">InSection 2 we give a brief overview of the translation framework TectoMT in which the experi ments are implemented.</S>
			<S sid ="23" ssid = "23">In Section 3 we describehow our translation models are constructed.</S>
			<S sid ="24" ssid = "24">Sec tion 4 summarizes the experimental results, and Section 5 contains a summary.</S>
	</SECTION>
	<SECTION title="Translation framework. " number = "2">
			<S sid ="25" ssid = "1">We use tectogrammatical (deep-syntactic) layer of language representation as the transfer layer in the presented MT experiments.</S>
			<S sid ="26" ssid = "2">Tectogrammatics was introduced in (Sgall, 1967) and further elaborated within the Prague Dependency Treebank project (Hajiˇc et al., 2006).</S>
			<S sid ="27" ssid = "3">On this layer, each sentence is represented as a tectogrammatical tree, whosemain properties (from the MT viewpoint) are fol lowing: (1) nodes represent autosemantic words, (2) edges represent semantic dependencies (a node is an argument or a modifier of its parent), (3) there are no functional words (prepositions, auxiliarywords) in the tree, and the autosemantic words appear only in their base forms (lemmas).</S>
			<S sid ="28" ssid = "4">Morpho logically indispensable categories (such as number with nouns or tense with verbs, but not number with verbs as it is only imposed by agreement) are stored in separate node attributes (grammatemes).The intuition behind the decision to use tectogrammatics for MT is the following: we be lieve that (1) tectogrammatics largely abstractsfrom language-specific means (inflection, agglu tination, functional words etc.) of expressing non-lexical meanings and thus tectogrammaticaltrees are supposed to be highly similar across languages,2 (2) it enables a natural transfer factorization,3 (3) and local tree contexts in tectogram matical trees carry more information (especially for lexical choice) than local linear contexts in the original sentences.4In order to facilitate transfer of sentence ‘syn tactization’, we work with tectogrammatical nodes enhanced with the formeme attribute (ˇZabokrtsk´yet al., 2008), which captures the surface mor phosyntactic form of a given tectogrammatical node in a compact fashion.</S>
			<S sid ="29" ssid = "5">For example, the value n:pˇred+4 is used to label semantic nouns that should appear in an accusative form in a prepositional group with the preposition pˇred in Czech.</S>
			<S sid ="30" ssid = "6">For English we use formemes such as n:subj (semantic noun (SN) in subject position), n:for+X (SN with preposition for), n:X+ago (SN with postposition ago), n:poss (possessive form ofSN), v:because+fin (semantic verb (SV) as a sub ordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), adj:attr (semantic adjective (SA) in attributive position), adj:compl (SA in complement position).</S>
			<S sid ="31" ssid = "7">We have implemented our experiments in theTectoMT software framework, which already of fers tool chains for analysis and synthesis of Czech and English sentences (ˇZabokrtsk´y et al., 2008).</S>
			<S sid ="32" ssid = "8">The translation scenario proceeds as follows.</S>
			<S sid ="33" ssid = "9">1.</S>
			<S sid ="34" ssid = "10">The input English text is segmented into sen-.</S>
			<S sid ="35" ssid = "11">tences and tokens.</S>
			<S sid ="36" ssid = "12">2.</S>
			<S sid ="37" ssid = "13">The tokens are lemmatized and tagged with.</S>
			<S sid ="38" ssid = "14">Penn Treebank tags using the Morce tagger (Spoustov´a et al., 2007).</S>
			<S sid ="39" ssid = "15">2This claim is supported by error analysis of output of tectogrammatics-based MT system presented in (Popel and ˇZabok/rtsk´y, 2009), which shows that only 8 % of translation errors are caused by the (obviously too strong) assumptionthat the tectogrammatical tree of a sentence and the tree rep resenting its translation are isomorphic.3Morphological categories can be translated almost inde pendently from lemmas, which makes parallel training data ‘denser’, especially when translating from/to a language with rich inflection such as Czech.4Recall the house-is-somewhere-around feature in the in troduction; again, the fact that we know the dominating (or dependent) word should allow to construct a more compact translation model, compared to n-gram models.</S>
			<S sid ="40" ssid = "16">202 Figure 1: Intermediate sentence representations when translating the English sentence “However, this very week, he tried to find refuge in Brazil.”, leading to the Czech translation “Pˇresto se tento pr´avˇe t´yden snaˇzil najit ´utoˇciˇstˇe v Braz´ılii.”.</S>
	</SECTION>
	<SECTION title="Then the Maximum Spanning Tree parser. " number = "3">
			<S sid ="41" ssid = "1">(McDonald et al., 2005) is applied and a surface-syntax dependency tree (analytical tree in the PDT terminology) is created for each sentence (Figure 1a).</S>
	</SECTION>
	<SECTION title="This tree is converted to a tectogrammatical. " number = "4">
			<S sid ="42" ssid = "1">tree (Figure 1b).</S>
			<S sid ="43" ssid = "2">Each autosemantic wordwith its associated functional words is col lapsed into a single tectogrammatical node,labeled with lemma, formeme, and semantically indispensable morphologically categories; coreference is also resolved.</S>
			<S sid ="44" ssid = "3">Collaps ing edges are depicted by wider lines in the Figure 1a.</S>
	</SECTION>
	<SECTION title="The transfer phase follows, whose most dif-. " number = "5">
			<S sid ="45" ssid = "1">ficult part consists in labeling the tree with target-side lemmas and formemes5 (changesof tree topology are required relatively infre quently).</S>
			<S sid ="46" ssid = "2">See Figure 1c.</S>
	</SECTION>
	<SECTION title="Finally, surface sentence shape (Figure 1d) is. " number = "6">
			<S sid ="47" ssid = "1">synthesized from the tectogrammatical tree, which is basically a reverse operation for the 5In this paper we focus on using maximum entropy for translating lemmas, but it can be used for translating formemes as well.tectogrammatical analysis: adding punctuation and functional words, spreading morphological categories according to grammat ical agreement, performing inflection (using Czech morphology database (Hajiˇc, 2004)), arranging word order etc. 3 Training the two models.</S>
			<S sid ="48" ssid = "2">In this section we describe two translation mod els used in the experiments: a baseline translation model based on maximum likelihood estimates (3.2), and a maximum entropy based model (3.3).</S>
			<S sid ="49" ssid = "3">Both models are trained using the same data (3.1).</S>
			<S sid ="50" ssid = "4">In addition, we describe a target-language tree model (3.4), which can be combined with both the translation models using the Hidden Tree Markov Model approach and tree-modified Viterbi search, similarly to the approach of (ˇZabokrtsk´y and Popel, 2009).</S>
			<S sid ="51" ssid = "5">3.1 Data preprocessing common for both.</S>
			<S sid ="52" ssid = "6">models We used CzechEnglish parallel corpus CzEng 0.9 (Bojar and ˇZabokrtsk´y, 2009) for training the translation models.</S>
			<S sid ="53" ssid = "7">CzEng 0.9 contains about8 million sentence pairs, and also their tectogram matical analyses and node-wise alignment.</S>
			<S sid ="54" ssid = "8">203 We used only trees from training sections (about 80 % of the whole data), which contain around 30 million pairs of aligned tectogrammatical nodes.</S>
			<S sid ="55" ssid = "9">From each pair of aligned tectogrammatical nodes, we extracted triples containing the source (English) lemma, the target (Czech) lemma, and the feature vector.</S>
			<S sid ="56" ssid = "10">In order to reduce noise in the training data,we pruned the data in two ways.</S>
			<S sid ="57" ssid = "11">First, we disregarded all triples whose lemma pair did not oc cur at least twice in the whole data.</S>
			<S sid ="58" ssid = "12">Second, we computed forward and backward maximum likelihood (ML) translation models (target lemma given source lemma and vice versa) and deleted all triples whose probability according to one of the two models was lower than the threshold 0.01.</S>
			<S sid ="59" ssid = "13">Then the forward ML translation model was reestimated using only the remaining data.</S>
			<S sid ="60" ssid = "14">For a given pair of aligned nodes, the featurevector was of course derived only from the source side node or from the tree which it belongs to.</S>
			<S sid ="61" ssid = "15">Asalready mentioned in the introduction, the advan tage of the maximum entropy approach is that arich and diverse set of features can be used, with out limiting oneself to linearly local context.</S>
			<S sid ="62" ssid = "16">Thefollowing features (or, better to say, feature templates, as each categorical feature is in fact con verted to a number of 01 features) were used: • formeme and morphological categories of the given node,• lemma, formeme and morphological cate gories of the governing node, • lemmas and formemes of all child nodes, • lemmas and formemes of the nearest linearly preceding and following nodes.</S>
			<S sid ="63" ssid = "17">3.2 Baseline translation model.</S>
			<S sid ="64" ssid = "18">The baseline TM is basically the ML translationmodel resulting from the previous section, lin early interpolated with several translation models making use of regular word-formative derivations,which can be helpful for translating some less frequent (but regularly derived) lemmas.</S>
			<S sid ="65" ssid = "19">For exam ple, one of the derivation-based models estimates the probability p(zaj´ımavˇejinterestingly) (possibly unseen pair of deadjectival adverbs) by the valueof p(zaj´ımav´yjinteresting).</S>
			<S sid ="66" ssid = "20">More detailed descrip tion of these models goes beyond the scope of this paper; their weights in the interpolation are very small anyway.</S>
			<S sid ="67" ssid = "21">3.3 MaxEnt translation model.</S>
			<S sid ="68" ssid = "22">The MaxEnt TM was created as follows: 1.</S>
			<S sid ="69" ssid = "23">training triples (source lemma, target lemma, feature vector) were disregarded if the source lemma was not seen at least 50 times (onlythe baseline model will be used for such lem mas),2.</S>
			<S sid ="70" ssid = "24">the remaining triples were grouped by the En glish lemma (over 16 000 groups), 3.</S>
			<S sid ="71" ssid = "25">due to computational issues, the maximum number of triples in a group was reduced to 1000 by random selection, 4.</S>
			<S sid ="72" ssid = "26">a separate maximum entropy classifier was trained for each group (i.e., one classifier per source-side lemma) using AI::MaxEntropy Perl module,6 5.</S>
			<S sid ="73" ssid = "27">due to the more aggressive pruning of the training data, coverage of this model issmaller than that of the baseline model; in order not to loose the coverage, the two mod els were combined using linear interpolation (1:1).</S>
			<S sid ="74" ssid = "28">Selected properties of the maximum entropy translation model (before the linear interpolation with the baseline model) are shown in Figure 2.</S>
			<S sid ="75" ssid = "29">We increased the size of the training data from10 000 training triples up to 31 million and eval uated three relative quantities characterizing the translation models:• coverage - relative frequency of source lem mas for which the translation model offers at least one translation, • first - relative frequency of source lemmas for which the target lemmas offered as the first by the model (argmax) are the correct ones, • oracle - relative frequency of source lemmas for which the correct target lemma is among the lemmas offered by the translation model.</S>
			<S sid ="76" ssid = "30">As mentioned in Section 3.1, there are context features making use both of local linear context and local tree context.</S>
			<S sid ="77" ssid = "31">After training the MaxEnt model, there are about 4.5 million features with nonzero weight, out of which 1.1 million features 6http://search.cpan.org/perldoc?AI:: MaxEntropy 204Figure 2: Three measures characterizing the Max Ent translation model performance, depending on the training data size.</S>
			<S sid ="78" ssid = "32">Evaluated on aligned node pairs from the dtest portion of CzEng 0.9.</S>
			<S sid ="79" ssid = "33">are derived from the linear context and 2.4 million features are derived from the tree context.</S>
			<S sid ="80" ssid = "34">This shows that the MaxEnt translation model employs the dependency structure intensively.</S>
			<S sid ="81" ssid = "35">A preliminary analysis of feature weights seems to support our intuition that the linear contextis preferred especially in the case of more stable collocations.</S>
			<S sid ="82" ssid = "36">For example, the most impor tant features for translating the lemma bare arebased on the lemma of the following noun: target lemma bos´y (barefooted) is preferred if the fol lowing noun on the source side is foot, while hol´y (naked, unprotected) is preferred if hand follows.</S>
			<S sid ="83" ssid = "37">The contribution of dependency-based features can be illustrated on translating the word drop.</S>
			<S sid ="84" ssid = "38">The greatest weight for choosing kapka (a droplet)as the translation is assigned to the feature captur ing the presence of a node with formeme n:of+X among the node’s children.</S>
			<S sid ="85" ssid = "39">The greatest weights in favor of odhodit (throw aside) are assigned to features capturing the presence of words such as gun or weapon, while the greatest weights in favorof klesnout (to come down) are assigned to fea tures saying that there is the lemma percent or the percent sign among the children.</S>
			<S sid ="86" ssid = "40">Of course, the lexical choice is influenced also by the governing lemmas, as can be illustratedwith the word native.</S>
			<S sid ="87" ssid = "41">One can find a high value feature for rodil´y (native-born) saying that the source-side parent is speaker; similarly for mateˇrsk´y (mother) with governing tongue, and rodn´y (home) with land.</S>
			<S sid ="88" ssid = "42">Linear and tree features are occasionally used simultaneously: there are high-valued positive configuration baseline TM MaxEnt TM baseline TM + TreeLM MaxEnt TM + TreeLMTable 1: BLEU and NIST evaluation of four con figurations of our MT system; the WMT 2010 test set was used.</S>
			<S sid ="89" ssid = "43">weights for translating order as objednat (reserve, give an order for st.) assigned both to tree-based features saying that there are words such as pizza, meal or goods and to linear features saying that the very following word is some or two.</S>
			<S sid ="90" ssid = "44">3.4 Target-language tree model.</S>
			<S sid ="91" ssid = "45">Although the MaxEnt TM captures some contex tual dependencies that are covered by language models in the standard noisy-channel SMT, it maystill be beneficial to exploit target-language models, because these can be trained on huge monolingual corpora.</S>
			<S sid ="92" ssid = "46">We use a target-language depen dency tree model differing from standard n-gram model in two aspects: • it uses tree context instead of linear context,• it predicts tectogrammatical attributes (lem mas and formemes) instead of word forms.</S>
			<S sid ="93" ssid = "47">In particular, our target-language tree model (TreeLM) predicts the probability of node’s lemma and formeme given its parent’s lemma andformeme.</S>
			<S sid ="94" ssid = "48">The optimal (lemma and formeme) la beling is found by tree-modified Viterbi search; for details see (ˇZabokrtsk´y and Popel, 2009).</S>
			<S sid ="95" ssid = "49">4 Experiments.</S>
			<S sid ="96" ssid = "50">When included into the above described transla tion scenario, the MaxEnt TM outperforms thebaseline TM, be it used together with or without TreeLM.</S>
			<S sid ="97" ssid = "51">The results are summarized in Table 1.</S>
			<S sid ="98" ssid = "52">The improvement is statistically signif icant according to paired bootstrap resampling test (Koehn, 2004).</S>
			<S sid ="99" ssid = "53">In the configuration without TreeLM the improvement is greater (1.33 BLEU) than with TreeLM (0.81 BLEU), which confirms our hypothesis that MaxEnt TM captures some of the contextual dependencies resolved otherwise by language models.</S>
			<S sid ="100" ssid = "54">BLEU NIST 10.44 4.795 11.77 5.135 11.77 5.038 12.58 5.250 205 5 Conclusions.</S>
			<S sid ="101" ssid = "55">We have introduced a maximum entropy translation model in dependency-based MT which enables exploiting a large number of feature func tions in order to obtain more accurate translations.The BLEU evaluation proved significant improvement over the baseline solution based on the trans lation model with maximum likelihood estimates.However, the performance of this system still be low the state of the art (which is around BLEU 16 for the English-to-Czech direction).</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="102" ssid = "56">This research was supported by the grantsMSM0021620838, MˇSMT ˇCR LC536, FP7ICT 20094-247762 (Faust), FP7ICT-20073-231720 (EuroMatrix Plus), GA201/09/H057, and GAUK 116310.</S>
			<S sid ="103" ssid = "57">We thank two anonymous reviewers for helpful comments.</S>
	</SECTION>
</PAPER>
