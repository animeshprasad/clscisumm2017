<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">The translation features typically used in Phrase-Based Statistical Machine Translation (PBSMT) model dependencies between the source and target phrases, but not among the phrases in the source language themselves.</S>
		<S sid ="2" ssid = "2">A swathe of research has demonstrated that integrating source context modelling directly into log-linear PBSMT can positively influence the weighting and selection of target phrases, and thus improve translation quality.</S>
		<S sid ="3" ssid = "3">In this contribution we present a revised, extended account of our previous work on using a range of contextual features, including lexical features of neighbouring words, supertags, and dependency information.</S>
		<S sid ="4" ssid = "4">We add a number of novel aspects, including the use of semantic roles as new contextual features in PBSMT, adding new language pairs, and examining the scalability of our research to larger amounts of training data.</S>
		<S sid ="5" ssid = "5">While our results are mixed across feature selections, classifier hyperparameters, language pairs, and learning curves, we observe that including contextual features of the source sentence in general produces improvements.</S>
		<S sid ="6" ssid = "6">The most significant improvements involve the integration of long-distance contextual features, such as dependency relations in combination with part-of-speech tags in Dutch-to-English subtitle translation, the combination of dependency parse and semantic role information in English-to-Dutch parliamentary debate translation, or supertag features in English-to-Chinese translation.</S>
		<S sid ="7" ssid = "7">R. Haque · S. K. Naskar · A. Way CNGL, School of Computing, Dublin City University, Dublin 9, Ireland A. van den Bosch (B) ILK Research Group, Tilburg center for Cognition and Communication, Tilburg University, Tilburg, The Netherlands email: Antal.vdnBosch@uvt.nl 123 Keywords Statistical machine translation · Phrase-based statistical machine translation · Syntax in machine translation · Translation modelling · Word alignment · Memory-based classification</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">In log-linear phrase-based statistical machine translation (PBSMT: Koehn et al. 2003), the probability P(eˆk | fˆk ) of a target phrase eˆk given a source phrase fˆk is modelled as a log-linear combination of features which typically consist of a finite set of translation features, and a language model (Och and Ney 2002).</S>
			<S sid ="9" ssid = "9">The translation features normally used in such models express dependencies between the source and target phrases, but not among phrases or tokens in the source language themselves.</S>
			<S sid ="10" ssid = "10">Stroppa et al.</S>
			<S sid ="11" ssid = "11">(2007) observed that incorporating source-language context using neighbouring words and part-of-speech tags had the potential to improve translation quality.</S>
			<S sid ="12" ssid = "12">This has led to a whole tranche of research, of which we provide an overview in Sect.</S>
			<S sid ="13" ssid = "13">3, which has shown that integrating source context modelling into PBSMT can positively influence the weighting and selection of target phrases, and thus improve translation quality.</S>
			<S sid ="14" ssid = "14">Approaches to include source-language context to help select more appropriate target phrases have partly been inspired by methods used in word-sense disambiguation (WSD), where rich contextual features are employed to determine the most likely sense of a polysemous word given that context.</S>
			<S sid ="15" ssid = "15">These contextual features may include lexical features of words appearing in the immediate context (Giménez and Màrquez 2007; Stroppa et al. 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith 2008), and full sentential context (Carpuat and Wu 2007).</S>
			<S sid ="16" ssid = "16">Studies in which syntactic features are employed have made use of part-of-speech taggers (Stroppa et al. 2007), supertaggers (Haque et al. 2009a), and shallow and deep syntactic parsers (Gimpel and Smith 2008; Haque et al. 2009b).</S>
			<S sid ="17" ssid = "17">In prior work, we have shown that exploring local sentential context information in the form of both supertags (Haque et al. 2009a) and syntactic dependencies (Haque et al. 2009b) can be successfully integrated into a PBSMT model.</S>
			<S sid ="18" ssid = "18">Here we provide a revised, extended account of this previous research.</S>
			<S sid ="19" ssid = "19">We add a number of novel aspects, including using semantic roles as new contextual features in PBSMT, adding new language pairs, and examining the scalability of our research to larger amounts of training data.</S>
			<S sid ="20" ssid = "20">Our results allow us to conclude that incorporating source-language contextual features benefits a range of different language pairs, both with English as source language (translating to Dutch, Chinese, Japanese, Hindi, Spanish, and Czech) and target language (from Dutch), on different types of data such as news articles and commentary, parliamentary debates, patents, and subtitles, according to a range of automatic evaluation measures.</S>
			<S sid ="21" ssid = "21">The remainder of this contribution is organized as follows.</S>
			<S sid ="22" ssid = "22">Section 2 provides some motivation for work in this direction, and related work is discussed in Sect.</S>
			<S sid ="23" ssid = "23">3.</S>
			<S sid ="24" ssid = "24">Section 4 provides a brief overview of PBSMT, which acts as the baseline throughout the study.</S>
			<S sid ="25" ssid = "25">In Sect.</S>
			<S sid ="26" ssid = "26">5 we describe the range of context-informed features we add to the baseline PBSMT model.</S>
			<S sid ="27" ssid = "27">Section 6 describes the memory-based classification approach and how we integrated the output of the memory-based classifier into a state-of-the-art PBSMT system.</S>
			<S sid ="28" ssid = "28">In Sect.</S>
			<S sid ="29" ssid = "29">7 we present the results obtained.</S>
			<S sid ="30" ssid = "30">We formulate our conclusions in Sect.</S>
			<S sid ="31" ssid = "31">8, and offer some avenues for further work.</S>
	</SECTION>
	<SECTION title="Motivation. " number = "2">
			<S sid ="32" ssid = "1">In PBSMT, the selection of appropriate target phrases for a source phrase depends only on the source phrase itself, regardless of its context, and the target language model.</S>
			<S sid ="33" ssid = "2">The target language model does represent contextual information, but only of the target language.</S>
			<S sid ="34" ssid = "3">In other words, the sense-disambiguation task inherent to PBSMT is modelled suboptimally as it ignores source-side context when translating a source phrase.</S>
			<S sid ="35" ssid = "4">We argue that the disambiguation of a source phrase can be enhanced by taking into account the context of the source phrase.</S>
			<S sid ="36" ssid = "5">Figure 1 shows translation examples for the polysemous English word ‘play’, which has many translation equivalents in Chinese, some of which (‘xi’, ‘wan’, ‘banYan’, ‘boFang’) are shown in Fig.</S>
			<S sid ="37" ssid = "6">1.</S>
			<S sid ="38" ssid = "7">Figure 1 also shows four English sentences, each containing ‘play’, translated into four different Chinese words depending on the context.</S>
			<S sid ="39" ssid = "8">For example, the most suitable translation of the word ‘play’ for the first English sentence ‘He wrote a play’ is ‘xi’ amongst the four Chinese candidate translations.</S>
			<S sid ="40" ssid = "9">The translation of ‘play’ in this sentence depends on the neighbouring word ‘wrote’.</S>
			<S sid ="41" ssid = "10">Similarly, the appropriate translation of ‘play’ in the English sentence ‘Can you play my favourite old record?’ is ‘boFang’ amongst the four Chinese candidate translations.</S>
			<S sid ="42" ssid = "11">In this English sentence, the translation of ‘play’ depends on the distant word ‘record’.</S>
			<S sid ="43" ssid = "12">In PBSMT, translation of a source sentence begins by generating all possible source phrases and gathering all candidate target phrases for each source phrase.</S>
			<S sid ="44" ssid = "13">In the translation process, potentially thousands of translation hypotheses are statistically generated using the pool of all target phrases to form the candidate translations.</S>
			<S sid ="45" ssid = "14">Thus, the decoder considers all target phrases for a given source phrase as possible candidate translations of that source phrase.</S>
			<S sid ="46" ssid = "15">The candidate phrases with higher translation probabilities have a better chance of occuring in the most likely candidate translations.</S>
			<S sid ="47" ssid = "16">On the other hand, the candidate phrases with lower translation Fig.</S>
			<S sid ="48" ssid = "17">1 Examples of ambiguity for the English word play, together with different translations depending on the context 123 probabilities have a higher risk of being pruned out during the formation of translation hypotheses due to the decoder’s beam size limit.</S>
			<S sid ="49" ssid = "18">The phrase translation probabilities are measured based on the frequency of occurrences of the source and target phrase pairs in the training corpus, ignoring the context in which those phrases appear.</S>
			<S sid ="50" ssid = "19">Let us go back to the example of the ambiguous word ‘play’ in Fig.</S>
			<S sid ="51" ssid = "20">1.</S>
			<S sid ="52" ssid = "21">Suppose the translation probability of ‘play’ into ‘xi’ is higher than that of ‘play’ into any of the remaining Chinese words (‘wan’, ‘banYan’, ‘boFang’).</S>
			<S sid ="53" ssid = "22">During translation of any of the English sentences in Fig.</S>
			<S sid ="54" ssid = "23">1, the decoder would ignore whatever contextual dependency the source phrase ‘play’ has in the source sentence, consider all Chinese words as probable candidate translations, and always give preference to ‘xi’ while generating candidate translations.</S>
			<S sid ="55" ssid = "24">To study whether we can counterbalance the important role of the target language model in the selection of candidate phrases, in this work we incorporate various aspects of source-language context into a state-of-the-art PBSMT model, Moses (Koehn et al. 2003, 2007), in order to perform discriminative translation filtering (cf.</S>
			<S sid ="56" ssid = "25">Sect.</S>
			<S sid ="57" ssid = "26">3.1.2) by learning context-sensitive translation probabilities which in effect should improve target phrase selection.</S>
			<S sid ="58" ssid = "27">We see from Fig.</S>
			<S sid ="59" ssid = "28">1 that the translations of ‘play’ may depend on the neighbouring lexical context (‘wrote’, in the first example sentence) as well as quite distant lexical context (‘record’, in the last example sentence).</S>
			<S sid ="60" ssid = "29">We investigate the incorporation of basic contextual features (words and POS tags), lexical syntactic descriptions (supertags), deep syntactic information (grammatical dependency relations) and semantic roles into PBSMT.</S>
			<S sid ="61" ssid = "30">We conjecture that such kinds of complex and rich syntactic and semantic knowledge sources, some of which inherently capture long-distance word-to-word dependencies in a sentence, may be useful to improve PBSMT lexical selection.</S>
	</SECTION>
	<SECTION title="Related work. " number = "3">
			<S sid ="62" ssid = "1">Context has been incorporated into both the source and target sides of the translation pair, in order to improve translational choice and the quality of translation.</S>
			<S sid ="63" ssid = "2">Techniques to incorporate context into SMT can be broadly divided into two categories: source- context modelling (Carpuat and Wu 2007; Giménez and Màrquez 2007; Stroppa et al. 2007; etc.) and target-context modelling (Berger et al. 1996; Hasan et al. 2008; etc.).</S>
			<S sid ="64" ssid = "3">In the first two subsections of this section we describe twenty nine studies of context modelling with English as the target language.</S>
			<S sid ="65" ssid = "4">In the third subsection we highlight six studies that use English as the source language.</S>
			<S sid ="66" ssid = "5">In the three subsections we group related work according to six key aspects.</S>
			<S sid ="67" ssid = "6">Each aspectual overview of related work is accompanied by a table highlighting the contrastive features of the studies discussed (Tables 1, 2, 3, 4, 5 and 6).</S>
			<S sid ="68" ssid = "7">In each table we list the contextual features employed by each study and the types of SMT models employed.</S>
			<S sid ="69" ssid = "8">In the second column of each table ‘SL→TL’ stands for ‘source language→target language’, referring to the translation pair and direction; ‘DS’ refers to the ‘data sets’ used to train SMT models; and ‘S/L’ stands for ‘small/large’, indicating a division between training set sizes below and above 500,000 words that we use to structure our Table 1 Related research integrating context into word-based SMT (WBSMT) models Authors SL→TL[DS][S/L] Contextual features Integrated into Brown et al.</S>
			<S sid ="70" ssid = "9">(1991) Fr→En[CPH][L] SL:neighbouring words and basic POS Vickrey et al.</S>
			<S sid ="71" ssid = "10">(2005) Fr→En[Europarl][L] SL:POS and neighbouring words Carpuat and Wu (2005) Zh→En[UN:LDC][L] SL:position-sensitive syntactic, and local collocations WBSMT model WBSMT model WBSMT model En English, Fr French, Zh Chinese, CPH Canadian Parliament Hansards, UN United Nations, LDC Linguistic Data Consortium Table 2 Related research integrating context into PBSMT models Authors SL→TL[DS][S/L] Contextual features Integrated into Stroppa et al.</S>
			<S sid ="72" ssid = "11">(2007) Zh→En[IWSLT][S] SL:neighbouring words and POS tags PBSMT model It→En[IWSLT][S] Carpuat and Wu (2007) Zh→En[IWSLT][S] SL:bag-of-words, collocations, POS and dependency features PBSMT model Zh→En[NIST][L] Giménez and Màrquez (2007, 2009) Sp→En[Europarl][L] SL:local context, n-grams, POS, lemmas, chunk label and bag-of-words PBSMT model En English, It Italian, Sp Spanish, Zh Chinese Table 3 Related research integrating context into Hiero models Authors SL→TL[DS][S/L] Contextual Features Integrated into Chan et al.</S>
			<S sid ="73" ssid = "12">(2007) Zh→En[FBIS][L] SL:local collocations, POS tags and neighbouring words Shen et al.</S>
			<S sid ="74" ssid = "13">(2009) Ar→En[NIST 06, 08][L] SL:nonterminal labels, length, context LM and dependency LM Hiero Hiero Zh→En[NIST 06, 08][L] Chiang et al.</S>
			<S sid ="75" ssid = "14">(2009) Zh→En[NIST][L] SL:neighbouring words Hiero and syntax-based model En English, Zh Chinese, Ar Arabic, FBIS Foreign broadcast information service experiments.</S>
			<S sid ="76" ssid = "15">In the third column, ‘SL’ and ‘TL’ respectively stand for ‘source’ and ‘target’ languages of the translation pair from which the listed contextual features are extracted.</S>
			<S sid ="77" ssid = "16">Table 4 Related research integrating context into alternative SMT models Authors SL→TL[DS][S/L] Contextual features Integrated into Bangalore et al.</S>
			<S sid ="78" ssid = "17">(2007) Ar→En[UN][L] SL:bag-of-words FST-based MT model Fr→En[CPH][L] Zh→En[IWSLT][S] Ittycheriah et al.</S>
			<S sid ="79" ssid = "18">(2007) Ar→En[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) De→En[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SL→TL[DS][S/L] Contextual features Integrated into Berger et al.</S>
			<S sid ="80" ssid = "19">(1996) Fr→En[CPH][L] TL:neighbouring words IBM model De→En[Verbmobil][L] García-Varea et al.</S>
			<S sid ="81" ssid = "20">(2001, 2002) De→En[Verbmobil][S] SL and TL:neighbouring words and word class IBM model Fr→En[CPH][L] Mauser et al.</S>
			<S sid ="82" ssid = "21">(2009) Zh→En[GALE][L] TL:neighbouring words and SL:sentence level lexical feature IBM model and proposed discriminative WA model Ar→En[NIST 08][L] Zh→En[NIST 08][L] Patry and Langlais (2009) Fr→En[Europarl][S] SL:bag-of-words Proposed WA model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, WA word alignment 3.1 Source context modelling.</S>
			<S sid ="83" ssid = "22">Approaches to integrating source-language contextual information into different stages in the SMT model can in turn be broadly divided into: (i) discriminative word alignment (e.g. Brunning et al. 2009; Patry and Langlais 2009) for creating improved word-to-word translation lexicons, and (ii) discriminative translation filtering (e.g. Carpuat and Wu 2007; Chan et al. 2007; Stroppa et al. 2007) by learning context- dependent translation probabilities.</S>
			<S sid ="84" ssid = "23">3.1.1 Discriminative word alignment García-Varea et al.</S>
			<S sid ="85" ssid = "24">(2001, 2002) present a MaxEnt approach to integrate contextual dependencies of both the source and target sides of the statistical alignment model Table 6 Related research using English as source language Authors SL→TL[DS][S/L] Contextual features Integrated into Max et al.</S>
			<S sid ="86" ssid = "25">(2008) En→Fr[Europarl][S] SL:neighbouring words and POS, and dependency relations PBSMT model Gimpel and Smith (2008) Zh→En[NIST 08, UN][L] De→En[WMT 07][L] En→De[WMT 07, 08][L] SL:lexical, shallow syntactic and positional features PBSMT model Venkatapathy and Bangalore (2007) En→Hi[News][S] SL:bag-of-words Proposed global lexical selection mod el Specia et al.</S>
			<S sid ="87" ssid = "26">(2008) En→Pt[Europarl][L] SL:morphological features (person, tense and number) Dependency treelet system Hasan et al.</S>
			<S sid ="88" ssid = "27">(2008) Zh→En[IWSLT][S] TL:words IBM model Sp→En[EPPS][L] En→Sp[EPPS][L] Brunning et al.</S>
			<S sid ="89" ssid = "28">(2009) Ar→En[NIST 08][S] SL and TL:POS tag MTTK WA model En→Ar[NIST 08][S] En English, Fr French, De German, Sp Spanish, Zh Chinese, Ar Arabic, Pt Portuguese, Hi Hindi, WA Word alignment to develop a refined context-dependent lexicon model.</S>
			<S sid ="90" ssid = "29">They report better alignment quality in terms of improved alignment error rate (AER) (Och and Ney 2000).</S>
			<S sid ="91" ssid = "30">Patry and Langlais (2009) propose an alignment model which does not assume word alignments and considers all source words jointly when evaluating the probability of a target word.</S>
			<S sid ="92" ssid = "31">They use a multilayer perceptron classifier to estimate this probability.</S>
			<S sid ="93" ssid = "32">The word alignment results surpass IBM model 1 when their model is extended to include alignment information.</S>
			<S sid ="94" ssid = "33">3.1.2 Discriminative translation filtering Discriminative translation filtering in SMT, in which contextual information from the source language is used to weight or select from the potentially large set of lexical or phrasal translations, can furthermore be divided into two categories: (i) hard interaction (e.g. Carpuat and Wu 2005) and (ii) soft interaction (e.g. Chan et al. 2007; Stroppa et al. 2007).</S>
			<S sid ="95" ssid = "34">Alternatively, the same techniques can also be classified according to their use of (i) hard constraints (e.g. Stroppa et al. 2007) or (ii) soft constraints (e.g. Carpuat and Wu 2007; Marton and Resnik 2008; Xiong et al. 2010).</S>
			<S sid ="96" ssid = "35">• Hard vs. soft interaction: In soft interaction, WSD-like translation predictions from context-informed translation models are allowed to interact with other log-linear models (e.g. the target language model, a distortion model, additional translation models, etc.) in the decoder.</S>
			<S sid ="97" ssid = "36">The additional context-informed model thus adds its influence to the mix of models integrated during decoding.</S>
			<S sid ="98" ssid = "37">In hard interaction, the WSD-like translation predictions are used for pre-processing or post-processing, and do not interfere with the SMT process itself.</S>
			<S sid ="99" ssid = "38">In other words, the weights of the context-dependent translations of a source phrase are not integrated with other SMT models to select the best candidate translations.</S>
			<S sid ="100" ssid = "39">• Hard vs. soft constraints: In the soft constraints model, the decoder is allowed to use all possible candidate phrases for a source phrase, while soft constraints such as weights are introduced to influence the decoder’s lexical selection model.</S>
			<S sid ="101" ssid = "40">In the hard constraints model, the decoder is forced to use a restricted but supposedly more appropriate set of candidate phrases for a source phrase; in addition, the context-informed model imposes weights on the candidate phrases on the basis of additional contextual information to influence the decoder’s lexical selection model.</S>
			<S sid ="102" ssid = "41">According to this division, analogous to the work of (Stroppa et al. 2007), our context-informed models interact ‘softly’ with the other SMT models, and we impose hard constraints on the decoder.</S>
			<S sid ="103" ssid = "42">Discriminative translation filtering in SMT can further be divided into the following four categories according to its deployment into different types of SMT engines: • Word-based SMT : Table 1 lists related research that integrates context into word- based SMT models.</S>
			<S sid ="104" ssid = "43">Brown et al.</S>
			<S sid ="105" ssid = "44">(1991) were the first to propose the use of dedicated WSD models in word-based SMT systems, using an English-to-French translation task as their testbed.</S>
			<S sid ="106" ssid = "45">An instance of a word is assigned a sense based on mutual information with the word’s translation.</S>
			<S sid ="107" ssid = "46">Evaluation is limited to the case of binary disambiguation, i.e. deciding between only the two most probable translation candidates, and to a reduced set of common words.</S>
			<S sid ="108" ssid = "47">A significant improvement in translation quality is reported, according to manual evaluation.</S>
			<S sid ="109" ssid = "48">Vickrey et al.</S>
			<S sid ="110" ssid = "49">(2005) build classifiers inspired by those used in WSD to fill in blanks in a partially completed translation.</S>
			<S sid ="111" ssid = "50">This blank-filling task is a limited subtask of the translation task, in which the (possibly incorrect) target context surrounding the word translation is already available.</S>
			<S sid ="112" ssid = "51">Carpuat and Wu (2005) integrate a WSD model into a word-based SMT system in two ways: (i) the WSD model constrains the set of potential senses considered by the decoder; and (ii) the SMT output is post-processed by directly replacing translation candidates with the WSD predictions.</S>
			<S sid ="113" ssid = "52">The integration of the WSD model into the SMT system is performed in a hard manner, and both approaches are found to hurt translation quality.</S>
			<S sid ="114" ssid = "53">• Phrase-Based SMT : Table 2 summarizes related research on integrating contextinto PBSMT models.</S>
			<S sid ="115" ssid = "54">Stroppa et al.</S>
			<S sid ="116" ssid = "55">(2007) successfully add source-side contex tual features into a log-linear PBSMT system (Koehn et al. 2003) by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch 2005).</S>
			<S sid ="117" ssid = "56">Several proposals have recently been formulated that exploit the accuracy and the flexibility of discriminative learning (Liang et al. 2006; Tillmann and Zhang 2006).</S>
			<S sid ="118" ssid = "57">Work of this type generally requires a redefinition of the training procedure; in contrast, Stroppa et al.</S>
			<S sid ="119" ssid = "58">(2007) introduce a discriminative component that can be built into PBSMT systems, retaining the strength of the latter.</S>
			<S sid ="120" ssid = "59">More recent approaches of integrating state-of-the-art WSD methods into PBSMT (Carpuat and Wu 2007; Giménez and Màrquez 2007; Gimpel and Smith 2008; Max et al. 2008) have also met with success in improving the overall translation quality.</S>
			<S sid ="121" ssid = "60">Giménez and Màrquez (2007) extend the work of Vickrey et al.</S>
			<S sid ="122" ssid = "61">(2005) by (i) considering the more general case of frequent phrases and moving to full translation rather than the blank-filling task on the target side, and (ii) moving from word translation to phrase translation.</S>
			<S sid ="123" ssid = "62">Giménez and Màrquez (2009) show that their discriminative models yield significantly improved lexical choice over a PBSMT model, which in turn does not necessarily lead to improved gram- maticality.</S>
			<S sid ="124" ssid = "63">Carpuat and Wu (2007) incorporate a phrase-sense disambiguation model directly into a state-of-the-art PBSMT system, producing consistent gains across all evaluation metrics on the IWSLT 2006 and NIST Chinese-to-English translation tasks.</S>
			<S sid ="125" ssid = "64">• Hierarchical phrase-based SMT : Table 3 lists related research that integrates context into hierarchical PBSMT models.</S>
			<S sid ="126" ssid = "65">Chan et al.</S>
			<S sid ="127" ssid = "66">(2007) were the first to use a WSD system to integrate additional features in hierarchical phrase-based SMT (HPBSMT) (Chiang 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation.</S>
			<S sid ="128" ssid = "67">More recently, Shen et al.</S>
			<S sid ="129" ssid = "68">(2009) proposed a method to include linguistic and contextual information in the HPBSMT model.</S>
			<S sid ="130" ssid = "69">While their source-side dependency language model does not produce improvements, the other features seem to be effective in Arabic-to-English and Chinese-to-English translation.</S>
			<S sid ="131" ssid = "70">Chiang et al.</S>
			<S sid ="132" ssid = "71">(2009) define new translational features using neighbouring word context of the source phrase, which are directly integrated into both the translation model of the Hiero system (Chiang 2007) and the syntax-based system of Galley et al.</S>
			<S sid ="133" ssid = "72">(2006).</S>
			<S sid ="134" ssid = "73">• Alternative SMT architectures: Table 4 lists related research that integrates context into alternative SMT models.</S>
			<S sid ="135" ssid = "74">Bangalore et al.</S>
			<S sid ="136" ssid = "75">(2007) propose an SMT architecture based on stochastic finite state transducers that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence.</S>
			<S sid ="137" ssid = "76">Ittycheriah et al.</S>
			<S sid ="138" ssid = "77">(2007) introduce the Direct Translation Model 2 (DTM2) which employs discriminative MaxEnt models to obtain the translation likelihoods.</S>
			<S sid ="139" ssid = "78">Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).</S>
			<S sid ="140" ssid = "79">3.2 Target context modelling.</S>
			<S sid ="141" ssid = "80">Table 6 enumerates related research that integrates context into word alignment models.</S>
			<S sid ="142" ssid = "81">Berger et al.</S>
			<S sid ="143" ssid = "82">(1996) suggest context-sensitive modelling of word translations in order to integrate local contextual information into their IBM translation models using a MaxEnt model.</S>
			<S sid ="144" ssid = "83">Probability distributions are estimated by MaxEnt based on position- sensitive local collocation features in a window of three words around the target word.</S>
			<S sid ="145" ssid = "84">This work is not supported by significant evaluation results.</S>
			<S sid ="146" ssid = "85">Mauser et al.</S>
			<S sid ="147" ssid = "86">(2009) extend the work of Hasan et al.</S>
			<S sid ="148" ssid = "87">(2008) by integrating additional discriminative word lexicons into the PBSMT model, by using sentence-level source information to predict appropriate target words.</S>
			<S sid ="149" ssid = "88">3.3 English as source language.</S>
			<S sid ="150" ssid = "89">It is a common belief that translating into a less inflected language (such as English) from a highly inflected language should be more effective than the other way round.</S>
			<S sid ="151" ssid = "90">This belief is hardly challenged in the related work cited in this section; all above-men- tioned twenty nine studies translate to English.</S>
			<S sid ="152" ssid = "91">Nonetheless, Table 6 lists six studies that take English as the source language.</S>
			<S sid ="153" ssid = "92">Most of these studies employ contextual features computed on the English input; the ample availability of Natural Language Processing (NLP) tools for English, such as part-of-speech taggers and parsers, makes this possible.</S>
			<S sid ="154" ssid = "93">Both Max et al.</S>
			<S sid ="155" ssid = "94">(2008) and Gimpel and Smith (2008) work with a state- of-the-art PBSMT system (Koehn et al. 2003) and focus on language pairs where the target is not English.</S>
			<S sid ="156" ssid = "95">Max et al.</S>
			<S sid ="157" ssid = "96">(2008) conduct experiments on English-to-French and show modest gains over a PBSMT baseline model according to manual evaluation.</S>
			<S sid ="158" ssid = "97">Gimpel and Smith (2008) work with two different English-to-German data sets (WMT’07 and WMT’08) and perform a range of experiments.</S>
			<S sid ="159" ssid = "98">Most gains are not statistically significant with the WMT’07 translation task, but with the WMT’08 task most gains are statistically significant.</S>
			<S sid ="160" ssid = "99">Venkatapathy and Bangalore (2007) conduct experiments on a small amount of English-to-Hindi training data with a global lexical selection and sentence reconstruction model.</S>
			<S sid ="161" ssid = "100">Their bag-of-words model considers all words of the source sentence as features, regardless of their positions.</S>
			<S sid ="162" ssid = "101">Using these features, a MaxEnt-based classifier predicts target words that should occur in the target sentence.</S>
			<S sid ="163" ssid = "102">The target sentence is determined by permuting the generated target words using a language model.</S>
			<S sid ="164" ssid = "103">In an English-to-Portuguese translation task, Specia et al.</S>
			<S sid ="165" ssid = "104">(2008) work with a syntactically motivated PBSMT system (Quirk et al. 2005), which they enrich by a WSD model limited to disambiguating ten highly frequent and ambiguous verbs.</S>
			<S sid ="166" ssid = "105">Two more studies (Hasan et al. 2008; Brunning et al. 2009) consider English as the source language.</S>
			<S sid ="167" ssid = "106">Both approaches focus on improving word alignment for creating refined word-to-word translation lexicons.</S>
			<S sid ="168" ssid = "107">Hasan et al.</S>
			<S sid ="169" ssid = "108">(2008) present an integration of target context modelling into SMT using a triplet lexicon model that captures long- distance dependencies.</S>
			<S sid ="170" ssid = "109">Brunning et al.</S>
			<S sid ="171" ssid = "110">(2009) introduce context-dependent alignment models for MT that exploit source-language context information to estimate word- to-word translation probabilities using a decision tree algorithm.</S>
			<S sid ="172" ssid = "111">We see from the above summary tables that a range of contextual features have been employed at different stages in the SMT model.</S>
			<S sid ="173" ssid = "112">In the present work, we integrate a range of contextual features into a state-of-the-art PBSMT model, Moses (Koehn et al. 2003, 2007), including neighbouring words and POS tags of the source phrases as in (Giménez and Màrquez 2007; Stroppa et al. 2007).</S>
			<S sid ="174" ssid = "113">We introduce lexical syntactic descriptions in the form of supertags and semantic roles as new contextual features in the PBSMT model.</S>
			<S sid ="175" ssid = "114">Moreover, we investigate the integration of deep syntactic information (grammatical dependency relations) into the PBSMT models as in (Carpuat and Wu 2007; Max et al. 2008).</S>
			<S sid ="176" ssid = "115">A more detailed account of the difference between our approach and those of (Carpuat and Wu 2007; Max et al. 2008) with respect to various aspects is given in Sect.</S>
			<S sid ="177" ssid = "116">5.4.</S>
			<S sid ="178" ssid = "117">In this contribution we report on a range of experiments on several data sets, mostly adopting English as the source language of the translation pairs (English-to-Hindi, English-to-Czech, English-to-Dutch, English-to-Chinese, English- to-Japanese, English-to-Spanish).</S>
			<S sid ="179" ssid = "118">We examine the scalability of our research to larger amounts of training data.</S>
			<S sid ="180" ssid = "119">Furthermore, we report on experiments with Dutch- to-English translation, using experimental data from two different domains.</S>
			<S sid ="181" ssid = "120">In the next section we provide a brief overview of our baseline PBSMT system.</S>
	</SECTION>
	<SECTION title="PB-SMT baseline. " number = "4">
			<S sid ="182" ssid = "1">The translation task in SMT can be viewed as a search problem (Brown et al. 1993) in which the goal is to find the most probable candidate translation e I = e1 ,..., eI for the given input sentence 1 = f1 ,..., f J . The best translation can be obtained applying the noisy channel model (Brown et al. 1990) of translation by maximizing P(e I | f J ), as in (1): 1 1 argmax P(e I | f J ) = argmax P( f J |e I ).</S>
			<S sid ="183" ssid = "2">P (e I ) (1) 1 1 I ,e I I ,e I 1 1 1 where P( f J |e I ) and P(e I ) denote respectively the translation model and the target 1 1 1 language model (Brown et al. 1993).</S>
			<S sid ="184" ssid = "3">The log-linear translation model (Och and Ney 2002) is a special case of the noisy channel model of translation, in which the posterior probability P(e I | f J ) is directly 1 1modelled as a (log-linear) combination of features, that usually comprise M transla tional features, and the language model, as in (2): M log P(e I | f J ) = &quot;) λm hm ( f J , e I , s K ) + λLM log P(e I ) (2) 1 1 m=1 1 1 1 1 where s K = s1 ,..., sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases ( fˆ1 ,..., fˆk ) and (eˆ1 ,..., eˆk ) such that (we set i0 := 0): ∀k ∈ [1, K ] sk := (ik ; bk , jk ), (bk corresponds to starting index of fk ) eˆk := eˆik−1 +1 ,..., eˆik , fˆk := fˆbk ,..., fˆjk Each feature hm in Eq. 2 can be rewritten as in (3): K hm ( f J , e I , s K ) = &quot;) hˆ m ( fˆk , eˆk , sk ) (3) 1 1 1 k=1 In theory, log-linear PBSMT features can apply to the entire sentence, but in practice, those features apply to a single phrase-pair ( fˆk , eˆk ).</S>
			<S sid ="185" ssid = "4">Thus translational features in (2) can be rewritten as in (4): M K K M &quot;) λm &quot;) hˆ m ( fˆk , eˆk , sk ) = &quot;) h˜ ( fˆk , eˆk , sk ), with h˜ = &quot;) λm hˆ m (4) m=1 k=1 k=1 m=1 In Eq. 4, hˆ m is a feature defined on phrase-pairs ( fˆk , eˆk ), and λm is the feature weight of hˆ m . One intuitively natural feature is the phrase translation log-probability (hˆ m = log P(eˆk | fˆk )) where probabilities are estimated using relative frequency count for a phrase pair ( fˆk , eˆk ) independent of any other context information.</S>
			<S sid ="186" ssid = "5">Other typical features used in PBSMT (Koehn et al. 2003) are derived from the inverse phrase translation probability (log P( fˆk |eˆk )), the lexical probability (log Plex (eˆk | fˆk )), and its inverse (log Plex ( fˆk |eˆk )).</S>
			<S sid ="187" ssid = "6">Our context-informed model will be expressed as additional features in the model.</S>
	</SECTION>
	<SECTION title="Context-informed features. " number = "5">
			<S sid ="188" ssid = "1">We conjecture that context-dependent phrase translation can be expressed as a multi- class classification problem, where a source phrase with given additional context information is classified into a distribution over possible target phrases.</S>
			<S sid ="189" ssid = "2">The size of this distribution is possibly limited, and would ideally omit improbable or irrelevant target phrase translations that the standard PBSMT approach would normally include.</S>
			<S sid ="190" ssid = "3">A context-informed feature hˆ mbl can be viewed as the conditional probability of the target phrase eˆk given the source phrase fˆk and its context information (CI), as in (5): hˆ mbl = log P(eˆk | fˆk , CI( fˆk )) (5) Here, CI may include any feature (lexical, syntactic, semantic etc.), which can provide useful information to disambiguate the given source phrase.</S>
			<S sid ="191" ssid = "4">The features used in our experiments are described in the following subsections.</S>
			<S sid ="192" ssid = "5">5.1 Lexical features.</S>
			<S sid ="193" ssid = "6">Lexical features include the immediately neighbouring words within l token positions to the left and right (resp.</S>
			<S sid ="194" ssid = "7">fik −l ... fik −1 and f jk +1 ... f jk +l ) of a given focus phrase fˆk = fik ... f jk . Lexical features thus form a window of size 2l . The lexical contextual information (CIlex ) can be described as in (6): CIlex ( fˆk ) = { fik −l ,..., fik −1 , f jk +1 ,..., f jk +l } (6) Taking the example sentence ‘Can you play my favourite old record?’ from Fig.</S>
			<S sid ="195" ssid = "8">1, for the single-word focus phrase ‘play’, CIlex = {can, you, my, favourite} with l = 2, i.e. a left and right context of two neighbouring words.</S>
			<S sid ="196" ssid = "9">5.2 Part-of-speech tags.</S>
			<S sid ="197" ssid = "10">In addition to the immediate lexical neighbours of a phrase it is possible to exploit other linguistic information sources characterizing the context.</S>
			<S sid ="198" ssid = "11">For example, we may consider the part-of-speech (POS) tags of the context words, as well as of the focus phrase itself.</S>
			<S sid ="199" ssid = "12">In our model, the POS tag of a multi-word focus phrase is the concatenation of the POS tags of the words composing that phrase.</S>
			<S sid ="200" ssid = "13">We generate a window of size 2l + 1 features, including the concatenated complex POS tag of the focus phrase.</S>
			<S sid ="201" ssid = "14">Accordingly, the POS-based contextual information (CIpos ) is described as in (7): CIpos ( fˆk ) = {pos( fik −l ),..., pos( fik −1 ), pos( fˆk ), pos( f jk +1 ), ..., pos( f jk +l )} (7) For the example sentence (‘Can you play my favourite old record?’) (Fig.</S>
			<S sid ="202" ssid = "15">1), the POS-based CI for the focus phrase ‘play’ is formed as: CIpos = {pos(can), pos(you), pos(play), pos(my), pos(favourite)} = {MD, PRP, VB, PRP$, JJ} (with l = 2).</S>
			<S sid ="203" ssid = "16">5.3 Supertags.</S>
			<S sid ="204" ssid = "17">Supertags represent complex linguistic categories that express the specific syntactic behaviour of a word in terms of the arguments it takes, and more generally the syntactic environment in which it appears.</S>
			<S sid ="205" ssid = "18">In our experiments two types of supertags are employed: those from lexicalized tree-adjoining grammar, LTAG (Bangalore and Joshi 1999), and combinatory categorial grammar, CCG (Steedman 2000).</S>
			<S sid ="206" ssid = "19">Both the LTAG (Chen et al. 2006) and the CCG (Hockenmaier 2003) supertag sets were acquired from the WSJ section of the PennII Treebank using hand-built extraction rules.</S>
			<S sid ="207" ssid = "20">Here we use both the LTAG (Bangalore and Joshi 1999) and CCG (Clark and Curran 2004) super- taggers.</S>
			<S sid ="208" ssid = "21">In LTAG, a lexical item is associated with an elementary tree, while in CCG the supertag constitutes a CCG lexical category with a set of word-to-word dependencies.</S>
			<S sid ="209" ssid = "22">The two alternative supertag descriptions can be viewed as closely related functional descriptors of words.</S>
			<S sid ="210" ssid = "23">Figures 2 and 3 show the CCG and LTAG supertags respectively for the example sentence ‘Can you play my favourite old record?’ Both CCG and LTAG supertags can be combined into parse trees with specific operations (see Figs.</S>
			<S sid ="211" ssid = "24">2 and 3).</S>
			<S sid ="212" ssid = "25">The supertag of a word encodes potentially long-distance syntactic relations with other words in the Can you play my favourite old record?</S>
			<S sid ="213" ssid = "26">(S/(S\NP))/NP NP (S\NP)/NP NP/N N/N N/N N S/(S\NP) N N N P S\NP S Fig.</S>
			<S sid ="214" ssid = "27">2 Example of CCG supertags.</S>
			<S sid ="215" ssid = "28">CCG supertags are combined under the operations of forward and backward applications into a parse tree SQ NP S SQ NP MD S MD S* Can PRP you NP VP VB NP PRP$ my ADJP* Can NP PRP VP VB NP play you play PRP$ ADJP ADJP ADJP NP my JJ ADJP JJ ADJP* JJ NP* NNS favourite JJ NP favourite old record?</S>
			<S sid ="216" ssid = "29">old NNS record?</S>
			<S sid ="217" ssid = "30">Fig.</S>
			<S sid ="218" ssid = "31">3 Example of LTAG supertags.</S>
			<S sid ="219" ssid = "32">LTAG supertags are combined under the operations of substitution and adjunction into a parse tree sentence, which could be informative as a feature both of the focus phrase as well as of words neighboring the phrase.</S>
			<S sid ="220" ssid = "33">As with CIpos , we define the contextual information (CIst ) using supertags as in (8): CIst ( fˆk ) = {st( fik −l ), . . .</S>
			<S sid ="221" ssid = "34">, st( fik −1 ), st( fˆk ), st( f jk +1 ),..., st( f jk +l )} (8) Similar to the CIpos feature, we form the supertag for a multi-word focus phrase by concatenating the supertags of the words composing it.</S>
			<S sid ="222" ssid = "35">Thus, the supertagbasedCI constitutes a window of size 2l + 1 features.</S>
			<S sid ="223" ssid = "36">In our experiments, we consider con text widths of ±1 and ±2 (i.e. l = 1, 2) surrounding the focus phrase.</S>
			<S sid ="224" ssid = "37">For example, the CI of the focus phrase ‘play’ in Fig.</S>
			<S sid ="225" ssid = "38">2 with CCG supertags is formed as: CIst = {st(you), st(play), st(my)} = {NP, (S\NP)/NP, NP/N} (with l = 1).</S>
			<S sid ="226" ssid = "39">We also carried out experiments joining the two supertag types (CCG and LTAG) (cf.</S>
			<S sid ="227" ssid = "40">Sect.</S>
			<S sid ="228" ssid = "41">7.2.2).</S>
			<S sid ="229" ssid = "42">5.4 Dependency relations.</S>
			<S sid ="230" ssid = "43">So far, the context information (CI) of a source phrase ( fˆk ) is modeled as the sequence of features immediately before and after the focus phrase ( fˆk ).</S>
			<S sid ="231" ssid = "44">Although it can be argued that they offer a rich source of information to disambiguate the translation of a source phrase, they remain position-specific and local, and may therefore not provide all information needed for disambiguation.</S>
			<S sid ="232" ssid = "45">In order to compensate for this, we model position-independent contextual features related to the focus phrase; we choose to model grammatical dependencies linking from and to the head word of the focus phrase ( fˆk ) with words occurring elsewhere in the sentence.</S>
			<S sid ="233" ssid = "46">The identification of the head word of a phrase is nontrivial, as SMT phrases are not restricted to linguistically coherent phrases, so the identification of a head word cannot be done with linguistic rules of thumb (e.g. select the head noun from the noun phrase).</S>
			<S sid ="234" ssid = "47">In our work, we identify head words of SMT phrases with the use of a dependency tree generated for the sentence.</S>
			<S sid ="235" ssid = "48">For all words in a given source phrase, the word that occupies hierarchically the highest position in the dependency tree is aux sub root obj poss nmod nmod Can you play my favourite old record?</S>
			<S sid ="236" ssid = "49">Head word An SMT phrase PR= {frame_you_record} OE = {aux_obj_sub} PW= {null} Fig.</S>
			<S sid ="237" ssid = "50">4 The dependency parse tree of the English sentence Can you play my favourite old record?</S>
			<S sid ="238" ssid = "51">and the dependency features extracted from it for the SMT phrase play my favourite chosen as the head word.</S>
			<S sid ="239" ssid = "52">For example, Fig.</S>
			<S sid ="240" ssid = "53">4 shows the above English sentence (‘Can you play my favourite old record?’) and its dependency parse tree.</S>
			<S sid ="241" ssid = "54">In Fig.</S>
			<S sid ="242" ssid = "55">4 we see that the head word of an English phrase ‘play my favourite’ identified by the PBSMT system is ‘play’ according to the tree structure.</S>
			<S sid ="243" ssid = "56">We consider the following dependency features, drawing on the syntactic dependencies emanating from or pointing to the head word of the source focus phrase (see also Fig.</S>
			<S sid ="244" ssid = "57">4): OE (outgoing edges)—For the head word of the focus phrase, we extract a list of zero or more relations with other words of which the word is the parent (i.e. the dependency type labels on all modifying dependency relations).</S>
			<S sid ="245" ssid = "58">The list of relations is concatenated and sorted uniquely and alphabetically into a single feature.</S>
			<S sid ="246" ssid = "59">This feature is denoted as OE, for ‘outgoing edges’.</S>
			<S sid ="247" ssid = "60">For example, the head word (‘play’) of the focus phrase (‘play my favourite’) has three outgoing edges: auxiliary, subject and object (see Fig.</S>
			<S sid ="248" ssid = "61">4).</S>
			<S sid ="249" ssid = "62">Therefore, the OE feature is formed as: OE = {aux_obj_sub}.</S>
			<S sid ="250" ssid = "63">PR (parent relation)—For the head word of the focus phrase we extract the relation it has with its parent.</S>
			<S sid ="251" ssid = "64">If the head word is a verb, then the subcategorization frame information is extracted and used as this feature.</S>
			<S sid ="252" ssid = "65">This feature is denoted as PR, for ‘parent relation’.</S>
			<S sid ="253" ssid = "66">For example, the head word ‘play’ is a verb, and so we extract its subcategorization frame information from the tree, namely {frame_you_record}.</S>
			<S sid ="254" ssid = "67">PW (parent word)—Extending the PR feature, we encode the identity of the parent word of the head word of the focus phrase.</S>
			<S sid ="255" ssid = "68">This feature is denoted as PW, for ‘parent word’.</S>
			<S sid ="256" ssid = "69">For example, the head word ‘play’ is root according to the tree structure, and so we set the PW feature to null.</S>
			<S sid ="257" ssid = "70">Together we refer to these dependency features as the grammatical dependency information (CIdi ( fˆk )) of the focus phrase ( fˆk ).</S>
			<S sid ="258" ssid = "71">These dependency features can be applied both individually and jointly.</S>
			<S sid ="259" ssid = "72">For instance, a combination of three dependency features (PR, OE and PW) defines the contextual information CIdi ( fˆk ) as in (9): CIdi ( fˆk ) = {OE, PR, PW} (9) Two published studies are closely related to our work on integrating dependency features.</S>
			<S sid ="260" ssid = "73">Carpuat and Wu (2007) mention in passing that their WSD system uses basic dependency relations, but the nature of this information is not further described, nor is its effect.</S>
			<S sid ="261" ssid = "74">Max et al.</S>
			<S sid ="262" ssid = "75">(2008) exploit grammatical dependency information, in addition to information extracted from the immediate context of a source phrase.</S>
			<S sid ="263" ssid = "76">Our approach differs with (Max et al. 2008) at least in three respects: 1.</S>
			<S sid ="264" ssid = "77">Max et al.</S>
			<S sid ="265" ssid = "78">(2008) select a set of the 16 most informative dependency relations.</S>
			<S sid ="266" ssid = "79">for their experiments.</S>
			<S sid ="267" ssid = "80">Dependencies are considered that link any of the tokens in the given source phrase to tokens outside the phrase.</S>
			<S sid ="268" ssid = "81">Each dependency type is represented in the vector by the outside word it involves, or by the symbol ‘nil’, which indicates that this type of dependency does not occur in the phrase under consideration.</S>
			<S sid ="269" ssid = "82">In contrast to this approach, we used all (26) dependency relations in our experiments, while only extracting features from the head words of the SMT phrases.</S>
			<S sid ="270" ssid = "83">2.</S>
			<S sid ="271" ssid = "84">They filter out phrases from the phrase table for which P(eˆk | fˆk )&lt; 0.0002.</S>
			<S sid ="272" ssid = "85">In.</S>
			<S sid ="273" ssid = "86">contrast, we keep all phrase pairs.</S>
			<S sid ="274" ssid = "87">3.</S>
			<S sid ="275" ssid = "88">Their experimental data contains 95K English-to-French training sentence pairs,.</S>
			<S sid ="276" ssid = "89">while we carried out a range of experiments considering different data sizes, domains, and language pairs, elaborated further in Sect.</S>
			<S sid ="277" ssid = "90">7.</S>
			<S sid ="278" ssid = "91">We compare the dependency features with incorporating words, part-of-speech tags and supertags as context, in order to observe the relative effects of position-inde- pendent and position-dependent features.</S>
			<S sid ="279" ssid = "92">While supertags represent an abstract view upwards the tree or graph, excluding other lexical nodes and anything below the lowest common ancestors in the tree between other lexical nodes and the words captured in the contextual features, dependency relations directly encode relations between tokens.</S>
			<S sid ="280" ssid = "93">One can follow a dependency and retrieve the lexical modifier or head at any distance.</S>
			<S sid ="281" ssid = "94">5.5 Semantic roles.</S>
			<S sid ="282" ssid = "95">Semantic role labeling is an established benchmark task in NLP research since the 2004 CoNLL shared task (Carreras and Márquez 2004).</S>
			<S sid ="283" ssid = "96">The task is to identify the.</S>
			<S sid ="284" ssid = "97">semantic arguments associated with a clause’s predicate, and their classification into specific semantic roles with respect to the predicate.</S>
			<S sid ="285" ssid = "98">Typical roles include agent, theme, temporal or locative modifier, etc. The CoNLL 2008 shared task (Surdeanu et al. 2008) introduced a unified dependency-based formalism that modeled both syntactic dependencies and semantic roles for English.</S>
			<S sid ="286" ssid = "99">A dependency-based semantic role labeler (SRL) (Johansson and Nugues 2008) finds all semantic graphs around each predicate verb or noun in an input sentence in addition to the dependency parse tree.</S>
			<S sid ="287" ssid = "100">Recently, Wu and Fung (2009) utilized semantic roles in improving SMT accuracy by enforcing consistency between the semantic predicates and arguments across both the input sentence and the translation output.</S>
			<S sid ="288" ssid = "101">Inspired by Wu and Fung (2009), we introduce semantic information as a new contextual feature in PBSMT for a English- to-Dutch translation task.</S>
			<S sid ="289" ssid = "102">In order to obtain the semantic graph information of English AM_MOD A0 A1 A1 AM_TMP Can you play my favourite old record?</S>
			<S sid ="290" ssid = "103">PRED Head predicate An SMT phrase PRE D AL= {A0_A1_AM_MOD} PS = {04} Fig.</S>
			<S sid ="291" ssid = "104">5 The semantic graph of an English sentence and the semantic features extracted from it for an SMT phrase sentences, we used the LTH semantic parser1 (Johansson and Nugues 2008), which assigns both predicative (PropBank-based) and nominative (NomBank-based) graphs.The semantic information (CIsi ) of a source phrase ( fˆk ) originates from the seman tic (verbal or nominal) predicate captured in that phrase.</S>
			<S sid ="292" ssid = "105">This introduces three possible cases: (a) there is no predicate in the source phrase, (b) there is only one predicate in the source phrase, in which case this is chosen as the head predicate to define (CIsi ), and (c) more than one predicate occurs in the source phrase; for such cases, the predicate that occupies the hierarchically superior position in the dependency parse tree is chosen as the head predicate.</S>
			<S sid ="293" ssid = "106">For example, Fig.</S>
			<S sid ="294" ssid = "107">5 shows a English sentence (‘Can you play my favourite old record?’) and two semantic graphs around its two predicates (verbal: ‘play’ and nominal: ‘record’) identified by SRL.</S>
			<S sid ="295" ssid = "108">Figure 5 also shows an English phrase ‘play my favourite’ identified by our baseline PBSMT system, Moses, which contains only the verbal predicate ‘play’.</S>
			<S sid ="296" ssid = "109">In our experiments, two semantic features were generated for the head predicate: AL (argument labels)—We extract the list of one or more predicate roles (argument labels) of the head predicate of a source phrase.</S>
			<S sid ="297" ssid = "110">The list of roles is concatenated and sorted uniquely and alphabetically into a single feature.</S>
			<S sid ="298" ssid = "111">This feature is denoted as AL, for ‘argument labels’.</S>
			<S sid ="299" ssid = "112">For example, Fig.</S>
			<S sid ="300" ssid = "113">4 illustrates that the head predicate (‘play’) of the focus phrase (‘play my favourite’) has three semantic dependencies (argument labels): acceptor (A0), thing accepted (A1) and modal (AM_MOD).</S>
			<S sid ="301" ssid = "114">PS (predicate sense)—In addition to the semantic roles of a predicate, SRL attempts to disambiguate the sense of the predicate in the source sentence.</S>
			<S sid ="302" ssid = "115">We extract the sense of the head predicate of the source phrase.</S>
			<S sid ="303" ssid = "116">This feature is denoted as PS, for ‘predicate sense’.</S>
			<S sid ="304" ssid = "117">For example, the sense of the head predicate (‘play’) in the sentence in Fig.</S>
			<S sid ="305" ssid = "118">5 is {04}.</S>
			<S sid ="306" ssid = "119">The two features AL and PS are applied both individually and jointly.</S>
			<S sid ="307" ssid = "120">For instance, a combination of two semantic features defines the contextual information CIsi ( fˆk ) of the source phrase ( fˆk ) as in (10): CIsi ( fˆk ) = {AL, PS} (10) 1 http://nlp.cs.lth.se/software/.</S>
	</SECTION>
	<SECTION title="Memory-based classification. " number = "6">
			<S sid ="308" ssid = "1">Stroppa et al.</S>
			<S sid ="309" ssid = "2">(2007) pointed out that directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic.</S>
			<S sid ="310" ssid = "3">Indeed, Zens and Ney (2004) showed that the estimation of P(eˆk | fˆk ) using relative frequencies resultsin overestimation of the probabilities of long phrases; consequently, smoothing fac tors in the form of lexical-based features are often used to counteract this bias (Foster et al. 2006).</S>
			<S sid ="311" ssid = "4">In the case of context-informed features, this estimation problem can only become worse.</S>
			<S sid ="312" ssid = "5">As an alternative, we make use of memory-based machine learning classifiers that are able to estimate P(eˆk | fˆk , CI( fˆk )) by similarity-based reasoning over mem orized nearest-neighbour examples of source–target phrase translations to a new source phrase to be translated.</S>
			<S sid ="313" ssid = "6">In this work, we use two approximate memory-based classifiers: IGTree and TRIBL2 (Daelemans and van den Bosch 2005).</S>
			<S sid ="314" ssid = "7">Both algorithms approximate the unabridged (and computationally expensive) memory-based or k-nearest neighbour (k-NN) classifier (Aha et al. 1991).</S>
			<S sid ="315" ssid = "8">6.1 Fast approximate memory-based classification: IGTree.</S>
			<S sid ="316" ssid = "9">IGTree makes a heuristic approximation of k-NN search (Aha et al. 1991) by storing examples of source-target translation instances in the form of lossless-compressed decision trees, and performing a top-down traversal of this tree (Daelemans et al. 1997a).</S>
			<S sid ="317" ssid = "10">As a normal k-NN classifier, IGTree retains the labeling information of all training examples, but in a compressed form.</S>
			<S sid ="318" ssid = "11">In our case, a labeled example is a fixed-length feature-value vector representing the source phrase (as an atomic feature: both single-word and multi-word source phrases are treated as concatenated single values, just as its POS tags or supertags are) and its contextual information, associated with a symbolic class label representing the associated target phrase found through an alignment procedure.</S>
			<S sid ="319" ssid = "12">A weighting metric such as information gain (IG) is used to determine the order in which features are tested in the tree (Daelemans et al. 1997a).</S>
			<S sid ="320" ssid = "13">The source phrase itself is intuitively the feature with the highest prediction power; it should take precedence in the similarity-based reasoning, and indeed it does, as it always receives the highest IG value.</S>
			<S sid ="321" ssid = "14">Prediction in IGTree is a straightforward traversal of the decision tree from the root node down, where a step is triggered by a match between a feature value of the new example and an arc fanning out of the current node.</S>
			<S sid ="322" ssid = "15">When traversal ends in a leaf node, the homogeneous class (i.e. a single phrase translation) stored at that node is returned; when no match is found with an arc fanning out of the current node, the distribution of possible class labels at the current node is returned.</S>
			<S sid ="323" ssid = "16">In our case, this would be a weighted distribution of target phrase translations, where the weights denote the counts in the subset of the training set represented at the current node.</S>
			<S sid ="324" ssid = "17">As the source 2 An implementation of IGTree and TRIBL is freely available as part of the TiMBL software package,.</S>
			<S sid ="325" ssid = "18">which can be downloaded from http://ilk.uvt.nl/timbl.</S>
			<S sid ="326" ssid = "19">phrase in focus is the first feature tested, when an input mismatches on the source phrase, the prior target phrase distribution in the training set is returned.</S>
			<S sid ="327" ssid = "20">6.2 A hybrid between k-NN and IGTree: TRIBL TRIBL, which stands for Tree-based approximation of Instance-Based Learning, a hybrid combination of IGTree and unabridged k-NN classification, performs heuristic approximate nearest neighbour search (Daelemans et al. 1997b).</S>
			<S sid ="328" ssid = "21">A parameter n determines the switching point in the feature ordering from IGTree to normal k-NN classification.</S>
			<S sid ="329" ssid = "22">The TRIBL approximation performs an initial decision-tree split of the database of training examples on the n most informative features, like IGTree would.</S>
			<S sid ="330" ssid = "23">Throughout our experiments we set n = 1; thus, we split on the values of the single most informative feature according to information gain (IG).</S>
			<S sid ="331" ssid = "24">During classification, after sub-selecting training examples matching on the most informative feature, the nearest-neighbour distance function is applied to the remaining features (weighted by their IG) to arrive at the set of nearest-neighbours.</S>
			<S sid ="332" ssid = "25">When predicting a target phrase given a source phrase and its context information, the identity of the source phrase is effectively (and also intuitively) the feature with the highest prediction power.</S>
			<S sid ="333" ssid = "26">This implies that nearest neighbours always match on the source phrase, and are most similar (preferably, identical) with respect to their contextual features.</S>
			<S sid ="334" ssid = "27">A parameter k determines the k closest radii of distances around the source phrase that encompass the nearest neighbours; then, the distribution of target phrases associated with these nearest neighbours is taken as the output of the classification step.</S>
			<S sid ="335" ssid = "28">The contribution of a single nearest neighbour in this set can be weighted by its distance to the source phrase to be translated, e.g. by assigning higher weights to closer neigh- bours.</S>
			<S sid ="336" ssid = "29">In our experiments, we empirically set the value of k, and use exponential decay for the distance-weighted class voting (Daelemans and van den Bosch 2005).</S>
			<S sid ="337" ssid = "30">Choosing the optimal setting of k can be handled empirically, as are other hyperparameter settings for the k-NN part of TRIBL.</S>
			<S sid ="338" ssid = "31">We used a heuristic-automated hyperparameter estimation method based on wrapped progressive sampling (Van den Bosch 2004)3 throughout our experiments.</S>
			<S sid ="339" ssid = "32">Thus, TRIBL’s hyperparameters are retuned with each experiment.</S>
			<S sid ="340" ssid = "33">A TRIBL classification produces a class distribution derived from the aggregate distance-weighted class voting generated by all found nearest neighbours, from whichwe estimate P(eˆk | fˆk , CI( fˆk )) for all possible eˆk . By normalizing the class votes gen erated by TRIBL, we obtain the posterior probability distributions we are interested in.</S>
			<S sid ="341" ssid = "34">Although TRIBL is a fast approximation of unrestricted k-NN, it retains its relatively large memory consumption, which becomes hard to handle on current computing machinery when the number of examples is of the order of 107 or higher.</S>
			<S sid ="342" ssid = "35">In the experiments reported in this paper, experiments with small-scale data sets are typically performed with TRIBL; experiments on large-scale data sets are performed with IGTree.</S>
			<S sid ="343" ssid = "36">In Sect.</S>
			<S sid ="344" ssid = "37">7.4.1 we compare the two approaches directly in a learning curve study.</S>
			<S sid ="345" ssid = "38">6.3 Feature integration.</S>
			<S sid ="346" ssid = "39">The output of memory-based k-NN classification is a set of weighted class labels, representing the possible target phrases (eˆk ) given a source phrase ( fˆk ) and its context information (CI).</S>
			<S sid ="347" ssid = "40">Once normalized, these weights can be seen as the posterior probabilities of the target phrases (eˆk ) which give access to P(eˆk | fˆk , CI( fˆk )).</S>
			<S sid ="348" ssid = "41">Thus, from the classifier’s output we can derive the feature hˆ mbl defined in Eq. 5.</S>
			<S sid ="349" ssid = "42">In addition to hˆ mbl , we derive a simple two-valued feature hˆ best , defined as in Eq. 11: hˆ best = 1 if eˆk maximizes P(eˆk | fˆk , CI( fˆk )) (11) 0 otherwise where hˆ best is set to 1 when eˆk is one of the target phrases with highest probability according to P(eˆk | fˆk , CI(eˆk )); otherwise, hˆ best is set to 0.000001.</S>
			<S sid ="350" ssid = "43">We performed experiments by integrating these two features hˆ mbl and hˆ best directly into the log linear model of Moses.</S>
			<S sid ="351" ssid = "44">Their weights are optimized using minimum error-rate training (MERT)(Och 2003) on a held-out development set for each of the experiments.</S>
			<S sid ="352" ssid = "45">As Stroppa et al.</S>
			<S sid ="353" ssid = "46">(2007) point out, PBSMT decoders such as Pharaoh (Koehn 2004a) or Moses (Koehn et al. 2007) rely on a static phrase table, represented as a list of aligned phrases accompanied by several estimated metrics.</S>
			<S sid ="354" ssid = "47">Since these features do not express the context information in which those phrases occur, no context information is kept in the phrase table, and there is no way to recover this information from the phrase table.</S>
			<S sid ="355" ssid = "48">In order to take into account the context-informed features within such decoders, the test set to be translated is preprocessed.</S>
			<S sid ="356" ssid = "49">Each word appearing in the test set (or, during development, in the development set) is assigned a unique identifier.</S>
			<S sid ="357" ssid = "50">First we derive the phrase table from the training data.</S>
			<S sid ="358" ssid = "51">Subsequently, we generate all possible phrases from the test set.</S>
			<S sid ="359" ssid = "52">These phrases are then looked up in the phrase table, and when found, the phrase along with its contextual information is given to the memory-based classifier to be classified.</S>
			<S sid ="360" ssid = "53">As stated above, memory-based classifiers produce target phrase distributions according to the training examples found within the k-nearest distance radii around the source phrase to be classified.</S>
			<S sid ="361" ssid = "54">We derive target phrase probabilities from this distribution and temporarily insert them into a new phrase table with the original phrase table estimates, to directly take our feature functions into account in the log-linear model.</S>
			<S sid ="362" ssid = "55">Thus we create an updated phrase table.</S>
			<S sid ="363" ssid = "56">A lexicalized reordering model is used for all the experiments undertaken on the development and test sets.</S>
			<S sid ="364" ssid = "57">The source phrases in the reordering table are replaced by the sequence of unique identifiers when the new phrase table is created.</S>
			<S sid ="365" ssid = "58">After replacing all words by their unique identifiers, we perform MERT using our updated phrase table to optimize the feature weights.</S>
			<S sid ="366" ssid = "59">6.4 Classification example.</S>
			<S sid ="367" ssid = "60">In this section, we give an example to illustrate how a source phrase with given additional context information is classified into a distribution over possible targetphrases.</S>
			<S sid ="368" ssid = "61">We consider a particular contextual feature (CCG±1) to illustrate the classi fication task.</S>
			<S sid ="369" ssid = "62">We select a source English sentence (‘let me make a suggestion to the commission as to how this problem could be tackled’) from the development set of the English-to-Spanish translation task.4 This sentence contains the ambiguous single- word phrase ‘make’, of which the contextually appropriate Spanish translation would be ‘hacer’.</S>
			<S sid ="370" ssid = "63">Following Eq. 8, we take the CCG supertags of the neighbouring (±1) words around the source phrase ‘make’ in order to form its context information, namely: CI (make) = {st(me), st(make), st(a)} = {(S\NP)/NP, NP, NP/N}.</S>
			<S sid ="371" ssid = "64">Thus, we form a test example (make, CI± which is given to the classifier for classification.</S>
			<S sid ="372" ssid = "65">As part of the earlier offline training phase, millions of training examples are generated that take the same form as the test examples, labeled with classes (aligned target phrases); one example for each alignment in the training data.</S>
			<S sid ="373" ssid = "66">A memory-based classifier is then trained on these millions of training examples.</S>
			<S sid ="374" ssid = "67">During decoding, we generate all possible test examples from the test set and give them to the classifier in order to obtain possible translations of the source phrases.</S>
			<S sid ="375" ssid = "68">For the above test example ((make, CI±) we classify it with the relevant classifier, whichgives us a class distribution in the form of a list of target phrases which are context-sen sitive translations of the source phrase ‘make’.</S>
			<S sid ="376" ssid = "69">A weight is associated with each class.</S>
			<S sid ="377" ssid = "70">From these weights we estimate the probabilities of translation into target phrases (eˆk ) from the source phrase ‘make’ with its additional context information, which areP(eˆk |make, CI(make)) (where k denotes the distribution size).</S>
			<S sid ="378" ssid = "71">According to the clas sification result, we found that ‘hacer’ is the translation with the highest weight for the source phrase ‘make’, i.e. the translation probability (P(hacer|make, CI(make))) is the highest.</S>
			<S sid ="379" ssid = "72">Table 7 shows some of the possible Spanish translations of the English phrase ‘make’ including ‘hacer’ with their memory-based context-dependent translation probabilities (i.e. memory-based scores: P(eˆk | fˆk , CI( fˆk ))) compared with con text-independent translation probabilities (i.e. baseline scores: P(eˆk | fˆk )).</S>
			<S sid ="380" ssid = "73">Table 7 shows that the memory-based classifier assigns a relatively higher score to the most suitable Spanish phrase ‘hacer’, while it assigns lower scores to three of the four alternative translations listed.</S>
			<S sid ="381" ssid = "74">The baseline phrase translation probability is estimated using the relative frequency counts of source and target phrases.</S>
			<S sid ="382" ssid = "75">Additionally we report the target phrase distribution size (TPDS, bottom line in Table 7) for the source phrase ‘make’ in the baseline system, 388 phrases, as well as in our memory-based model, 181 phrases.</S>
			<S sid ="383" ssid = "76">This illustrates how the memory-based classifier typically produces a reduced set of target phrases for a given source phrase in context.</S>
			<S sid ="384" ssid = "77">As an additional point of analysis, we also compared the log-linear weights (λi ) of the context-informed memory-based features hˆ mbl (cf.</S>
			<S sid ="385" ssid = "78">Eq. 5) and hˆ best (cf.</S>
			<S sid ="386" ssid = "79">Eq. 11) with the weight of a baseline feature derived from the forward phrase translation probability (hˆ base = log P(eˆk | fˆk )) for the above experiment (CCG±1, an English-to-Spanish Table 7 Some of the possible Spanish translations of the English phrase make with their memory-based context-dependent translation probabilities (rightmost column) compared against context-independent translation probabilities of the baseline system B a s e li n e P (e ˆ k | fˆk ) C C G ± 1 P ( e ˆ k | f ˆ k , C I f ˆ k ) hace r 0 . 2 3 5 3 0 . 3 4 1 2 haga n 0 . 0 8 5 1 0 . 0 0 5 7 reali zar 0 . 0 2 7 7 0 . 0 3 0 5 hace n 0 . 0 2 4 5 0 . 0 0 7 3 haga 0 . 0 1 4 2 0 . 0 1 1 3 ...</S>
			<S sid ="387" ssid = "80">TPD S 3 8 8 1 8 1 TPD S targe t phra se distri butio n size Table 8 Weights of different log-linear features of the CCG±1 system hˆ base hˆ mbl hˆ best 0.0263951 0.0463334 0.00507119 translation task described further in Sect.</S>
			<S sid ="388" ssid = "81">7.4.1).</S>
			<S sid ="389" ssid = "82">The λi of the various log-linear features directly affects the phrase scores during translation; thus, λi plays a crucial role in selecting the most appropriate candidate phrases.Table 8 indicates that our context-informed models (hˆ mbl , hˆ best ) contribute posi tively to the phrase-scoring process during translation.</S>
			<S sid ="390" ssid = "83">Moreover, MERT (Och 2003)assigns a notably higher weight to the context-informed feature hˆ mbl than the baseline feature hˆ base , directly indicating the importance of the memory-based context informed models.</S>
	</SECTION>
	<SECTION title="Experiments and results. " number = "7">
			<S sid ="391" ssid = "1">We carried out experiments by systematically applying various source-side contextual features for different language pairs with varying training data sizes.</S>
			<S sid ="392" ssid = "2">The system outputs are evaluated across a wide range of automatic evaluation metrics: BLEU (Papineni et al. 2002), NIST (Doddington 2002), METEOR (Lavie and Agarwal 2007), TER (Snover et al. 2006), WER, and PER.</S>
			<S sid ="393" ssid = "3">Additionally we performed statistical significance tests using bootstrap resampling methods on BLEU (Koehn 2004b).</S>
			<S sid ="394" ssid = "4">The confidence level (%) of the improvements obtained by the best performing context- informed systems with respect to the PBSMT baseline are reported.</S>
			<S sid ="395" ssid = "5">An improvement in system performance at a confidence level above 95% is assumed to be statistically significant.</S>
			<S sid ="396" ssid = "6">We divide the reports on our experiments into five subsections.</S>
			<S sid ="397" ssid = "7">Section 7.1 provides an overview of statistical properties of the corpora we used in our experiments.</S>
			<S sid ="398" ssid = "8">Section 7.2 reports on small-scale data sets representing the language pairs Dutch to-English, English-to-Hindi, and English-to-Czech, with less than 300,000 training examples.</S>
			<S sid ="399" ssid = "9">Section 7.3 reports on large-scale data sets with more than 500,000 training sentence-pairs, representing the language pairs Dutch-to-English, English-to-Dutch, English-to-Japanese, and English-to-Chinese.</S>
			<S sid ="400" ssid = "10">In Sect.</S>
			<S sid ="401" ssid = "11">7.4, we present the results of learning curve experiments we carried out on three different language pairs: English- to-Spanish, Dutch-to-English, and English-to-Dutch.</S>
			<S sid ="402" ssid = "12">In Sect.</S>
			<S sid ="403" ssid = "13">7.5 we present a manual qualitative analysis of the MT outputs.</S>
			<S sid ="404" ssid = "14">7.1 Data.</S>
			<S sid ="405" ssid = "15">The various corpora we used for carrying out our experiments are listed in Table 9.</S>
			<S sid ="406" ssid = "16">In this table we list for each data set the number of sentences, source (S) and target (T) vocabulary size (VS), and average sentence length (ASL).</S>
			<S sid ="407" ssid = "17">The corpora have the following origins: subtitles S source, T target, VS vocabulary size, ASL average sentence length Dutch-to-English (Open Subtitles): This corpus is collected as part of the Opus collection of freely available parallel corpora (Tiedemann and Nygaard 2004).5 The corpus contains user-contributed translations of movie subtitles.</S>
			<S sid ="408" ssid = "18">English-to-Hindi (EILMT): This small EILMT tourism domain corpus was released for the shared task on English-to-Hindi SMT (Venkatapathy 2008).6 English-to-Czech (WMT 2010): For the English-to-Czech translation task we employed the News Commentary training data set released in the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR (WMT-Met- ricsMATR 2010)7 . To tune the system during development, we used the WMT 2008 test set of 2,051 sentences.</S>
			<S sid ="409" ssid = "19">For evaluation purposes we used two different test sets: the WMT 2009 test set of 2,525 sentences, and the WMT 2010 testset of 2,489 sentences.</S>
			<S sid ="410" ssid = "20">Dutch-to-English (Europarl): The Dutch-to-English Europarl parallel corpus is extracted from the proceedings of the European Parliament (Koehn 2005).8 English-to-Japanese (NTCIR8): The experimental data sets were taken from the NTCIR8 Patent Translation Task.9 For the purpose of evaluation, we used two different test sets: the first test set contains 927 sentences (henceforth referred to as ‘EJTestset1’), and the second test set contains 1,119 sentences (‘EJTestset2’).</S>
			<S sid ="411" ssid = "21">English-to-Chinese (NIST08): Our English-to-Chinese training text contains sentence pairs of benchmark news text from the NIST Open Machine Translation 2009 Evaluation (MT09).10 We used the NIST MT05 test set sentences (development set) for tuning, and the NIST MT08 ‘current’ test set sentences for evaluation.</S>
			<S sid ="412" ssid = "22">English-to-Spanish (Europarl): This training corpus was provided in the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR (WMT-Met- ricsMATR 2010).11 We use the WMT ‘test2006’ set as a development set, and the WMT ‘test2008’ set as the test set; both sets contain 2,000 sentence pairs.</S>
			<S sid ="413" ssid = "23">7.2 Experiments on small-scale data sets.</S>
			<S sid ="414" ssid = "24">7.2.1 Dutch-to-English Small-scale experiments were performed on the Dutch-to-English Open Subtitles corpus (cf.</S>
			<S sid ="415" ssid = "25">Sect.</S>
			<S sid ="416" ssid = "26">7.1).</S>
			<S sid ="417" ssid = "27">To generate dependency features, the Dutch sentences were parsed using Frog,12 a robust morphosyntactic analyzer and dependency parser (Van den Bosch et al. 2007) that generates approximately the double amount of errors in labeled relations as compared to the English equivalent dependency parser used in our study.</S>
			<S sid ="418" ssid = "28">5 http://urd.let.rug.nl/tiedeman/OPUS/OpenSubtitles.php.</S>
			<S sid ="419" ssid = "29">6 http://ltrc.iiit.ac.in/nlptools2008/index.html.</S>
			<S sid ="420" ssid = "30">7 http://www.statmt.org/wmt10/.</S>
			<S sid ="421" ssid = "31">8 http://www.statmt.org/europarl/.</S>
			<S sid ="422" ssid = "32">9 http://www.cl.cs.titech.ac.jp/~fujii/ntc8patmt/.</S>
			<S sid ="423" ssid = "33">10 http://www.itl.nist.gov/iad/mig//tests/mt/2009/.</S>
			<S sid ="424" ssid = "34">11 http://www.statmt.org/wmt10/.</S>
			<S sid ="425" ssid = "35">12 http://ilk.uvt.nl/frog/.</S>
			<S sid ="426" ssid = "36">Table 10 Experiments with words and parts-of-speech as contextual features Expe rime nts B L E U N IS T M E T E O R T E R W E R P E R Base line 32 .3 9 6.</S>
			<S sid ="427" ssid = "37">11 55 .3 9 50 .1 5 49 .6 7 4 3.</S>
			<S sid ="428" ssid = "38">1 2 Wor d±2 32 .4 8 6.</S>
			<S sid ="429" ssid = "39">11 55 .7 2 50 .4 0 50 .4 3 4 2.</S>
			<S sid ="430" ssid = "40">9 1 POS ±2 33 .0 7 6.</S>
			<S sid ="431" ssid = "41">13 56 .1 7 50 .0 7 49 .3 8 4 2.</S>
			<S sid ="432" ssid = "42">8 5 POS ±2† 33 .2 9 (7 4 % ) 6.</S>
			<S sid ="433" ssid = "43">17 55 .7 2 49 .5 6 48 .9 1 4 2.</S>
			<S sid ="434" ssid = "44">7 7 Wor d±2 + POS ±2 32 .5 9 6.</S>
			<S sid ="435" ssid = "45">09 55 .3 6 50 .1 1 49 .6 3 4 3.</S>
			<S sid ="436" ssid = "46">1 0 Table 11 Experiments with dependency relations Expe rime nts B L E U N I S T M E T E O R T E R W E R P E R Base line 3 2.</S>
			<S sid ="437" ssid = "47">3 9 6.</S>
			<S sid ="438" ssid = "48">1 1 5 5.</S>
			<S sid ="439" ssid = "49">3 9 5 0.</S>
			<S sid ="440" ssid = "50">1 5 4 9.</S>
			<S sid ="441" ssid = "51">6 7 4 3 . 1 2 PR 3 2.</S>
			<S sid ="442" ssid = "52">6 9 6.</S>
			<S sid ="443" ssid = "53">0 8 5 5.</S>
			<S sid ="444" ssid = "54">0 8 5 0.</S>
			<S sid ="445" ssid = "55">4 8 5 0.</S>
			<S sid ="446" ssid = "56">1 1 4 3 . 5 8 OE 3 2.</S>
			<S sid ="447" ssid = "57">6 1 6.</S>
			<S sid ="448" ssid = "58">0 0 5 5.</S>
			<S sid ="449" ssid = "59">5 3 5 2.</S>
			<S sid ="450" ssid = "60">4 0 5 1.</S>
			<S sid ="451" ssid = "61">5 6 4 5 . 0 9 PR+ PW 3 2.</S>
			<S sid ="452" ssid = "62">7 4 6.</S>
			<S sid ="453" ssid = "63">0 6 5 5.</S>
			<S sid ="454" ssid = "64">9 8 5 1.</S>
			<S sid ="455" ssid = "65">1 5 5 0.</S>
			<S sid ="456" ssid = "66">7 5 4 3 . 6 1 PR+ OE 3 3.</S>
			<S sid ="457" ssid = "67">0 6 (6 0 % ) 6.</S>
			<S sid ="458" ssid = "68">2 0 5 5.</S>
			<S sid ="459" ssid = "69">7 0 4 9.</S>
			<S sid ="460" ssid = "70">4 5 4 8.</S>
			<S sid ="461" ssid = "71">8 3 4 2 . 4 4 PR+ OE+ PW 3 2.</S>
			<S sid ="462" ssid = "72">7 9 6.</S>
			<S sid ="463" ssid = "73">1 8 5 5.</S>
			<S sid ="464" ssid = "74">3 7 4 9.</S>
			<S sid ="465" ssid = "75">5 1 4 9.</S>
			<S sid ="466" ssid = "76">0 3 4 2 . 4 3 We performed three series of experiments using the TRIBL classifier.</S>
			<S sid ="467" ssid = "77">In the first series, words, parts-of-speech, and their combination are added as contextual information.</S>
			<S sid ="468" ssid = "78">The experimental results are reported in Table 10.</S>
			<S sid ="469" ssid = "79">In all cases, the width of the left and right contexts is 2.</S>
			<S sid ="470" ssid = "80">An additional experiment (labeled POS±2† )13 was performed in which the concatenated parts-of-speech of the focus phrases were not included as a feature.</S>
			<S sid ="471" ssid = "81">As can be observed from Table 10, the POS±2† experiment produces the best improvements (0.90 BLEU points; 2.78% relative) over the baseline.</S>
			<S sid ="472" ssid = "82">However, the improvement is not statistically significant.</S>
			<S sid ="473" ssid = "83">A second series of experiments was performed involving dependency relations as source context.</S>
			<S sid ="474" ssid = "84">The results are shown in Table 11.</S>
			<S sid ="475" ssid = "85">Five experiments were performed combining dependency features (outgoing edges: OE, parent relation: PR, and parent word: PW).</S>
			<S sid ="476" ssid = "86">The combination of PR and OE produces the best results in terms of BLEU and NIST: we observe a 0.67 absolute improvement corresponding to a 2.07% relative improvement in terms of BLEU, which is not statistically significant.In the third series we combined the position-independent PR+OE dependency fea tures with the position-dependent word and part-of-speech features.</S>
			<S sid ="477" ssid = "87">The combined experimental results are reported in Table 12.</S>
			<S sid ="478" ssid = "88">We observe that combining POS±2† with PR+OE yields the highest BLEU improvement (1.0 BLEU point; 3.08% relative) over the baseline, which is statistically significant at a 98.7% level of confidence.</S>
			<S sid ="479" ssid = "89">The best METEOR score (an improvement of 1.18 METEOR points over the baseline; 2.14% relative) is obtained when PR+OE is combined with POS±2.</S>
			<S sid ="480" ssid = "90">13 Signalling one particular exception, we use the dag(†) symbol for experiments in which syntactic infor-.</S>
			<S sid ="481" ssid = "91">mation of the focus phrase is ignored.</S>
			<S sid ="482" ssid = "92">Table 12 Experiments combining dependency relations, words and part-of-speech Expe rime nts B L E U NI S T M E T E O R T E R W E R P E R Base line 32 .3 9 6.</S>
			<S sid ="483" ssid = "93">11 55 .3 9 50 .1 5 49 .6 7 4 3.</S>
			<S sid ="484" ssid = "94">1 2 PR+ OE+ Wor d±2 33 .0 5 6.</S>
			<S sid ="485" ssid = "95">11 56 .0 2 50 .6 2 49 .8 2 4 3.</S>
			<S sid ="486" ssid = "96">6 8 PR+ OE+ POS ±2 33 .3 0 6.</S>
			<S sid ="487" ssid = "97">09 56 .5 7 50 .5 2 50 .1 7 4 3.</S>
			<S sid ="488" ssid = "98">8 1 PR+ OE+ POS ±2† 33 .3 9 (9 8.</S>
			<S sid ="489" ssid = "99">7 % ) 6.</S>
			<S sid ="490" ssid = "100">11 56 .3 0 50 .4 3 50 .3 4 4 3.</S>
			<S sid ="491" ssid = "101">5 4 3.5e+06 3e+06 Linked Modifier Words Parent Word 2.5e+06 2e+06 1.5e+06 1e+06 500000 0 0 1 2 3 4 5 6 Distance Fig.</S>
			<S sid ="492" ssid = "102">6 Distances found between phrase boundaries with linked modifier words and with parent words In sum, the small-scale Dutch-to-English translation task shows the POS contextual feature to produce the largest single-feature improvement over the baseline, while the difference in score between POS-based and dependency-based contextual models is negligible.</S>
			<S sid ="493" ssid = "103">Moreover, the highest improvement in BLEU over the baseline is obtained employing the combination of the POS- and dependency-based features, which is statistically significant.</S>
			<S sid ="494" ssid = "104">As an additional analysis, Fig.</S>
			<S sid ="495" ssid = "105">6 displays the distribution of distances (number of tokens) between the source phrase boundary and the words outside the phrase linked through a dependency relation.</S>
			<S sid ="496" ssid = "106">There are about twice as many outgoing modifier dependency relations linking to modifier words outside the focus phrase than to phrase-internal modifiers.</S>
			<S sid ="497" ssid = "107">About half of the phrases have the root of the dependency graph as the parent, i.e. they are the main verbs.</S>
			<S sid ="498" ssid = "108">For the remaining phrases, the parent of the headword is a phrase-external word.</S>
			<S sid ="499" ssid = "109">From the distance distribution statistics, we find that the average distance of head-modifying words to the phrase boundary is only 0.75 tokens when including phrase-internal relations, indicating that modifiers of the phrase are usually not too far away, and are mostly immediate neighbours.</S>
			<S sid ="500" ssid = "110">In contrast, parent words of the phrase’s head word are found relatively further away, at an average distance of 1.69 tokens outside the phrase boundary.</S>
			<S sid ="501" ssid = "111">Table 13 Experiments applying individual features in English-to-Hindi translation Expe rime nts B L E U NI S T M E T E O R T E R W E R P E R Base line 10 .9 3 4.</S>
			<S sid ="502" ssid = "112">54 28 .5 9 74 .8 7 8 2.</S>
			<S sid ="503" ssid = "113">0 6 5 6.</S>
			<S sid ="504" ssid = "114">6 7 Wor d±1 10 .7 6 4.</S>
			<S sid ="505" ssid = "115">53 28 .2 7 75 .4 2 8 2.</S>
			<S sid ="506" ssid = "116">7 5 5 7.</S>
			<S sid ="507" ssid = "117">2 6 Wor d±2 11 .2 4 (8 5 % ) 4.</S>
			<S sid ="508" ssid = "118">58 28 .2 7 74 .8 9 8 2.</S>
			<S sid ="509" ssid = "119">1 8 5 6.</S>
			<S sid ="510" ssid = "120">4 5 POS ±1 10 .8 2 4.</S>
			<S sid ="511" ssid = "121">55 28 .5 9 74 .7 6 8 1.</S>
			<S sid ="512" ssid = "122">8 5 5 6.</S>
			<S sid ="513" ssid = "123">6 5 POS ±2 11 .0 0 4.</S>
			<S sid ="514" ssid = "124">55 28 .9 0 74 .6 7 8 1.</S>
			<S sid ="515" ssid = "125">8 9 5 6.</S>
			<S sid ="516" ssid = "126">7 2 CCG ±1 11 .1 4 4.</S>
			<S sid ="517" ssid = "127">58 27 .9 4 74 .8 4 8 2.</S>
			<S sid ="518" ssid = "128">1 9 5 6.</S>
			<S sid ="519" ssid = "129">7 6 CCG ±2 11 .0 7 4.</S>
			<S sid ="520" ssid = "130">57 28 .5 9 74 .7 6 8 1.</S>
			<S sid ="521" ssid = "131">8 5 5 6.</S>
			<S sid ="522" ssid = "132">6 5 LTA G±1 11 .1 9 4.</S>
			<S sid ="523" ssid = "133">55 28 .2 8 74 .6 7 8 1.</S>
			<S sid ="524" ssid = "134">4 8 5 6.</S>
			<S sid ="525" ssid = "135">7 8 LTA G±2 11 .1 7 4.</S>
			<S sid ="526" ssid = "136">57 28 .5 9 74 .7 3 8 1.</S>
			<S sid ="527" ssid = "137">9 8 5 6.</S>
			<S sid ="528" ssid = "138">6 3 CCG LTA G±1 11 .0 1 4.</S>
			<S sid ="529" ssid = "139">53 28 .7 3 75 .3 4 8 2.</S>
			<S sid ="530" ssid = "140">6 2 5 6.</S>
			<S sid ="531" ssid = "141">8 9 CCG ±1+ LTA G±1 † 11 .0 4 4.</S>
			<S sid ="532" ssid = "142">55 28 .7 3 74 .9 4 8 2.</S>
			<S sid ="533" ssid = "143">1 4 5 6.</S>
			<S sid ="534" ssid = "144">6 6 Super pair ±1† 11 .0 2 4.</S>
			<S sid ="535" ssid = "145">58 27 .6 2 74 .4 5 8 1.</S>
			<S sid ="536" ssid = "146">4 5 5 6.</S>
			<S sid ="537" ssid = "147">4 5 Super pair ±2† 11 .1 5 4.</S>
			<S sid ="538" ssid = "148">58 28 .2 7 74 .8 7 8 2.</S>
			<S sid ="539" ssid = "149">2 2 5 6.</S>
			<S sid ="540" ssid = "150">4 5 PR+ OE 11 .0 2 4.</S>
			<S sid ="541" ssid = "151">58 28 .2 8 74 .6 5 8 1.</S>
			<S sid ="542" ssid = "152">7 5 5 6.</S>
			<S sid ="543" ssid = "153">5 5 7.2.2 English-to-Hindi For the English-to-Hindi translation pair we conduct experiments using supertags and dependency context features, both individually and jointly, comparing these to using lexical and POS contextual features.</S>
			<S sid ="544" ssid = "154">Like other Indian languages, Hindi is a free word order language.</S>
			<S sid ="545" ssid = "155">Experiments were carried out using the relatively small EILMT tourism corpus (cf.</S>
			<S sid ="546" ssid = "156">Sect.</S>
			<S sid ="547" ssid = "157">7.1).</S>
			<S sid ="548" ssid = "158">In order to obtain the dependency parse information for English sentences, the Malt dependency parser14 (Nivre et al. 2006) was used.</S>
			<S sid ="549" ssid = "159">The experimental results for individual and joint features, using the TRIBL classifier, are displayed in Tables 13 and 14, respectively.</S>
			<S sid ="550" ssid = "160">For the English-to-Hindi translation task, we copied the previously best-performing setup obtained from the Dutch-to-English translation task.</S>
			<S sid ="551" ssid = "161">Experimental results in Table 13 show that among the homogeneous features, Word±2 produces the bestimprovement (0.31 BLEU points, 2.84% relative) over the baseline.</S>
			<S sid ="552" ssid = "162">This improve ment is not statistically significant, yet close to the significance level.</S>
			<S sid ="553" ssid = "163">Other context- informed features also produce small but consistent improvements over the baseline in terms of BLEU.</S>
			<S sid ="554" ssid = "164">However, these improvements with respect to the baseline are not statistically significant.</S>
			<S sid ="555" ssid = "165">The other evaluation metrics show similar improvements.</S>
			<S sid ="556" ssid = "166">The results of combining the dependency features PR+OE with various contextual features are shown in Table 14.</S>
			<S sid ="557" ssid = "167">Combining PR+OE with POS±2 and the concatena tion of CCG and LTAG supertag features, referred to as PR + OE + POS ± 2 + CCG + LTAG ± 1† (see last row in Table 14), we achieve the overall best improvement (0.41BLEU points; 3.7% relative) over the baseline.</S>
			<S sid ="558" ssid = "168">Again, the improvement is not statisti 14 http://maltparser.org/download.html.</S>
			<S sid ="559" ssid = "169">Table 14 Experiments applying combinations of features in English-to-Hindi translation Expe rime nts BL EU NI ST M ET EO R TE R W E R P E R Base line 10.</S>
			<S sid ="560" ssid = "170">93 4.5 4 28.</S>
			<S sid ="561" ssid = "171">59 74.</S>
			<S sid ="562" ssid = "172">87 82 .0 6 56 .6 7 PR+ OE +· · · POS ±2 11.</S>
			<S sid ="563" ssid = "173">08 4.5 6 28.</S>
			<S sid ="564" ssid = "174">59 75.</S>
			<S sid ="565" ssid = "175">39 82 .6 7 56 .6 5 Wor d±2 11.</S>
			<S sid ="566" ssid = "176">02 4.5 5 28.</S>
			<S sid ="567" ssid = "177">59 75.</S>
			<S sid ="568" ssid = "178">13 82 .2 7 56 .8 5 CCG ±1 11.</S>
			<S sid ="569" ssid = "179">02 4.5 7 28.</S>
			<S sid ="570" ssid = "180">27 74.</S>
			<S sid ="571" ssid = "181">77 82 .0 8 56 .5 6 LTA G±1 11.</S>
			<S sid ="572" ssid = "182">08 4.5 6 28.</S>
			<S sid ="573" ssid = "183">27 75.</S>
			<S sid ="574" ssid = "184">39 82 .6 7 56 .6 5 CCG ±1+ LTA G±1 † 11.</S>
			<S sid ="575" ssid = "185">02 4.5 7 28.</S>
			<S sid ="576" ssid = "186">27 74.</S>
			<S sid ="577" ssid = "187">77 82 .0 8 56 .5 6 Super pair ±1† 11.</S>
			<S sid ="578" ssid = "188">02 4.5 4 28.</S>
			<S sid ="579" ssid = "189">27 75.</S>
			<S sid ="580" ssid = "190">00 82 .0 0 56 .8 2 Super pair ±2† 11.</S>
			<S sid ="581" ssid = "191">23 4.5 7 28.</S>
			<S sid ="582" ssid = "192">59 74.</S>
			<S sid ="583" ssid = "193">74 81 .9 2 56 .5 5 POS ±2+ CCG +LT AG ±1† 11.</S>
			<S sid ="584" ssid = "194">34 (89 %) 4.5 8 27.</S>
			<S sid ="585" ssid = "195">94 74.</S>
			<S sid ="586" ssid = "196">70 82 .0 0 56 .4 4 cally significant, yet is close to the significance level.</S>
			<S sid ="587" ssid = "197">Similar trends are observed on other evaluation metrics for the combined features.</S>
			<S sid ="588" ssid = "198">In sum, the word contextual model produces the biggest single-feature improvement over the baseline in terms of BLEU.</S>
			<S sid ="589" ssid = "199">The combination of dependency, supertag, and POS features brings about the highest BLEU score in this translation task, although the improvements over the baseline are not statistically significant.</S>
			<S sid ="590" ssid = "200">7.2.3 English-to-Czech For the English-to-Czech translation task (Penkale et al. 2010) we employed the News Commentary training data set (cf.</S>
			<S sid ="591" ssid = "201">Sect.</S>
			<S sid ="592" ssid = "202">7.1).</S>
			<S sid ="593" ssid = "203">We employed the previously best performing setup in terms of context width and feature combinations, and the TRIBL classifier.</S>
			<S sid ="594" ssid = "204">The evaluation results on the WMT 2009 test set are reported in Table 15.</S>
			<S sid ="595" ssid = "205">We observe that small improvements over the Moses baseline are achieved for CCG±1, PR and PR+OE features in terms of BLEU.</S>
			<S sid ="596" ssid = "206">Moderate improvements in the METEOR and TER evaluation scores are achieved for all features except LTAG±1 and Word±2.The highest METEOR score over the baseline is obtained for the dependency par ent relation (PR: 0.31 METEOR points improvement; 0.91% relative).</S>
			<S sid ="597" ssid = "207">On the TER evaluation metric, the best performing setup, CCG±1, yields an absolute reduction of 0.42 TER points over the baseline.</S>
			<S sid ="598" ssid = "208">Similar trends are also observed for WER and PER distance metrics.</S>
			<S sid ="599" ssid = "209">Moreover, gains for the CCG±1 and PR+OE features over the baseline are seen across the most evaluation metrics.</S>
			<S sid ="600" ssid = "210">Experimental results on the WMT 2010 test set are shown in Table 16.</S>
			<S sid ="601" ssid = "211">We observe that the improvements on this test set are similar to the improvements on the WMT 2009 test set.</S>
			<S sid ="602" ssid = "212">CCG±1 yields the highest improvements across all evaluation metrics except METEOR.</S>
			<S sid ="603" ssid = "213">As far as the METEOR evaluation metric is concerned, the Super- Pair±1 feature produces the best improvement (0.32 METEOR points gains, 0.92% relative) over the baseline.</S>
			<S sid ="604" ssid = "214">CCG±1 yields a 0.21 BLEU points gain (2.68% relative increase) and a 0.43 TER points reduction over the baseline.</S>
			<S sid ="605" ssid = "215">In contrast, POS±2 and Word±2 do not bring about any improvements across any of the evaluation metrics.</S>
			<S sid ="606" ssid = "216">Table 15 Experimental results on the WMT 2009 test set Experiments BLEU NIST METEOR TER WER PER Base line 7.</S>
			<S sid ="607" ssid = "217">8 3 3 . 9 0 3 4.</S>
			<S sid ="608" ssid = "218">1 3 8 7 . 6 6 8 0 . 5 3 6 7 . 8 8 CCG ±1 7.</S>
			<S sid ="609" ssid = "219">8 8 3 . 9 5 3 4.</S>
			<S sid ="610" ssid = "220">2 3 8 7 . 2 4 8 0 . 1 5 6 7 . 3 9 LTA G±1 7.</S>
			<S sid ="611" ssid = "221">6 7 3 . 8 9 3 4.</S>
			<S sid ="612" ssid = "222">0 0 8 7 . 9 0 8 0 . 8 6 6 8 . 0 0 CCG LTA G±1 7.</S>
			<S sid ="613" ssid = "223">8 0 3 . 9 0 3 4.</S>
			<S sid ="614" ssid = "224">3 5 8 8 . 2 4 8 1 . 1 7 6 8 . 1 6 Super pair ±1 7.</S>
			<S sid ="615" ssid = "225">8 2 3 . 9 0 3 4.</S>
			<S sid ="616" ssid = "226">3 8 8 7 . 9 6 8 0 . 8 4 6 8 . 1 8 POS ±2 7.</S>
			<S sid ="617" ssid = "227">8 0 3 . 9 0 3 4.</S>
			<S sid ="618" ssid = "228">2 5 8 7 . 8 7 8 0 . 8 4 6 7 . 9 5 Wor d±2 7.</S>
			<S sid ="619" ssid = "229">5 0 3 . 8 3 3 3.</S>
			<S sid ="620" ssid = "230">8 4 8 8 . 7 3 8 1 . 6 8 6 8 . 6 7 PR 7.</S>
			<S sid ="621" ssid = "231">8 5 3 . 9 2 3 4.</S>
			<S sid ="622" ssid = "232">4 4 8 7 . 5 3 8 0 . 5 7 6 7 . 8 8 PR+ OE 7.</S>
			<S sid ="623" ssid = "233">8 6 3 . 9 2 3 4.</S>
			<S sid ="624" ssid = "234">2 9 8 7 . 5 5 8 0 . 5 3 6 7 . 8 0 Table 16 Experimental results on the WMT 2010 test set Expe rime nts B L E U N I S T M E T E O R T E R W E R P E R Base line 8 . 0 5 3 . 9 7 3 4 . 6 1 8 6 . 0 1 7 8 . 5 4 6 7 . 4 8 CCG ±1 8 . 2 6 4 . 0 2 3 4 . 7 6 8 5 . 5 8 7 8 . 0 6 6 6 . 9 6 LTA G±1 8 . 0 0 3 . 9 5 3 4 . 5 7 8 6 . 4 1 7 8 . 9 5 6 7 . 7 2 CCG LTA G±1 8 . 0 9 3 . 9 6 3 4 . 9 0 8 6 . 6 2 7 9 . 1 8 6 7 . 9 1 Super Pair ±1 8 . 1 1 3 . 9 5 3 4 . 9 3 8 6 . 6 2 7 9 . 0 5 6 8 . 0 8 POS ±2 7 . 9 1 3 . 9 4 3 4 . 5 7 8 6 . 5 7 9 . 0 3 6 7 . 8 4 Wor d±2 7 . 5 7 3 . 8 8 3 4 . 1 6 8 7 . 1 3 7 9 . 7 7 6 8 . 3 9 PR 8 . 0 6 4 . 0 0 3 4 . 8 9 8 5 . 9 8 7 8 . 6 2 6 7 . 4 3 PR+ OE 8 . 0 3 3 . 9 9 3 4 . 8 1 8 5 . 9 7 7 8 . 6 3 6 7 . 4 4 In sum, slight improvements over the baseline are seen for supertag and dependency features, while POS and word contexts do not improve the baseline at all.</S>
			<S sid ="625" ssid = "235">None of the improvements over the baseline models are statistically significant in terms of BLEU.</S>
			<S sid ="626" ssid = "236">If we compare the effectiveness of the various contextual features both collectively and individually, in all small-scale translation tasks we see that supertags seem to be the most effective context features, as compared to neighbouring words and part- of-speech.</S>
			<S sid ="627" ssid = "237">Arguably, one can surmise that the differences in word order between the source and target languages in our experiments are best treated by a feature which is not entirely restricted to local context.</S>
			<S sid ="628" ssid = "238">7.3 Experiments on large-scale data sets.</S>
			<S sid ="629" ssid = "239">7.3.1 Dutch-to-English To explore the question whether similar improvements to the ones reported in the previous section can be achieved with large-scale data sets, we carried out a similar series of experiments.</S>
			<S sid ="630" ssid = "240">Our first experimental data set is Dutch-to-English Europarl data (cf.</S>
			<S sid ="631" ssid = "241">Table 17 Results on large-scale Dutch-to-English translation Experiments BLEU NIST METEOR TER WER PER Baseline 27.29 6.686 56.81 58.65 63.97 45.18 Words and part-of-speech tags Word±2 27.13 6.66 56.78 59.1 64.44 45.41 POS±2 26.93 6.67 56.51 59.06 64.19 45.51 POS±2† 26.9 6.67 56.61 58.94 64.10 45.52 Dependency relations PR 27.47 (66%) 6.726 57.02 58.5 63.59 45.10 OE 27.40 6.737 56.95 58.3 63.56 44.92 PR+OE 27.53 (63%) 6.721 57.15 58.64 63.93 45.08 PR+PW 27.17 6.690 56.86 58.94 64.09 45.34 PR+OE+PW 27.29 6.725 56.89 58.68 63.82 45.18 Combinations of words, part-of-speech tags and dependency relations PR+OE+Word±2 27.02 6.69 56.7 59.00 64.23 45.44 PR+OE+POS±2 27.14 6.64 56.68 59.39 64.56 45.76 PR+OE+POS±2† 27.16 6.66 56.58 59.00 64.17 45.53 Sect.</S>
			<S sid ="632" ssid = "242">7.1).</S>
			<S sid ="633" ssid = "243">Analogous to the experiments on small-scale data sets, we experimented with adding contextual information features representing words, part-of-speech tags, dependency relations, and their combinations.</S>
			<S sid ="634" ssid = "244">We used the IGTree classifier to carry out these experiments, as TRIBL’s memory needs become too demanding with data sets of this size.15 We used the same experimental settings as used with the small-scale Open Subtitles data set reported in Sect.</S>
			<S sid ="635" ssid = "245">7.2.1.</S>
			<S sid ="636" ssid = "246">Experimental results are reported in Table 17, where we see that word and part-of-speech contexts are unable to yield any improvement over the Moses baseline.</S>
			<S sid ="637" ssid = "247">However, some of the dependency features produce small improvements over the baseline.</S>
			<S sid ="638" ssid = "248">Among the dependency features, PR+OE producesthe largest improvement (0.24 BLEU points; 0.88% relative increase) over the base line.</S>
			<S sid ="639" ssid = "249">However, none of the improvements are statistically significant with respect to the baseline score.</S>
			<S sid ="640" ssid = "250">Furthermore, we combine the best performing dependency feature combination (PR+OE) with Word±2, POS±2 and POS±2† , the results of which are shown in the last rows of the Table 17.</S>
			<S sid ="641" ssid = "251">Nevertheless, like lexical and POS features, none of the combinations are able to produce an improvement over the baseline.</S>
			<S sid ="642" ssid = "252">The other evaluation metrics tend to follow the trends of the BLEU evaluation metric.</S>
			<S sid ="643" ssid = "253">In sum, word- and POS-based models do not show any improvements over a baseline PBSMT model.</S>
			<S sid ="644" ssid = "254">In contrast, we achieve small but consistent improvements over the baseline for all evaluation metrics when dependency relations are employed as the source-language contextual feature.</S>
			<S sid ="645" ssid = "255">15 For example, the memory structure built by TRIBL takes about 90 GB when trained on a training set.</S>
			<S sid ="646" ssid = "256">of 70 million instances generated on the 1.3 million training sentences dataset of English-to-Dutch, when only the Word±2 features are included.</S>
			<S sid ="647" ssid = "257">Table 18 Results on English-to-Dutch translation employing homogeneous features Experiments BLEU NIST METEOR TER WER PER Baseline 24.26 6.177 52.68 64.37 68.81 50.02 Words and part-of-speech tags Word±2 24.50 (80%) 6.248 52.78 63.96 68.46 49.59 POS±2 24.04 6.150 52.17 64.44 68.69 50.1 Supertags CCG±1 24.58 6.229 52.46 63.79 68.2 49.85 LTAG±1 24.33 6.267 52.51 63.53 68.00 49.13 CCG±1 + LTAG±1† 24.45 6.250 52.54 63.87 68.30 49.74 Super-Pair±1† 24.35 6.184 52.31 64.45 68.71 50.32 Super-Pair±2† 24.14 6.160 52.34 64.56 68.92 50.31 CCGLTAG±1 24.64 (96%) 6.235 52.79 63.90 68.27 49.58 Super-Pair±1 24.34 6.224 52.70 64.03 68.42 49.6 Dependency relations PR 24.72 (99.9%) 6.245 52.75 63.87 68.36 49.76 OE 24.32 6.219 52.62 64.00 68.52 49.87 PR+OE 24.62 6.235 52.82 63.95 68.26 49.81 PR+PW 24.58 6.260 52.80 63.62 68.33 49.49 PR+PW+OE 24.26 6.204 52.46 64.24 68.70 50.03 Semantic roles AL 24.56 (90.9%) 6.237 52.66 63.95 68.09 49.54 AL+PS 24.50 (91.6%) 6.221 52.50 64.15 68.33 49.79 7.3.2 English-to-Dutch The first set of experiments to incorporate supertags on a large-scale data set were carried out on the same Dutch-to-English Europarl data set described in Sect.</S>
			<S sid ="648" ssid = "258">7.3.1, but in the reverse direction.</S>
			<S sid ="649" ssid = "259">We also incorporated dependency features in this large- scale translation task as computed by the Malt dependency parser (cf.</S>
			<S sid ="650" ssid = "260">Sect.</S>
			<S sid ="651" ssid = "261">7.2.2).</S>
			<S sid ="652" ssid = "262">Moreover, we introduce semantic roles as new contextual features in PBSMT, and in addition we tried different combinations of lexical, syntactic and semantic features to test whether further improvements could be achieved.</S>
			<S sid ="653" ssid = "263">Experimental results for individual contextual features are displayed in Table 18.</S>
			<S sid ="654" ssid = "264">As can be seen from the table, Word±2 yields a small (statistically insignificant) BLEU improvement (0.24 BLEU points, 0.98% relative increase) over the Moses baseline; POS±2 is unable to yield any improvement.</S>
			<S sid ="655" ssid = "265">Among the supertag-based experiments, CCGLTAG±1 yields the highest improvement (0.38 BLEU points; 1.57% relativeincrease) over the baseline, which is statistically significant at the 96% level of confi dence.</S>
			<S sid ="656" ssid = "266">Among the dependency features, PR produces the highest improvement (0.46 BLEU points; 1.90% relative increase) over the baseline, which is statistically significant at the 99.9% level of confidence.</S>
			<S sid ="657" ssid = "267">Among the semantic features, AL yields Table 19 Results on English-to-Dutch translation combining best performing homogeneous features Expe rime nts BL EU NI ST M E T E O R TE R W E R PE R Base line 24.</S>
			<S sid ="658" ssid = "268">26 6.1 77 52 .6 8 64.</S>
			<S sid ="659" ssid = "269">37 68 .8 1 50.</S>
			<S sid ="660" ssid = "270">02 PR+ Wor d±2 24.</S>
			<S sid ="661" ssid = "271">66 (92 .9 %) 6.3 02 52 .8 9 63.</S>
			<S sid ="662" ssid = "272">36 67 .9 5 49.</S>
			<S sid ="663" ssid = "273">09 PR+ CCG LTA G±1 24.</S>
			<S sid ="664" ssid = "274">51 6.3 01 52 .5 5 63.</S>
			<S sid ="665" ssid = "275">14 67 .3 2 49.</S>
			<S sid ="666" ssid = "276">04 PR+ CCG LTA G±1 24.</S>
			<S sid ="667" ssid = "277">55 6.2 32 52 .5 8 63.</S>
			<S sid ="668" ssid = "278">72 68 .0 1 49.</S>
			<S sid ="669" ssid = "279">48 AL+ PR 24.</S>
			<S sid ="670" ssid = "280">70 (92 .9 %) 6.2 58 52 .7 9 63.</S>
			<S sid ="671" ssid = "281">70 68 .1 0 49.</S>
			<S sid ="672" ssid = "282">55 AL+ CCG LTA G±1 24.</S>
			<S sid ="673" ssid = "283">55 6.2 36 52 .5 9 63.</S>
			<S sid ="674" ssid = "284">99 68 .2 6 49.</S>
			<S sid ="675" ssid = "285">81 AL+ PS+ PR 24.</S>
			<S sid ="676" ssid = "286">72 (98 .7 %) 6.2 54 52 .6 4 63.</S>
			<S sid ="677" ssid = "287">89 68 .1 4 49.</S>
			<S sid ="678" ssid = "288">53 AL+ PS+ CCG LTA G±1 24.</S>
			<S sid ="679" ssid = "289">50 6.2 18 52 .7 7 64.</S>
			<S sid ="680" ssid = "290">23 68 .3 5 49.</S>
			<S sid ="681" ssid = "291">73 the highest score (a 0.30 BLEU points improvement; 1.24% relative increase) over the baseline, but this is not statistically significant, although close to the significance level.</S>
			<S sid ="682" ssid = "292">Overall, PR remains the best performing feature among the individual context features.</S>
			<S sid ="683" ssid = "293">Similar to the previous approaches, the best performing settings were combined to see whether further improvements could be achieved.</S>
			<S sid ="684" ssid = "294">Experimental results for the combined features are reported in Table 19.</S>
			<S sid ="685" ssid = "295">We see from Table 19 that a combined setup (AL+PS+PR) equals the BLEU score obtained with the PR feature, and this improvement is statistically significant at the 98.2% level of confidence.</S>
			<S sid ="686" ssid = "296">Among the other combinations, Word±2 added to PR produces the second highest improvement (0.40 BLEU points, 1.64% relative increase) over the baseline, although this is not a statistically significant increase.</S>
			<S sid ="687" ssid = "297">In sum, for English-to-Dutch translation (the reverse direction as compared to the previously reported experiment), improvements over the baseline for the dependency and supertag-based context-informed models are statistically significant in terms of BLEU at 99.9 and 96% levels of confidence respectively.</S>
			<S sid ="688" ssid = "298">In contrast, improvements for the word context are not statistically significant, and the POS-based model performs below the baseline PBSMT model.</S>
			<S sid ="689" ssid = "299">Our novel semantic role contextual feature achieved modest gains over the baseline, both when used individually and in collaboration with other features.</S>
			<S sid ="690" ssid = "300">While this is encouraging, we note that semantic parsing is computationally expensive, so any gains in translation accuracy need to be offset against slower processing speed.</S>
			<S sid ="691" ssid = "301">7.3.3 English-to-Japanese Our next sets of experiments were carried out on a large-scale English-to-Japanese data set (cf.</S>
			<S sid ="692" ssid = "302">Sect.</S>
			<S sid ="693" ssid = "303">7.1).</S>
			<S sid ="694" ssid = "304">As with the other large-scale experiments, the experiments were carried out using IGTree classifiers.</S>
			<S sid ="695" ssid = "305">Experimental results are shown in Table 20.</S>
			<S sid ="696" ssid = "306">None of the contextual features are able to improve on the Moses baseline with both test sets.</S>
			<S sid ="697" ssid = "307">Supertag-based features perform slightly better than POS and lexical features.</S>
			<S sid ="698" ssid = "308">As well as providing the largest amount of training data of all our experiments, the Table 20 Experimental results for large-scale English-to-Japanese translation Experiments BLEU NIST TER WER PER Baseline 27 .30 6.746 63.31 80.01 43.36 Evaluation results on EJTestset1 CCG±1 27.11 6.722 63.97 80.40 43.84 LTAG±1 27.18 6.736 63.53 80.06 43.51 CCGLTAG±1 27.13 6.690 64.19 80.81 44.04 Super-Pair±1 27.10 6.727 63.84 80.44 43.59 POS±2 27.03 6.728 64.03 80.85 43.67 Word±2 26.65 6.656 64.18 80.20 43.83 Evaluation results on EJTestset2 Baseline 27 .76 6.838 60.64 77 .49 42.61 CCG±1 27.41 6.768 61.49 78.38 43.23 LTAG±1 27.37 6.771 61.19 78.04 43.13 CCGLTAG±1 27.31 6.734 61.68 78.51 43.27 Super-Pair±1 27.40 6.773 61.19 78.29 43.14 POS±2 27.39 6.744 61.65 78.79 43.18 Word±2 27.15 6.752 61.53 78.25 43.13 Table 21 Experimental results for large-scale English-to-Chinese translation NTCIR data has been reported to be very noisy (Okita et al. 2010), which may have affected the results.</S>
			<S sid ="699" ssid = "309">7.3.4 English-to-Chinese Our next set of experiments were carried out on an English-to-Chinese data set (cf.</S>
			<S sid ="700" ssid = "310">Sect.</S>
			<S sid ="701" ssid = "311">7.1).</S>
			<S sid ="702" ssid = "312">Experiments were carried out with two types of supertags (CCG±1, LTAG±1), using both IGTree and TRIBL.</S>
			<S sid ="703" ssid = "313">Experimental results are displayed in Table 21.</S>
			<S sid ="704" ssid = "314">When IGTree classifiers were used, LTAG±1 shows no improvement over the Moses baseline across all evaluation metrics, while CCG±1 shows a slight improvement on only the BLEU evaluation metric.</S>
			<S sid ="705" ssid = "315">On the other hand, when we use TRIBL classifiers, CCG±1 yields a 0.37 BLEU points improvement (3.76% relative increase) over the baseline, a statistically significant improvement at a 99.3% level of confidence.</S>
			<S sid ="706" ssid = "316">LTAG±1 produces the highest BLEU improvements (0.54 BLEU points; 5.48% relative increase) over the baseline, and the improvement is statistically significant at a 99.9% level of confidence.</S>
			<S sid ="707" ssid = "317">In sum, while for large-scale English-to-Japanese translation none of the contextual features showed an improvement over the baseline, for large-scale English-to-Chinese translation a slight improvement over the baseline in terms of BLEU was observed for the CCG supertag context when IGTree is used for the classification task.</S>
			<S sid ="708" ssid = "318">We also carried out experiments using TRIBL as the classifier, and achieved significant improvements over the baseline for both the CCG and LTAG supertag features.</S>
			<S sid ="709" ssid = "319">Comparing the effectiveness of the classifiers on large-scale translation tasks, IGTree proved useful for Dutch-to-English and English-to-Dutch, but not for English- to-Japanese or English-to-Chinese.</S>
			<S sid ="710" ssid = "320">In contrast, TRIBL was effective for the latter language direction.</S>
			<S sid ="711" ssid = "321">In terms of contextual features, overall supertags, dependency relations and semantic roles seemed to be more effective than word- and POS-based models.</S>
			<S sid ="712" ssid = "322">7.4 Learning curve experiments.</S>
			<S sid ="713" ssid = "323">7.4.1 English-to-Spanish Thus far, combining two different approximate memory-based classifiers (of which TRIBL has empirically tuned hyperparameters), different data set sizes, different language pairs, text genres and domains, various source-side context features and context widths, we have obtained a mixed bag of results.</S>
			<S sid ="714" ssid = "324">We observe that the context-informed models tend to perform better than the baseline PBSMT model on small-scale training data, but the relative gain tends to diminish when we use larger training sets, which may be partly due to the approximate behaviour of the IGTree classifier compared to unrestriced k-nearest neighbour classification.</S>
			<S sid ="715" ssid = "325">With the English-to-Chinese experiments we observed that the improvements may be somewhat larger with the TRIBL classifier, a closer approximation of k-nearest neighbour classification, than with the faster IGTree algorithm.</S>
			<S sid ="716" ssid = "326">So far, however, we have not systematically varied the amount of training data sizes given a particular data set, to see whether the relative advantage of TRIBL over IGTree changes with the amount of training data available, and how their performance relates to the baseline with varying amounts of training data available.</S>
			<S sid ="717" ssid = "327">In this section we explore a new language pair, English-to-Spanish.</S>
			<S sid ="718" ssid = "328">We conduct learning curve experiments on increasing training data sets while adding an optimized set of contextual features.</S>
			<S sid ="719" ssid = "329">We segment the English-to-Spanish data set into several incremental slices of increasing size, and perform a series of experiments on each of these data sets.</S>
			<S sid ="720" ssid = "330">To conduct learning curve experiments, we employ the Spanish-to-English data set (cf.</S>
			<S sid ="721" ssid = "331">Sect.</S>
			<S sid ="722" ssid = "332">7.1).</S>
			<S sid ="723" ssid = "333">We segmented the English-to-Spanish training set into eight pseudo- exponentially increasing training sets: 10 K, 20 K, 50 K, 100 K, 200 K, 500 K, 1 M, and 1,639,764 training sentences.</S>
			<S sid ="724" ssid = "334">To perform experiments on this sequence of training sets, we used both IGTree and TRIBL.</S>
			<S sid ="725" ssid = "335">We were only able to use the TRIBL classifier 32 30 28 26 Baseline LTAG ± 1:IGTree 24 PR:IGTree.</S>
			<S sid ="726" ssid = "336">Super-Pair ± 1:TRIBL 22 PR:TRIBL.</S>
			<S sid ="727" ssid = "337">1 0.5 00.51 Baseline LTAG ± 1:IGTree PR:IGTree Super-Pair ± 1:TRIBL PR:TRIBL 10 100 1000 x 1000 Training Sentences 10 100 1000 x 1000 Training Sentences Fig.</S>
			<S sid ="728" ssid = "338">7 BLEU learning curves (left) and difference curves (right) comparing the Moses baseline against two IGTree (LTAG±1 and PR) and TRIBL (Super-Pair±1 and PR) classifiers with training sets containing up to 100 K sentences due to TRIBL’s relatively high memory requirements.</S>
			<S sid ="729" ssid = "339">We plot the BLEU score learning curves of the two best-performing context- informed models for TRIBL (Super-Pair±1 and PR) and for IGTree (LTAG±1 and PR) as well as the Moses baseline in the left part of Fig.</S>
			<S sid ="730" ssid = "340">7.</S>
			<S sid ="731" ssid = "341">The figure adopts a logarithmic horizontal axis, representing the number of training sentences.</S>
			<S sid ="732" ssid = "342">In addition, the right-hand side graph of Fig.</S>
			<S sid ="733" ssid = "343">7 shows the BLEU difference curves of the four classifier experiments against the baseline, highlighting the gains and losses against the baseline.</S>
			<S sid ="734" ssid = "344">The curves of IGTree extend up to the maximum of 1.64 M training sentences; as noted, due to limitations in memory, the TRIBL experiment extends to up to 100 K training sentences.</S>
			<S sid ="735" ssid = "345">Figure 7 shows that the Super-Pair±1 and PR curves of TRIBL start just under the baseline curve, then increasingly improve over themselves and the baseline curve.</S>
			<S sid ="736" ssid = "346">The figure also illustrates that the LTAG±1 and PR curves of IGTree start at a lower level than the baseline curve, and end at the same level as the baseline curve at the largest training set size.</S>
			<S sid ="737" ssid = "347">To summarize, (a) TRIBL appears to be effective on both small and large-scale data sets, though its memory needs prohibit it from being used with the largest-sized training sets; on the other hand, it does not improve the context-informed models on the smallest amounts of training data tested (e.g. 10 K sentences); (b) IGTree does not offer improvements over the baseline either with the small or the large-scale context-informed models; the performance of the large-scale context-informed models with the IGTree classifier are merely close or equal to the performance of the Moses baseline.</S>
			<S sid ="738" ssid = "348">As an additional point of analysis, Fig.</S>
			<S sid ="739" ssid = "349">8 compares the Moses baseline with both TRIBL and IGTree using the CCG±1 feature, in terms of the average number of target phrases considered for a source phrase for varying training data sizes.</S>
			<S sid ="740" ssid = "350">The CCG±1 feature produces a similar performance to LTAG±1 and Super-Pair±1 when IGtree and TRIBL classifiers are used, respectively.</S>
			<S sid ="741" ssid = "351">The graph in Fig.</S>
			<S sid ="742" ssid = "352">8 shows that the TRIBL curve lies between the IGTree curve and the Moses baseline curve; the Moses baseline uses an increasing number of target phrases with more training data, reaching an average of several hundreds of phrases at 500 450 400 B a s e l i n e C C G + / 1 : I G T r e e C C G + / 1 : T R I B L 350 300 250 200 150 100 50 0 10 100 1000 x 1000 Training Sentences Fig.</S>
			<S sid ="743" ssid = "353">8 Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree compared to the Moses baseline the maximal training set sizes.</S>
			<S sid ="744" ssid = "354">The TRIBL curve starts close to the IGTree curve, but rises at 100 K training sentences; nevertheless, the TRIBL curve remains under the baseline curve.</S>
			<S sid ="745" ssid = "355">Thus, both the TRIBL and IGTree classifiers produce smaller, more constrained distributions of the target phrases given a source phrase and its context information.</S>
			<S sid ="746" ssid = "356">Expanding on the results displayed in Fig.</S>
			<S sid ="747" ssid = "357">7, we analyse four specific learning curves for different context-informed models using the TRIBL classifier.</S>
			<S sid ="748" ssid = "358">Figure 9 visualizes the losses and gains in terms of BLEU score of experiments with four individual types of contextual features compared against the baseline: using supertags (Super-Pair±1), dependency relations (PR), POS tags, and words.</S>
			<S sid ="749" ssid = "359">The figure shows that the Super-Pair±1, POS, PR and word curves start below the baseline curve at the smallest training set size (10,000 sentences), but then start to deviate positively and increasingly from the baseline curve when more training data is added.</S>
			<S sid ="750" ssid = "360">7.4.2 Dutch-to-English Originally we ran our large-scale experiments on the Dutch-to-English and English- to-Dutch language pairs with the IGTree classifiers (cf.</S>
			<S sid ="751" ssid = "361">Sect.</S>
			<S sid ="752" ssid = "362">7.3).</S>
			<S sid ="753" ssid = "363">As mentioned earlier, we observe in the English-to-Chinese translation task (cf.</S>
			<S sid ="754" ssid = "364">Sect.</S>
			<S sid ="755" ssid = "365">7.3.4) and the English-to-Spanish learning curve experiments (cf.</S>
			<S sid ="756" ssid = "366">Sect.</S>
			<S sid ="757" ssid = "367">7.4.1) that TRIBL seems to be a more effective classifier than IGTree in improving the performances of the context-informed SMT systems, but we were able to use TRIBL classifiers only for the small-scale translations due to its relatively high memory requirements.</S>
			<S sid ="758" ssid = "368">However, 0.6 0.5 0.4 0.3 0.2 0.1 00.10.2 B a s e l i n e S u p e r P a i r ± 1 P R P O S ± 2 W o r d ± 2 10 100 x 1 0 0 0 T r a i n i n g S e n t e n c e s Fig.</S>
			<S sid ="759" ssid = "369">9 BLEU difference curves of four context-informed models using TRIBL TRIBL was reprogrammed recently, and can now efficiently handle large number of examples.</S>
			<S sid ="760" ssid = "370">This inspires us to deploy TRIBL classifiers for the large-scale translation.</S>
			<S sid ="761" ssid = "371">To investigate the consequences of the different context-informed SMT systems on the increasing sizes of training sets while employing TRIBL as the classifier, we carried out experiments on the Dutch-to-English and English-to-Dutch language pairs.</S>
			<S sid ="762" ssid = "372">First, like the division of English-to-Spanish data set (cf.</S>
			<S sid ="763" ssid = "373">Sect.</S>
			<S sid ="764" ssid = "374">7.4.1), we segmented the Dutch-to-English training set (cf.</S>
			<S sid ="765" ssid = "375">Sect.</S>
			<S sid ="766" ssid = "376">7.1) into eight pseudo-exponentially increasing training sets: 10 K, 20 K, 50 K, 100 K, 200 K, 500 K, 1 M, and 1,311,111 training sentences.</S>
			<S sid ="767" ssid = "377">In this section, we report the outcomes of the Dutch-to-English learning curve experiments.</S>
			<S sid ="768" ssid = "378">In order to perform the learning curve experiments, we chose the previously best- performing experimental setups for each feature type (dependency relations (PR, OE), part-of-speech tags (POS±2), and neighbouring words (Word±2)).</S>
			<S sid ="769" ssid = "379">Figure 10 shows the learning curves and score-difference curves comparing the Dutch-to-English Moses baseline against the four context-based models (PR, OE, POS±2, Word±2).</S>
			<S sid ="770" ssid = "380">The three left-hand side graphs in Fig.</S>
			<S sid ="771" ssid = "381">10 show respectively BLEU (top), METEOR (centre) and TER (bottom) learning curves representing the performance of four context-informed models compared against the baseline.</S>
			<S sid ="772" ssid = "382">The three right-hand side graphs in Fig.</S>
			<S sid ="773" ssid = "383">10 show respectively BLEU (top), METEOR (centre) and TER (bottom) score- difference curves, highlighting the gains and losses against the baseline.</S>
			<S sid ="774" ssid = "384">We observe that the BLEU and METEOR curves (learning and score-difference) of all the context-informed models always remain above the baseline curve from the starting point (10 K training data) to the end point (1.31 M training data).</S>
			<S sid ="775" ssid = "385">Figure 10 also shows that the PR and OE learning curves (BLEU and METEOR) start at a lower level than the POS±2 and Word±2 curves at smaller training set sizes (10–500 K training set), but end at the same level as the Word±2 curve, and at a higher level 276 R. Haque et al. 28 0.8 27 0.6 26 25 0.4 24 23 0.2 22 21 Baseline 20 PR.</S>
			<S sid ="776" ssid = "386">OE 19 POS ± 2.</S>
			<S sid ="777" ssid = "387">Word ± 2 18 00.20.4 Baseline PR OE POS ± 2 Word ± 2 10 100 1000 x 1000 Training Sentences 10 100 1000 x 1000 Training Sentences 58 0.8 56 0.6 54 0.4 52 0.2 50 Baseline PR 48 OE.</S>
			<S sid ="778" ssid = "388">POS ± 2 Word ± 2 46 00.20.4 Baseline PR OE POS ± 2 Word ± 2 10 100 1000 x 1000 Training Sentences 10 100 1000 x 1000 Training Sentences 70 Baseline PR 68 OE.</S>
			<S sid ="779" ssid = "389">POS ± 2 Word ± 2 66 0.4 0.2 00.2 640.4 620.6 600.8 581 Baseline PR OE POS ± 2 Word ± 2 10 1 0 0 1000 1 0 1 0 0 1000 x 1 0 0 0 T r a i n i n g S e n t e n c e s x 1 0 0 0 T r a i n i n g S e n t e n c e s Fig.</S>
			<S sid ="780" ssid = "390">10 Dutch-to-English Learning curves (left-hand side graphs) and difference curves (right-hand side graphs) comparing the Moses baseline against four context-informed models (PR, OE, POS±2 and Word±2).</S>
			<S sid ="781" ssid = "391">These curves are plotted with scores obtained using three evaluation metrics: BLEU (top), METEOR (centre) and TER (bottom) than the POS±2 curve for the larger training set sizes (1–1.31 M training set).</S>
			<S sid ="782" ssid = "392">Note that TER is an error metric, so lower scores indicate better performance.</S>
			<S sid ="783" ssid = "393">The bottom two graphs of Fig.</S>
			<S sid ="784" ssid = "394">10 show that the TER curves (learning and score-difference) of all context-informed models (PR, OE, POS±2, Word±2) mostly remain below the baseline curve, which indicates the effectiveness of source-language context in this translation task also in terms of TER.</S>
			<S sid ="785" ssid = "395">In summary, in the Dutch-to-English translation task the dependency relations and neighbouring words appear to be more effective source-language context features than POS tags according to the performance measured by all evaluation metrics.</S>
			<S sid ="786" ssid = "396">7.4.3 English-to-Dutch In this section, we report the outcomes of the English-to-Dutch learning curve experiments.</S>
			<S sid ="787" ssid = "397">In order to conduct English-to-Dutch learning curve experiments we consider the previously best-performing experimental setups comprising each feature type: su pertags (CCG±1, LTAG±1), dependency relations (PR), semantic roles (PSAL), and basic context features (POS±2, Word±2).</S>
			<S sid ="788" ssid = "398">Figure 11 illustrates BLEU (top), METEOR(centre) and TER (bottom) learning curves (left-hand side graphs) and score-differ ence curves (right-hand side graphs) comparing the Moses baseline against the six context-informed models (CCG±1, LTAG±1, PR, PSAL, POS±2 and Word±2).</S>
			<S sid ="789" ssid = "399">We see from the top-left and -right graphs in Fig.</S>
			<S sid ="790" ssid = "400">11 that the semantic and dependency feature-based BLEU curves (PSAL, PR) show consistency in residing mostly above the baseline BLEU curve from the starting point (10 K training data) to the end point (1.31 M training data).</S>
			<S sid ="791" ssid = "401">Supertag-based BLEU learning curves (CCG±1, LTAG±1) start close to the baseline curve, go upwards and cross the baseline curve while adding more training data.</S>
			<S sid ="792" ssid = "402">Interestingly, LTAG±1 and CCG±1 producedrespectively the highest and second highest BLEU improvements over the Moses base line for the larger amounts of training data.</S>
			<S sid ="793" ssid = "403">We observed that most of the improvements over the baseline with supertag context features are statistically significant.</S>
			<S sid ="794" ssid = "404">In contrast, we found that the most of the improvements for word and POS-based models over the baseline are not statistically significant.</S>
			<S sid ="795" ssid = "405">In short, supertags appear to be the most effective context features in PBSMT according to the performance measured by the BLEU evaluation metric.</S>
			<S sid ="796" ssid = "406">Centre-left and -right graphs of Fig.</S>
			<S sid ="797" ssid = "407">11 show METEOR learning and score-dif- ference curves, respectively.</S>
			<S sid ="798" ssid = "408">We see that the most of the METEOR curves (learning and score-difference) do not resemble the BLEU curves (top-left and -right graphs in Fig.</S>
			<S sid ="799" ssid = "409">11).</S>
			<S sid ="800" ssid = "410">The PSAL and PR-based METEOR curves show consistency in residing mostly above the baseline curve for all amounts of training data, while the Word ± 2-based METEOR curve shows consistency in residing above the baseline curve at larger amounts (500 K, 1 M and 1.31 M) of training data.</S>
			<S sid ="801" ssid = "411">Interestingly, the supertag- and POS-based METEOR curves reside mostly beneath the baseline curve.</S>
			<S sid ="802" ssid = "412">The bottom-left and -right graphs in Fig.</S>
			<S sid ="803" ssid = "413">11 show respectively TER learning and score-difference curves.</S>
			<S sid ="804" ssid = "414">We see from the bottom-part of the Fig.</S>
			<S sid ="805" ssid = "415">11 that most of the TER learning and score-difference curves show consistency in residing below the baseline.</S>
			<S sid ="806" ssid = "416">Interestingly, both LTAG±1 and CCG±1 produce higher TER scores than 278 R. Haque et al. 25 24 23 22 21 20 Baseline 19 CCG ±1.</S>
			<S sid ="807" ssid = "417">LTAG ±1 18 PR.</S>
			<S sid ="808" ssid = "418">POS ±2 17 Word ± 2.</S>
			<S sid ="809" ssid = "419">PSAL 16 10 100 1000 x 1000 Training Sentences 0.8 0.6 0.4 0.2 00.20.40.60.81 Baseline CCG ±1 LTAG ±1 PR POS ±2 Word ± 2 PSAL.</S>
			<S sid ="810" ssid = "420">10 100 1000 x 1000 Training Sentences 53 52 51 50 49 48 47 Baseline.</S>
			<S sid ="811" ssid = "421">46 CCG ±1.</S>
			<S sid ="812" ssid = "422">LTAG ±1 45 PR.</S>
			<S sid ="813" ssid = "423">POS ±2 44 Word ± 2.</S>
			<S sid ="814" ssid = "424">PSAL 43 10 100 1000 x 1000 Training Sentences 0.8 0.6 0.4 0.2 00.20.40.60.81 Baseline CCG ±1 LTAG ±1 PR POS ±2 Word ± 2 PSAL.</S>
			<S sid ="815" ssid = "425">10 100 1000 x 1000 Training Sentences 74 Baseline CCG ±1 72 LTAG ±1.</S>
			<S sid ="816" ssid = "426">PR POS ±2 70 Word ± 2.</S>
			<S sid ="817" ssid = "427">PSAL 1 0.5 0 Baseline CCG ± 1 LTAG ± 1 PR POS ± 2 Word ± 2 PSAL 680.5 661 641.5 62 10 100 1000 x 1000 Training Sentences 10 100 1000 x 1000 Training Sentences Fig.</S>
			<S sid ="818" ssid = "428">11 English-to-Dutch Learning curves (left-hand side graphs) and difference curves (right-hand side graphs) comparing the Moses baseline against four context-informed models (CCG±1, LTAG±1, PR,PSAL, POS±2 and Word±2).</S>
			<S sid ="819" ssid = "429">These curves are plotted with scores obtained using three evaluation met rics: BLEU (top), METEOR (centre) and TER (bottom) the baseline and other context-informed models (PR, PSAL, POS±2, Word±2) at all amounts of training data.</S>
			<S sid ="820" ssid = "430">In summary, in this translation task, dependency (PR) and semantic (PSAL) features appear to be the most effective source-language context features in PBSMT according to the performance measured by all evaluation metrics.</S>
			<S sid ="821" ssid = "431">Nevertheless, the BLEU and TER metrics point at supertags (CCG and LTAG) as the most effective context features, while the METEOR evaluation metric suggests otherwise.</S>
			<S sid ="822" ssid = "432">As the METEOR metric does not support the Dutch language (Lavie and Agarwal 2007), we operated it with its default English settings.</S>
			<S sid ="823" ssid = "433">We suspect this might be the reason why the METEOR shows inconsistency while evaluating the Dutch sentences.</S>
			<S sid ="824" ssid = "434">7.5 Translation examples.</S>
			<S sid ="825" ssid = "435">We performed manual qualitative analysis comparing the translated outputs of the best-performing systems with those of the Moses baseline systems.</S>
			<S sid ="826" ssid = "436">In order to carry out the manual evaluation, we randomly sampled fifty (50) test set sentences from the translated output of each system.</S>
			<S sid ="827" ssid = "437">7.5.1 Dutch-to-English translation examples First, we looked at the translated output of our best-performing system (PR+OE+POS±2† ) against that of the Moses baseline in the small-scale Dutch-to-English transla tion task (cf.</S>
			<S sid ="828" ssid = "438">Sect.</S>
			<S sid ="829" ssid = "439">7.2.1).</S>
			<S sid ="830" ssid = "440">We observed that the (PR+OE+POS±2† ) system generates a more fluent as well as more adequate output than the baseline for most sentences.</S>
			<S sid ="831" ssid = "441">The following are two translation examples: (5) Dutch: heeft mijn vader je gestuurd ? Reference: did my father send you ? PR+OE+POS±2† : did my father send you ? Baseline: my father has sent you ?</S>
			<S sid ="832" ssid = "442">(6) Dutch: daarna vraag je om informatie.</S>
			<S sid ="833" ssid = "443">Reference: then you call for information.</S>
			<S sid ="834" ssid = "444">PR+OE+POS±2† : then you ask for information.</S>
			<S sid ="835" ssid = "445">Baseline: then ask you to get information.</S>
			<S sid ="836" ssid = "446">In Example (5) we observe that the baseline does not select the word order of a question, and selects a less optimal (more literal) translation of the Dutch auxiliary verb heeft, has, while the PR+OE+POS system generates a translation identical to the reference translation.</S>
			<S sid ="837" ssid = "447">In example (6), the baseline system again selects a less appropriate (Dutch) word order.</S>
			<S sid ="838" ssid = "448">7.5.2 English-to-Dutch translation examples We also analysed the translated output of our best-performing system (LTAG±1) against that of the Moses baseline in the English-to-Dutch translation task (cf.</S>
			<S sid ="839" ssid = "449">Sect.</S>
			<S sid ="840" ssid = "450">7.4.3).</S>
			<S sid ="841" ssid = "451">We observed that the baseline Moses system frequently mistranslates English function words.</S>
			<S sid ="842" ssid = "452">The following are the two translation examples which illustrate how our best-performing system (LTAG±1) surpasses the Moses baseline in this translation task: (7) English: European agriculture is not uniform.</S>
			<S sid ="843" ssid = "453">Reference: De Europese landbouw is verre van eenvormig.</S>
			<S sid ="844" ssid = "454">LTAG±1: De Europese landbouw is niet uniform.</S>
			<S sid ="845" ssid = "455">Baseline: Europese landbouw niet uniform is.</S>
			<S sid ="846" ssid = "456">(8) Baseline: Apart from a limited budget, the European Union has little political interest in Tajikistan.</S>
			<S sid ="847" ssid = "457">Reference: Naast een beperkte begroting heeft de Europese Unie politiek gezien weinig in Tadzjikistan te zoeken.</S>
			<S sid ="848" ssid = "458">LTAG±1: Afgezien van een beperkte begroting heeft de Europese Unie weinig politieke belang in Tadzjikistan.</S>
			<S sid ="849" ssid = "459">Baseline: Afgezien van een beperkte begroting, de Europese Unie heeft weinig politieke belang in Tadzjikistan.</S>
			<S sid ="850" ssid = "460">In the translation example (7), the translation produced by LTAG±1 is fluent androughly synonymous to the reference translation, while the baseline generates a trans lation with a wrong word order, and also misses the initial article ‘De’.</S>
			<S sid ="851" ssid = "461">Translation example (8) resembles (7) in that the LTAG±1 system generates a fluent and grammat ical translation save for one agreement issue (the adjective politieke should be politiek as the noun belang has neuter gender), while the baseline system also generates a faulty word order.</S>
	</SECTION>
	<SECTION title="Conclusions and future work. " number = "8">
			<S sid ="852" ssid = "1">In this paper, we presented a revised, extended account of our previous work on using a range of features as source-language context to better enable a state-of-the-art PBSMT system to select appropriate target language phrases for consideration in the generation of the most probable translation given the input.</S>
			<S sid ="853" ssid = "2">Such features include neighbouring position-specific lexical and part-of-speech features of words surrounding the phrase to be translated, as well as information linking the head word of the phrase to its syntactic context in terms of supertags or dependency relations.</S>
			<S sid ="854" ssid = "3">While parts of this appeared in previously published research (Haque et al. 2009a,b), in this paper we added a number of novel aspects, including using semantic roles as new contextual features in PBSMT, adding new language pairs, and examining the scalability of our research to larger amounts of training data.</S>
			<S sid ="855" ssid = "4">The most significant improvements observed in our experiments involve the integration of long-distance contextual features, such as dependency relations in combination with part-of-speech tags in Dutch-to-English subtitle translation, the combination of dependency parse and semantic role information in English-to-Dutch parliamentary debate translation, or CCG and LTAG supertag features in English-to-Chinese translation.</S>
			<S sid ="856" ssid = "5">As far as scalability is concerned, when our PBSMT systems were trained with larger amounts of parallel data, the effects of the source-language context are lessened somewhat, but in some cases remain statistically significant.</S>
			<S sid ="857" ssid = "6">For English-to-Dutch, for example, while the POS-based model failed to contribute positively, our dependency- and supertag-based improvements continued to be effective.</S>
			<S sid ="858" ssid = "7">Furthermore, our novel use of semantic roles as a source-language discriminative feature showed encouraging improvements over the PBSMT baseline.</S>
			<S sid ="859" ssid = "8">When varying the amounts of English-to-Spanish Europarl training data used from 10,000 to 1.64 million sentences in a learning curve experiment, the resulting curves demonstrate that gains attained by our source-language contextual models cannot be expected to occur given any amount of training data.</S>
			<S sid ="860" ssid = "9">We observe that the TRIBL classifier attains gains at small training set sizes, though not at the smallest sizes (10,000 training sentences).</S>
			<S sid ="861" ssid = "10">IGTree, on the other hand, disappoints by requiring the maximal amount of training data (1.64 million sentences) to equal the baseline.</S>
			<S sid ="862" ssid = "11">Furthermore, learning curve experiments on the Dutch-to-English and English-to-Dutch language pairs show that rich and complex syntactic features surpass basic features (words and POS tags) as source-language context features in the small-scale as well as the large- scale translations.</S>
			<S sid ="863" ssid = "12">Moreover, outcomes of the manual analysis conducted on the MT outputs of the several context-informed models against the respective Moses baselines justify our claims established on the basis of the gains obtained with several automatic evaluation measures.</S>
			<S sid ="864" ssid = "13">We argue that, in general, learning curve experiments give a more complete overview of relative gains when more data is available.</S>
			<S sid ="865" ssid = "14">For attaining higher performance, using more training data remains the best advice.</S>
			<S sid ="866" ssid = "15">To summarize our findings, we have shown that whatever language pair might need to be deployed, using source-language context is guaranteed to produce better translations compared to a baseline PBSMT system.</S>
			<S sid ="867" ssid = "16">To be more precise, if one has a parser available for the source language at hand, integrating syntactic dependency information pertaining to the current input string can generate improved translation quality.</S>
			<S sid ="868" ssid = "17">Alternatively, if no such parser is available, then POS or supertag information can be useful, but if even this is absent, then taking the neighbouring words into account is also likely to be effective.</S>
			<S sid ="869" ssid = "18">Such source-language contextual models become less effective when scaling to large amounts of parallel data, yet even here, statistically significant scores are still to be seen.</S>
			<S sid ="870" ssid = "19">Furthermore, our experiments have been carried out on a wide range of language pairs, and on a variety of domains of training material.</S>
			<S sid ="871" ssid = "20">As for future work, we aim to conduct a suite of experiments to investigate the effectiveness of our models on state-of-the-art hierarchical phrase-based SMT systems (Chiang 2007); our initial effort in this direction (Haque et al. 2010) is encouraging.</S>
			<S sid ="872" ssid = "21">In addition, apart from experimenting with still more language pairs and different types of training data, we would like to provide a comprehensive guide on how best to combine different source-language contextual features where more than one type is available, and if possible, to predict a priori—perhaps based on the combination of language pair and training data type—the optimal features to use in such circumstances.</S>
			<S sid ="873" ssid = "22">As a first step we would investigate the influence of the degree to which a domain triggers formulaic language.</S>
			<S sid ="874" ssid = "23">If a domain contains largely formulaic language, selecting only simple lexical features such as neighbouring words could already be effective, as the generalizing power of more abstract linguistic features is not needed.</S>
			<S sid ="875" ssid = "24">The reverse may be the case in more open, less formulaic domains.</S>
			<S sid ="876" ssid = "25">Our memory-based classifiers could be used to provide a quantitative estimate of how similar unseen sequences are to training sentences, much like a fuzziness score in translation memories or example-based machine translation.</S>
			<S sid ="877" ssid = "26">Acknowledgements This work is supported by Science Foundation Ireland (grant no. 07/CE/ I1142) and the Irish Centre for High-End Computing.16 The work of Van den Bosch is supported by the Netherlands Organisation for Scientific Research (NWO) as part of the “Implicit Linguistics” Vici project, which also provided computing infrastructure.</S>
			<S sid ="878" ssid = "27">We would like to thank Yifan He and Sergio Penkale for their input on the presentation of translation examples in Chinese and Spanish languages, respectively.</S>
	</SECTION>
</PAPER>
