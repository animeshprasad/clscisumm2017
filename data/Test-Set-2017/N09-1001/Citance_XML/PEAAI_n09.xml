<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">The present work describes a classiﬁcation schema for irony detection in Greek political tweets.</S>
		<S sid ="2" ssid = "2">Our hypothesis states that humorous political tweets could predict actual election results.</S>
		<S sid ="3" ssid = "3">The irony detection concept is based on subjective perceptions, so only relying on human- annotator driven labor might not be the best route.</S>
		<S sid ="4" ssid = "4">The proposed approach relies on limited labeled training data, thus a semi-supervised approach is followed, where collective-learning algorithms take both labeled and unlabeled data into consideration.</S>
		<S sid ="5" ssid = "5">We compare the semi- supervised results with the supervised ones from a previous research of ours.</S>
		<S sid ="6" ssid = "6">The hypothesis is evaluated via a correlation study between the irony that a party receives on Twitter, its respective actual election results during the Greek parliamentary elections of May 2012, and the difference between these results and the ones of the preceding elections of 2009.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Irony as a para-linguistic element is used to ﬁguratively express a concept with a semantic meaning that is very different from its actual initial purpose.</S>
			<S sid ="8" ssid = "8">It is a challenging ﬁeld for computational linguistics and natural language processing due to the high ambiguity and the difﬁculty to detect it, objectively.</S>
			<S sid ="9" ssid = "9">Language use is vigorous and creative; there is no predeﬁned consensual agreement on how to recognize an ironic expression, due to the high subjectivity involved.</S>
			<S sid ="10" ssid = "10">In the last decade, irony expression has been thriving on social networks and particularly Twitter, because of the 140 characters restraint on the status updates, being a perfect ﬁt for good old one- liners.</S>
			<S sid ="11" ssid = "11">As a public social medium, users realize that their writings may be read and reproduced by potentially everyone, gaining popularity and followers.</S>
			<S sid ="12" ssid = "12">But this publicity, contrary to Facebook&apos;s real name policy, often has no direct consequences to their everyday lives, since the majority participate anonymously, using an avatar and a nickname.</S>
			<S sid ="13" ssid = "13">This no-censorship state contributes to the freedom of expressing personal thoughts on tough, taboo, unpopular or controversial issues, part of which contains the political satire.</S>
			<S sid ="14" ssid = "14">n Corresponding author.</S>
			<S sid ="15" ssid = "15">Tel.: þ 30 6946354612.</S>
			<S sid ="16" ssid = "16">(D. Spathis).</S>
			<S sid ="17" ssid = "17">Email addresses: p11char@ionio.gr (B. Charalampakis), p11spat@ionio.gr, sdimitris@csd.auth.gr (D. Spathis), p11kous@ionio.gr (E. Kouslis), kerman@ionio.gr (K. Kermanidis).</S>
			<S sid ="18" ssid = "18">Political satire is a signiﬁcant part of comedy which specializes in drawing entertainment from politics.</S>
			<S sid ="19" ssid = "19">Most of the times, it aims just to please.</S>
			<S sid ="20" ssid = "20">By nature, it does not offer a constructive view by itself; when it is used as part of criticism, it tends to simply pinpoint the unexpected or different.</S>
			<S sid ="21" ssid = "21">The high topicality of Twitter, combined with the ephemerality of political news, forms a state which is described as&apos;echo chamber&apos;, a group-thinking effect on virtually enclosed spaces, ampli- ﬁed by repetition (Colleoni et al., 2014).</S>
			<S sid ="22" ssid = "22">As a result, the occasional user might write something political just to &apos;jump on the bandwagon&apos;, without an initial conscious aim to criticize.</S>
			<S sid ="23" ssid = "23">Adding to that, politics is a topic that almost everybody is familiar with and makes more sense from the engagement and attention side to write about Obama instead of an obscure book you just read.</S>
			<S sid ="24" ssid = "24">Studies focus on the simultaneous usage of Twitter and the TV on circumstances like a political debate, where meta-talk tweets reveal critical scrutiny of the agenda or &apos;the debate about the debate&apos; (Kalsnes et al., 2014).</S>
			<S sid ="25" ssid = "25">In the rapidly changing web, there is a plethora of available text, especially from social networks, which is unlabeled, raw or unprocessed.</S>
			<S sid ="26" ssid = "26">Adding to the traditional supervised methods, there are quite a few techniques that enable us to take these huge unstructured data into account.</S>
			<S sid ="27" ssid = "27">An insight from our previous work was the subjectivity involved during the tagging of a text as ironic.</S>
			<S sid ="28" ssid = "28">Three of our authors who took up the tedious task of annotation could not agree on what should be considered as ironic or not.</S>
			<S sid ="29" ssid = "29">As a result, there cannot be a gold standard corpus of ironic tweets.</S>
			<S sid ="30" ssid = "30">This was our main motivation to explore semi-supervised techniques, since they take http://dx.doi.org/10.1016/j.engappai.2016.01.007 09521976/&amp; 2016 Elsevier Ltd. All rights reserved.</S>
			<S sid ="31" ssid = "31">into account both train and test data.</S>
			<S sid ="32" ssid = "32">To be speciﬁc, the technique we chose is collective classiﬁcation: a type of semi-supervised learning that presents an interesting method for optimizing the classiﬁcation of partially-labeled data.</S>
			<S sid ="33" ssid = "33">Considering the above, our empirical study tries to detect irony on a corpus of Greek political tweets by training a classiﬁer, using appropriate linguistic features, some of which are proposed for the ﬁrst time herein for irony detection.</S>
			<S sid ="34" ssid = "34">Our goal is to ﬁnd a relation between the ironic tweets that refer to the political parties and leaders in Greece in the pre-election period of May 2012, and their actual election results.</S>
			<S sid ="35" ssid = "35">We compare the semi-supervised results with the supervised ones from a previous research of ours.</S>
			<S sid ="36" ssid = "36">Regarding the novelty of our study, this is a ﬁrst exploration on the ﬁeld of irony detection with semi-supervised learning and an application in politics.</S>
			<S sid ="37" ssid = "37">The remainder of this paper is organized as follows: In Section 2, we present the related literature on the topics of irony detection, Twitter sentiment analysis and political expression.</S>
			<S sid ="38" ssid = "38">The next Sections 3 and 4 are dedicated to data preprocessing and its representation schema through the set of linguistic features that affect irony detection.</S>
			<S sid ="39" ssid = "39">The Section 5 describes the training procedure, the evaluation of the algorithms’ performance and their test procedure on a large unlabeled dataset.</S>
			<S sid ="40" ssid = "40">An overview of the study limitations, future research prospects and a summary of the empirical study are described in Section 6.</S>
	</SECTION>
	<SECTION title="Related  work. " number = "2">
			<S sid ="41" ssid = "1">The greater part of the literature on irony detection in computational linguistics is focused on English, but this is a ﬁrst attempt to explore this area in the Greek language, to the authors&apos; knowledge.</S>
			<S sid ="42" ssid = "2">Reyes et al.</S>
			<S sid ="43" ssid = "3">(2013) attempt to detect irony by examining the corpus on the following features: signatures (concerning point- edness, counter-factuality, and temporal compression), unexpectedness (concerning temporal imbalance and contextual imbalance), style (as captured by character-grams (c-grams), skip-grams (s-grams), and polarity skip-grams (ps-grams)) and emotional scenarios (concerning activation, imagery, and pleasantness).</S>
			<S sid ="44" ssid = "4">These features work better when they are used as part of a coherent framework rather than used individually.</S>
			<S sid ="45" ssid = "5">They used multiple datasets in order to evaluate their hypothesis and achieved a precision of 0.79 at best.</S>
			<S sid ="46" ssid = "6">Classiﬁcation is performed by Naïve Bayes and Decision Trees.</S>
			<S sid ="47" ssid = "7">Also a crisis management case study of the hashtag #Toyota is described.</S>
			<S sid ="48" ssid = "8">A study by Rajadesingan and Liu (2014) discovered an interesting aspect of Twitter usage, an &apos;orientation phase&apos; in which the user is gradually introduced to irony as one gains followers.</S>
			<S sid ="49" ssid = "9">The threshold of this phase is one&apos;s 30 initial tweets.</S>
			<S sid ="50" ssid = "10">The top features in decreasing order of importance for sarcasm detection are the following: Percentage of emoticons in the tweet, percentage of adjectives in the tweet, percentage of past words with sentiment score, number of polysyllables per word in the tweet, lexical density of the tweet.</S>
			<S sid ="51" ssid = "11">They evaluate using a J48 decision tree, logistic regression, and SVM to obtain an accuracy of 78.06%, 83.46%, and 83.05%, respectively.</S>
			<S sid ="52" ssid = "12">The usual approach on similar irony detection studies on Twitter is to identify the two classes by hashtag analysis.</S>
			<S sid ="53" ssid = "13">However, this method creates noisy results with low accuracy (GonzálezIbánez et al., 2011; Liebrecht et al., 2013).</S>
			<S sid ="54" ssid = "14">Features used by Gonzalez were Lexical (unigrams, affective language, interjections and punctuation) and Pragmatic (positive smileys, negative smileys, and “@toUser” signs if a twitter is directed to another user).</S>
			<S sid ="55" ssid = "15">Algorithms used were SVM with SMO and Logistic Regression.</S>
			<S sid ="56" ssid = "16">Overall SMO outperformed LogR, with the best accuracy of 57% being an indication of the difﬁculty of the task.</S>
			<S sid ="57" ssid = "17">On the other hand, Liebrecht approached the same problem with the Balanced Winnow algorithm for classiﬁcation.</S>
			<S sid ="58" ssid = "18">The strongest linguistic markers of sarcastic utterances were markers that can be seen as synonyms for #sarcasm hashtag.</S>
			<S sid ="59" ssid = "19">Testing the classiﬁer on the top 250 of the tweets it ranked as most likely to be sarcastic, it attains a 30% average precision.</S>
			<S sid ="60" ssid = "20">Twitter lexical analysis on Greek tweets has been the main subject of the research by Kermanidis and Maragoudakis (2013), examining the sentimental tagging in a supervised environment.</S>
			<S sid ="61" ssid = "21">Their hypothesis is focused on the positive / negative distinction, using statistical metrics such as count and frequency distributions.</S>
			<S sid ="62" ssid = "22">The alignment between actual political results and web sentiment in both directions was investigated and conﬁrmed that there is a relation between political results and web sentiment.</S>
			<S sid ="63" ssid = "23">We use the same corpus of tweets in our study.</S>
			<S sid ="64" ssid = "24">Apart from Twitter, similar techniques have been applied on Amazon reviews as well, making use of structured information of reviews versus the unstructured nature of Twitter.</S>
			<S sid ="65" ssid = "25">The accuracy results are encouraging due to the semi-supervised technique and the huge dataset, requiring human-annotator labor though, Davidov et al.</S>
			<S sid ="66" ssid = "26">(2010); Tsur et al.</S>
			<S sid ="67" ssid = "27">(2010).</S>
			<S sid ="68" ssid = "28">Features used were high- frequency words, content words, sentence length and punctuation.</S>
			<S sid ="69" ssid = "29">Results on the Twitter dataset are better than those obtained on the Amazon dataset, with accuracy of 0.947 with a k-nearest neighbors implementation.</S>
			<S sid ="70" ssid = "30">Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).</S>
			<S sid ="71" ssid = "31">Their approach involves Wordnet, like us, and they propose a subjectivity measure of each Wordnet entry.</S>
			<S sid ="72" ssid = "32">They suggest a semi-supervised minimum-cut framework that makes use of both WordNet deﬁnitions and its relation structure.</S>
			<S sid ="73" ssid = "33">Minimum-cut is a technique used at graph theory, which uses pairwise relationships between the data points in order to learn from both labeled and unlabeled data.</S>
			<S sid ="74" ssid = "34">The semi- supervised approach achieves the same results as the supervised framework with less than 20% of the training data.</S>
			<S sid ="75" ssid = "35">In the emerging area of active learning, where the learning algorithm is able to interactively query the researcher to obtain the desired outputs at new data points, there is some ongoing research.</S>
			<S sid ="76" ssid = "36">Gokhan et al.</S>
			<S sid ="77" ssid = "37">(2005) wanted to reduce the labeling effort for spoken language understanding from data gathered at AT&amp;T call centers.</S>
			<S sid ="78" ssid = "38">The examples that are classiﬁed with higher con- ﬁdence scores (not selected by active learning) are exploited using two semi-supervised learning methods.</S>
			<S sid ="79" ssid = "39">This enables them to exploit all collected data and alleviates the data imbalance problem caused by employing only active or semi-supervised learning.</S>
			<S sid ="80" ssid = "40">Their results indicate that it is possible to reduce human labeling effort signiﬁcantly.</S>
			<S sid ="81" ssid = "41">Similar technique, namely collective learning, was followed by Santos et al.</S>
			<S sid ="82" ssid = "42">(2011), where they propose a new method that adopts a collective learning approach to detect unknown malware.</S>
			<S sid ="83" ssid = "43">Their empirical research demonstrates that the labeling efforts are lower than when supervised learning is used, while maintaining high accuracy rates.</S>
			<S sid ="84" ssid = "44">Collective classiﬁcation is an approach that uses the relational structure of the combined labeled and unlabeled dataset to enhance classiﬁcation accuracy (Neville and Jensen, 2003).</S>
			<S sid ="85" ssid = "45">Research by de-la-Peña-Sordo et al.</S>
			<S sid ="86" ssid = "46">(2013) studied the comparison between collective learning and supervised techniques, pretty similar with our methodology.</S>
			<S sid ="87" ssid = "47">Apart from that, quite similar was their topic of detecting trolling comments on a Spanish platform like Digg or Reddit and their lexical features selection, since irony and trolling may seem indistinguishable in some cases.</S>
			<S sid ="88" ssid = "48">Their approach obtains nearly the same accuracy than the best supervised learning approaches.</S>
			<S sid ="89" ssid = "49">Another study dealing with online opinion and reviews, again by Reyes and Rosso (2011), examined Amazon and Slashdot.com Fig.</S>
			<S sid ="90" ssid = "50">1.</S>
			<S sid ="91" ssid = "51">Structure and data preprocessing of the initial dataset and the cleaned one after preprocessing.</S>
			<S sid ="92" ssid = "52">customer reviews trained on Naive Bayes, Decision Trees and Support Vector Machines.</S>
			<S sid ="93" ssid = "53">Accuracy results were satisfying and feature selection ranked as top the features of POS 3-gram (frequent sequence of trigrams) and Pleasantness (dictionary approach to pleasant and unpleasant words).Considering the above attempts in the ﬁeld of computa tional linguistics, the novelty of our study lies on the leverage of these above techniques in Greek, based on the hypothesis that sarcasm and irony in Twitter messages may be linked to actual elections results.</S>
			<S sid ="94" ssid = "54">We conduct a comparison between traditional supervised and semi-supervised learning.</S>
			<S sid ="95" ssid = "55">Regarding the tools we developed, we used NLTK, Weka and Word- net, but the feature extraction / text mining was done in Python code developed by us, and the methodology is described in the following sections (Fig.</S>
			<S sid ="96" ssid = "56">1).</S>
	</SECTION>
	<SECTION title="Data preprocessing. " number = "3">
			<S sid ="97" ssid = "1">The dataset contains 61.427 Greek tweets collected on the week before and the week after the May 2012 parliamentary elections in Greece (Figs.</S>
			<S sid ="98" ssid = "2">1 and 2).</S>
			<S sid ="99" ssid = "3">The dataset is divided in 2 sub-datasets: parties and leaders.</S>
			<S sid ="100" ssid = "4">For each one, there are two subsections: before and after the elections.</S>
			<S sid ="101" ssid = "5">The dataset structure before the cleanup is presented below: The dataset is available for research purposes1 (Table 1 and 2).</S>
			<S sid ="102" ssid = "6">The ﬁrst step was to eliminate the duplicate tweets in order to avoid frequency bias.</S>
			<S sid ="103" ssid = "7">Duplicate tweets are identical tweets and “retweets” (RTs) that do not contribute to our linguistic goals.The second step was to delete the useless, unstructured arti facts (unformatted tweets) that were fetched by the Twitter API.</S>
			<S sid ="104" ssid = "8">In order to form a unibody test set, we merged the above sub- datasets.</S>
			<S sid ="105" ssid = "9">We decided to keep the tweets that contain links, because our hypothesis supports that tweets with links, for instance newspaper article tweets, are neutral, not ironic.</S>
			<S sid ="106" ssid = "10">After the cleanup, the size of tweets was 44.438.</S>
			<S sid ="107" ssid = "11">The semantic analysis was assigned to Balkanet (Tuﬁs et al., 20 04), a Greek edition of the WordNet (OMW: Open Multilingual Wordnet), a popular lexical database that groups words into sets of cognitive synonyms (synsets), each expressing a distinct concept.</S>
			<S sid ="108" ssid = "12">Also, the Python natural language package, NLTK (Bird et al., 2009), was used in order to support Wordnet.</S>
			<S sid ="109" ssid = "13">The machine learning and training process was performed using the Weka software (Fig.</S>
			<S sid ="110" ssid = "14">2).2</S>
	</SECTION>
	<SECTION title="Features. " number = "4">
			<S sid ="111" ssid = "1">We approached irony detection as a text classiﬁcation problem.</S>
			<S sid ="112" ssid = "2">The decision if a tweet is ironic-or-not is a binary decision.</S>
			<S sid ="113" ssid = "3">We tag each tweet with ﬁve features, taking into consideration structural sentence formations and unexpectedness occurrences.</S>
			<S sid ="114" ssid = "4">Some of the features are designed to detect imbalance and unexpectedness, others to detect common patterns in the structure of the ironic tweets (like type of punctuation, length, and emoticons).</S>
			<S sid ="115" ssid = "5">(Barbieri and Saggion, 2014) Our features are grouped into the following model: • Spoken (spoken style applied in writings) • Rarity (the frequency occurrences of the most rare words) • Meanings (the number of Wordnet synsets as a measure of ambiguity) • Lexical (punctuation, prosodic repeated letters, metaphors) • Emoticons (smiley faces etc) We analyzed the features with the pearson correlation and found low correlation between the variables.</S>
			<S sid ="116" ssid = "6">To be precise, the highest correlations were between the dependent and the independent variables: rarityScore and isIronic (0.398), lexicalScore and isIronic (0.350).</S>
			<S sid ="117" ssid = "7">These relations are conﬁrmed by the Feature Selection process as well (see 5.3).</S>
			<S sid ="118" ssid = "8">The only correlation between independent variables (features) was between rarityScore and emoticonScore (0.329) which is considered relatively low, on statistical terms.</S>
			<S sid ="119" ssid = "9">4.1.</S>
			<S sid ="120" ssid = "10">Spoken.</S>
			<S sid ="121" ssid = "11">The verbal irony in Twitter is often expressed as everyday-life chats between potentially real characters, using heavily dashes (-) and asterisks (*).</S>
			<S sid ="122" ssid = "12">Their occurrences in tweets count positively in our classiﬁer.</S>
			<S sid ="123" ssid = "13">The use of spoken language is often related to unexpectedness.</S>
			<S sid ="124" ssid = "14">In political context, dashes may be used to quote an actual quote, but the 1 http://di.ionio.gr/hilab/doku.php?id ¼ start:websent.</S>
			<S sid ="125" ssid = "15">2 http://www.cs.waikato.ac.nz/ml/weka.</S>
			<S sid ="126" ssid = "16">Fig.</S>
			<S sid ="127" ssid = "17">2.</S>
			<S sid ="128" ssid = "18">Schematic ﬂowchart of the workﬂow we followed, regarding the datasets, the training techniques and the operations.</S>
			<S sid ="129" ssid = "19">Table 1 Number of tweets mentioning political parties.</S>
			<S sid ="130" ssid = "20">Par ty A N T A R S Y A P A S O K D H M A R K K E N D S Y R I Z A X A A N E L Bef ore elec tion s 1 6 1 5 2 9 0 9 1 3 7 3 1 5 8 8 7 9 2 5 1 5 9 2 7 3 1 7 9 1 Afte r elec tion s 1 6 2 8 9 9 8 4 5 0 6 1 5 9 4 1 2 7 2 2 2 9 4 3 3 0 0 1 1 0 9 reason is usually to add a sarcastic comment.</S>
			<S sid ="131" ssid = "21">The asterisk character showcases movements or nonverbal actions in tweets, such as *sigh* or *faints*, adding an emotional level.</S>
			<S sid ="132" ssid = "22">If there is at least one of the above characters in the tweet, the value of the feature is &apos;true&apos;, otherwise it is&apos;false&apos;.</S>
			<S sid ="133" ssid = "23">Thus, the spoken feature is binary.</S>
			<S sid ="134" ssid = "24">We grouped the attributes as one variable because the continuous score does not check the existence or not of spoken speech.</S>
			<S sid ="135" ssid = "25">4.2.</S>
			<S sid ="136" ssid = "26">Rarity.</S>
			<S sid ="137" ssid = "27">A frequency dictionary for all the words of the original dataset was created.</S>
			<S sid ="138" ssid = "28">The tweets are split into tokens, and token occurrences (excluding URL links) are counted.</S>
			<S sid ="139" ssid = "29">Thus, we isolated the rarest words and limited the upper bound to three occurrences.</S>
			<S sid ="140" ssid = "30">The resulted frequency dictionary consists of 25.898 words.</S>
			<S sid ="141" ssid = "31">The Table 2 Number of tweets mentioning party leaders.</S>
			<S sid ="142" ssid = "32">Lea der Tsi pra s Ka m me no s Sa ma ras Mi ch alo liak os Bef ore elec tion s 32 82 351 1 451 3 105 9 Afte r elec tion s 29 92 172 1 317 1 253 0 rarest cases had 1 occurrence.</S>
			<S sid ="143" ssid = "33">If a word had 3 occurrences, it was less rare.</S>
			<S sid ="144" ssid = "34">In order to invert this scale, proper weights were attached to each token: weights 10, 5 and 2 were assigned to frequencies 1, 2 and 3 respectively.</S>
			<S sid ="145" ssid = "35">The distance of the ﬁrst weight (10) and the second (5) shows the signiﬁcance of the most rare words.</S>
			<S sid ="146" ssid = "36">The ﬁnal score is the following equation: m P wordn ðweightÞ n ¼ 1 Fig.</S>
			<S sid ="147" ssid = "37">3.</S>
			<S sid ="148" ssid = "38">Example tweet assigned with feature scores.</S>
			<S sid ="149" ssid = "39">4.5.</S>
			<S sid ="150" ssid = "40">Emoticons.</S>
			<S sid ="151" ssid = "41">The Emoticon feature detects all the possible variations of smiley, sad and mocking faces such as :), :-(, :P etc. Irony can be detected by the existence of emoticons, due to emotional charge.</S>
			<S sid ="152" ssid = "42">The value of the Emoticon feature is binary: &apos;true&apos; if at least one m The formula that estimates the rarity score of each word of the examined tweet, where: m¼number of words (tokens) of each tweet and n¼current word (token) under examination.</S>
			<S sid ="153" ssid = "43">This formula looks up every word of the given tweet in our frequency dictionary.</S>
			<S sid ="154" ssid = "44">If the word is found there, it attaches a weight according to the scale we described above.</S>
			<S sid ="155" ssid = "45">For normalization purposes, we divide the weight scores with the number of words of each tweet.</S>
			<S sid ="156" ssid = "46">The three occurrences threshold is over the whole dataset.</S>
			<S sid ="157" ssid = "47">We qualitatively examined the dataset with 4 þ occurrences and there were many frequent words inside that do not provide linguistic value, so we set the limit to three.</S>
			<S sid ="158" ssid = "48">The descriptive statistics of this variable distribution show high concentration around the 0.4 score with a maximum value of 10.</S>
			<S sid ="159" ssid = "49">Almost half of the dataset tweets have a score of zero.</S>
			<S sid ="160" ssid = "50">4.3.</S>
			<S sid ="161" ssid = "51">Meanings.</S>
			<S sid ="162" ssid = "52">We used the Balkanet packet of Wordnet to extract the meanings of each word, because the use of a word with multiple meanings implies ambiguity and eventually irony.</S>
			<S sid ="163" ssid = "53">For instance, the one-liner &apos;Change is inevitable, except from a vending machine&apos; exploits the ambiguity, and consequently wrong expectations, induced by the word change (Mihalcea and Strapparava, 2006).</S>
			<S sid ="164" ssid = "54">Our algorithm looks up in Balkanet every word of each tweet.</S>
			<S sid ="165" ssid = "55">If a word has multiple synsets (meanings), we count their number and add them to the score.</S>
			<S sid ="166" ssid = "56">This process is repeated for every tweet.</S>
			<S sid ="167" ssid = "57">The descriptive statistics of this variable distribution show high concentration between scores 0.2 and 3 with a maximum value of 85.</S>
			<S sid ="168" ssid = "58">Almost a third of the dataset tweets have a score of zero.</S>
			<S sid ="169" ssid = "59">4.4.</S>
			<S sid ="170" ssid = "60">Lexical.</S>
			<S sid ="171" ssid = "61">The lexical attributes of each tweet were: repeated letters, metaphor words and punctuation.</S>
			<S sid ="172" ssid = "62">Twitter&apos;s users use repeated letters to express a spoken-verbal emotion.</S>
			<S sid ="173" ssid = "63">This phenomenon is called prosody, altering the intonation of speech like singing (Cheang and Pell, 2008).</S>
			<S sid ="174" ssid = "64">Also, we track the occurrences of words that showcase ﬁgurative language.</S>
			<S sid ="175" ssid = "65">For the example, the word “like” in Greek is written as σαν, σα &amp; σάν.</S>
			<S sid ="176" ssid = "66">The punctuation feature is the aggregation of exclamation marks, question marks, dots and semicolons.</S>
			<S sid ="177" ssid = "67">The semicolon is used in Greek instead of the &apos;?&apos; symbol.</S>
			<S sid ="178" ssid = "68">The descriptive statistics of this variable distribution show high concentration of scores 1 and 2 with a maximum value of 5.</S>
			<S sid ="179" ssid = "69">Two thirds of the dataset tweets have a score of zero.</S>
			<S sid ="180" ssid = "70">emoticon appears in the tweet, &apos;false&apos; otherwise.</S>
			<S sid ="181" ssid = "71">The above example (Fig.</S>
			<S sid ="182" ssid = "72">3) displays a random political Greek tweet about a party leader and translates roughly to “Kammenos looks like sunburnt”.</S>
			<S sid ="183" ssid = "73">This tweet exploits a wordplay with the name of the party leader, which in Greek sounds like the word “sunburnt”.</S>
			<S sid ="184" ssid = "74">As a result, the lexical score is above zero, since it uses ﬁgurative speech (“like”), as well as the rarity score, expressing a colloquial term of the word sunburnt in Greek.</S>
	</SECTION>
	<SECTION title="Test and results. " number = "5">
			<S sid ="185" ssid = "1">5.1.</S>
			<S sid ="186" ssid = "2">Supervised technique.</S>
			<S sid ="187" ssid = "3">5.1.1.</S>
			<S sid ="188" ssid = "4">Training After the data preprocessing and the automatic feature scoring, we have a complete dataset, ready for labeling.</S>
			<S sid ="189" ssid = "5">We labeled a small amount of tweets manually (n ¼ 126) in order to train the classiﬁer.</S>
			<S sid ="190" ssid = "6">The distribution of the dependent variable is: 74 ironic and 52 non-ironic tweets, gathered by randomly sampling the big dataset.</S>
			<S sid ="191" ssid = "7">The resulting set was loaded on Weka and was trained on multiple algorithms according to the 10-fold-cross validation technique.</S>
			<S sid ="192" ssid = "8">Apart from probabilistic algorithms, we involved decision trees as well, in order to be able to rank the signiﬁcance of the features.</S>
			<S sid ="193" ssid = "9">The training algorithms with the best performance were: J48-the Weka version of C4.5 (Quinlan, 1993), SVM (Chang and Lin, 2011), Neural Networks, Naive Bayes (John and Langley, 1995), Functional Trees, KStar (Cleary and Trigg, 1995) and Random Forests (Breiman, 2001).</S>
			<S sid ="194" ssid = "10">The best performing algorithm on average was the Functional Trees (Precision ¼ 82.4).</S>
			<S sid ="195" ssid = "11">Functional Trees combine a univariate decision tree with a linear function by means of constructive induction.</S>
			<S sid ="196" ssid = "12">Decision trees created from the model are able to use decision nodes with multivariate tests, and leaf nodes that make predictions using linear functions (Gama, 2004).</S>
			<S sid ="197" ssid = "13">5.1.2.</S>
			<S sid ="198" ssid = "14">Testing The classiﬁcation model created from the Functional Trees algorithm was applied to the unlabeled datasets of each party in order to get the irony predictions.</S>
			<S sid ="199" ssid = "15">Due to the fact that our test dataset is unlabeled, we can’t evaluate the model&apos;s validity directly.</S>
			<S sid ="200" ssid = "16">An indirect, qualitative evaluation is attempted in the following sections, comparing the volume of irony in tweets with the actual election results.</S>
			<S sid ="201" ssid = "17">5.2.</S>
			<S sid ="202" ssid = "18">Semi-supervised technique.</S>
			<S sid ="203" ssid = "19">5.2.1.</S>
			<S sid ="204" ssid = "20">Training-collective classiﬁcation Collective classiﬁcation is a combinatorial optimization problem, in which we are given a set of documents, or nodes, D ¼ {d1, …,dn} and a neighborhood function N, where Ni D D/{Di}, which describes the underlying network structure.</S>
			<S sid ="205" ssid = "21">Being D a random collection of documents, it is divided into two sets X and Y where X corresponds to the documents for which we know the correct values and Y are the documents whose values need to be found (Santos et al., 2011; Namata et al. 2009) We applied the collective-tree algorithm, which is similar to the Random Tree classiﬁer, but takes into account both labeled and unlabeled data.</S>
			<S sid ="206" ssid = "22">This algorithm combines the training and prediction phase, so it receives as training the manual small dataset and the big unlabeled one as testing.</S>
			<S sid ="207" ssid = "23">According to the documentation3, the collective-tree algorithm splits the attribute at that position that divides the current subset of instances (of training and test instances) into (roughly) two halves.</S>
			<S sid ="208" ssid = "24">The tree is stopped from growing, if one of the following conditions is met: a. only training instances would be covered (the labels for these instances are already known!)</S>
			<S sid ="209" ssid = "25">b. only test instances in the leaf taking the distribution from the parent node c. only training instances of one class all test instances are considered to have this class 5.2.2.</S>
			<S sid ="210" ssid = "26">Testing The resulting predicted dataset is used as gold corpus training dataset against each unlabeled party dataset.</S>
			<S sid ="211" ssid = "27">5.2.3.</S>
			<S sid ="212" ssid = "28">Validation In order to evaluate the semi-supervised predictions, we used the resulting predicted dataset as train-set against the manual small dataset, enabling us to compare the performance of supervised vs semi-supervised techniques.</S>
			<S sid ="213" ssid = "29">The same algorithms as above were used (see Fig.</S>
			<S sid ="214" ssid = "30">4).</S>
			<S sid ="215" ssid = "31">The best performing algorithm is Random Forest (precision ¼ 83.1), while Naive Bayes once again behaves the worst.</S>
			<S sid ="216" ssid = "32">5.3.</S>
			<S sid ="217" ssid = "33">Hypothesis evaluation.</S>
			<S sid ="218" ssid = "34">In this section, we count the ironic and non-ironic tweets that were picked by our supervised and semi-supervised classiﬁers.</S>
			<S sid ="219" ssid = "35">Interestingly, the actual election results are not directly correlated but there is a trend between the parties that receive irony and their election votes&apos; percentage ﬂuctuation retrospectively.</S>
			<S sid ="220" ssid = "36">Table 3 shows that precision in both cases is quite similar so that the positive predicted outcome matches the manual positive tagging.</S>
			<S sid ="221" ssid = "37">On the other hand, recall is quite lower on the semi-supervised, meaning that the false negative rate is higher.</S>
			<S sid ="222" ssid = "38">Namely, semi- supervised classiﬁes more frequently as non-ironic what human tagged as ironic.</S>
			<S sid ="223" ssid = "39">Probabilistic and instance-based methods perform worse with more data, while decision trees in some cases outperform the traditional models.</S>
			<S sid ="224" ssid = "40">We cannot claim that one algorithm is better than another, but that it performs better on the given data.</S>
			<S sid ="225" ssid = "41">As we mention in Section 5.1, we include Decision Trees (J48, FT, Random Forest), Probabilistic (Naive Bayes), Instance-based (K-Star), Kernel-based (SVM) and Neural Networks.</S>
			<S sid ="226" ssid = "42">From theory, we know the considerations when choosing a ML algorithm are: accuracy, training time, linearity, number of parameters and number of features.</S>
			<S sid ="227" ssid = "43">Accuracy is covered thoroughly in our results tables and ﬁgures.</S>
			<S sid ="228" ssid = "44">In training time, our metrics show that naive bayes is the fastest while NNs expectedly require more training time.</S>
			<S sid ="229" ssid = "45">Linearity is a characteristic available only in SVMs and Neural Networks in our case.</S>
			<S sid ="230" ssid = "46">Qualitatively, our feature scatterplots do not present linearity, that is why those algorithms do not perform the best.</S>
			<S sid ="231" ssid = "47">Also, each algorithm Fig.</S>
			<S sid ="232" ssid = "48">4.</S>
			<S sid ="233" ssid = "49">Performance of the training algorithms, supervised against semi-supervised techniques.</S>
			<S sid ="234" ssid = "50">The semi-supervised precision is evaluated indirectly by using the predicted dataset as train-set against the human-annotated manual small dataset.</S>
			<S sid ="235" ssid = "51">needs different ﬁne-tuning regarding its parameters, with NNs being the most complex ones (many parameters) and probabilistic ones being more simple.</S>
			<S sid ="236" ssid = "52">Finally, the number of features remains the same (ﬁve Scores) in each experiment.</S>
			<S sid ="237" ssid = "53">Another observation, the semi-supervised ironic tweets are signiﬁcantly fewer, but their relative distance to the supervised ones is pretty much the same.</S>
			<S sid ="238" ssid = "54">We guess that the unlabeled dataset contributed to that, so that the smaller percent of ironic tweets of the semi-technique might be more representative and closer to the reality.</S>
			<S sid ="239" ssid = "55">Reasonably, the supervised ironic results seem a little too high; for instance, 70% of one party&apos;s tweets cannot be humorous.</S>
			<S sid ="240" ssid = "56">Table 4 consists of the sub-dataset of &apos;parties before elections&apos;.</S>
			<S sid ="241" ssid = "57">One should note that the irony percent shows a trend that may be interpreted as the hype for every party.</S>
			<S sid ="242" ssid = "58">The &apos;Election results&apos; column refers to the actual results of the May 2012 elections.</S>
			<S sid ="243" ssid = "59">The ﬂuctuation (last column) describes the difference between the May 2012 election results and the previous of 2009.</S>
			<S sid ="244" ssid = "60">The ﬂuctuation percent shows a trend on the edges, so that the &apos;loser&apos; parties of ND and PASOK are getting the most ironic tweets as well as the&apos;winner&apos; parties SYRIZA and XA.</S>
			<S sid ="245" ssid = "61">Both machine-learning techniques predict roughly the same result.</S>
			<S sid ="246" ssid = "62">5.4.</S>
			<S sid ="247" ssid = "63">Feature selection.</S>
			<S sid ="248" ssid = "64">The features signiﬁcance rank, ordered by decreasing signiﬁcance, based on Information Gain, is the following: 1) Rarity (0.1732), 2) Lexical (0.0841), Emoticon (0.0451), Meanings (0) and Spoken (0).</S>
			<S sid ="249" ssid = "65">As mentioned in Section 4, we do not have adequate scores for meanings and spoken, so Information Gain does not consider them to be signiﬁcant in feature selection.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "6">
			<S sid ="250" ssid = "1">Twitter in Greece is not as popular and established compared to other countries.</S>
			<S sid ="251" ssid = "2">Greek Twitter users are just 3.7% of the total population4.</S>
			<S sid ="252" ssid = "3">The average user is usually young, well-educated and liberal, something important for our political context.</S>
			<S sid ="253" ssid = "4">As a result, our ﬁndings are ﬁltered through this demographic.</S>
			<S sid ="254" ssid = "5">3 https://github.com/fracpete/collective-classiﬁcation-weka-package.</S>
			<S sid ="255" ssid = "6">4 statistic from social media analysis site trending.gr.</S>
			<S sid ="256" ssid = "7">Table 3 Performance measures of the training algorithms.</S>
			<S sid ="257" ssid = "8">Green indicates the best performance, while red the worst.</S>
			<S sid ="258" ssid = "9">Alg orit hm s Supe rvise d Semi supe rvise d Preci sion R e c a l l F m e a s u r e Preci sion R e c a ll F m e a s u r e J48 78.4 7 7 . 8 7 7 . 0 82.7 7 3 . 8 7 3 . 3 Nai ve bay es 72.8 7 1 . 4 7 1 . 6 69.5 5 9 . 5 5 7 . 8 Fun ctio nal tree s 82.4 8 0 . 2 7 9 . 1 81.2 7 3 . 0 7 2 . 6K Star 76.9 7 7 . 0 7 6 . 7 68.6 6 1.</S>
			<S sid ="259" ssid = "10">1 6 0 . 2 Ran do m fore st 77.4 7 7 . 0 7 6 . 3 83.1 7 4 . 6 7 4 . 2 SV M 73.6 7 3 . 8 7 3 . 5 71.7 6 5 . 9 6 5 . 6 Neu ral net wor k 79.9 7 8 . 6 7 7 . 6 63.8 5 5 . 6 5 3 . 8 Table 4 Ironic tweets that received every party and their election results.</S>
			<S sid ="260" ssid = "11">The ﬂuctuation describes the difference between the May 2012 election results and the previous.</S>
			<S sid ="261" ssid = "12">Party Ironic Total Ironic/total % 2012 Greek election results % Fluctuation % of 2012 results from 2009 greek election results Supervised Semi-supervised Supervised Semi-supervised ANT ARS YA 511 1 7 5 1 2 4 0 41 1 4 1 . 1 9 0 . 8 3 PAS OK 1440 5 9 2 2 2 4 0 64 2 6 1 3 . 1 8 3 0 . 7 4 DH MA R 497 2 2 4 1 2 2 3 41 1 8 6 . 1 1 N e w KKE 504 2 1 8 1 2 6 2 40 1 7 8 . 4 8 0 . 9 4 ND 252 1 0 4 5 6 0 45 1 8 1 8 . 8 0 1 4 . 6 0 SYR IZA 2649 1 0 5 4 4 3 7 1 61 2 4 1 6 . 7 0 1 2 . 1 0 XA 1040 4 1 0 1 8 2 1 57 2 2 6 . 9 0 6 . 6 8 ANE L 280 7 7 6 8 0 41 1 1 1 0 . 6 0 N e w On the technical side, we did not use a stemmer or a lemma- tizer, because our hypothesis is depending on rare words or wordplays which would be eliminated.</S>
			<S sid ="262" ssid = "13">Furthermore, the informal nature of the text would render the performance of such tools rather useless for a morphologically ﬂuent language such as Greek.</S>
			<S sid ="263" ssid = "14">Another restraint for our study was the shortage of tested NLP tools for the Greek language.</S>
			<S sid ="264" ssid = "15">Some available tools are not well documented or not accessible.</S>
			<S sid ="265" ssid = "16">As a result, our study is focused mainly on self-developed tools for mining the features from the text, which we are going to open source in the near future.</S>
			<S sid ="266" ssid = "17">On the semantic analysis, the meanings score was not effective due to the fact that the Balkanet framework does not support grammatical conjugation, resulting to fewer results.</S>
			<S sid ="267" ssid = "18">It accepted only the nominative case.</S>
			<S sid ="268" ssid = "19">Comparing the two machine learning techniques, semi- and supervised learning, we note that their performance is in some cases quite similar.</S>
			<S sid ="269" ssid = "20">Even semi-supervised techniques are based on a small seed train-set, which is why they are called that way after all.</S>
			<S sid ="270" ssid = "21">There is some literature on unsupervised text classiﬁcation but it is more useful when the main interest is clustering, not explicit classiﬁcation on predeﬁned classes (irony or not).</S>
			<S sid ="271" ssid = "22">We discussed earlier the value of semi-supervised learning in irony annotation, due to the fact that manual labeling is very subjective and not easily available.</S>
			<S sid ="272" ssid = "23">Humor per se is one the most disputed and personal virtues.</S>
			<S sid ="273" ssid = "24">As future work we could attempt an approach with word vectors or deep learning.We researched on the theoretical ground of the Twitter use cases, especially on what inﬂuences and motivates the individual to criticize and joke about politics.</S>
			<S sid ="274" ssid = "25">The empirical study, attempts to detect irony on Greek political tweets, to automatically label a big unlabeled dataset of them and to seek underlying relations between the irony that the parties receive and their actual election results.</S>
			<S sid ="275" ssid = "26">The performance of the two machine learning techniques is reasonably acceptable (supervised &quot;&apos; 82%, semi-supervised &quot;&apos; 83%) and produces similar results on predicting the ﬂuctuation from previous election results, establishing our initial hypothesis.</S>
			<S sid ="276" ssid = "27">The collective learning approach detected fewer ironic tweets, that in our opinion is closer to the reality.</S>
			<S sid ="277" ssid = "28">The big unlabeled dataset assisted and contributed to this result.</S>
			<S sid ="278" ssid = "29">The real-world application of irony detection could be useful to polling companies to get the pulse of social media in election periods as well as to the parties to get feedback.</S>
			<S sid ="279" ssid = "30">Another business- oriented aspect could be its use by brands in crisis management situations to leverage the opinion of the web.</S>
			<S sid ="280" ssid = "31">Zooming out, humor detection was always one of the desired targets of computational intelligence, where the machines will be able to empathize with humans in all aspects of speech, ﬁgurative or literal.</S>
	</SECTION>
</PAPER>
