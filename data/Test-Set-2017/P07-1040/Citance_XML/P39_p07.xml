<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Hypergraphs are used in several syntax­ inspired methods of machine translation to compactly encode exponentially many trans­ lation hypotheses.</S>
		<S sid ="2" ssid = "2">The hypotheses closest to given reference translations therefore cannot be found via brute force, particularly for pop­ ular measures of closeness such as BLEU.</S>
		<S sid ="3" ssid = "3">We develop a dynamic program for extracting the so called oracle-best hypothesis from a hyper­ graph by viewing it as the problem of finding the most likely hypothesis under an n-gram language model trained from only the refer­ ence translations.</S>
		<S sid ="4" ssid = "4">We further identify and re­ move massive redundancies in the dynamic program state due to the sparsity of n-grams present in the reference translations, resulting in a very efficient program.</S>
		<S sid ="5" ssid = "5">We present run­ time statistics for this program, and demon­ strate successful application of the hypothe­ ses thus found as the targets for discriminative training of translation system components.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">A hypergraph, as demonstrated by Huang and Chi­ ang (2007), is a compact data-structure that can en­ code an exponential number of hypotheses gener­ ated by a regular phrase-based machine translation (MT) system (e.g., Koehn et al.</S>
			<S sid ="7" ssid = "7">(2003)) or a syntax­ based MT system (e.g., Chiang (2007)).</S>
			<S sid ="8" ssid = "8">While the hypergraph represents a very large set of transla­ tions, it is quite possible that some desired transla­ tions (e.g., the reference translations) are not con­ tained in the hypergraph, due to pruning or inherent deficiency of the translation model.</S>
			<S sid ="9" ssid = "9">In this case, one is often required to find the translation(s) in the hy­ pergraph that are most similar to the desired transla­ tions, with similarity computed via some automatic metric such as BLEU (Papineni et al., 2002).</S>
			<S sid ="10" ssid = "10">Such maximally similar translations will be called oracle­ best translations, and the process of extracting them oracle extraction.</S>
			<S sid ="11" ssid = "11">Oracle extraction is a nontrivial task because computing the similarity of any one hypothesis requires information scattered over many items in the hypergraph, and the exponentially large number of hypotheses makes a brute-force linear search intractable.</S>
			<S sid ="12" ssid = "12">Therefore, efficient algorithms that can exploit the structure of the hypergraph are required.</S>
			<S sid ="13" ssid = "13">We present an efficient oracle extraction algo­ rithm, which involves two key ideas.</S>
			<S sid ="14" ssid = "14">Firstly, we view the oracle extraction as a bottom-up model scoring process on a hypergraph, where the model is &quot;trained&quot; on the reference translation(s).</S>
			<S sid ="15" ssid = "15">This is sim­ ilar to the algorithm proposed for a lattice by Dreyer et al.</S>
			<S sid ="16" ssid = "16">(2007).</S>
			<S sid ="17" ssid = "17">Their algorithm, however, requires maintaining a separate dynamic programming state for each distinguished sequence of &quot;state&quot; words and the number of such sequences can be huge, mak­ ing the search very slow.</S>
			<S sid ="18" ssid = "18">Secondly, therefore, we present a novel look-ahead technique, called equiv­ alent oracle-state maintenance, to merge multiple states that are equivalent for similarity computation.</S>
			<S sid ="19" ssid = "19">Our experiments show that the equivalent oracle­ state maintenance technique significantly speeds up (more than 40 times) the oracle extraction.</S>
			<S sid ="20" ssid = "20">Efficient oracle extraction has at least three im­ portant applications in machine translation.</S>
			<S sid ="21" ssid = "21">Discriminative Training: In discriminative train­ ing, the objective is to tune the model parameters, e.g. weights of a perceptron model or conditional random field, such that the reference translations are preferred over competitors.</S>
			<S sid ="22" ssid = "22">However, the reference translations may not be reachable by the translation system, in which case the oracle-best hypotheses should be substituted in training.</S>
			<S sid ="23" ssid = "23">9 Proceedings of NAACL HLT 2009: Short Papers, pages 912, Boulder, Colorado, June 2009.</S>
			<S sid ="24" ssid = "24">@2009 Association for Computational Linguistics System Combination: In a typical system combi­ nation task, e.g. Rosti et al.</S>
			<S sid ="25" ssid = "25">(2007), each compo­ nent system produces a set of translations, which are then grafted to form a confusion network.</S>
			<S sid ="26" ssid = "26">The confusion network is then rescored, often employ­ ing additional (language) models, to select the fi­ nal translation.</S>
			<S sid ="27" ssid = "27">When measuring the goodness of a hypothesis in the confusion network, one requires its score under each component system.</S>
			<S sid ="28" ssid = "28">However, some translations in the confusion network may not be reachable by some component systems, in which case a system&apos;s score for the most similar reachable translation serves as a good approximation.</S>
			<S sid ="29" ssid = "29">Multi-source Translation: In a multi-source translation task (Och and Ney, 2001) the input is given in multiple source languages.</S>
			<S sid ="30" ssid = "30">This leads to a situation analogous to system combination, except that each component translation system now corresponds to a specific source language.</S>
	</SECTION>
	<SECTION title="Oracle Extraction on a Hypergraph. " number = "2">
			<S sid ="31" ssid = "1">In this section, we present the oracle extraction al­ gorithm: it extracts one or more translations in a hy­ pergraph that have the maximum BLEU score1 with respect to the corresponding reference translation(s).</S>
			<S sid ="32" ssid = "2">The BLEU score of a hypothesis h relative to a reference r may be expressed in the log domain as, logBLEU(r,h) =min[111 ,o]+ logpn.</S>
			<S sid ="33" ssid = "3">The first component is the brevity penalty when I hI&lt; I r I, while the second component corresponds to the geometric mean of n-gram precisions Pn (with clipping).</S>
			<S sid ="34" ssid = "4">While BLEU is normally defined at the corpus level, we use the sentence-level BLEU for the purpose of oracle extraction.</S>
			<S sid ="35" ssid = "5">Two key ideas for extracting the oracle-best hy­ pothesis from a hypergraph are presented next.</S>
			<S sid ="36" ssid = "6">2.1 Oracle Extraction as Model Scoring.</S>
			<S sid ="37" ssid = "7">Our first key idea is to view the oracle extraction as a bottom-up model scoring process on the hy­ pergraph.</S>
			<S sid ="38" ssid = "8">Specifically, we train a 4-gram language model (LM) on only the reference translation( s ), 1We believe our method is general and can be extended to other metrics capturing only n-gram dependency and other com­ pact data structures, e.g. lattices.</S>
			<S sid ="39" ssid = "9">and use this LM as the only model to do a Viterbi search on the hypergraph to find the hypothesis that has the maximum (oracle) LM score.</S>
			<S sid ="40" ssid = "10">Essentially, the LM is simply a table memorizing the counts of n-grams found in the reference translation(s), and the LM score is the log-BLEU value (instead oflog­ probability, as in a regular LM).</S>
			<S sid ="41" ssid = "11">During the search, the dynamic programming (DP) states maintained at each item include the left- and right-side LM context, and the length of the partial translation.</S>
			<S sid ="42" ssid = "12">To compute the n-gram precisions Pn incrementally during the search, the algorithm also memorizes at each item a vector of maximum numbers of n-gram matches between the partial translation and the ref­ erence(s).</S>
			<S sid ="43" ssid = "13">Note however that the oracle state of an item (which decides the uniqueness of an item) de­ pends only on the LM contexts and span lengths, not on this vector of n-gram match counts.</S>
			<S sid ="44" ssid = "14">The computation of BLEU also requires the brevity penalty, but since there is no explicit align­ ment between the source and the reference(s), we cannot get the exact reference length lrl at an inter­ mediate item.</S>
			<S sid ="45" ssid = "15">The exact value of brevity penalty is thus not computable.</S>
			<S sid ="46" ssid = "16">We approximate the true refer­ ence length for an item with a product between the length of the source string spanned by that item and a ratio (which is between the lengths of the whole reference and the whole source sentence).</S>
			<S sid ="47" ssid = "17">Another approximation is that we do not consider the effect of clipping, since it is a global feature, making the strict computation intractable.</S>
			<S sid ="48" ssid = "18">This does not signifi­ cantly affect the quality of the oracle-best hypothesis as shown later.</S>
			<S sid ="49" ssid = "19">Table 1 shows an example how the BLEU scores are computed in the hypergraph.</S>
			<S sid ="50" ssid = "20">The process above may be used either in a first­ stage decoding or a hypergraphrescoring stage.</S>
			<S sid ="51" ssid = "21">In the latter case, if the hypergraph generated by the first-stage decoding does not have a set of DP states that is a superset of the DP states required for ora­ cle extraction, we need to split the items of the first­ stage hypergraph and create new items with suffi­ ciently detailed states.</S>
			<S sid ="52" ssid = "22">It is worth mentioning that if the hypergraph items contain the state information necessary for extract­ ing the oracle-best hypothesis, it is straightforward to further extract the k-best hypotheses in the hyper­ graph (according to BLEU) for any k 2: 1 using the algorithm of Huang and Chiang (2005).</S>
			<S sid ="53" ssid = "23">I Item I lhl I lrl matches I log BLEU I Ite m A lte m B 5 1 0 6.</S>
			<S sid ="54" ssid = "24">2 9.</S>
			<S sid ="55" ssid = "25">8 ( 3 , 2 , 2 , 1 ) ( 8 , 7 , 6 , 5 ) 0 . 8 2 0 . 2 7 lte m C 1 7 18 .3 (1 2, 10 , 9, 6) 0 . 6 2 Table 1: Example computation when items A and B are combined by a rule to produce item c. lrl is the approxi­ mated reference length as described in the text.</S>
			<S sid ="56" ssid = "26">2.2 Equivalent Oracle State Maintenance.</S>
			<S sid ="57" ssid = "27">The process above, while able to extract the oracle­ best hypothesis from a hypergraph, is very slow due to the need to maintain a dedicated item for each or­ acle state (i.e., a combination ofleftLM state, right­ LM state, and hypothesis length).</S>
			<S sid ="58" ssid = "28">This is especially true if the baseline system uses a LM whose order is smaller than four, since we need to split the items in the original hypergraph into many sub-items during the search.</S>
			<S sid ="59" ssid = "29">To speed up the extraction, our second key idea is to maintain an equivalent oracle state.</S>
			<S sid ="60" ssid = "30">Roughly speaking, instead of maintaining a dif­ ferent state for different language model words, we collapse them into a single state whenever it does not affect BLEU.</S>
			<S sid ="61" ssid = "31">For example, if we have two left-side LM states a b c and a b d, and we know that the reference(s) do not have any n-gram ending with them, then we can reduce them both to a band ig­ nore the last word.</S>
			<S sid ="62" ssid = "32">This is because the combination of neither left-side LM state (a b cor a b d) can contribute an n-gram match to the BLEU computa­ tion, regardless of which prefix in the hypergraph they combine with.</S>
			<S sid ="63" ssid = "33">Similarly, if we have two right­ side LM states a b c and d b c, and if we know that the reference(s) do not have any n-gram starting with either, then we can ignore the first word and re­ duce them both to b c. We can continue this reduc­ tion recursively as shown in Figures 1 and 2, where IS-A-PREFIX( ei) (or IS-A-SUFFIX(ef)) checks if ei (resp.</S>
			<S sid ="64" ssid = "34">ei) is a prefix (suffix) of any n-gram in the reference translation(s).</S>
			<S sid ="65" ssid = "35">For BLEU, 1 :::; n :::; 4.</S>
			<S sid ="66" ssid = "36">This equivalent oracle state maintenance tech­ nique, in practice, dramatically reduces the number of distinct items preserved in the hypergraph for or­ acle extraction.</S>
			<S sid ="67" ssid = "37">To understand this, observe that if all hypotheses in the hypergraph together contain m unique n-grams, for any fixed n, then the total num­ ber of equivalent items takes a multiplicative factor that is O(m2 ) due to left- and right-side LM state EQ-L-STATE (ef) 1 els +- ef 2 for i+- m to 1 C&gt; right to left 3 if IS-A-SUFFIX(ei) 4 break C&gt; stop reducing e1s 5 else 6 els +- ei1 C&gt; reduce state 7 return els Figure 1: Equivalent Left LM State Computation.</S>
			<S sid ="68" ssid = "38">EQ-R-STATE (ef) 1 ers +- ef 2 for i+- 1 to m C&gt; left to right 3 if IS-A-PREFIX (ei) 4 break C&gt; stop reducing e r s 5 else 6 ers +- eH-1 C&gt; reduce state 7 return ers Figure 2: Equivalent Right LM State Computation.</S>
			<S sid ="69" ssid = "39">maintenance of Section 2.1.</S>
			<S sid ="70" ssid = "40">This multiplicative fac­ tor under the equivalent state maintenance above is O(m2 ), where m is the number of unique n-grams in the reference translations.</S>
			<S sid ="71" ssid = "41">Clearly, m « m by several orders of magnitude, leading to effectively much fewer items to process in the chart.</S>
			<S sid ="72" ssid = "42">One may view this idea of maintaining equivalent states more generally as an outside look-ahead dur­ ing bottom-up inside parsing.</S>
			<S sid ="73" ssid = "43">The look-ahead uses some external information, e.g. IS-A-SUFFIX(·), to anticipate whether maintaining a detailed state now will be of consequence later; if not then the in­ side parsing eliminates or collapses the state into a coarser state.</S>
			<S sid ="74" ssid = "44">The technique proposed by Li and Khudanpur (2008a) for decoding with large LMs is a special case of this general theme.</S>
	</SECTION>
	<SECTION title="Experimental Results. " number = "3">
			<S sid ="75" ssid = "1">We report experimental results on a Chinese to En­ glish task, for a system that is trained using a similar pipeline and data resource as in Chiang (2007).</S>
			<S sid ="76" ssid = "2">3.1 Goodness of the Oracle-Best Translations.</S>
			<S sid ="77" ssid = "3">Table 2 reports the average speed (seconds/sentence) for oracle extraction.</S>
			<S sid ="78" ssid = "4">Hypergraphs were generated with a trigram LM and expanded on the fly for 4- gram BLEU computation.</S>
			<S sid ="79" ssid = "5">Basic DP Collapse equiv.</S>
			<S sid ="80" ssid = "6">states speedup 25.4 sec/sent 0.6 sec/sent X 42 Table 2: Speed of oracle extraction from hypergraphs.</S>
			<S sid ="81" ssid = "7">The basic dynamic program (Sec.</S>
			<S sid ="82" ssid = "8">2.1) improves signifi­ cantly by collapsing equivalent oracle states (Sec.</S>
			<S sid ="83" ssid = "9">2.2).</S>
			<S sid ="84" ssid = "10">Table 3 reports the goodness of the oracle-best hy­ potheses on three standard data sets.</S>
			<S sid ="85" ssid = "11">The highest achievable BLEU score in a hypergraph is clearly much higher than in the 500-best unique strings.</S>
			<S sid ="86" ssid = "12">This shows that a hypergraph provides a much better basis, e.g., for reranking than an n-best list.</S>
			<S sid ="87" ssid = "13">As mentioned in Section 2.1, we use several ap­ proximations in computing BLEU (e.g., no clipping and approximate reference length).</S>
			<S sid ="88" ssid = "14">To justify these approximations, we first extract 500-best unique or­ acles from the hypergraph, and then rerank the ora­ cles based on the true sentence-level BLEU.</S>
			<S sid ="89" ssid = "15">The last row of Table 3 reports the reranked one-best oracle BLEU scores.</S>
			<S sid ="90" ssid = "16">Clearly, the approximations do not hurt the oracle BLEU very much.</S>
			<S sid ="91" ssid = "17">I Hypothesis space I MT&apos;04 I MT&apos;05 I MT&apos;06 I I 1-best (Baseline) I 35.7 I 32.6 I 28.3 I I 500-unique-best I 44.0 I 41.2 I 35.1 I Hypergraph 500-best oracles Table 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST ChineseEnglish MT datasets.</S>
			<S sid ="92" ssid = "18">3.2 Discriminative HypergraphReranking.</S>
			<S sid ="93" ssid = "19">Oracle extraction is a critical component for hypergraph-based discriminative reranking, where millions of model parameters are discriminatively tuned to prefer the oracle-best hypotheses over oth­ ers.</S>
			<S sid ="94" ssid = "20">Hypergraphreranking in MT is similar to the forest-reranking for monolingual parsing (Huang, 2008).</S>
			<S sid ="95" ssid = "21">Moreover, once the oracle-best hypothesis is identified, discriminative models may be trained on hypergraphs in the same way as on n-best lists (cf e.g. Li and Khudanpur (2008b)).</S>
			<S sid ="96" ssid = "22">The results in Table 4 demonstrate that hypergraphreranking with a discriminative LM or TM improves upon the base­ line models on all three test sets.</S>
			<S sid ="97" ssid = "23">Jointly training both the LM and TM likely suffers from over-fitting.</S>
			<S sid ="98" ssid = "24">I Test Set I MT&apos;04 I MT&apos;05 I MT&apos;06 I I Baseline 1 35.7 1 32.6 1 28.3 1 D is cr i m . L M D is cr i m . T M D is cr i m . T M + L M 3 5 . 9 3 6 . 1 3 6 . 0 3 3 . 0 3 3 . 2 3 3 . 1 2 8 . 2 2 8 . 7 2 8 . 6 Table 4: BLEU scores after discriminative hypergraph­ reranking.</S>
			<S sid ="99" ssid = "25">Only the language model (LM) or the transla­ tion model (TM) or both (LM+TM) may be discrimina­ tively trained to prefer the oracle-best hypotheses.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "4">
			<S sid ="100" ssid = "1">We have presented an efficient algorithm to extract the oracle-best translation hypothesis from a hyper­ graph.</S>
			<S sid ="101" ssid = "2">To this end, we introduced a novel technique for equivalent oracle state maintenance, which sig­ nificantly speeds up the oracle extraction process.</S>
			<S sid ="102" ssid = "3">Our algorithm has clear applications in diverse tasks such as discriminative training, system combination and multi-source translation.</S>
	</SECTION>
</PAPER>
