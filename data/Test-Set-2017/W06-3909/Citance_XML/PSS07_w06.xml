<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">There has been recent research in open-ended information extraction from text that finds relational triples of the form (arg1, relation phrase, arg2), where the relation phrase is a text string that expresses a relation between two arbitrary noun phrases.</S>
		<S sid ="2" ssid = "2">While such a relational triple is a good first step, much further work is required to turn such a textual relation into a logical form that supports inferencing.</S>
		<S sid ="3" ssid = "3">The strings from arg1 and arg2 must be normalized, disambiguated, and mapped to a formal taxonomy.</S>
		<S sid ="4" ssid = "4">The relation phrase must likewise be normalized and mapped to a clearly defined logical relation.</S>
		<S sid ="5" ssid = "5">Some relation phrases can be mapped to a set of predefined relations such as Part-0f and Causes.</S>
		<S sid ="6" ssid = "6">We focus instead on arbitrary relation phrases that are discovered from text.</S>
		<S sid ="7" ssid = "7">For this, we need to automatically merge synonymous relations and discover meta-properties such as entail- ment.</S>
		<S sid ="8" ssid = "8">Ultimately, we want the coverage of a bottom-up approach together with the rich set of axioms associated with a top-down approach.</S>
		<S sid ="9" ssid = "9">We have begun exploratory work in “ontologizing” the output of TextRunner, an open information extraction system that finds arbitrary relational triples from text.</S>
		<S sid ="10" ssid = "10">Our test domain is 2.5 million Web pages on health and nutrition, which yields relational triples such as (orange, contains, vitamin C) and (fruits, are rich in, antioxidants).</S>
		<S sid ="11" ssid = "11">We automatically disambiguate the strings arg1 and arg2, mapping them to WordNet synsets.</S>
		<S sid ="12" ssid = "12">We also learn entailments between normalized relation strings (e.g. “be rich in” entails “contain”).</S>
		<S sid ="13" ssid = "13">This enhanced ontology enables reasoning about relationships that are not seen in the corpus, but can be inferred by inheritance and entailment.</S>
		<S sid ="14" ssid = "14">Further, we define ontology-based relationships between the extracted triples themselves, and experimentally show that these can be used in significantly improving probability estimation for the triples.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="15" ssid = "15">A dream from the early days of Artificial Intelligence is to build a system that can read autonomously with deep understanding.</S>
			<S sid ="16" ssid = "16">In the 1970’s and 1980’s natural language processing (NLP) research tackled this problem directly, but in a way that was too brittle to apply to unrestricted text.</S>
			<S sid ="17" ssid = "17">While we may dismiss this body of research as “toy systems” that required carefully selected input text, they genuinely performed deep reasoning about those texts.</S>
			<S sid ="18" ssid = "18">Since the 1990’s the mainstream of NLP research shifted to robust information extraction (IE) systems with shallow reasoning, often statistically based.</S>
			<S sid ="19" ssid = "19">Such IE systems can handle unseen text that the earlier NLP systems could not handle, but have the more modest goal of scanning for a limited number of predefined relations, and ignoring everything else.</S>
			<S sid ="20" ssid = "20">Current IE systems have hardly any mechanisms to make inferences beyond what is explicitly stated in the text.</S>
			<S sid ="21" ssid = "21">However, the time may be ripe to combine the best of both eras, and build robust NLP systems that support deep reasoning.</S>
			<S sid ="22" ssid = "22">Consider textual relations with instances in the form (arg1, relation phrase, arg2) that have text strings for the arguments and the relation phrase.</S>
			<S sid ="23" ssid = "23">The argument strings must be normalized and mapped to a formal taxonomy to enable inheritance-based reasoning.</S>
			<S sid ="24" ssid = "24">Similarly, the relation string must be normalized and mapped to an axiomatized set of relations where possible.</S>
			<S sid ="25" ssid = "25">Learning entailments between relations can enable further inferencing.</S>
			<S sid ="26" ssid = "26">The normalized instances of relations must be converted into probabilistic logical statements with clearly defined semantics.</S>
			<S sid ="27" ssid = "27">Suppose we have an instance (X, relation, Y) where X and Y are mapped to non-leaf nodes of a WordNet-like taxonomy.</S>
			<S sid ="28" ssid = "28">Does this mean that for all instances of X there exists an instance of Y where the relationship holds, or some other combination of universal and existential quantifiers?</S>
			<S sid ="29" ssid = "29">The ontologized instances should be in a format suitable for an inference engine that can make plausible inferences.</S>
			<S sid ="30" ssid = "30">We have taken some early steps in this research direction, but the bulk of these ideas still need to be tested and validated.</S>
			<S sid ="31" ssid = "31">The remainder of this paper is organized as follows.</S>
			<S sid ="32" ssid = "32">Section 2 gives a brief overview of NLP research from the 1970’s up to the present.</S>
			<S sid ="33" ssid = "33">Section 3 presents a case study of the steps needed to transform textual relations into ontologized relations.</S>
			<S sid ="34" ssid = "34">Section 4 gives some preliminary results where we use relationships between the ontologized relations for better confidence estimation of these relations.</S>
			<S sid ="35" ssid = "35">We conclude in Section 5 with some general discussion.</S>
	</SECTION>
	<SECTION title="Trends in NLP Research. " number = "2">
			<S sid ="36" ssid = "1">NLP research has seen extreme swings in goals and methods over the last thirty years.</S>
			<S sid ="37" ssid = "2">Research in the 1970’s and Copyright Qc 2007, Association for the Advancement of Artificial 1980’s lacked robust techniques, but had lofty goals of deep Intelligence (www.aaai.org).</S>
			<S sid ="38" ssid = "3">All rights reserved.</S>
			<S sid ="39" ssid = "4">understanding.</S>
			<S sid ="40" ssid = "5">NLP systems by Roger Schank and his stu dents could read that John went into a restaurant, and infer that John was hungry and had a goal of eating, that he sat at a table, ordered food, ate it, paid, and left the restaurant.</S>
			<S sid ="41" ssid = "6">Good overviews of this work are given in (Schank &amp; Abelson 1977; Dyer 1983; Lehnert 1988).</S>
			<S sid ="42" ssid = "7">These deep understanding systems parsed the text into unambiguous meaning, represented by a network of semantic primitives (Schank &amp; Reisbeck 1982) and a rich knowledge base used to make plausible inferences from this semantic representation.</S>
			<S sid ="43" ssid = "8">Unfortunately, neither the parsing nor the reasoning could scale to the variablility of naturally occurring text.</S>
			<S sid ="44" ssid = "9">NLP research since the early 1990’s has focused primarily on robustness, at the expense of deep reasoning.</S>
			<S sid ="45" ssid = "10">The Message Understanding Conferences (MUC) (Sundheim 1991; Chinchor 1998) spurred the NLP community to create systems that could selectively extract relevant information from previously unseen texts.</S>
			<S sid ="46" ssid = "11">However, hardly any inferencing is done beyond normalizing the information that goes into an output template.</S>
			<S sid ="47" ssid = "12">Statistical and machine learning systems have almost completely replaced the intricate handcrafted knowledge of earlier language understanding systems.</S>
			<S sid ="48" ssid = "13">Some recent IE systems have extracted domain- independent relations, but still narrowly focused on a few relations.</S>
			<S sid ="49" ssid = "14">Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti &amp; Pantel 2006).</S>
			<S sid ="50" ssid = "15">Snow, Jurafsky, and Ng used a hyponym classifier to extract is-a relations from a large collection of newswire and encyclopedia text (Snow, Jurafsky, &amp; Ng 2006).</S>
			<S sid ="51" ssid = "16">Recently, a few systems have been created that do “open IE”.</S>
			<S sid ="52" ssid = "17">This is general purpose information extraction that operates bottom-up from a text corpus, without a predefined notion of what information is relevant.</S>
			<S sid ="53" ssid = "18">Open IE gathers all relational triples it finds, in the form (arg1, relation phrase, arg2), where arg1 and arg2 are noun phrases and the relation phrase expresses a logical relation of some kind between the two arguments.</S>
			<S sid ="54" ssid = "19">The most advanced open IE system is TextRunner from Etzioni’s research group (Banko et al. 2007), which extracts relational triples from text corpora using domain- independent, self-supervised learning.</S>
			<S sid ="55" ssid = "20">TextRunner merges duplicate triples and assigns a redundancy-based probability of correctness to each triple (Downey, Etzioni, &amp; Soder- land 2005).</S>
			<S sid ="56" ssid = "21">Examples of triples found from a corpus of Web pages on nutrition are (tomatoes, are high in, lycopene) and (antioxidant, can help prevent, many forms of cancer).</S>
			<S sid ="57" ssid = "22">Recently some NLP systems are moving toward deep reasoning.</S>
			<S sid ="58" ssid = "23">An impressive example is the HALO project (Fried- land et al. 2004), which combines a common sense knowledge base with knowledge gleaned from chemistry textbooks.</S>
			<S sid ="59" ssid = "24">The chemistry knowledge is input by a combination of automatic NLP and manual knowledge engineering.</S>
			<S sid ="60" ssid = "25">This general purpose and domain-specific knowledge enables an NLP system to read and answer questions from a standardized Advanced Placement test in Chemistry, and perform at One of the three research teams for HALO used the University of Texas KM1 component library (Barker, Porter, &amp; Clark 2001) as their knowledge base.</S>
			<S sid ="61" ssid = "26">This knowledge base contains hundreds of axiomatized concepts, such as the concept Be-Contained, which is defined as a relation between a Tangible-Entity as the object (passive participant in an event) and a Tangible-Entity as the base (relatively fixed thing).</S>
			<S sid ="62" ssid = "27">The KM system can reason about how Be-Contained is related to Spatial-Entities as origin and destination and how Be-Contained is related to the concepts Encloses, Has- Region, Is-Inside, Is-Region-Of, Move-Into, and Move-Out- Of.</S>
			<S sid ="63" ssid = "28">Such fully axiomatized concepts, along with the KM reasoner, provide the capability of deep understanding equal to that of Schank’s semantic primitives.</S>
	</SECTION>
	<SECTION title="Towards Deep Information Extraction. " number = "3">
			<S sid ="64" ssid = "1">In this section, we will discuss the individual steps involved in ontologizing textual relations, and getting them ready for use in an automated reasoning system.</S>
			<S sid ="65" ssid = "2">We will use as our running example, textual relations from TextRunner for a corpus of nutrition Web pages.</S>
			<S sid ="66" ssid = "3">We enumerate the steps below.</S>
			<S sid ="67" ssid = "4">We will discuss many of these in greater detail in later sections.</S>
			<S sid ="68" ssid = "5">1.</S>
			<S sid ="69" ssid = "6">Normalize the argument phrases.</S>
			<S sid ="70" ssid = "7">This involves map-.</S>
			<S sid ="71" ssid = "8">ping the argument strings to the “objects” of some logical domain.</S>
			<S sid ="72" ssid = "9">In this paper, we consider WordNet noun synsets as our objects domain.</S>
			<S sid ="73" ssid = "10">Normalizing arguments thus becomes a problem of word sense disambiguation.</S>
			<S sid ="74" ssid = "11">We need to be able to enhance this domain and add new synsets, if we want to handle unrestricted text.</S>
			<S sid ="75" ssid = "12">We don’t yet cover this case in our implementation.</S>
			<S sid ="76" ssid = "13">Also note that normalization solves the important problem of identity resolution – do two strings si and sj refer to the same real world entity or not?</S>
			<S sid ="77" ssid = "14">2.</S>
			<S sid ="78" ssid = "15">Normalize the relation phrase.</S>
			<S sid ="79" ssid = "16">This involves normal-.</S>
			<S sid ="80" ssid = "17">izing to drop unimportant differences between phrases, such as adverbs or verb morphological form.</S>
			<S sid ="81" ssid = "18">It may also involve mapping predicate phrases to a predefined set of concepts.</S>
			<S sid ="82" ssid = "19">3.</S>
			<S sid ="83" ssid = "20">Formalize the semantics.</S>
			<S sid ="84" ssid = "21">Ontologized relations must be.</S>
			<S sid ="85" ssid = "22">given precise logical semantics before automated reasoning can be applied.</S>
			<S sid ="86" ssid = "23">Does (x, rel, y) mean that for all hyponyms of x the relation holds for all hyponyms of y, or some other combination of the two quantifiers?</S>
	</SECTION>
	<SECTION title="Learn meta-properties of relation phrases. Are two re-. " number = "4">
			<S sid ="87" ssid = "1">lation phrases synonymous, or does relation r1 existing between x and y imply that relation r2 also holds between them (entailment)?</S>
			<S sid ="88" ssid = "2">Is a relation transitive with respect to another relation?</S>
	</SECTION>
	<SECTION title="Assign probability of correctness.  Automatically ex-. " number = "5">
			<S sid ="89" ssid = "1">tracted relations will inevitably include errors.</S>
			<S sid ="90" ssid = "2">Can we use relationships between the ontologized relations to better estimate their probabilities.</S>
			<S sid ="91" ssid = "3">We later define such relationships, using the hyponymy and siblinghood relationships that we have between objects (available from Word a level close to that of typical high school students who take that test.</S>
			<S sid ="92" ssid = "4">1 http://ww w.cs.utexa s.edu/users /mfkb/RK F/clib.html Net), and the entailment relationships that we have derived between relation phrases.</S>
	</SECTION>
	<SECTION title="Inference engine.  The inference engine should be able. " number = "6">
			<S sid ="93" ssid = "1">to combine the relations that were derived from text with other relations and axioms in a knowledge base.</S>
			<S sid ="94" ssid = "2">3.1 Case Study: Fruit and Antioxidants.</S>
			<S sid ="95" ssid = "3">We first present some examples of TextRunner relations to better motivate the issues involved in executing the steps we listed above.</S>
			<S sid ="96" ssid = "4">Figure 1 shows three such relations.</S>
			<S sid ="97" ssid = "5">These illustrate textual relations that we want to ontologize.</S>
			<S sid ="98" ssid = "6">Some normalization has been done to the original source text phrases: modifiers and adverbs have been dropped, and nouns and verbs are transformed into their morphological base.</S>
			<S sid ="99" ssid = "7">This normalization, however, is not sufficient to enable machine reasoning about the relations.</S>
			<S sid ="100" ssid = "8">1.</S>
			<S sid ="101" ssid = "9">(orange, contain, vitamin C) “One orange contains all the vitamin C your body needs for the day!” 2.</S>
			<S sid ="102" ssid = "10">(fruit, be rich in, antioxidant) “Whole grains, fruits, and vegetables are also rich in antioxidants, such as vitamins C and E, and beta-carotene.” 3.</S>
			<S sid ="103" ssid = "11">(green bean, should be alert to, problem) “Growers who rotate with soybeans or green beans should be alert to potential problems with fields infested with soybean cyst nematodes.” Figure 1: Examples of textual relations extracted from a corpus of Web pages on nutrition, along with a sentence from which the relation was extracted.</S>
			<S sid ="104" ssid = "12">While the first two of these textual relations seem perfectly reasonable, they have limited utility for an automated reasoner.</S>
			<S sid ="105" ssid = "13">The first two relations have no terms in common and, thus, appear to be totally unrelated strings.</S>
			<S sid ="106" ssid = "14">A language understanding system should be able to reason that oranges are a type of fruit, that vitamin C is a type of antioxidant, and that being “rich in” x can imply containing x. None of this reasoning is possible at the level of text phrases.</S>
			<S sid ="107" ssid = "15">While the first two relations have interrelated meaning, and thus corroborate each other, the third relation can be immediately labeled as false by a human reader.</S>
			<S sid ="108" ssid = "16">Green beans or any other vegetable having cognition runs counter to any other true assertions in a human reader’s knowledge, and thus can be assigned low belief.</S>
			<S sid ="109" ssid = "17">3.2 Normalizing Argument Noun Phrases.</S>
			<S sid ="110" ssid = "18">As mentioned above, we normalize the argument noun phrases of a relation by mapping them to WordNet noun synsets (Harabagiu, Miller, &amp; Moldovan 1999).</S>
			<S sid ="111" ssid = "19">Thus, we are dealing with a word sense disambiguation (WSD) problem, since most terms in WordNet occur in multiple synsets.</S>
			<S sid ="112" ssid = "20">For example “orange” could be a fruit, a color, a tree, a pigment, or a river in South Africa.</S>
			<S sid ="113" ssid = "21">The fruit sense is what we want in the context of example 1 in Figure 1.</S>
			<S sid ="114" ssid = "22">We now describe the technique we use for WSD.</S>
			<S sid ="115" ssid = "23">To disambiguate x in (x, rel, y), we consider all the sentences S1, . . .</S>
			<S sid ="116" ssid = "24">, Sk from which this relation was extracted.</S>
			<S sid ="117" ssid = "25">Using these, and a window size of 4 words on each side of x, we create a set C of context words for x. Words with similar meaning tend to be distributionally similar, and have sim ilar PMI (pointwise mutual information) values with other words.</S>
			<S sid ="118" ssid = "26">Thus these occurrences of x will on average have greater similarity with words similar to the correct sense compared to those similar to incorrect senses.</S>
			<S sid ="119" ssid = "27">For each sense, we compute the average similarity of x with all the WordNet synonyms, siblings and direct hyponyms of that sense.</S>
			<S sid ="120" ssid = "28">We measure similarity according to the metric proposed in (Dagan, Marcus, &amp; Markovitch 1993).</S>
			<S sid ="121" ssid = "29">It computes similarity between corresponding vectors of PMI values.</S>
			<S sid ="122" ssid = "30">The set of words forming the PMI vector space is just the set C and not all the words in the corpus (this is important).</S>
			<S sid ="123" ssid = "31">We assign the sense for which this average similarity is highest.</S>
			<S sid ="124" ssid = "32">In the experiments section, we report the results we got with this method.</S>
			<S sid ="125" ssid = "33">A formal taxonomy enables a system to make inferences that cannot be made otherwise.</S>
			<S sid ="126" ssid = "34">If (orange, contain, vitamin C), we can infer from inheritance that (orange, contain, vitamin) and that (orange, contain, antioxidant), since vitamin C is a subclass of vitamin and also a subclass of antioxidant.</S>
			<S sid ="127" ssid = "35">Similarly, (orange, contain, vitamin C) is a specialization of (citrus fruit, contain, vitamin C).</S>
			<S sid ="128" ssid = "36">A general-purpose taxonomy like WordNet is likely to have gaps for some domain concepts – a taxonomy may have “fat” but not its hyponym “unsaturated fat”.</S>
			<S sid ="129" ssid = "37">(Snow, Jurafsky, &amp; Ng 2006) have shown that adding terms to a taxonomy can be done with high precision, given a good classifier for expressions that denote is-a relations.</S>
			<S sid ="130" ssid = "38">Augmenting WordNet on the fly while normalizing relations is in our plan for future work.</S>
			<S sid ="131" ssid = "39">Proper names are another case where a taxonomy is unlikely to help.</S>
			<S sid ="132" ssid = "40">There has been considerable research re cently in named entity recognition (Collins &amp; Singer 1999; McCallum &amp; Li 2003) and in identity resolution across alternate forms of the same name (Dong, Halevey, &amp; Madhavan 2005; Kanani, McCallum, &amp; Pal 2007).</S>
			<S sid ="133" ssid = "41">We would also like to leverage these in future work.</S>
			<S sid ="134" ssid = "42">3.3 Normalizing Relation Phrases.</S>
			<S sid ="135" ssid = "43">We first normalize the relation phrase by dropping adverbs and reducing verbs to a morphological base form.</S>
			<S sid ="136" ssid = "44">Thus, “are also rich in” is normalized to “be rich in”.</S>
			<S sid ="137" ssid = "45">We also learn entailments between relation phrases – (X, be rich in, Y) implies that (X, contains, Y) is also true with high probability, as described in Section 3.5.</S>
			<S sid ="138" ssid = "46">This allows inheritance between relations that goes beyond what would be provided by a verb taxonomy.</S>
			<S sid ="139" ssid = "47">Our future plans are to tie the normalized relation phrases to formal concepts such as Be-Contained from the KM component library that was described in Section 2.3.</S>
			<S sid ="140" ssid = "48">Each of these formal concepts is fully axiomatized to enable a rich set of inferences.</S>
			<S sid ="141" ssid = "49">KM currently has a few hundred of such general purpose concepts, and ultimately may need several thousand axiomatized concepts for adequate coverage of free text.</S>
			<S sid ="142" ssid = "50">A scalable approach to mapping relation phrases to a set of formal concepts is to associate each formal concept with a small set of the most common terms used to refer to the concept.</S>
			<S sid ="143" ssid = "51">This is already done for each of the manually engineered KM concepts.</S>
			<S sid ="144" ssid = "52">The set of relations that map to a given concept is then expanded by learning entailments.</S>
			<S sid ="145" ssid = "53">If (X, contain, Y) is manually mapped to Be-Contained, and (X, be rich in, Y) entails (X, contain, Y), then “be rich in” is implicitly mapped to Be-Contained as well.</S>
			<S sid ="146" ssid = "54">3.4 Quantification.</S>
			<S sid ="147" ssid = "55">Human language is typically underspecified about scoping and quantification.</S>
			<S sid ="148" ssid = "56">Automated reasoning systems that are based on first order logic require a clear resolution to scoping and quantification.</S>
			<S sid ="149" ssid = "57">Does example 2 in Figure 1 mean that all instances of fruit are rich in all instances of antioxidants (interpretation A).</S>
			<S sid ="150" ssid = "58">Or does it mean that at all instances of fruit are rich in at least one instance of antioxidant (B)?</S>
			<S sid ="151" ssid = "59">Or does it mean that at least one instance of fruit is rich in all antioxidants (C) or that at least one fruit is rich in at least one antioxidant(D)?</S>
			<S sid ="152" ssid = "60">A. ∀x ∈ fruit ∀y ∈ antioxidant, rich in(x, y) B. ∀x ∈ fruit ∃y ∈ antioxidant, rich in(x, y) C. ∃x ∈ fruit ∀y ∈ antioxidant, rich in(x, y) D. ∃x ∈ fruit ∃y ∈ antioxidant, rich in(x, y) Figure 2: Possible semantics of the textual relation (fruit, rich in, antioxidant).</S>
			<S sid ="153" ssid = "61">The most likely interpretation is a probabilistic version of B: most fruits contain at least one antioxidant.</S>
			<S sid ="154" ssid = "62">Interpretations A and C contradict domain knowledge about nutrition, and interpretation D is a trivially weak version of B. Example 1 with its generic use of “the orange”, is an even knottier problem for determining quantification automatically.</S>
			<S sid ="155" ssid = "63">Even with domain knowledge, determining correct quantification from natural language is a difficult problem.</S>
			<S sid ="156" ssid = "64">We choose option B as the default quantification.</S>
			<S sid ="157" ssid = "65">This enables inference of relations where arg1 is replaced with a hyponym (specialization).</S>
			<S sid ="158" ssid = "66">If the system believes that (citrus fruit, contain, vitamin C), it can infer with equal certainty that (grapefruit, contain, vitamin C) and (pink grapefruit, contain, vitamin C).</S>
			<S sid ="159" ssid = "67">Arg2 may similarly be replaced with a hypernym (generalized class).</S>
			<S sid ="160" ssid = "68">If (citrus fruit, contain, vitamin C) then (citrus fruit, contain, antioxidant) and, somewhat vacuously, (citrus fruit, contain, substance).</S>
			<S sid ="161" ssid = "69">3.5 Learning Meta-properties of Relation Phrases.</S>
			<S sid ="162" ssid = "70">Our goal is to automatically extract relational triples spanning a large set of relation phrases, which precludes manually engineering axioms for these relations.</S>
			<S sid ="163" ssid = "71">Instead, we can learn meta-properties of relation phrases, such as entail- ment, transitivity, and transitivity-through.</S>
			<S sid ="164" ssid = "72">Relation r entails another relation r’ if an instance (x, r, y) implies that (x, r’, y) is also true.</S>
			<S sid ="165" ssid = "73">For example, in the domain of nutrition, (x, be rich in, y) entails (x, contain, y).</S>
			<S sid ="166" ssid = "74">Transitivity holds for a relation r if (a, r, b) and (b, r, c) imply that (a, r, c) is also true.</S>
			<S sid ="167" ssid = "75">Another meta-property that can be learned is transitive-through.</S>
			<S sid ="168" ssid = "76">Relation r’ is transitive through relation r if whenever (a, r, b) and (b, r’, c) are true, then so is (a, r’, c).</S>
			<S sid ="169" ssid = "77">For example, if (tomato, contain, lycopene) and (lycopene, help prevent, cancer) , then (tomato, help prevent, cancer) We now describe how we compute entailments in our implementation.</S>
			<S sid ="170" ssid = "78">For any relation r, we say the ordered pair (x, y) is an instance of r if (x, r, y) has been extracted from the corpus.</S>
			<S sid ="171" ssid = "79">Given relations r1, r2 , let their respective in stance sets be I1 , I2.</S>
			<S sid ="172" ssid = "80">If r1 ⇒ r2 holds, then we should have I1 ⊆ I2.</S>
			<S sid ="173" ssid = "81">Due to data sparseness, we don’t expect this containment to actually hold.</S>
			<S sid ="174" ssid = "82">However, we would expect a measure like |I1 ∩ I2 |/|I1| to be a good score to give to this entailment.</S>
			<S sid ="175" ssid = "83">In our experiments, this did not turn out to work well.</S>
			<S sid ="176" ssid = "84">The distribution we had for instance set sizes for relations was very skewed.</S>
			<S sid ="177" ssid = "85">It varied from 6 to more than 10, 000.</S>
			<S sid ="178" ssid = "86">If I2 is orders of magnitude larger than I1 then this score can be high just by chance overlap.</S>
			<S sid ="179" ssid = "87">We need to con sider |I2 | too in assigning a score.</S>
			<S sid ="180" ssid = "88">We experimented with several metrics, and the one that was found to work best was |I1 ∩ I2 |/|I1| log|I2 |.</S>
			<S sid ="181" ssid = "89">We chose a threshold 0.01 for this score, and chose all entailments with score greater than this value.</S>
			<S sid ="182" ssid = "90">3.6 Assigning Probabilities.</S>
			<S sid ="183" ssid = "91">A set of relations that have been extracted automatically from text are bound to contain some errors.</S>
			<S sid ="184" ssid = "92">Probability estimation for extracted relations is an important component of information extraction systems.</S>
			<S sid ="185" ssid = "93">We consider three types of evidence for such probability estimates.</S>
			<S sid ="186" ssid = "94">1.</S>
			<S sid ="187" ssid = "95">Language Based Features: These are features derived.</S>
			<S sid ="188" ssid = "96">from the output of POS taggers and deep parsers on the corpus text.</S>
			<S sid ="189" ssid = "97">These include the POS tags themselves, their bigrams and trigrams, the number of words separating the argument noun phrases, features derived from paths connecting them in the parse tree, and so on (Kambhatla 2004).</S>
			<S sid ="190" ssid = "98">State of the art relation extraction systems are mostly based on these features.</S>
			<S sid ="191" ssid = "99">2.</S>
			<S sid ="192" ssid = "100">Redundancy: The more frequently an instance has been.</S>
			<S sid ="193" ssid = "101">extracted from a large corpus, the more likely it is to be correct (Downey, Etzioni, &amp; Soderland 2005).</S>
			<S sid ="194" ssid = "102">When redundancy is present, this has been shown to be a very effective source of evidence.</S>
			<S sid ="195" ssid = "103">However, an open IE system like TextRunner is bound to give a lot of good extractions with low counts, in addition to good extractions with high counts.</S>
			<S sid ="196" ssid = "104">3.</S>
			<S sid ="197" ssid = "105">Ontological Corroboration: This is a new source of ev-.</S>
			<S sid ="198" ssid = "106">idence that we introduce.</S>
			<S sid ="199" ssid = "107">It becomes available from our ontologizing the relations.</S>
			<S sid ="200" ssid = "108">It is derived from ontology- based relationships between the extracted relations themselves.</S>
			<S sid ="201" ssid = "109">We describe these next.</S>
			<S sid ="202" ssid = "110">We define the following relationships between ontologized relations.</S>
			<S sid ="203" ssid = "111">Acc Precision Recall F Naive Bayes – BFS Naive Bayes – OFS Log Reg – BFS Log Reg – OFS SVM – BFS SVM – OFS 69% 74% 72% 75% 73% 75% 0.57 0.65 0.67 0.70 0.70 0.71 0.61 0.61 0.46 0.54 0.43 0.50 0.59 0.63 0.54 0.61 0.53 0.59 Figure 3: Illustration of relationships between ontologized triples 1.</S>
			<S sid ="204" ssid = "112">Hyponymy: We define a relation (x′ , r′ , y′) to be a hy-.</S>
			<S sid ="205" ssid = "113">ponym of (x, r, y) if x′ is a hyponym or synonym of x, y′ is a hyponym or synonym of y, and relation phrase r′ entails r. For example, in Figure 3, (tomato, rich in, lycopene) can be seen to be a hyponym of (vegetable, contains, antioxidant).</S>
			<S sid ="206" ssid = "114">2.</S>
			<S sid ="207" ssid = "115">Hypernymy: We define hypernymy to just be the inverse.</S>
			<S sid ="208" ssid = "116">relationship of hyponymy.</S>
			<S sid ="209" ssid = "117">3.</S>
			<S sid ="210" ssid = "118">Siblinghood: We define a relation (x′ , r′ , y′) to be a sib-.</S>
			<S sid ="211" ssid = "119">ling of (x, r, y) if x′ is a sibling or synonym of x, y′ is a sibling or synonym of y, and r and r′ are related by an entailment in either direction.</S>
			<S sid ="212" ssid = "120">For example, in Figure 3, (tomato, rich in, lycopene) can be seen to be a sibling of (carrot, loaded with, beta-carotene).</S>
			<S sid ="213" ssid = "121">It is easy to see why, if two relations are related by hyponymy, this provides additional evidence that they are both true.</S>
			<S sid ="214" ssid = "122">The hyponym relation can be thought of as an “instance” of the hypernym relation.</S>
			<S sid ="215" ssid = "123">We discussed this earlier when we talked about the semantics of relations between non-leaf nodes in the ontology.</S>
			<S sid ="216" ssid = "124">We claim that two relations related by siblinghood present a similar scenario.</S>
			<S sid ="217" ssid = "125">Relation phrases, like predicates in logic, tend to have fixed argument classes.</S>
			<S sid ="218" ssid = "126">Siblinghood between relations is evidence for particular argument classes for the involved relation phrases.</S>
			<S sid ="219" ssid = "127">4.</S>
			<S sid ="220" ssid = "128">Preliminary Results.</S>
			<S sid ="221" ssid = "129">Our dataset consisted of 272, 960 relations extracted from a corpus of web pages on health and nutrition.</S>
			<S sid ="222" ssid = "130">These spanned around 27, 000 WordNet synsets, and 7, 000 distinct relation phrases.</S>
			<S sid ="223" ssid = "131">Our entailment threshold of 0.01 gave us 20, 550 entailments.</S>
			<S sid ="224" ssid = "132">Based on a sample of 60 drawn from these, we obtained a precision of 0.67.</S>
			<S sid ="225" ssid = "133">For word sense disambiguation, we tagged a sample of 125 relations.</S>
			<S sid ="226" ssid = "134">70% of these were found to have correct senses assigned to both their arguments.</S>
			<S sid ="227" ssid = "135">The baseline WSD that always selected the first WordNet synset had accuracy 55%.</S>
			<S sid ="228" ssid = "136">The baseline was pretty good, presumably because WordNet numbers senses according to decreasing frequency of usage.</S>
			<S sid ="229" ssid = "137">We now turn to probability estimation for these relations.</S>
			<S sid ="230" ssid = "138">We trained classifiers to predict labels (true or false) for them, and assign them truth probabilities.</S>
			<S sid ="231" ssid = "139">The Baseline Feature Set (BFS) consists of POS tags, bigrams of POS tags, size of the relation phrase, and the extraction frequency of the relation.</S>
			<S sid ="232" ssid = "140">We used feature selection methods to choose Table 1: Classifier performance comparison between the two feature sets eight features from the large candidate set of POS tags and their bigrams.</S>
			<S sid ="233" ssid = "141">The Ontology Feature Set (OFS) uses all the features in BFS.</S>
			<S sid ="234" ssid = "142">In addition, it uses three additional features derived from the kind of ontological evidence that we discussed earlier.</S>
			<S sid ="235" ssid = "143">For each relation, we use the counts for the number of hyponyms, hypernyms, and siblings that the relation has in the entire set of 272, 960 relations, as features.</S>
			<S sid ="236" ssid = "144">We expect nonzero values for these features to provide additional evidence that the relation is true.</S>
			<S sid ="237" ssid = "145">We first evaluate the accuracy and F-measure performance.</S>
			<S sid ="238" ssid = "146">We tried three classifiers – Naive Bayes, Logistic Regression, and SVMs with a RBF kernel using (Frank et al. 2005).</S>
			<S sid ="239" ssid = "147">We used a set of 355 tagged relations, and performed 5-fold cross-validation.</S>
			<S sid ="240" ssid = "148">The set had 127 correct relations, giving an overall precision of 0.36.</S>
			<S sid ="241" ssid = "149">The results we obtained are shown in Table 1.</S>
			<S sid ="242" ssid = "150">Evaluating the quality of the assigned probabilities is of greater interest to us.</S>
			<S sid ="243" ssid = "151">We will assess this by using the probabilities as a ranking function on the relations, and determining the precision – recall tradeoff that occurs as we descend this ranked list.</S>
			<S sid ="244" ssid = "152">We determine precision and recall values for the top k relations for a range of k values.</S>
			<S sid ="245" ssid = "153">We determine precision by tagging a random sample of size 50.</S>
			<S sid ="246" ssid = "154">For recall, we had a common, fixed set of 100 correct relations, randomly chosen from all relations.</S>
			<S sid ="247" ssid = "155">The fraction of this set contained in the top k relations gave the corresponding recall value.</S>
			<S sid ="248" ssid = "156">Figure 4 compares the precision–recall curves given by Logistic Regression and Naive Bayes when using the two feature sets.</S>
			<S sid ="249" ssid = "157">We see significant improvements when using the OFS feature set compared to BFS.</S>
			<S sid ="250" ssid = "158">The results seems to indicate that while these ontological features are not active for all correct relations, when they are active they are very accurate evidence for the relation being true.</S>
			<S sid ="251" ssid = "159">5.</S>
			<S sid ="252" ssid = "160">Discussion.</S>
			<S sid ="253" ssid = "161">We have outlined the basic steps needed to “ontologize” instances of relations of the form (arg1, relation phrase, arg2).</S>
			<S sid ="254" ssid = "162">This maps the argument strings to a formal taxonomy, adding new concepts to the taxonomy as required, normalizes the relation phrases, and learns meta-relations that form semantic links between relations.</S>
			<S sid ="255" ssid = "163">This enables a reasoner to draw plausible inferences beyond what is stated directly in the original text.</S>
			<S sid ="256" ssid = "164">We have presented experimental results for several of these steps.</S>
			<S sid ="257" ssid = "165">We have shown that current technology is sufficient to disambiguate noun phrases, to normalize relation phrases, and to learn entailments between relations.</S>
			<S sid ="258" ssid = "166">We also 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 L R O F S L R B F S 0 0.2 0.4 0.6 0.8 1 R e c a l l N B O F S N B B F S 0 0.2 0.4 0.6 0.8 1 R e c a l l Named Entity Classifica tion.</S>
			<S sid ="259" ssid = "167">In Procs.</S>
			<S sid ="260" ssid = "168">of the Joint SIGDAT Conferen ce on Empirical Methods in Natural Language Processin g and Very Large Corpora, 100–111.</S>
			<S sid ="261" ssid = "169">Dagan, I.; Marcus, S.; and Markovit ch, S. 1993.</S>
			<S sid ="262" ssid = "170">Contextual word similarity and estimatio n from sparse data.</S>
			<S sid ="263" ssid = "171">In Proceedin gs of the 31st ACL, 164–171.</S>
			<S sid ="264" ssid = "172">Dong, X.; Halevey, A. Y.; and Madhava n, J. 2005.</S>
			<S sid ="265" ssid = "173">Reference reconcilia tion in complex informati on spaces.</S>
			<S sid ="266" ssid = "174">In Procs.</S>
			<S sid ="267" ssid = "175">of SIGMOD Conferen ce.</S>
			<S sid ="268" ssid = "176">Downey, D.; Etzioni, O.; and Soderland , S. 2005.</S>
			<S sid ="269" ssid = "177">A Probabilistic Model of Redundan cy in Informati on Extractio n. In Procs.</S>
			<S sid ="270" ssid = "178">of IJCAI.</S>
			<S sid ="271" ssid = "179">Dyer, M. 1983.</S>
			<S sid ="272" ssid = "180">In Depth Understa nding.</S>
			<S sid ="273" ssid = "181">MIT Press.</S>
			<S sid ="274" ssid = "182">Frank, E.; Hall, M. A.; Holmes, G.; Kirkby, R.; and Pfahringe r, B. 2005.</S>
			<S sid ="275" ssid = "183">Weka - a machine learning work bench for data mining.</S>
			<S sid ="276" ssid = "184">In The Data Mining and Knowledg e D i s c o v e r y H a n d b o o k . 1 3 0 5 – 1 3 1 4 . Friedland , N. S.; Allen, P. G.; Matthews , G.; Witbrock, M.; Baxter, D.; Curtis, J.; Shepard, B.; P, M.; Angele, J.; Moench, E.; Opperma nn, H.; Wenke, D.; Israel, D.; Chaudhri, V.; Porter, B.; Barker, K.; Fan, J.; Chaw, S. Y.; Yeh, P.; Tecuci, D.; and Clark, P. 2004.</S>
			<S sid ="277" ssid = "185">Project Halo: towards a digital aristotle.</S>
			<S sid ="278" ssid = "186">AI Magazine . Figure 4: Using ontological feature set (OFS) for a Logistic Regression (LR) or Naive Bayes (NB) classifier gives a large boost to the precision-recall compared with a baseline feature set (BFS).</S>
			<S sid ="279" ssid = "187">demonstrated that the resulting ontology can assists in information extraction by validating extractions that are corroborated by the ontology.</S>
			<S sid ="280" ssid = "188">In preliminary experiments the recall- precision curve was raised considerably when we incorporated ontology-based features in estimating relation probabilities.</S>
			<S sid ="281" ssid = "189">The high precision end of the curve was raised from precision 0.51 to precision 0.76, and area under the curve was increased by 32%.</S>
			<S sid ="282" ssid = "190">Much work remains to move from extraction of textual relations to formal ontologies that support automatic reasoning.</S>
			<S sid ="283" ssid = "191">While deep reasoning at a human level is beyond our reach in the near future, a first pass at each of the steps outlined in this paper can be accomplished with current techniques in natural language processing and knowledge representation.</S>
	</SECTION>
</PAPER>
