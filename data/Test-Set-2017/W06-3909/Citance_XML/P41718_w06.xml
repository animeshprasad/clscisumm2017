<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents a crowdsourcing project on the creation of a publicly available corpus of sentential paraphrases for Russian.</S>
		<S sid ="2" ssid = "2">Collected from the news headlines, such corpus could be applied for information extraction and text summarization.</S>
		<S sid ="3" ssid = "3">We collect news headlines from diﬀerent agencies in real- time; paraphrase candidates are extracted from the headlines using an unsuper‐ vised matrix similarity metric.</S>
		<S sid ="4" ssid = "4">We provide user-friendly online interface for crowdsourced annotation which is available at paraphraser.ru.</S>
		<S sid ="5" ssid = "5">There are 5181 annotated sentence pairs at the moment, with 4758 of them included in the corpus.</S>
		<S sid ="6" ssid = "6">The annotation process is going on and the current version of the corpus is freely available at http://paraphraser.ru.</S>
		<S sid ="7" ssid = "7">Keywords: Russian paraphrase corpus · Lexical similarity metric · Unsupervised paraphrase extraction · Crowdsourcing</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">Our aim is to create a publicly available Russian paraphrase corpus which could be applied for information extraction (IE), text summarization (TS) and compression.</S>
			<S sid ="9" ssid = "9">We believe that such corpus can be helpful for paraphrase identiﬁcation and generation for Russian and that is why we focus on the sentential paraphrases.</S>
			<S sid ="10" ssid = "10">Indeed, a sentential corpus does not impose any speciﬁc methods of further paraphrase identiﬁcation or generation on the researcher.</S>
			<S sid ="11" ssid = "11">If such corpus is representative enough, it can serve as a dataset for the experiments on the extraction of word-, phrase- and syntactic level para‐ phrases.</S>
			<S sid ="12" ssid = "12">Paraphrase is restatement of a text: it conveys the same meaning in another form.</S>
			<S sid ="13" ssid = "13">Such natural language processing (NLP) tasks as paraphrase identiﬁcation and genera‐ tion have been shown to be helpful for IE [25], question answering [14], machine trans‐ lation [7], TS [19], text simpliﬁcation [29], etc. Paraphrase identiﬁcation is used to detect plagiarism [6] and to remove redundancies in TS [19] and IE [25], while paraphrase generation – to expand queries in information retrieval and question answering [14] and patterns – in IE.</S>
			<S sid ="14" ssid = "14">Paraphrase generation is also useful for text normalization [28] and textual entailment recognition tasks [9].</S>
			<S sid ="15" ssid = "15">As far as the deﬁnition of paraphrase is concerned, it generally implies that the same message is expressed in diﬀerent words, but it does not prescribe which portion of text is replaced by paraphrasing.</S>
			<S sid ="16" ssid = "16">Neither does it state whether common knowledge can be © Springer International Publishing Switzerland 2016 P. Braslavski et al.</S>
			<S sid ="17" ssid = "17">(Eds.): RuSSIR 2015, CCIS 573, pp.</S>
			<S sid ="18" ssid = "18">146–157, 2016.</S>
			<S sid ="19" ssid = "19">DOI: 10.1007/9783-31941718-9_8 used when judging on the similarity of the two messages.</S>
			<S sid ="20" ssid = "20">As a consequence of this ambiguity, some researchers believe that paraphrases should have absolute semantic equivalence while others allow for bidirectional textual entailment, when the two messages convey roughly the same meaning.</S>
			<S sid ="21" ssid = "21">Let us consider an example from our corpus: (1) BTБ мoжeт пpoдaть дoлю в Tele2 в ближaйшиe нeдeли.</S>
			<S sid ="22" ssid = "22">/VTB might sell its shares in TELE2 in the nearest weeks/ (2) BTБ aнoнcиpoвaл пpoдaжy Tele2.</S>
			<S sid ="23" ssid = "23">/VTB announced the sale of TELE2/ Although it is clear that the two sentences describe the same event, the ﬁrst one has additional details: indication of the time and the fact that the shares are going to be sold.</S>
			<S sid ="24" ssid = "24">A human judger, with his/her knowledge about the world, might consider these sentences paraphrases.</S>
			<S sid ="25" ssid = "25">But if we intend to teach a machine to identify semantically equivalent paraphrases, a threshold for paraphrases should be higher.</S>
			<S sid ="26" ssid = "26">On the other hand, the second sentence can be considered a summarization of the ﬁrst one, and therefore such types of paraphrases can be used in automatic TS.</S>
			<S sid ="27" ssid = "27">In our research, we intend to construct paraphrase corpus for IE and TS.</S>
			<S sid ="28" ssid = "28">We believe that the former task requires semantically equivalent, or precise paraphrases while the latter one demands roughly similar ones (so-called loose paraphrases) like those in our example.</S>
			<S sid ="29" ssid = "29">Thus, it is important for us to distinguish precise paraphrases (PP) and loose paraphrases (LP) while constructing our paraphrase corpus.</S>
			<S sid ="30" ssid = "30">Today there are already a number of available paraphrase resources, Microsoft Para‐ phrase Corpus being the most well-known of them [13].</S>
			<S sid ="31" ssid = "31">A wide number of metrics for paraphrase identiﬁcation (for English) are evaluated against this corpus.</S>
			<S sid ="32" ssid = "32">For Russian there are no publicly available paraphrase resources known to us, with the only exception of the dataset published by Ganitkevich et al. as part of The Para‐ phrase Database project [17].</S>
			<S sid ="33" ssid = "33">The latter includes paraphrases on the word-, phrase- and syntactic levels, and each paraphrase pair is annotated with the set of count- and prob‐ ability-based features.</S>
			<S sid ="34" ssid = "34">Such corpus can be used for both IE and TS, but it lacks infor‐ mation on the context of paraphrases.</S>
			<S sid ="35" ssid = "35">We believe that if such context (the original sentences) was provided, it could improve both these NLP tasks.</S>
			<S sid ="36" ssid = "36">That is why we aim at constructing a sentential corpus.</S>
			<S sid ="37" ssid = "37">Thus, our task is to construct a corpus with both PPs and LPs and to make it helpful for paraphrase identiﬁcation and generation in IE, TS and text compression tasks.</S>
			<S sid ="38" ssid = "38">Our research is a part of an ongoing crowdsourcing project available at para‐ phraser.ru, with our current results available at paraphraser.ru/scorer/stat.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="39" ssid = "1">In paraphrase identiﬁcation/generation, unlike many other NLP applications, the data is usually hard to get.</S>
			<S sid ="40" ssid = "2">Paraphrases do not emerge naturally, like users’ clicks or query logs, and gathering them manually is a tedious task.</S>
			<S sid ="41" ssid = "3">Moreover, one usually needs large amounts of data to collect just a few paraphrases.</S>
			<S sid ="42" ssid = "4">Paraphrase corpora can be constructed from “natural” or “artiﬁcial” sources.</S>
			<S sid ="43" ssid = "5">The former usually include parallel multilingual corpora [2] and comparable monolingual corpora [22] (diﬀerent translations of the same texts [11]; news texts/clusters [1, 10, 13, 27]; texts on similar topics, e.g., from the social networks (e.g., Twitter Paraphrase Corpus) [28] or students’ answers to the questions [9]; social media [3], Wikipedia [26]; diﬀerent descriptions of the same videos [8]), etc. “Artiﬁcial” sources are texts para‐ phrased by humans [20, 21, 24].</S>
			<S sid ="44" ssid = "6">Gold standard paraphrases are typically extracted from the candidates set using either experts’ [13, 20] or crowdsourced [1, 6, 8] annotation.</S>
			<S sid ="45" ssid = "7">2-way and 3-way annotation is a common approach, but sometimes a complex system of characteristics is introduced (e.g., in [20] paraphrases are annotated along 10 dimensions of paraphrase characteris‐ tics on a 6 point scale).</S>
			<S sid ="46" ssid = "8">A detailed overview of all the existing paraphrase corpora is beyond the scope of this paper.</S>
			<S sid ="47" ssid = "9">A thorough and insightful review of diﬀerent sentential paraphrase datasets can be found in [21] where the authors present recommendations on paraphrase corpora construction and raise a number of important problems to the community.</S>
			<S sid ="48" ssid = "10">Due to the aim of our research and space limitations we inevitably focus on the well- known Microsoft Research Paraphrase Corpus (MSRP) and on The Paraphrase Data‐ base, the only publicly available resource of Russian paraphrases known to us.</S>
			<S sid ="49" ssid = "11">MSRP is not the oldest paraphrase corpus, but is deﬁnitely the one which greatly inspired research in paraphrase community.</S>
			<S sid ="50" ssid = "12">It was constructed as a broad-domain corpus of sentential paraphrases which would be amenable to statistical machine translation (SMT) techniques [13].</S>
			<S sid ="51" ssid = "13">It consists of 5801 pairs of English sentences collected from news clusters and annotated by 2 experts.</S>
			<S sid ="52" ssid = "14">An initial set of paraphrases is extracted using Levenshtein edit distance.</S>
			<S sid ="53" ssid = "15">The authors only consider ﬁrst 3 sentences of the articles and apply several criteria to their length and lexical distance between the sentences.</S>
			<S sid ="54" ssid = "16">The resulting dataset is extracted using SVM with morphological, lexical, string similarity and composite features.</S>
			<S sid ="55" ssid = "17">Although MSRP is widely used as the gold standard in the experiments on paraphrase extraction methods, it is often criticized by researchers for its loose deﬁnition of para‐ phrase, for its 2-way annotation, high lexical overlap, etc. While constructing a Russian corpus, we try to solve the problem of paraphrase ambiguity by distinguishing 2 types of paraphrases: precise and loose ones.</S>
			<S sid ="56" ssid = "18">We have 3- way annotation: precise paraphrases, loose paraphrases and non-paraphrases.</S>
			<S sid ="57" ssid = "19">As for the lexical overlap problem, we consider this overlap acceptable and even helpful in our case.</S>
			<S sid ="58" ssid = "20">Russian is a language with free word order, and pairs of sentences which consist of the same words put in diﬀerent order could be used for learning syntactic patterns for paraphrase generation.</S>
			<S sid ="59" ssid = "21">As we have already mentioned, there is one publicly available Russian paraphrase resource known to us: the dataset published by Ganitkevich et al. as part of The Para‐ phrase Database project (PPDB) [17].</S>
			<S sid ="60" ssid = "22">The authors collected an impressively large data‐ base of paraphrases on word-, phrase- and syntactic levels.</S>
			<S sid ="61" ssid = "23">Syntactic level paraphrases are annotated with nonterminal symbols (constituents, in terms of phrase structure grammar) and contain placeholders which can be substituted with any paraphrase matching its syntactic type.</S>
			<S sid ="62" ssid = "24">In addition, all types of paraphrases are annotated with count and probability-based features.</S>
			<S sid ="63" ssid = "25">These features include the diﬀerence in the number of words/characters/average word length between the original phrase and the paraphrase, the probability of the original phrase given the paraphrase, alignment features, etc. Some features are derived from the syntactic rules, e.g., the probability of the lefthand side nonterminal symbol given the paraphrase (and vice versa).</S>
			<S sid ="64" ssid = "26">The training data for Russian is substantial in PPDB (over 2 million sentence pairs), and the resulting dataset is large as well.</S>
			<S sid ="65" ssid = "27">It is collected from the corpora typically used in SMT: CommonCrawl, Yandex 1M corpus and News Commentary.</S>
			<S sid ="66" ssid = "28">The authors use a language independent method to extract paraphrases from parallel bilingual texts: paraphrases are found in a single language by “pivoting” over a shared translation in another language.</S>
			<S sid ="67" ssid = "29">Such approach was introduced by Bannard and CallisonBurch in 2005 [2] and since then it has been successfully applied by many researchers.</S>
			<S sid ="68" ssid = "30">The authors acknowledge that in morphologically rich languages diﬀerent forms of the same word tend to group into the same paraphrase clusters because English phrases are chosen as the pivot ones (in Russian diﬀerent forms of the same word are considered paraphrases in PPDB).</S>
			<S sid ="69" ssid = "31">While for some tasks it could be desirable, for others it is deﬁnitely not (it could cause generation of incorrect paraphrases).</S>
			<S sid ="70" ssid = "32">Moreover, such grouping leads to the rapid growth of the dataset, and, with a number of available morphological parsers today, it seems unnecessary.</S>
			<S sid ="71" ssid = "33">We also believe that we should use language-speciﬁc methods (in contrast with language-independent ones) when dealing with a morphologically rich language.</S>
			<S sid ="72" ssid = "34">Unlike other paraphrase resources, our corpus is not intended to be a general-purpose one.</S>
			<S sid ="73" ssid = "35">According to our tasks (IE and TS) we collect it from the news texts.</S>
			<S sid ="74" ssid = "36">The corpus consists of sentential paraphrases, and lower level paraphrase pairs can be extracted from it using any of the existing methods (e.g., SMT methods).</S>
	</SECTION>
	<SECTION title="Unsupervised Paraphrase Extraction. " number = "3">
			<S sid ="75" ssid = "1">3.1 Data: Method.</S>
			<S sid ="76" ssid = "2">We adopt a sentence-level approach and extract paraphrases from the news articles published on the Web.</S>
			<S sid ="77" ssid = "3">The latter is a truly rich source of paraphrases: the articles describing the same events appear in diﬀerent newspapers every day.</S>
			<S sid ="78" ssid = "4">Due to the lack of training data for Russian, our approach is unsupervised.</S>
			<S sid ="79" ssid = "5">It extends the one described by Fernando and Stevenson [15]: their method yields the best results against MSRP among the latest unsupervised approaches.</S>
			<S sid ="80" ssid = "6">Our hypothesis is that para‐ phrases can be successfully extracted from the Russian news texts based on a lexical similarity metric.</S>
			<S sid ="81" ssid = "7">We automatically extract articles published by several newspapers on the same day during the last 2 years.</S>
			<S sid ="82" ssid = "8">Then we adopt the strategy by Wubben et al. [27] and proceed with pairwise comparisons of headlines.</S>
			<S sid ="83" ssid = "9">A headline of an article can be considered its compression1, and we suppose that the headlines of the articles describing the same events are similar and may even be paraphrases of each other.</S>
			<S sid ="84" ssid = "10">Moreover, headlines 1 This statement can only be applied to the informative news texts (the ones intended to inform,.</S>
			<S sid ="85" ssid = "11">and not to persuade the reader) and not to the publicistic texts (exerting inﬂuence on the reader in the ﬁrst place).</S>
			<S sid ="86" ssid = "12">A publicistic headline is often designed to attract readers’ attention.</S>
			<S sid ="87" ssid = "13">However, both publicistic and informative texts can be used as a source of paraphrases.</S>
			<S sid ="88" ssid = "14">comparison is much faster than the comparison of all the sentences from the two articles.</S>
			<S sid ="89" ssid = "15">We do not take into account too short headlines (less than 3 words long) as they are unlikely to add to the representativeness of the resulting corpus.</S>
			<S sid ="90" ssid = "16">The overall scheme is as follows: we iterate over all possible pairs of headlines (with the same date of publication) from diﬀerent media agencies and calculate a similarity metric for each pair.</S>
			<S sid ="91" ssid = "17">Then the pairs with scores below the threshold value are pruned with the exception of a small portion of the necessary negative instances.</S>
			<S sid ="92" ssid = "18">The resulting dataset is evaluated by the annotators.</S>
			<S sid ="93" ssid = "19">Having the annotated data, we further adopt a supervised approach and optimize the unsupervised similarity metric.</S>
			<S sid ="94" ssid = "20">3.2 Sentence Similarity Metric.</S>
			<S sid ="95" ssid = "21">To extract paraphrases, we use an unsupervised lexical similarity metric based on the one proposed by Fernando and Stevenson [15]: (1) where is a similarity matrix and and – word vectors of the two sentences.</S>
			<S sid ="96" ssid = "22">Each element of the matrix represents the similarity between the words and . Diagonal elements obviously equal 1.</S>
			<S sid ="97" ssid = "23">Other elements equal 0 for diﬀerent or for similar words.</S>
			<S sid ="98" ssid = "24">To capture lexical similarity, the authors use several metrics mainly based on the “is-a” hierarchy from WordNet [15].</S>
			<S sid ="99" ssid = "25">In our research we use a matrix metric with lexical similarity scores based on the synonymy relation.</S>
			<S sid ="100" ssid = "26">As far as the source of synonymy relations is concerned, there exists a famous Russian dictionary of synonyms by Abramov [30] created over a century ago.</S>
			<S sid ="101" ssid = "27">Despite its numerous merits, the dictionary is deplorably outdated.</S>
			<S sid ="102" ssid = "28">The lack of modern synonymy resources has spurred a number of attempts at creating databases of syno‐ nyms.</S>
			<S sid ="103" ssid = "29">Although most resources are designed for practical purposes (rewriting texts in the web and automatically producing unique content), they can also be useful for NLP in general, and for our task in particular.</S>
			<S sid ="104" ssid = "30">We use one of such collections which consists of about 6 thousand articles.</S>
			<S sid ="105" ssid = "31">In fact, each article (a word and the list of its synonyms) can be considered a synset.</S>
			<S sid ="106" ssid = "32">For every pair of words we may further calculate the number of times they occur together in the same synset.</S>
			<S sid ="107" ssid = "33">In terms of information retrieval, every synset is a document, and it is known that semantically close are more likely to appear in the same documents than in the diﬀerent ones.</S>
			<S sid ="108" ssid = "34">To compute lexical similarity, we use such metrics as normalized pointwise Mutual Information (npmi) [4], Dice coeﬃcient [12] and Jaccard index [18].</S>
			<S sid ="109" ssid = "35">Unlike the original metric from [15] (which uses WordNet relations), ours is calcu‐ lated according to the list of scoring rules designed to capture not only synonymy rela‐ tions but also conjugate words2: 2 The latter might be of no importance for English, but they are essential for detecting Russian.</S>
			<S sid ="110" ssid = "36">sentential paraphrases.</S>
			<S sid ="111" ssid = "37">– Identical words starting with capital letters -&gt; 1.2 score (a slight bias towards the simultaneous occurrence of the same named entities in the sentences).</S>
			<S sid ="112" ssid = "38">– Identical words -&gt; 1.</S>
			<S sid ="113" ssid = "39">– Synonyms -&gt; Npmi, Dice or Jaccard coeﬃcient multiplied by 0.8.</S>
			<S sid ="114" ssid = "40">– One of the words is a substring of the other -&gt; the score equal to the length of the smaller word divided by the length of the larger word and multiplied by 0.7.</S>
			<S sid ="115" ssid = "41">– The words have common preﬁx (at least 3 characters) -&gt; the score equal to the preﬁx length divided by the length of the lesser word and multiplied by 0.</S>
			<S sid ="116" ssid = "42">– Otherwise -&gt; 0.</S>
			<S sid ="117" ssid = "43">The original metric varies from 0 to 1.</S>
			<S sid ="118" ssid = "44">With our modiﬁcations it no longer satisﬁes this condition but it does not aﬀect paraphrase extraction process.</S>
			<S sid ="119" ssid = "45">The scores (1.2, 1, 0.8, etc.) are obtained from the preliminary experiments conducted on the small subset of the corpus.</S>
			<S sid ="120" ssid = "46">Based on the results of these experiments, we select Jaccard index as the synonymy coeﬃcient in our metric.</S>
			<S sid ="121" ssid = "47">Let us calculate the metric for the two sentences from our dataset: 1.</S>
			<S sid ="122" ssid = "48">КHДP aннyлиpoвaлa дoгoвop o нeнaпaдeнии c Южнoй Кopeeй.</S>
			<S sid ="123" ssid = "49">/DPRK annulled the non-aggression treaty with South Korea/ 2.</S>
			<S sid ="124" ssid = "50">КHДP вышлa из coглaшeний o нeнaпaдeнии c Южнoй Кopeeй.</S>
			<S sid ="125" ssid = "51">/DPRK withdrew from the non-aggression agreement with South Korea/ We lemmatize the sentences using TreeTagger [23] and cut oﬀ auxiliary words.</S>
			<S sid ="126" ssid = "52">After these manipulations we represent the sentences as binary vectors (Fig.</S>
			<S sid ="127" ssid = "53">1): Fig.</S>
			<S sid ="128" ssid = "54">1.</S>
			<S sid ="129" ssid = "55">Example of word vectors for the two sentences One can see that there are 4 overlapping words, 3 of them starting with an uppercase letter, and 2 synonyms: “coглaшeниe” (agreement) and “дoгoвop” (treaty).</S>
			<S sid ="130" ssid = "56">The word “coглaшeниe” occurs in 5 synsets in the dictionary of synonyms, while “дoгoвop” – only once (and their appear in one synset together).</S>
			<S sid ="131" ssid = "57">Jaccard index equals 1/ (1 + 5 − 1) = 0.2 for the two given words.</S>
			<S sid ="132" ssid = "58">This score is multiplied by the pruning coeﬃcient: 0.2 * 0.8 = 0.16.</S>
			<S sid ="133" ssid = "59">The similarity matrix for the two sentences is shown in Fig.</S>
			<S sid ="134" ssid = "60">2.</S>
			<S sid ="135" ssid = "61">According to (1), the resulting similarity score equals 0.763.</S>
			<S sid ="136" ssid = "62">We apply the described metric to the pairs of article headlines and prune the ones with the Jaccard index-based similarity score below the empirically deﬁned threshold value of 0.5.</S>
			<S sid ="137" ssid = "63">Thus, our approach is based on the existing similarity metric, but according to our goal – the construction of paraphrase corpus for IE and TS – we introduce a list of scoring Fig.</S>
			<S sid ="138" ssid = "64">2.</S>
			<S sid ="139" ssid = "65">Similarity matrix example rules which capture the linguistic phenomena (synonymy relations, conjugate words, matching names entities) required for our corpus.</S>
	</SECTION>
	<SECTION title="Corpus Annotation and Analysis. " number = "4">
			<S sid ="140" ssid = "1">4.1 Annotation.</S>
			<S sid ="141" ssid = "2">Potential paraphrases with similarity metric values above the threshold are evaluated by the annotators.</S>
			<S sid ="142" ssid = "3">To obtain negative instances, we also include a portion of random sentence pairs with metric value below 0.5 in the corpus (roughly speaking, every fourth sentence pair in the corpus has a score below 0.5).</S>
			<S sid ="143" ssid = "4">At the moment there are 5424 pairs of sentences in the corpus, with 5181 annotated pairs.</S>
			<S sid ="144" ssid = "5">Out of these 5181 pairs of sentences, we select 4758 pairs and include them in the corpus (by pruning inconsistent or potentially unreliable results).</S>
			<S sid ="145" ssid = "6">We developed an online interface: http://paraphraser.ru/scorer for crowdsourced annotation.</S>
			<S sid ="146" ssid = "7">A user is shown two sentences at a time, and he/she is to decide whether the sentences convey the same meaning (class “1”), similar meanings (class “0”) or diﬀerent meanings (class “−1”).</S>
			<S sid ="147" ssid = "8">The users are advised to user their own judgement and intuition and are not given any speciﬁc instructions.</S>
			<S sid ="148" ssid = "9">We try to make the annotation process less tedious by introducing an entertainment element: the users are shown various facts (about diﬀerent events like the invention of something or the birth of a famous scientist/artist, etc.) and pictures at random intervals and are encouraged to annotate further.</S>
			<S sid ="149" ssid = "10">It is well known that crowdsourcing poses a challenge concerning the reliability of the obtained results.</S>
			<S sid ="150" ssid = "11">To prune unreliable results, we only consider sentence pairs anno‐ tated by at least 3 users.</S>
			<S sid ="151" ssid = "12">If a paraphrase pair is annotated by less than 4 users, and two of them provide opposite judgments (“−1” and “1”), such pair is cut oﬀ.</S>
			<S sid ="152" ssid = "13">In future we also plan to involve expert linguists in the annotation process.</S>
			<S sid ="153" ssid = "14">To assign a class to each sentence pair (“−1” for non-paraphrases, “0” for LPs and “1” – for PPs), we compute the median of all the scores given to the pair by the anno‐ tators.</S>
			<S sid ="154" ssid = "15">It can obviously take one of the following values: {− 1, −0.5, 0, 0.5, 1}.</S>
			<S sid ="155" ssid = "16">As we would like to have only 3 classes, in case of ties we adopt a pessimistic strategy and round the value down to the previous integer (−0.5 is reduced to −1, 0.5 – to 0).</S>
			<S sid ="156" ssid = "17">4.2 Paraphrase Classes.</S>
			<S sid ="157" ssid = "18">As stated earlier, we distinguish non-paraphrases, loose paraphrases (LPs) and precise paraphrases (PPs).</S>
			<S sid ="158" ssid = "19">Their distribution in our corpus is presented in Table 1.</S>
			<S sid ="159" ssid = "20">Table 1.</S>
			<S sid ="160" ssid = "21">Distribution of paraphrase classes in the corpus Para phra se clas s Nu mb er of inst anc es Perc enta ge of inst anc es Non para phra ses 159 9 33.6 % Loo se para phra ses 196 9 41.4 % Prec ise para phra ses 119 0 25 % Tot al 475 8 100 % Although one cannot say that the dataset is severely unbalanced, there is a slight bias towards loose paraphrases.</S>
			<S sid ="161" ssid = "22">To analyze the diﬀerences between precise and loose para‐ phrases in the corpus we follow the approach adopted in [13]: we randomly select 100 PPs and 100 LPs and manually annotate them with the linguistic features: – diﬀerent content (sentences diﬀer in words or phrases which carry additional infor‐ mation and make the sentences semantically diﬀerent); – diﬀerent time (diﬀerent grammar tenses are used to described the same event); – context knowledge (sentences diﬀer in the words or phrases, and added words/ phrases have no counterparts in the other sentences (see “diﬀerent content”), but nevertheless it is clear from the context of the phrases that the same notion/event is being referred to, and that there is no semantic diﬀerence); – metaphor (a metaphor takes place in one of the sentences); – metonymy (sentences diﬀer in some named entities, and one of these entities is used metonymically); – numeral (sentence pairs diﬀer in the representation of the same numerals or the number is rounded in one of the sentences); – phrasal synonymy (sentences diﬀer in the synonymous multiword expressions); – reordering (sentences consist of the same words in diﬀerent order); – word-level synonymy (sentence pairs diﬀer in the synonymous words); – syntactic synonymy (the same information is expressed in the sentences using diﬀerent constituents or the same constituents with diﬀerent grammatical character‐ istics, e.g., a verbal phrase in active and passive voice respectfully).</S>
			<S sid ="162" ssid = "23">Each pair of sentences can be annotated with more than one linguistic feature (e.g., syntactic synonymy is often accompanied by reordering and word-level synonymy).</S>
			<S sid ="163" ssid = "24">For each of the features we calculate the portion of sentence pairs it occurs in (see Table 2).</S>
			<S sid ="164" ssid = "25">These portions are calculated for PPs and LPs separately.</S>
			<S sid ="165" ssid = "26">It can be seen that metaphor, metonymy and diﬀerent representations of numbers are rare events in both types of paraphrases in our sample.</S>
			<S sid ="166" ssid = "27">While most (76 %) LPs diﬀer in the meaning they convey (see “diﬀerent content”), PPs are richer in word-, phrase- and syntactic-level synonymy.</S>
			<S sid ="167" ssid = "28">In fact, such results are quite predictable, and it is just what we expect these two paraphrase classes to be like.</S>
			<S sid ="168" ssid = "29">However, the portion of diﬀerent content (18 %) among PPs (this feature is undesirable for PPs in our corpus) is not what one could call neglectable.</S>
			<S sid ="169" ssid = "30">Indeed, deciding on the semantic equivalence is a challenging task even for linguist experts, setting aside mere native speakers.</S>
			<S sid ="170" ssid = "31">Thus, in future we plan to involve experts’ annotation to reduce the portion of semantically diﬀerent phrases among PPs.</S>
			<S sid ="171" ssid = "32">Table 2.</S>
			<S sid ="172" ssid = "33">Linguistic characteristics of precise and loose paraphrases Feat ure PP LP Feat ure PP LP Con text kno wle dge 32 % 25 % Nu mer al 9 % 4 % Diﬀ eren t cont ent 18 % 76 % Phr asal syn ony my 16 % 6 % Diﬀ eren t time 6 % 15 % Reo rder ing 17 % 10 % Met aph or 0 % 1 % Word leve l syn ony my 18 % 7 % Met ony my 3 % 2 % Syn tact ic syn ony my 33 % 17 %</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "5">
			<S sid ="173" ssid = "1">We evaluate the unsupervised metric used in the corpus construction by comparing it with the annotation results (see Table 3).</S>
			<S sid ="174" ssid = "2">Table 3.</S>
			<S sid ="175" ssid = "3">Unsupervised paraphrase extraction: results Sco re abo ve thre shol d Sco re belo w thre shol d Tot al Pre cise para phra ses 11 79 1 1 11 90 Loo se para phra ses 19 19 5 0 19 69 Non para phra ses 7 6 3 83 6 15 99 Tot al 38 61 89 7 47 58 As we do not distinguish between PPs and LPs when collecting sentence pairs using Jaccard-based metric, we only evaluate the quality of the metric in the task of classifying sentence pairs into similar (PPs and LPs) and diﬀerent ones.</S>
			<S sid ="176" ssid = "4">Thus, its precision equals 80.24 %.</S>
			<S sid ="177" ssid = "5">We believe that the evaluation via traditional recall and F1 measures would be unreliable in our case.</S>
			<S sid ="178" ssid = "6">We do not focus on the collection of a balanced dataset with “proper” negative instances at the moment (approximately every fourth candidate is randomly selected as a potential negative instance), and recall and F1-score would over‐ estimate our metric.</S>
			<S sid ="179" ssid = "7">Our unsupervised metric extends the one used by Fernando and Stevenson [15].</S>
			<S sid ="180" ssid = "8">They evaluated it against MSRP and achieved 75.2 % precision.</S>
			<S sid ="181" ssid = "9">Thus, our result seems prom‐ ising although one should bear in mind that it only reﬂects the quality of classifying sentence pairs into 2 classes, when PPs and LPs are merged into one class.</S>
			<S sid ="182" ssid = "10">Having obtained the annotated data, we optimized our metric: the threshold and the scores changed, Dice coeﬃcient was selected instead of Jaccard index, and the overall performance improved, but due to space limitations we cannot give full details of the experiments here.</S>
			<S sid ="183" ssid = "11">At the moment we are working on a supervised approach towards paraphrase identiﬁcation and train a classiﬁer to distinguish between PPs and LPs, with the optimized similarity metric being used as one of the features.</S>
			<S sid ="184" ssid = "12">In future we intend to develop an approach which focuses on covering various paraphrase classes and linguistic phenomena [16, 21] because Russian is rich in these phenomena.</S>
	</SECTION>
	<SECTION title="Conclusion: Future Work. " number = "6">
			<S sid ="185" ssid = "1">In this paper we presented our work on the creation of a Russian sentential paraphrase corpus.</S>
			<S sid ="186" ssid = "2">The corpus consists of news headlines automatically collected from the Web and ﬁltered using the unsupervised similarity metric.</S>
			<S sid ="187" ssid = "3">Such resource can be used in information extraction and text summarization.</S>
			<S sid ="188" ssid = "4">It can also serve as a training dataset for paraphrase identiﬁcation models for Russian.</S>
			<S sid ="189" ssid = "5">There are 4758 sentence pairs in the corpus at the moment, and it is freely available at our website: paraphraser.ru.</S>
			<S sid ="190" ssid = "6">All the pairs are being annotated using crowdsourcing via the website, and one of the three classes (non-paraphrase, loose paraphrase or precise paraphrase) is assigned to each pair of sentences.</S>
			<S sid ="191" ssid = "7">To obtain reliable data, we ensure that each pair of sentences in the corpus is annotated by at least 3 users and cut oﬀ inconsistent annotation.</S>
			<S sid ="192" ssid = "8">Evaluated against crowdsourced annotation, the similarity metric achieves 80.24 % precision at classifying paraphrases.</S>
			<S sid ="193" ssid = "9">Thus, it conﬁrms our hypothesis that para‐ phrases can be extracted from the Russian news texts using methods based on lexical similarity.</S>
			<S sid ="194" ssid = "10">Our further step aims at the development of paraphrase identiﬁcation model and we are already working on it.</S>
			<S sid ="195" ssid = "11">This step includes using a better synonymy resource: Yet Another RussNet [5] (it is 8 times larger than our original one), and a dictionary of word formation families.</S>
			<S sid ="196" ssid = "12">We already use the optimized similarity metric in the paraphrase classiﬁer and experiment with features based on semantic distributional models; other features are derived from the morphological characteristics, syntactic and semantic structure of the sentences.</S>
			<S sid ="197" ssid = "13">Thus, we intend to develop a ﬁnegrained approach towards identifying paraphrases.</S>
			<S sid ="198" ssid = "14">As it might demand experts’ annotation, it is one of our future work directions, along with the comparison of experts’ annotation and the results of the automatic extraction of linguistic features.</S>
			<S sid ="199" ssid = "15">We acknowledge Saint-Petersburg State University for the research grant 30.38.305.2014.</S>
	</SECTION>
</PAPER>
