<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">It has long been a dream to build computer systems that learn automatically by reading text.</S>
		<S sid ="2" ssid = "2">This dream is generally considered infeasible, but some surprising developments in the US over the past three years have led to the funding of several short-term investigations into whether and how much the best current practices in Natural Language Processing and Knowledge Representation and Reasoning, when combined, actually enable this dream.</S>
		<S sid ="3" ssid = "3">This paper very briefly describes one of these efforts, the Learning by Reading project at ISI, which has converted a high school textbook of Chemistry into very shallow logical form and is investigating which semantic features can plausibly be added to support the kinds of inference required for answering standard high school text questions.</S>
	</ABSTRACT>
	<SECTION title="Background" number = "1">
			<S sid ="4" ssid = "4">From almost the beginnings of Artificial Intelligence, it was clear that automated systems require knowledge to reason intelligently, and that for multipurpose, wide-domain, robust reasoning, the amount required is nontrivial.</S>
			<S sid ="5" ssid = "5">Experience, especially with expert systems during the 1970s, illustrated just how hard it is to acquire enough of the right knowledge, and how difficult it is to formalize that knowledge in ways suitable for supporting reasoning.</S>
			<S sid ="6" ssid = "6">Naturally, then, the dream arose to enable systems to read text and learn by themselves.</S>
			<S sid ="7" ssid = "7">But this dream has never been realized.</S>
			<S sid ="8" ssid = "8">In fact, as research in Knowledge Representation and Reasoning (KR&amp;R) and Natural Language Processing (NLP) progressed, the two areas diverged, to the point where today they are more or less entirely separate, with unrelated conferences, journals, and research paradigms.</S>
			<S sid ="9" ssid = "9">A few years ago, three research groups, funded by Vulcan Inc., participated in an audacious experiment called Project Halo: to (manually) convert the information contained in one chapter of a high school textbook on Chemistry into knowledge representation statements, and then to have the knowledge representation system take a standard high school Advanced Placement (AP) exam.</S>
			<S sid ="10" ssid = "10">Surprisingly, two of the three systems passed, albeit at a relatively low level of performance.</S>
			<S sid ="11" ssid = "11">The project engendered wide interest; see (Friedland et al., 2003).</S>
			<S sid ="12" ssid = "12">Over the past year, DARPA has funded five groups in the US to conduct pilot studies that investigate the feasibility of building fully Learning by Reading (LbR) systems.</S>
			<S sid ="13" ssid = "13">The largest, Project Möbius, is a consortium of some 20 researchers from numerous institutions.</S>
			<S sid ="14" ssid = "14">Its goal Petr Sojka, Ivan Kopecˇ ek and Karel Pala (Eds.): TSD 2006, LNAI 4188, pp.</S>
			<S sid ="15" ssid = "15">3–12, 2006.</S>
			<S sid ="16" ssid = "16">Oc SpringerVerlag Berlin Heidelberg 2006 is to design a general framework for LbR systems in the future, and to advise DARPA on the wisdom of funding a new program in this area.</S>
			<S sid ="17" ssid = "17">Typical questions include: How feasible is fully automated LbR?</S>
			<S sid ="18" ssid = "18">What are the different phases/components/steps of LbR?</S>
			<S sid ="19" ssid = "19">What are the current levels of capability of the component technologies, and where are the major bottlenecks and failure points?</S>
			<S sid ="20" ssid = "20">What kind of research would best further the dream of LbR?</S>
			<S sid ="21" ssid = "21">What sorts of results could one expect after five years?</S>
			<S sid ="22" ssid = "22">The remaining four projects proceed independently but report back to Möbius.</S>
			<S sid ="23" ssid = "23">All are smaller, 9-month efforts, and each focuses on one or more specific aspects of the general LbR problem: – A project jointly at Boeing and SRI, led by Peter Clark of Boeing, focuses on the mismatch between English sentences and their equivalent knowledge representations propositions, with the methodology of building manually the representations for a carefully selected extract of 5 pages from the Chemistry textbook.</S>
			<S sid ="24" ssid = "24">– A project at Northwestern University, led by Ken Forbus, concentrates on the processes of self-guided inference that occurs after new information is read.</S>
			<S sid ="25" ssid = "25">Called introspection or rumination, these processes work in parallel with the reading, and serve as a source of expectations, questions, and background checking.</S>
			<S sid ="26" ssid = "26">This project focuses on a few selected sentences from the Chemistry textbook, and the thoughts that may arise from them.</S>
			<S sid ="27" ssid = "27">– A project at CYC Corp., led by Michael Witbrock, addresses the problem of learning the meaning of new, unknown, words in context.</S>
			<S sid ="28" ssid = "28">Starting with the knowledge already inside their very large ontology and reasoning system Cyc, researchers develop methods to apply inferences in order to build up likely interpretations of a sentence and from these hypothesize the meaning of the unknown word.</S>
			<S sid ="29" ssid = "29">– The fourth project is the subject of this paper.</S>
			<S sid ="30" ssid = "30">Located at ISI, we are investigating how much can be done by combining traditional and statistical NLP methods, and what kinds of KR&amp;R are absolutely required, at which points in the process.</S>
			<S sid ="31" ssid = "31">We have parsed the whole Chemistry textbook, have developed methods to convert the parses into shallow logical form, and are investigating the what types of -semantics should be added to support the reasoning required for question answering.</S>
			<S sid ="32" ssid = "32">In this paper we briefly outline the architecture and general aspects of ISI’s LbR project, which will finish in August, namely about three months after the time of writing this paper.</S>
	</SECTION>
	<SECTION title="The Text. " number = "2">
			<S sid ="33" ssid = "1">The text is Chemistry: The Central Science (9th ed.), by Brown, LeMay, Bursten, and Burdge, which is intended for senior high-school students.</S>
			<S sid ="34" ssid = "2">The publishers have kindly made an electronic version available to the projects, to be used for research purposes only.</S>
			<S sid ="35" ssid = "3">The book contains 313590 words (12722 different words).</S>
			<S sid ="36" ssid = "4">Of this, nouns comprise 146825 / 9279 tokens/types, verbs 28627 / 1387 tokens/types, and adjectives and adverbs together 35775 / 2795.</S>
			<S sid ="37" ssid = "5">The most frequent 50 nouns cover 67.2% of all noun tokens in the book; the most frequent 50 verbs cover 92.4% of the verb tokens, and the most frequent 50 adjectives/adverbs cover 83.7%.</S>
			<S sid ="38" ssid = "6">This is relatively good news: should one have to hand- construct core meanings of (most of) these words before the systems starts to read, it will suffice to treat only the most frequent 200 nouns (covering 96.1% of all noun tokens), 50 verbs, and 50 adjectives/adverbs.</S>
	</SECTION>
	<SECTION title="The Learning by Reading Architecture and Modules. " number = "3">
			<S sid ="39" ssid = "1">3.1 System Architecture.</S>
			<S sid ="40" ssid = "2">Given the very short time available—9 months from start to finish—the project is necessarily very incomplete; we decided to focus on just selected areas of interest and to simplify or ignore many of the interesting questions we encountered.</S>
			<S sid ="41" ssid = "3">Since our strength lies in language processing technology, we focus on the NLP areas, with the intent to discover how little KR&amp;R we needed, and how shallow semantics is possible, to still produce something interesting.</S>
			<S sid ="42" ssid = "4">We therefore consciously sidestep the many complex phenomena involved in semantic understanding, as opposed to syntactic parsing, including in particular wordsense disambiguation, coreference and anaphora resolution, quantification, negation, complex NP structure and modification, discourse structure, and so on.</S>
			<S sid ="43" ssid = "5">One might ask: if you ignore all this, what’s the point?</S>
			<S sid ="44" ssid = "6">And indeed, designing some kind of evaluation, or at least some criteria by which one can measure and/or describe the validity and utility of the outcome, is a significant challenge, as we discuss in Section 4.</S>
			<S sid ="45" ssid = "7">Nonetheless, it is extremely interesting and quite instructive to proceed under these very strong simplifications, and to then discover what phenomena of semantics are in fact required for a practical application of LbR (as opposed to what phenomena have received a lot of attention from researchers), as well as to investigate whether there are simple and robust methods to derive these phenomena using modern techniques.</S>
			<S sid ="46" ssid = "8">The overall system architecture is shown in Figure 1.</S>
			<S sid ="47" ssid = "9">The system was built in three 3- month stages: stage 1 covered textbook analysis, background knowledge structures, and syntactic parsing; stage 2 covered conversion of parse structures into shallow logical form, creation of notations for selected semantic phenomena, and building of inferences / transformation rules to create these representations; stage 3 covers instantiation of the final output for into a KR&amp;R system and experiments with question answering.</S>
			<S sid ="48" ssid = "10">3.2 The Skeletal Ontology.</S>
			<S sid ="49" ssid = "11">No human would dream of reading a book like this without any knowledge whatsoever of English words.</S>
			<S sid ="50" ssid = "12">Since we had only 9 months for the project, we decided to extract from our large general-purpose term taxonomy/ontology Omega (Philpot et al., 2005) just the terms used in the book, with their inheritance structure, verb case frame information, and selected other relationships such as partonyms and synonyms.</S>
			<S sid ="51" ssid = "13">(The largest part of Omega, the Middle Model, was derived by combining WordNet 2.1 (Fellbaum et al., 1998) with Mikrokosmos (Mahesh, 1996) and adding to it a variety of additional information, notably the verb frame structures of Framenet (Baker et al., 1998), Lexical-Conceptual Structures (Dorr and Habash, 1991), and the PropBank frame (Palmer et al, 2005).</S>
			<S sid ="52" ssid = "14">The LbR ‘ontology’ we derived consists of 10371 terms, of which 6117 were already in Omega (the remaining terms were multi-word phrases, unknown proper names, etc.).</S>
			<S sid ="53" ssid = "15">In a technical book such as this, complex NPs pose a serous problem for understanding.</S>
			<S sid ="54" ssid = "16">No current NLP engines do an adequate job of analyzing their internal structure.</S>
			<S sid ="55" ssid = "17">To avoid this problem, and to fill in the rather large gap in the ontology due to these terms, Pantel applied his Mutual Information-based method (see for example (Pantel, 2005; Pantel and Ravichandran, 2004) for similar word on noun clusters and concept formation) to identify Fig.</S>
			<S sid ="56" ssid = "18">1.</S>
			<S sid ="57" ssid = "19">ISI’s Learning by Reading system architecture.</S>
			<S sid ="58" ssid = "20">The top region was completed in stage 1; the middle region in stage 2, and the bottom (left and right) region in stage 3.</S>
			<S sid ="59" ssid = "21">bigrams, trigrams, and longer strings of nouns that occur unusually frequently in this corpus compared to general English.</S>
			<S sid ="60" ssid = "22">We obtained several thousand multi-word terms, including (starting with highest mutual information) practice exercise, Solution Analysis, equilibrium constant, periodic table, boiling point, equilibrium constant, partial pressures, and so on, which we added to the ontology through simple automated matching of the final noun, which we assumed was the head in all cases.</S>
			<S sid ="61" ssid = "23">Although each term in the book is represented in the skeletal ontology, this does not mean much is actually known about the concept.</S>
			<S sid ="62" ssid = "24">The system knows in all cases the English forms of a word (plural, past tense, etc.), its purported taxonomic location (according to Omega, which is not specialized for Chemistry), and the frame structure of some of the verbs.</S>
			<S sid ="63" ssid = "25">It does not however have the words in the textbook disambiguated by sense: to the system, for example, the noun water might mean the normal fluid we drink, a sea or lake, or any of several other meanings.</S>
			<S sid ="64" ssid = "26">(Our initial assumption that words would tend to be monosemous because we are dealing only with a single domain turned out not to hold; even in Chemistry, many words have different senses.</S>
			<S sid ="65" ssid = "27">For example, “water” might refer to the substance in general, to its formula in an equation, or to a specific instance of it as being discussed in an experiment, 3.3 Information Retrieval.</S>
			<S sid ="66" ssid = "28">We identified the need to support three modes of reading: skimming; general sentence-by- sentence processing, as one normally reads a book (which we called deep reading); and targeted reading, in which one focuses on a specific term and reads fragments about it from all over the text.</S>
			<S sid ="67" ssid = "29">As described in Section 3.6, we used skimming to extract relations from the book using statistical methods.</S>
			<S sid ="68" ssid = "30">In contrast, as discussed in Sections 3.4 and 3.5, targeted reading is required during more typical reading, both to find details about some term that may be lacking crucial information and to answer questions (Section 3.7).</S>
			<S sid ="69" ssid = "31">To enable targeted reading, Lin deployed over the textbook an instance of the Lucene information retrieval package (Lucene 2005), and built a small web-based interface for it, by which one can explore the book manually as well.</S>
			<S sid ="70" ssid = "32">This IR system and its interface we called Scout.</S>
			<S sid ="71" ssid = "33">3.4 Parsing and Conversion to Hobbs Normal Form.</S>
			<S sid ="72" ssid = "34">We decomposed the conversion of natural language text into logical form into a series of steps.</S>
			<S sid ="73" ssid = "35">In this process we used our Basic Element package, built at ISI earlier this year for automated summarization evaluation (Hovy et al., 2006), which can be downloaded from http1//www.isi.edu/�cyl/BE.</S>
			<S sid ="74" ssid = "36">The purpose of the package is to convert sentences of text into a list of relation-head-modifier triples and then to compare the lists obtained from automated text summarizers to the lists produced from gold-standard human summaries.</S>
			<S sid ="75" ssid = "37">For LbR, we required just the triples.</S>
			<S sid ="76" ssid = "38">Using the package, we applied both Charniak’s parser (Charniak, 2000) and MINIPAR (Lin, 1994) to the whole book, and for technical reasons chose the former.</S>
			<S sid ="77" ssid = "39">Using the BE package tree-decomposition rules, Lin converted the entire book’s contents into so-called flat form.</S>
			<S sid ="78" ssid = "40">Next using a set of conversion rules written manually by Lin, each flat form statement was converted into what we call Hobbs Normal Form (HNF), which is a list of clauses, one per triplet, combined using logical AND, that expresses the dependency structure of the sentence in quasi-logical format.</S>
			<S sid ="79" ssid = "41">During this process, Lin also replaced the syntactic relations Subject and Object with the appropriate case frame roles obtained from Omega, using a process trained on the PropBank corpus.</S>
			<S sid ="80" ssid = "42">A brief example of a heading is shown in Figure 2.</S>
			<S sid ="81" ssid = "43">This process has not been fully optimized, and still contains quite a few easily fixed errors (as well as some unfortunately not so easily fixed).</S>
			<S sid ="82" ssid = "44">Of the 12394 sentences in the book, 51.7% contain some kind of known error, including 2219 verbs without arguments (broken parse trees), 2621 cases in which some argument (case frame role) has been wrongly duplicated within a sentence, and 752 auxiliary verbs with main verb.</S>
			<S sid ="83" ssid = "45">Some of these errors can be ignored for later purposes; many of them can be fixed by inserting additional tree-cutting rules.</S>
			<S sid ="84" ssid = "46">Parsing the whole book took 39 minutes on a standard PC; creating the basic element triples took 340 minutes; and converting into HNF took an additional 15 minutes.</S>
			<S sid ="85" ssid = "47">02011.fr.txti# 2.1/1 The/2 Atomic/3 Theory/4 of/5 Matter/6 02011.fr.txti 02011.fr.txti [ X4 itype Theory/NN/theory 02011.fr.txti i#NNCD 2.1/CD/2.1&lt;* ( X1 ) 02011.fr.txti i#NNNNP The/NNP/the&lt;* ( X2 ) 02011.fr.txti i#NNJJ Atomic/JJ/atomic&lt;* ( X3 ) 02011.fr.txti i#NNNN Matter/NN/matter&lt;OF ( X6 ) ] 02011.fr.txti 02011.fr.txti @@@LF 2.1&apos; (e1,x1) &amp; the&apos;(e2,x2) &amp; nn&apos;(e3,x2,x1) &amp; atomic&apos;(e4,x1) &amp; theory&apos;(e5,x1) &amp; of&apos;(e6,x1,x3) &amp; matter&apos;(e7,x3) Fig.</S>
			<S sid ="86" ssid = "48">2.</S>
			<S sid ="87" ssid = "49">Input sentence, Basic Element triples, and HNF 3.5 From Hobbs Normal Form to Limited Semantics.</S>
			<S sid ="88" ssid = "50">HNF is our starting point en route to semantics.</S>
			<S sid ="89" ssid = "51">This notation is a simplified variant of the logical form developed over almost a decade by Jerry Hobbs (Hobbs, forthcoming).</S>
			<S sid ="90" ssid = "52">Our implementation avoids numerous complex problems, including wordsense disambiguation, quantifier and negation scoping, complex NP structure, and other issues.</S>
			<S sid ="91" ssid = "53">Nonetheless, it provides a starting point into which we could systematically add representations of the kinds of logical phenomena required to answer questions in the Chemistry domain.</S>
			<S sid ="92" ssid = "54">Guided by analysis of the text and questions, Hobbs and Mulkar implemented a set of abductive inference rules that takes as input HNF and returns a set of possible ‘deeper’ interpretations of them, in which some particular semantic phenomenon has been addressed each time.</S>
			<S sid ="93" ssid = "55">At the time of writing, the phenomena for which rules were created include: – Determiner interpretation: insertion of ∀ and ∃ quantifiers (without scope) – Plural handling: insertion of sets to represent aggregations, plus skolem variables to represent their individual item(s).</S>
			<S sid ="94" ssid = "56">These sets are denoted by S variables in the HNF An example will be helpful.</S>
			<S sid ="95" ssid = "57">Consider the sentence All atoms of a given element are identical.</S>
			<S sid ="96" ssid = "58">After parsing and basic element conversion, the flat logical form is all(s,e1) &amp; atom’(e1,x) &amp; plural(x,s) &amp; of ’(e2,x,y) &amp; given(y) &amp; element’(e3,y) &amp; identical1’(e4,s) &amp; MainAssertion(e4) Here each e variable expresses an eventuality, each s variable a set (as introduced by a plural), and each x or y a specific instance.</S>
			<S sid ="97" ssid = "59">After application of the determiner inference rule and canonicalization of symmetric predicates, we obtain FORALL(x,e1,e4) &amp; atom’(e1,x) &amp; of ’(e2,x,y) &amp; element’(e3,y) &amp; identical1’(e4,s) and after recursively collecting the properties of x, the final result is an axiom in chemical theory: (∀x)[atom(x1) &amp; atom(x2) &amp; of(x1,y) &amp; of(x2,y) &amp; element(y) → identical2(x1,x2)] The engine performing this transformation is a simplified version of the abductive theorem prover TACITUS (Hobbs, 1986).</S>
			<S sid ="98" ssid = "60">It combines weight scores, originally assigned to inferences, in order to obtain numerical goodness scores for each derived result, and then ranks them accordingly.</S>
			<S sid ="99" ssid = "61">Since each rule handles a different semantic phenomenon, the rules have to be written by hand.</S>
			<S sid ="100" ssid = "62">How many such rules will eventually be required, and will the rule set eventually grow unmanageable?</S>
			<S sid ="101" ssid = "63">This is impossible to estimate, but Hobbs is optimistic.</S>
			<S sid ="102" ssid = "64">In a passage of 7 clauses selected for its high content, for example, all clauses are of form (∀x )[Subject(x ) → (∃ y) VP(x , y)] The density of occurrence of the principal phenomena we have so far encountered in the book (which has a relatively homogeneous style of exposition) suggests that a relatively small number of rules/phenomena will do a lot of the work in translating sentences into content theory axioms.</S>
			<S sid ="103" ssid = "65">3.6 Skimming to Obtain Relations.</S>
			<S sid ="104" ssid = "66">In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.</S>
			<S sid ="105" ssid = "67">In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.</S>
			<S sid ="106" ssid = "68">Their harvester, called Espresso, uses weak supervision to learn lexical patterns that typically signal relations of interest, and then applies these relations to extract all likely instances of the relation from the book.</S>
			<S sid ="107" ssid = "69">These extractions are then reformulated as HNF axioms and added to the system’s knowledge.</S>
			<S sid ="108" ssid = "70">Espresso follows in the pattern-based extraction paradigm of Hearst (1992), Berland and Charniak (1999), Ravichandran and Hovy (2002), Fleischmann et al.</S>
			<S sid ="109" ssid = "71">(2003), and Girju et al.</S>
			<S sid ="110" ssid = "72">(2003).</S>
			<S sid ="111" ssid = "73">The patterns themselves are not manually built, but are learned from the text after a small set of seed instances is given by the user, following the general design of Ravichandran and Hovy.</S>
			<S sid ="112" ssid = "74">The most reliable patterns are identified using pointwise mutual information, and the overly general ones are discarded.</S>
			<S sid ="113" ssid = "75">The resulting patterns are applied throughout the textbook, to deliver a set of instances of the relation.</S>
			<S sid ="114" ssid = "76">Once instances have been collected, pointwise mutual information is again used to prefer the ones best associated with the most (reliable) patterns.</S>
			<S sid ="115" ssid = "77">Espresso was applied to the Chemistry textbook, in which the relations of particular interest (because they provide useful axioms in the domain) are is-a, part-of, react-with, and produces, as in HCl is-a strong acid, atom part-of molecule, hydrogen-gas reacts- with oxygen-gas, and ammonia produces nitrous oxide respectively.</S>
			<S sid ="116" ssid = "78">After quality filtering, Espresso produced 200 is-a instances (with average precision of 85.0%), 111 part-of (precision 60.0%), 40 reacts-with (precision 85.0%), and 196 produces (precision 72.5%).</S>
			<S sid ="117" ssid = "79">The resulting instance expressions were then reformulated as axioms as follows: R is-a S becomes R(x) ♦ S(x) R part-of S becomes (∀x)R(x) ♦ (∃y)[S(y) &amp; part-of(x,y)] 3.7 Answering Questions: Inferences in Powerloom1.</S>
			<S sid ="118" ssid = "80">Given the modules described above, not much more is required to answer questions.</S>
			<S sid ="119" ssid = "81">The parser’s grammar has to be extended to handle question syntax, and a rudimentary question typology is required, along the lines of the QA typology described in (Hovy et al., 2002).</S>
			<S sid ="120" ssid = "82">Most important, however, is the inclusion of a reasoning system to apply learned inferences to newly-acquired propositions or questions in order to derive appropriate connections with the existing knowledge.</S>
			<S sid ="121" ssid = "83">For this purpose we use Powerloom (Chalupsky and Russ, 2002), a knowledge representation and reasoning system that has been widely distributed and used in numerous projects over the past decade.</S>
			<S sid ="122" ssid = "84">All axioms and semantic propositions derived from HNF are asserted into Powerloom, which is responsible for their interconnection and maintenance.</S>
			<S sid ="123" ssid = "85">The process of asserting semantic propositions into Powerloom, performed by Chalupsky, makes very clear how much is taken for granted in the textbook as background knowledge, either of Chemistry or of English in general.</S>
			<S sid ="124" ssid = "86">For example, for the sentence each element 1 This work is still underway at the time of writing and hence this section does not include many.</S>
			<S sid ="125" ssid = "87">details.</S>
			<S sid ="126" ssid = "88">composed of extremely small particles called atoms the shallow semantic representation, derived from HNF and asserted to Powerloom, is (ASSERT (FORALL (?E34 ?E35 ?x) (=&gt; (AND (subject ?E34 ?E35) (element’ ?E35 ?x)) (EXISTS (?E62 ?E63 ?s1 ?e10 ?y ?e4 ?e9 ?e3 ?e5 ?a ?z ?e11 ?s2 ?e6) (AND (asserted ?E62 ?E63) (compose’ ?E63 ?x ?s1) (plural ?e10 ?y ?s1) (small’ ?e4 ?y) (extremely ?e9 ?e4) (particle’ ?e3 ?y) (call’ ?e5 ?a ?y ?z) (plural ?e11 ?z ?s2) (atom’ ?e6 ?z)))))) With this knowledge, Powerloom is able to answer question 1 but not question 2: 1.</S>
			<S sid ="127" ssid = "89">Is each element composed of particles?.</S>
			<S sid ="128" ssid = "90">2.</S>
			<S sid ="129" ssid = "91">Is each element composed of atoms?.</S>
			<S sid ="130" ssid = "92">because it does not know that the relation call’ denotes object identity—that it, it does not know the meaning of the word “called”.</S>
			<S sid ="131" ssid = "93">Probably the most problematic aspect of this project is the fact there exists no set of axioms that define the meanings of general English words, even to the rudimentary level required for the simplest questions.</S>
			<S sid ="132" ssid = "94">This lack places a serious limitation on the amount of text a short-term project such as this can handle.</S>
			<S sid ="133" ssid = "95">Fortunately, as mentioned in Section 2, one need not define more than a few hundreds nouns, verbs, adjectives, and adverbs in order to cover most of the book.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "4">
			<S sid ="134" ssid = "1">It remains unclear exactly how one should evaluate a Learning by Reading system such as this.</S>
			<S sid ="135" ssid = "2">The general QA paradigm used in Halo and planned for Möbius—obtain a set of AP exam questions, apply them to the system before reading and then again to the system after reading, and compare the results—is somewhat inappropriate because in a 9-month period one simply cannot build enough of the system, and include enough background knowledge in the form of axioms, to be able to understand enough text for a real-world exam.</S>
			<S sid ="136" ssid = "3">We therefore plan two approaches to evaluation.</S>
			<S sid ="137" ssid = "4">First, we are investigating the kinds of questions the system can be expected to handle, and are building a typology of the question types, arranged by complexity of reasoning.</S>
			<S sid ="138" ssid = "5">This typology will range from the simplest questions (being able to answer Is it true that X?</S>
			<S sid ="139" ssid = "6">after having been given proposition X ), to formula-oriented computations (such as balancing equations), to the most complex ones (involving deeper inference with several axioms, obtained from several places in the text).</S>
			<S sid ="140" ssid = "7">If possible, we will try to estimate the frequencies of these types in typical AP exams, in order to estimate how much of each type of background knowledge (definitions of English; formula reasoning; simple and complex inference; etc.) will be required of a full-blown LbR system.</S>
			<S sid ="141" ssid = "8">Second, we are assembling a set of questions about material about which the system will have been given the necessary background knowledge.</S>
			<S sid ="142" ssid = "9">We will apply these questions three times, at levels of increasing complexity: 1.</S>
			<S sid ="143" ssid = "10">Baseline: just in English, using the Scout IR engine on the untreated English text;.</S>
			<S sid ="144" ssid = "11">2.</S>
			<S sid ="145" ssid = "12">No inference: after the system has read the text and asserted its knowledge and axioms.</S>
			<S sid ="146" ssid = "13">in Powerloom, but without inference allowed; 3.</S>
			<S sid ="147" ssid = "14">With inference: after the system has read and ingested the text, with Powerloom.</S>
			<S sid ="148" ssid = "15">inference enabled.</S>
			<S sid ="149" ssid = "16">We will apply the same two relatively transparent evaluation metrics at each level, the first counting just how much relevant material/propositions have been found, and the second counting how many exact and correct answers have been found.</S>
			<S sid ="150" ssid = "17">(We are curious to learn the results, and hope that they show that inference is useful, in other words, that pure text-level IR is not enough for Chemistry.)</S>
			<S sid ="151" ssid = "18">These two approaches to evaluation do not constitute a real evaluation, but will be informative in helping to understand just how far one can expect a Learning by Reading system of this (language-oriented) type to go, and in just what kinds of areas it falls short.</S>
	</SECTION>
</PAPER>
