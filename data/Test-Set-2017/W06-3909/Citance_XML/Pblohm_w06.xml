<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">None</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="2" ssid = "2">For many applications it is important to provide a large amount of instances of domain-specific relations.</S>
			<S sid ="3" ssid = "3">This is the case, for example, for question answering systems.</S>
			<S sid ="4" ssid = "4">Answering a question like “Which European capitals have more than a million inhabitants?” requires that we know the capital of each country, the continent of each city as well as the number of inhabitants of every city.</S>
			<S sid ="5" ssid = "5">It is clear that manually acquiring the extension of each relation of interest is unfeasible.</S>
			<S sid ="6" ssid = "6">Thus, some sort of automatic support is indeed not only desirable but crucial.</S>
			<S sid ="7" ssid = "7">In the last years, several systems have been developed with the aim of automatically extracting relation tuples by matching certain lexico-syntactic patterns indicating the relation of interest in text data.</S>
			<S sid ="8" ssid = "8">The approach builds on the seminal work of Hearst [1] who manually defined lexico-syntactic patterns to extract isa-relations.</S>
			<S sid ="9" ssid = "9">Domain-specific relations can however certainly not be captured by a rigid set of a few lexico-syntactic patterns.</S>
			<S sid ="10" ssid = "10">Besides defining a few lexico-syntactic patterns for extraction of hypernym-relations, Marti Hearst also defined a basic procedure to find appropriate patterns indicating these relations: 1.</S>
			<S sid ="11" ssid = "11">Decide on a lexical relation R of interest, e.g. hyponymy/hypernymy..</S>
			<S sid ="12" ssid = "12">2.</S>
			<S sid ="13" ssid = "13">Gather a list of terms for which this relation is known to hold, e.g. hyponym(car,.</S>
			<S sid ="14" ssid = "14">vehicle).</S>
			<S sid ="15" ssid = "15">This list can be found automatically using the patterns already learned or by bootstrapping from an existing lexicon or knowledge base.</S>
			<S sid ="16" ssid = "16">3.</S>
			<S sid ="17" ssid = "17">Find expressions in the corpus where these terms occur syntactically near one.</S>
			<S sid ="18" ssid = "18">another.</S>
			<S sid ="19" ssid = "19">4.</S>
			<S sid ="20" ssid = "20">Find the commonalities and generalize the expressions in 3 to yield patterns that.</S>
			<S sid ="21" ssid = "21">indicate the relation of interest.</S>
			<S sid ="22" ssid = "22">5.</S>
			<S sid ="23" ssid = "23">Once a new pattern has been identified, gather more instances of the target relation.</S>
			<S sid ="24" ssid = "24">and go to step 3.</S>
			<S sid ="25" ssid = "25">Our long term goal is to build on this general approach and develop a system that automatically generates patterns and extracts relation instances.</S>
			<S sid ="26" ssid = "26">In order to enable large flexibility and to allow covering a large amount of relations, it is important to keep human intervention at a minimum.</S>
			<S sid ="27" ssid = "27">This however requires that many design choices are taken in advance and parameters are specified.</S>
			<S sid ="28" ssid = "28">The large amount of possible design choices is reflected in the number of systems that are currently being developed following Hearst’s general algorithm (compare [2, 3]).</S>
			<S sid ="29" ssid = "29">Important design choices are: – How many examples for which the relation of interest is known to hold do we need to specify?</S>
			<S sid ="30" ssid = "30">(We will refer to these as the seed examples.)</S>
			<S sid ="31" ssid = "31">– What does it mean for two terms to occur syntactically near one another?</S>
			<S sid ="32" ssid = "32">– How are expressions from step 3 and their generalizations represented?</S>
			<S sid ="33" ssid = "33">In particular one can take into account output from different levels of linguistic analysis.</S>
			<S sid ="34" ssid = "34">– How are the generalizations in step 4 derived and selected from the enormous space of possible generalizations.</S>
			<S sid ="35" ssid = "35">The task at hand can be considered a Machine Learning task that is based on minimal supervision.</S>
			<S sid ="36" ssid = "36">In the course of the above procedure, the system constantly takes decisions on which extracted relation instances and which generated patterns to accept.</S>
			<S sid ="37" ssid = "37">These decisions can only be guided by the knowledge the system has at that stage.</S>
			<S sid ="38" ssid = "38">It is therefore far from straightforward to come up with an objective function to guide the decisions.</S>
			<S sid ="39" ssid = "39">This is particularly so as ground truth knowledge for these judgments, namely the extension of the relation, is inherently unknown during extraction.</S>
			<S sid ="40" ssid = "40">All the available research systems have done some implicit choice on the above questions, but rarely made all their choices explicit.</S>
			<S sid ="41" ssid = "41">We have implemented a pattern learner and thus relation extractor inspired by Hearst’s approach as well.</S>
			<S sid ="42" ssid = "42">The contribution of our paper is on the one hand to present the concrete algorithm we use and explicitly mentioning our design choices with respect to the above issues.</S>
			<S sid ="43" ssid = "43">In particular, in this paper we focus on the objective evaluation functions to evaluate the patterns.</S>
			<S sid ="44" ssid = "44">We present different evaluation measures and provide preliminary results based on human inspection comparing the different measures.</S>
			<S sid ="45" ssid = "45">The research reported in this paper is in a preliminary stage but nevertheless interesting in our view in that it first of all systematizes the design choices of any pattern-based learner and second provides first insight into which evaluation function might work best.</S>
	</SECTION>
	<SECTION title="Algorithm  and Implementation. " number = "2">
			<S sid ="46" ssid = "1">In this section the extraction algorithm (Figure 1) and key functional units in an iterative pattern induction system are identified and described.</S>
			<S sid ="47" ssid = "2">The learned relation instances are considered tuples t = (t1 , t2 ) ∈ T . In addition to the set of accepted tuples S ⊂ T the algorithm maintains a set P of accepted extraction patterns.</S>
			<S sid ="48" ssid = "3">An initial set of tuples St and of patterns P t can serve as input to the algorithm, although either of both suffice in principle.</S>
			<S sid ="49" ssid = "4">Our implementation of the algorithm, the PRONTO system uses the World Wide Web as a corpus, relying the Google search engine for matching.</S>
			<S sid ="50" ssid = "5">Matching is done by issuing queries to Google via its API and processing the short text fractions (snippets) returned.</S>
			<S sid ="51" ssid = "6">The seeds are regarded as occurring near each other if there are at most 4 tokens separating them.</S>
			<S sid ="52" ssid = "7">The overall algorithm is shown in Figure 1.</S>
			<S sid ="53" ssid = "8">Match-Tuples constructs queries from individual relation instances t ∈ S, an additional matching mechanism then identifies occurrences of the relation instance within the snippets.</S>
			<S sid ="54" ssid = "9">Patterns consist of generalizations of these occurrences, and generalization is done by replacing individual text tokens by ∗ wildcards and restricting the pattern to a window of a few tokens around the occurrence.</S>
			<S sid ="55" ssid = "10">Queries for Match-Patterns are then constructed by surrounding the patterns by quotes in order to match the exact sequence.</S>
			<S sid ="56" ssid = "11">During processing, text is treated as a sequence of tokens.</S>
			<S sid ="57" ssid = "12">Iterative Pattern Induction(P atternsP t , T uplesSt ) 1 S ← St 2 P ← P t 3 while not Done 4 do Occt ← Match-Tuples(S) 5 P ← P + Learn-Patterns(Occt ) 6 Evaluate − P atterns(P ).</S>
			<S sid ="58" ssid = "13">7 P t ← {p ∈ P | Pattern-Filter-Condition(p)} 8 Occp ← Match-Patterns(P t ).</S>
			<S sid ="59" ssid = "14">9 S ← S + Extract-Tuples(Occp ) 10 Evaluate − T uples(S).</S>
			<S sid ="60" ssid = "15">11 S ← {t ∈ S | Tuple-Filter-Condition(t)} Fig.</S>
			<S sid ="61" ssid = "16">1.</S>
			<S sid ="62" ssid = "17">Iterative pattern induction algorithm starting with inital patterns P t and tuples St Learn-Patterns identifies occurrences in which the same tokens appear in the same positions (relative to the relations arguments).</S>
			<S sid ="63" ssid = "18">It then generates abstractions through a merging of occurrences.</S>
			<S sid ="64" ssid = "19">Thereby tokens shared by all merged occurrences are kept while others are replaced by wildcards.</S>
			<S sid ="65" ssid = "20">All patterns that have not been generated by a merger of occurrences of at least two different relation instances, are discarded, which ensures at the same time a certain minimum generality and some degree of appropriateness for the relation.</S>
			<S sid ="66" ssid = "21">The evaluation process and the diverse evaluation functions are discussed in further detail in section 3.</S>
			<S sid ="67" ssid = "22">In every iteration, only the top 100 patterns and the top 50% of the tuples are kept.</S>
			<S sid ="68" ssid = "23">The Extract-Tuples step can be considered part of the matching procedure at the level of abstraction of this description.</S>
			<S sid ="69" ssid = "24">A typical run of the PRONTO system currently generates around 3000 facts in approximately three hours starting with a list of 10 facts as seed examples, iterating 5 times the extraction procedure.</S>
			<S sid ="70" ssid = "25">Precision strongly depends on the type of relation that is to be learned as well as on the evaluation strategy used.</S>
			<S sid ="71" ssid = "26">(cf.</S>
			<S sid ="72" ssid = "27">section 4)</S>
	</SECTION>
	<SECTION title="Evaluation  Functions. " number = "3">
			<S sid ="73" ssid = "1">In general, tuples can be evaluated by estimating the confidence that they belong to the target relation.</S>
			<S sid ="74" ssid = "2">For the purpose of our experiments we do so by averaging over the confidence the system puts into the patterns that extracted that tuple.</S>
			<S sid ="75" ssid = "3">There are various ways to compute pattern confidence.</S>
			<S sid ="76" ssid = "4">In this work we present three approaches taken from the literature that we compare against our own approach and against a baseline condition randomly assigning confidence values.</S>
			<S sid ="77" ssid = "5">The most direct measures of pattern performance are precision and recall.</S>
			<S sid ="78" ssid = "6">However, those are impossible to assess due to lack of information on the extension of the relation at hand.</S>
			<S sid ="79" ssid = "7">In this work, we heuristically define precision by assuming that among the matches of a pattern m(p) only those are true positives which have been previously accepted in S. This is similar to what is done by [3].</S>
			<S sid ="80" ssid = "8">m(p) ∩ S| cprec (p) = | |S| This measure may heavily underestimate the actual performance of a pattern if that pattern is able to generate many previously unseen relation instances.</S>
			<S sid ="81" ssid = "9">The following strategies have been adopted to overcome this limitation.</S>
			<S sid ="82" ssid = "10">They rely on counts of matches of patterns with or without filling their argument slots with particular relation instances.</S>
			<S sid ="83" ssid = "11">The corpus frequencies derived in this manner are used to assess the coherence of patterns and the their extraction results via pointwise mutual information (PMI).</S>
			<S sid ="84" ssid = "12">PMI measures the strength of association between two random events A and B and is defined as: pmi(A, B) = log P (A, B) P (A)P (B) The KnowItAll[4] information extraction system uses PMI in the following way1 to assess coherence of a pattern-tuple pair (p, t) in: t1 , p, t2 | pmi1 (p, t) = | |t1 , ∗, t2 | Following [5] we write |t1 , p, t2 | to denote the number of search engine matches of a query generated by filling the components of tuple t = (t1 , t2 ) into the argument slots of pattern p. While ∗ means allowing arbitrary values for the pattern or the argument replaced.</S>
			<S sid ="85" ssid = "13">In [4] this measure is used to generate a feature vector for classification of patterns.</S>
			<S sid ="86" ssid = "14">In our work, we use an average of pmi1 values over a subset of S to quantify the patterns performance.</S>
			<S sid ="87" ssid = "15">In the Espresso system [5], PMI is used in a different way aiming at relating the event of the pattern occurring in the corpus and the event of the tuple occurring in the corpus.</S>
			<S sid ="88" ssid = "16">The intuition behind this is that a pattern is good if it occurs preferably in association with tuples from S and conversely tuples from S have a strong association with the pattern.</S>
			<S sid ="89" ssid = "17">t1 , p, t2 | pmi2 (p, t) = log | |∗, p, ∗| |t1 , ∗, t2 | We propose a third PMI-based confidence measure which relates the event of the first argument of a tuple t within the pattern and that of the second argument occurring in t in its respective position.</S>
			<S sid ="90" ssid = "18">The idea of this measure is to punish too general patterns (large denominator), too specific patterns (small |∗, p, ∗|) and bogus tuples (low association) at the same time.</S>
			<S sid ="91" ssid = "19">It is based on the following approximation: P (t1 , p, ∗ ∧ ∗, p, t2 |∗, p, ∗) |t1 , p, t2 ||∗, p, ∗| pmi3 (t) = log P (t , p, , p, )P ( , p, t , p, ) &quot;&apos; log t , p, , p, t 1 ∗|∗ ∗ ∗ 2 |∗ ∗ | 1 ∗||∗ 2 | 1 Although called PMI, the applications in [4] and [5] deviate from the original defini-.</S>
			<S sid ="92" ssid = "20">tion in that the logarithm is omitted (Etzioni only) and that absolute match counts replace probabilities.</S>
			<S sid ="93" ssid = "21">These deviations however do not affect the ranking derived.</S>
			<S sid ="94" ssid = "22">Fig.</S>
			<S sid ="95" ssid = "23">2.</S>
			<S sid ="96" ssid = "24">Yield and precision of the extraction experiments.</S>
			<S sid ="97" ssid = "25">Results for the same relation share the same line style those derived with the same evaluation strategy share one marking point style.</S>
			<S sid ="98" ssid = "26">A pattern confidence value is computed for the PMI strategies by averaging the PMI values over a random subset of the currently accepted tuples St . As a baseline condition, a pattern evaluator has been implemented that assigns random confidence values crandom (p) to all patterns.</S>
	</SECTION>
	<SECTION title="Experiments and Results. " number = "4">
			<S sid ="99" ssid = "1">We present in figure 2 yield counts (cumulative number of extracted tuples) and precision values of the output of the PRONTO system during 5 iterations.</S>
			<S sid ="100" ssid = "2">The located-in (city in country) and product-of (car model, car maker) relations have been chosen for evaluation.</S>
			<S sid ="101" ssid = "3">Precision has been assessed by taking a random sample of size 20 from of the results at each iteration and inspection of each individual tuple by one of the authors.</S>
			<S sid ="102" ssid = "4">All precision values decay between the first and the second iteration and show no clear trend afterwards.</S>
			<S sid ="103" ssid = "5">Precision values of the located-in relation range around 0.8 those of the product-of relation around 0.5.</S>
			<S sid ="104" ssid = "6">For located-in the precision- based evaluation performs slightly stronger, for product-of pmi2 performs best.</S>
			<S sid ="105" ssid = "7">The yield-count shows that the increase of extracted information is linear and equally large for all strategies mostly bounded by the constant number of Google queries run in each iteration.</S>
			<S sid ="106" ssid = "8">While being far from perfect, these preliminary results indicate several important points: – The system is able to extract relation instance with relatively high quality.</S>
			<S sid ="107" ssid = "9">– The random filtering of patterns is slightly outperformed by other evaluation strategies.</S>
			<S sid ="108" ssid = "10">– The precision level reached depends more on the type of relation that is to be extracted than on the evaluation strategy applied.</S>
			<S sid ="109" ssid = "11">– No strong performance difference can currently be observed among the informed pattern evaluation strategies.</S>
			<S sid ="110" ssid = "12">– There is no strong inherent degradation of results from one iteration to the next even though imperfect output is added to the seeds.</S>
			<S sid ="111" ssid = "13">The relatively strong performance of the “random” strategy may be explained by the learning algorithm excluding patterns that have not been derived by a merger of occurrences of at least two different relation instances.</S>
			<S sid ="112" ssid = "14">This criterion which is similar to the only filtering done in [2] is necessary to keep further processing computationally feasible.</S>
	</SECTION>
	<SECTION title="Related Work  and Conclusion. " number = "5">
			<S sid ="113" ssid = "1">An early rather generic implementation of the approach outlined by Hearst [1] is DIPRE[2] which has been extended by Snowball[3] mostly by adding explicit pattern and tuple evaluation strategies, which however assume that the relation that is to be learned is functional.</S>
			<S sid ="114" ssid = "2">PMI-based evaluation of patterns and tuples was done in KnowItAll[4] and Espresso [5].</S>
			<S sid ="115" ssid = "3">Recent work in the field includes [6] in which the focus lies on inducing patterns reflecting syntactic dependencies.</S>
			<S sid ="116" ssid = "4">In this paper we introduced our implementation of a pattern induction system that is like many others on iterative extension of a set of seed examples.</S>
			<S sid ="117" ssid = "5">Focusing on pattern evaluation we demonstrated the impact of different evaluation strategies on extraction quality.</S>
			<S sid ="118" ssid = "6">At the workshop we will be able to present evaluation results that have higher statistical validity and cover a larger number of relations as a fully automatic evaluation system is currently under development.</S>
			<S sid ="119" ssid = "7">The results will probably allow to identify features of evaluation functions that are crucial for the success of the extraction.</S>
	</SECTION>
			<S sid ="120" ssid = "8">1.</S>
			<S sid ="121" ssid = "9">M. A. Hearst, “Automatic acquisition of hyponyms from large text corpora,” in Proceedings of the 14th conference on Computational linguistics.</S>
			<S sid ="122" ssid = "10">Morristown, NJ, USA: Association for Computational Linguistics, 1992, pp.</S>
			<S sid ="123" ssid = "11">539–545.</S>
			<S sid ="124" ssid = "12">2.</S>
			<S sid ="125" ssid = "13">S. Brin, “Extracting patterns and relations from the world wide web,” in WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98, 1998.</S>
			<S sid ="126" ssid = "14">[Online].</S>
			<S sid ="127" ssid = "15">Available: citeseer.csail.mit.edu/brin98extracting.html 3.</S>
			<S sid ="128" ssid = "16">E. Agichtein and L. Gravano, “Snowball: extracting relations from large plain-text collections,” in DL ’00: Proceedings of the fifth ACM conference on Digital libraries.</S>
			<S sid ="129" ssid = "17">New York, NY, USA: ACM Press, 2000, pp.</S>
			<S sid ="130" ssid = "18">85–94.</S>
			<S sid ="131" ssid = "19">4.</S>
			<S sid ="132" ssid = "20">O. Etzioni, M. Cafarella, D. Downey, A.-M.</S>
			<S sid ="133" ssid = "21">Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates, “Unsupervised named-entity extraction from the web: an experimental study,” Artif.</S>
			<S sid ="134" ssid = "22">Intell., vol.</S>
			<S sid ="135" ssid = "23">165, no. 1, pp.</S>
			<S sid ="136" ssid = "24">91–134, 2005.5.</S>
			<S sid ="137" ssid = "25">M. Pennacchiotti and P. Pantel, “A bootstrapping algorithm for automatically har vesting semantic relations,” in Proceedings of Inference in Computational Semantics (ICoS06), Buxton, England.</S>
			<S sid ="138" ssid = "26">6.</S>
			<S sid ="139" ssid = "27">F. M. Suchanek, G. Ifrim, and G. Weikum, “Leila: Learning to extract information by linguistic analysis,” in Proceedings of the 2nd Workshop on Ontology Learning and Population: Bridging the Gap between Text and Knowledge.</S>
			<S sid ="140" ssid = "28">Sydney, Australia: Association for Computational Linguistics, July 2006, pp.</S>
			<S sid ="141" ssid = "29">18–25.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
</PAPER>
