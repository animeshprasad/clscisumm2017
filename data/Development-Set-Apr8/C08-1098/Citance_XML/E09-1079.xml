<PAPER>
	<ABSTRACT>
	<SECTION title="Urdu Language. " number = "1">
			<S sid ="1" ssid = "1">Urdu belongs to the IndoAryan language family.</S>
			<S sid ="2" ssid = "2">It is the national language of Pakistan and is one of the official languages of India.</S>
			<S sid ="3" ssid = "3">The majority of the speakers of Urdu spread over the area of South Asia, South Africa and the United King- dom1.</S>
			<S sid ="4" ssid = "4">Urdu is a free order language with general word order SOV.</S>
			<S sid ="5" ssid = "5">It shares its phonological, morphological and syntactic structures with Hindi.</S>
			<S sid ="6" ssid = "6">Some linguists considered them as two different dialects of one language (Bhatia and Koul, 2000).</S>
			<S sid ="7" ssid = "7">However, Urdu is written in Persoarabic script and inherits most of the vocabulary from Arabic and Persian.</S>
			<S sid ="8" ssid = "8">On the other hand, Hindi is written in Devanagari script and inherits vocabulary from Sanskrit.</S>
			<S sid ="9" ssid = "9">Urdu is a morphologically rich language.</S>
			<S sid ="10" ssid = "10">Forms of the verb, as well as case, gender, and number are expressed by the morphology.</S>
			<S sid ="11" ssid = "11">Urdu represents case with a separate character after the head noun of the noun phrase.</S>
			<S sid ="12" ssid = "12">Due to their separate occurrence and their place of occurrence, they are sometimes considered as postpositions.</S>
			<S sid ="13" ssid = "13">Considering them as case markers, Urdu has no 1 http://www.ethnologue.com/14/show_language.asp?</S>
			<S sid ="14" ssid = "14">code=URD minative, ergative, accusative, dative, instrumental, genitive and locative cases (Butt, 1995: pg 10).</S>
			<S sid ="15" ssid = "15">The Urdu verb phrase contains a main verb, a light verb describing the aspect, and a tense verb describing the tense of the phrase (Hardie, 2003; Hardie, 2003a).</S>
	</SECTION>
	</ABSTRACT>
	<SECTION title="Urdu Tagset. " number = "2">
			<S sid ="16" ssid = "1">There are various questions that need to be answered during the design of a tagset.</S>
			<S sid ="17" ssid = "2">The granularity of the tagset is the first problem in this regard.</S>
			<S sid ="18" ssid = "3">A tagset may consist either of general parts of speech only or it may consist of additional morpho-syntactic categories such as number, gender and case.</S>
			<S sid ="19" ssid = "4">In order to facilitate the tagger training and to reduce the lexical and syntactic ambiguity, we decided to concentrate on the syntactic categories of the language.</S>
			<S sid ="20" ssid = "5">Purely syntactic categories lead to a smaller number of tags which also improves the accuracy of manual tagging2 (Marcus et al., 1993).</S>
			<S sid ="21" ssid = "6">Urdu is influenced from Arabic, and can be considered as having three main parts of speech, namely noun, verb and particle (Platts, 1909; Javed, 1981; Haq, 1987).</S>
			<S sid ="22" ssid = "7">However, some grammarians proposed ten main parts of speech for Urdu (Schmidt, 1999).</S>
			<S sid ="23" ssid = "8">The work of Urdu grammar writers provides a full overview of all the features of the language.</S>
			<S sid ="24" ssid = "9">However, in the perspective of the tagset, their analysis is lackingthe computational grounds.</S>
			<S sid ="25" ssid = "10">The semantic, mor phological and syntactic categories are mixed in their distribution of parts of speech.</S>
			<S sid ="26" ssid = "11">For example,Haq (1987) divides the common nouns into sit uational (smile, sadness, darkness), locative (park, office, morning, evening), instrumental (knife, sword) and collective nouns (army, data).In 2003, Hardie proposed the first com putational part of speech tagset for Urdu (Hardie, 2 A part of speech tagger for Indian languages, available at http://shiva.iiit.ac.in/SPSAL2007 /iiit_tagset_guidelines.pdf Proceedings of the 12th Conference of the European Chapter of the ACL, pages 692–700, Athens, Greece, 30 March – 3 April 2009.</S>
			<S sid ="27" ssid = "12">Qc 2009 Association for Computational Linguistics 2003a).</S>
			<S sid ="28" ssid = "13">It is a morpho-syntactic tagset based on the EAGLES guidelines.</S>
			<S sid ="29" ssid = "14">The tagset contains 350 different tags with information about number, gender, case, etc.</S>
			<S sid ="30" ssid = "15">(van Halteren, 2005).</S>
			<S sid ="31" ssid = "16">The EAGLES guidelines are based on three levels, major word classes, recommended attributes and optional attributes.</S>
			<S sid ="32" ssid = "17">Major word classes include thirteen tags: noun, verb, adjective, pronoun/determiner, article, adverb, adposition, conjunction, numeral, interjection, unassigned, residual and punctuation.</S>
			<S sid ="33" ssid = "18">The recommended attributes include number, gender, case, finite- ness, voice, etc.3 In this paper, we will focus on purely syntactic distributions thus will not go into the details of the recommended attributes of the EAGLES guidelines.</S>
			<S sid ="34" ssid = "19">Considering the EAGLES guidelines and the tagset of Hardie in comparison with the general parts of speech of Urdu, there are no articles in Urdu.</S>
			<S sid ="35" ssid = "20">Due to the phrase level and semantic differences, pronoun and demonstrative are separate parts of speech in Urdu.</S>
			<S sid ="36" ssid = "21">In the Hardie tagset, the possessive pronouns like Y-..</S>
			<S sid ="37" ssid = "22">/mera/ (my), } t&quot;&quot;.: /tumhara/ (your), } &quot;&quot; /humara/ (our) are assigned to the category of possessive adjective.</S>
			<S sid ="38" ssid = "23">Most of the Urdu grammarians consider them as pronouns (Platts, 1909; Javed, 1981; Haq, 1987).</S>
			<S sid ="39" ssid = "24">However, all these possessive pronouns require a noun in their noun phrase, thus show a similar behavior as demonstratives.</S>
			<S sid ="40" ssid = "25">The locative and temporal adverbs (u t&apos; /yahan/ (here), u J /wahan/ (there), y /ab/ (now), etc.) and, the locative and temporal nouns (� .., /subah/ (morning), � ...</S>
			<S sid ="41" ssid = "26">/sham/ (evening), Y,.� /gher/ (home)) appear in a very similar syntactic context.</S>
			<S sid ="42" ssid = "27">In order to keep the structure of pronoun and noun consistent, locative and temporal adverbs are treated as pronouns.</S>
			<S sid ="43" ssid = "28">The tense and aspect of a verb in Urdu is represented by a sequence of auxiliaries.</S>
			<S sid ="44" ssid = "29">Consider the example4: c&apos;&quot; } .:Y � Hai raha Ja kerta kam Jan Is Doing Kept Work John John is kept on doing work “Table 1: The aspect of the verb .:Y /kerta/ (doing) is represented by two separate words /ja/ and } /raha/ and the last word of the sentence c&apos;&quot; /hai/ (is) shows the tense of the verb.”</S>
	</SECTION>
	<SECTION title="The details on the EAGLES guidelines can be found at:. " number = "3">
			<S sid ="45" ssid = "1">http://www.ilc.cnr.it/EAGLES/browse.html</S>
	</SECTION>
	<SECTION title="Urdu is written in right to left direction.. " number = "4">
			<S sid ="46" ssid = "1">The above considerations lead to the following tagset design for Urdu.</S>
			<S sid ="47" ssid = "2">The general parts of speech are noun, pronoun, demonstrative, verb, adjective, adverb, conjunction, particle, number and punctuation.</S>
			<S sid ="48" ssid = "3">The further refinement of the tagset is based on syntactic properties.</S>
			<S sid ="49" ssid = "4">The morphologically motivated features of the language are not encoded in the tagset.</S>
			<S sid ="50" ssid = "5">For example, an Urdu verb has 60 forms which are morphologically derived from its root form.</S>
			<S sid ="51" ssid = "6">All these forms are annotated with the same category i.e. verb.</S>
			<S sid ="52" ssid = "7">During manual tagging, some words are hard for the linguist to disambiguate reliably.</S>
			<S sid ="53" ssid = "8">In order to keep the training data consistent, such words are assigned a separate tag.</S>
			<S sid ="54" ssid = "9">For instance, the semantic marker c&apos;&quot;&quot; /se/ gets a separate tag due to its various confusing usages such as loca tive and instrumental (Platts, 1909).</S>
			<S sid ="55" ssid = "10">The tagset used in the experiments reported in this paper contains 42 tags including three special tags.</S>
			<S sid ="56" ssid = "11">Nouns are divided into noun (NN)and proper name (PN).</S>
			<S sid ="57" ssid = "12">Demonstratives are di vided into personal (PD), KAF (KD), adverbial (AD) and relative demonstratives (RD).</S>
			<S sid ="58" ssid = "13">All four categories of demonstratives are ambiguous withfour categories of pronouns.</S>
			<S sid ="59" ssid = "14">Pronouns are di vided into six types i.e. personal (PP), reflexive (RP), relative (REP), adverbial (AP), KAF (KP) and adverbial KAF (AKP) pronouns.</S>
			<S sid ="60" ssid = "15">Based on phrase level differences, genitive reflexive (GR)and genitive (G) are kept separate from pronouns.</S>
			<S sid ="61" ssid = "16">The verb phrase is divided into verb, aspectual auxiliaries and tense auxiliaries.</S>
			<S sid ="62" ssid = "17">Numer als are divided into cardinal (CA), ordinal (OR),fractional (FR) and multiplicative (MUL).</S>
			<S sid ="63" ssid = "18">Con junctions are divided into coordinating (CC) and subordinating (SC) conjunctions.</S>
			<S sid ="64" ssid = "19">All semantic markers except c&apos;&quot;&quot; /se/ are kept in one category.</S>
			<S sid ="65" ssid = "20">Adjective (ADJ), adverb (ADV), quantifier (Q), measuring unit (U), intensifier (I), interjection (INT), negation (NEG) and question words(QW) are handled as separate categories.</S>
			<S sid ="66" ssid = "21">Adjec tival particle (A), KER (KER), SE (SE) and WALA (WALA) are ambiguous entities which are annotated with separate tags.</S>
			<S sid ="67" ssid = "22">A complete listof the tags with the examples is given in appen dix A. The examples of the weird categories such as WALA, KAF pronoun, KAF demonstratives, etc. are given in appendix B. 3 Tagging Methodologies.</S>
			<S sid ="68" ssid = "23">The work on automatic part of speech tagging started in early 1960s.</S>
			<S sid ="69" ssid = "24">Klein and Simmons (1963) rule based POS tagger can be considered as the first automatic tagging system.</S>
			<S sid ="70" ssid = "25">In the rule based approach, after assigning each word its potential tags, a list of hand written disambiguation rules are used to reduce the number of tags to one (Klein and Simmons, 1963; Green and Rubin, 1971; Hindle, 1989; Chanod and Tapanainen 1994).</S>
			<S sid ="71" ssid = "26">A rule based model has the disadvantage of requiring lots of linguistic efforts to write rules for the language.</S>
			<S sid ="72" ssid = "27">Data-driven approaches resolve this problem by automatically extracting the information from an already tagged corpus.</S>
			<S sid ="73" ssid = "28">Ambiguity between the tags is resolved by selecting the most likely tag for a word (Bahl and Mercer, 1976; Church, 1988; Brill, 1992).</S>
			<S sid ="74" ssid = "29">Brill’s transformation based tagger uses lexical rules to assign each word the most frequent tag and then applies contextual rules over and over again to get a high accuracy.</S>
			<S sid ="75" ssid = "30">However, Brill’s tagger requires training on a large number of rules which reduces the efficiency of machine learning process.</S>
			<S sid ="76" ssid = "31">Statistical approaches usually achieve an accuracy of 96%-97% (Hardie, 2003: 295).</S>
			<S sid ="77" ssid = "32">However, statistical taggers require a large training corpus to avoid data sparseness.</S>
			<S sid ="78" ssid = "33">The problem of low frequencies can be resolved by applying different methods such as smoothing, decision trees, etc. In the next section, an overview of the statistical taggers is provided which are evaluated on the Urdu tagset.</S>
			<S sid ="79" ssid = "34">3.1 Probabilistic Disambiguation.</S>
			<S sid ="80" ssid = "35">The Hidden Markov model is the most widely used method for statistical part of speech tagging.</S>
			<S sid ="81" ssid = "36">Each tag is considered as a state.</S>
			<S sid ="82" ssid = "37">States are connected by transition probabilities which represent the cost of moving from one state to another.</S>
			<S sid ="83" ssid = "38">The probability of a word having a particular tag is called lexical probability.</S>
			<S sid ="84" ssid = "39">Both, the transitional and the lexical probabilities are used to select the tag of a particular word.</S>
			<S sid ="85" ssid = "40">As a standard HMM tagger, The TnT tagger is used for the experiments.</S>
			<S sid ="86" ssid = "41">The TnT tag- ger is a trigram HMM tagger in which the transition probability depends on two preceding tags.</S>
			<S sid ="87" ssid = "42">The performance of the tagger was tested on NEGRA corpus and Penn Treebank corpus.</S>
			<S sid ="88" ssid = "43">The average accuracy of the tagger is 96% to 97% (Brants, 2000).</S>
			<S sid ="89" ssid = "44">The second order Markov model used by the TnT tagger requires large amounts of taggeddata to get reasonable frequencies of POS tri grams.</S>
			<S sid ="90" ssid = "45">The TnT tagger smooths the probability with linear interpolation to handle the problem of data sparseness.</S>
			<S sid ="91" ssid = "46">The Tags of unknown words are predicted based on the word suffix.</S>
			<S sid ="92" ssid = "47">The longest ending string of an unknown word having one or more occurrences in the training corpus is considered as a suffix.</S>
			<S sid ="93" ssid = "48">The tag probabilities of a suffix are evaluated from all the words in the training corpus (Brants, 2000).</S>
			<S sid ="94" ssid = "49">In 1994, Schmid proposed a probabilistic part of speech tagger very similar to a HMM based tagger.</S>
			<S sid ="95" ssid = "50">The transition probabilities are calculated by decision trees.</S>
			<S sid ="96" ssid = "51">The decision tree merges infrequent trigrams with similar contexts until the trigram frequencies are large enough toget reliable estimates of the transition probabili ties.</S>
			<S sid ="97" ssid = "52">The TreeTagger uses an unknown word POS guesser similar to that of the TnT tagger.</S>
			<S sid ="98" ssid = "53">The TreeTagger was trained on 2 million words of the PennTreebank corpus and was evaluated on 100,000 words.</S>
			<S sid ="99" ssid = "54">Its accuracy is compared against a trigram tagger built on the same data.</S>
			<S sid ="100" ssid = "55">The TreeTagger showed an accuracy of 96.06% (Schmid, 1994a).In 2004, Giménez and Màrquez pro posed a part of speech tagger (SVM tool) basedon support vector machines and reported accura cy higher than all state-of-art taggers.</S>
			<S sid ="101" ssid = "56">The aim of the development was to have a simple, efficient, robust tagger with high accuracy.</S>
			<S sid ="102" ssid = "57">The support vector machine does a binary classification of the data.</S>
			<S sid ="103" ssid = "58">It constructs an N-dimensional hyperplane that separates the data into positive and negative classes.</S>
			<S sid ="104" ssid = "59">Each data element is considered as a vector.</S>
			<S sid ="105" ssid = "60">Those vectors which are close to the separating hyperplane are called support vectors5.</S>
			<S sid ="106" ssid = "61">A support vector machine has to be trained for each tag.</S>
			<S sid ="107" ssid = "62">The complexity is controlled by introducing a lexicon extracted from the training data.</S>
			<S sid ="108" ssid = "63">Each word tag pair in the training corpus is considered as a positive case for that tag class and all other tags in the lexicon are considered negative cases for that word.</S>
			<S sid ="109" ssid = "64">This feature avoids generating useless cases for the comparison of classes.</S>
			<S sid ="110" ssid = "65">The SVM tool was evaluated on the English Penn Treebank.</S>
			<S sid ="111" ssid = "66">Experiments were conducted using both polynomial and linear kernels.</S>
			<S sid ="112" ssid = "67">When using n-gram features, the linear kernel showed a significant improvement in speed and accuracy.</S>
			<S sid ="113" ssid = "68">Unknown words are considered as the most ambiguous words by assigning them all open class POS tags.</S>
			<S sid ="114" ssid = "69">The disambiguation of unknowns uses features such as prefixes, suffixes,</S>
	</SECTION>
	<SECTION title="Andrew Moore:. " number = "5">
			<S sid ="115" ssid = "1">http://www.autonlab.org/tutorials/svm.html upper case, lower case, word length, etc. On the Penn Treebank corpus, SVM tool showed an accuracy of 97.16% (Giménez and Màrquez, 2004).</S>
			<S sid ="116" ssid = "2">words by artificially marking some known words as unknown words and then learning the model.</S>
			<S sid ="117" ssid = "3">In 2008, Schmid and Florian proposed a probabilistic POS tagger for fine grained tagsets.</S>
			<S sid ="118" ssid = "4">The basic idea is to consider POS tags as sets of attributes.</S>
			<S sid ="119" ssid = "5">The context probability of a tag is the product of the probabilities of its attributes.</S>
			<S sid ="120" ssid = "6">The probability of an attribute given the previous tags is estimated with a decision tree.</S>
			<S sid ="121" ssid = "7">The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).</S>
			<S sid ="122" ssid = "8">“Table 2: Statistics of training and test data.”The RF tagger is well suited for lan guages with a rich morphology and a large fine grained tagset.</S>
			<S sid ="123" ssid = "9">The RF tagger was evaluated onthe German Tiger Treebank and Czech Academ ic corpus which contain 700 and 1200 POS tags, respectively.</S>
			<S sid ="124" ssid = "10">The RF tagger achieved a higher accuracy than TnT and SVMTool.</S>
			<S sid ="125" ssid = "11">Urdu is a morphologically rich language.</S>
			<S sid ="126" ssid = "12">Training a tagger on a large fine grained tagset requires a large training corpus.</S>
			<S sid ="127" ssid = "13">Therefore, the tagset which we are using for these experimentsis only based on syntactic distributions.</S>
			<S sid ="128" ssid = "14">However, it is always interesting to evaluate new dis ambiguation ideas like RF tagger on different languages.</S>
			<S sid ="129" ssid = "15">4 Experiments.</S>
			<S sid ="130" ssid = "16">A corpus of approx 110,000 tokens was taken from a news corpus (www.jang.com.pk).</S>
			<S sid ="131" ssid = "17">In the filtering phase, diacritics were removed from the text and normalization was applied to keep the Unicode of the characters consistent.</S>
			<S sid ="132" ssid = "18">The problem of space insertion and space deletion was manually solved and space is defined as the word boundary.</S>
			<S sid ="133" ssid = "19">The data was randomly divided into two parts, 90% training corpus and 10% test corpus.</S>
			<S sid ="134" ssid = "20">A part of the training set was also used as held out data to optimize the parameters of the taggers.</S>
			<S sid ="135" ssid = "21">The statistics of the training corpus and test corpus are shown in table 2 and table 3.</S>
			<S sid ="136" ssid = "22">The optimized parameters of the TreeTagger are context size 2, with minimum information gain for decision tree 0.1 and information gain at leaf node 1.4.</S>
			<S sid ="137" ssid = "23">For TnT, a default trigram tagger is used with suffix length of 10, sparse data mode 4 with lambda1 0.03 and lambda2 0.4.</S>
			<S sid ="138" ssid = "24">The RF tagger uses a context length of 4 with threshold of suffix tree pruning 1.5.</S>
			<S sid ="139" ssid = "25">The SVM tool is trained at right to left direction with model 4.</S>
			<S sid ="140" ssid = "26">Model 4 improves the detection of unknown “Table 3: Eight most frequent tags in the test corpus.” In the first experiment, no external lexicon was provided.</S>
			<S sid ="141" ssid = "27">The types from the training corpus were used as the lexicon by the tagger.</S>
			<S sid ="142" ssid = "28">SVM tool showed the best accuracy for both known and unknown words.</S>
			<S sid ="143" ssid = "29">Table 4 shows the accuracies of all the taggers.</S>
			<S sid ="144" ssid = "30">The baseline result where each word is annotated with its most frequent tag, irrespective of the context, is 88.0%.</S>
			<S sid ="145" ssid = "31">Tn T ta gg er Tr ee Ta gg er RF ta gg er SV M ta gg er 93 .4 0 % 93 .0 2 % 93 .2 8 % 94 .1 5 % K n o w n 95 .7 8 % 95 .6 0 % 95 .6 8 % 96 .1 5 % U n k n o w n 68 .4 4 % 65 .9 2 % 68 .0 8 % 73 .2 1 % “Table 4: Accuracies of the taggers without using any external lexicon.</S>
			<S sid ="146" ssid = "32">SVM tool shows the best result for both known and unknown words.” The taggers show poor accuracy while detecting proper names.</S>
			<S sid ="147" ssid = "33">In most of the cases, proper name is confused with adjective and noun.</S>
			<S sid ="148" ssid = "34">This is because in Urdu, there is no clear distinction between noun and proper name.</S>
			<S sid ="149" ssid = "35">Also, the usage of an adjective as a proper name is a frequent phenomenon in Urdu.</S>
			<S sid ="150" ssid = "36">The accuracies of open class tags are shown in table 5.</S>
			<S sid ="151" ssid = "37">The detailed discussion on the results of the taggers is done after providing an external lexicon to the taggers.</S>
			<S sid ="152" ssid = "38">T a g Tn T ta gg er T r e e - T a g g e r RF ta gg er SV M ta gg er V B 93 .2 0 % 91 .8 6 % 92 .6 8 % 94 .2 3 % N N 94 .1 2 % 96 .2 1 % 93 .8 9 % 96 .4 5 % P N 73 .2 0 % 66 .8 8 % 72 .7 7 % 68 .6 2 % A D V 75 .9 4 % 72 .7 8 % 74 .6 8 % 72 .1 5 % A DJ 85 .6 7 % 80 .7 8 % 86 .5 % 85 .8 8 % “Table 5: Accuracies of open class tags without having an external lexicon” In the second stage of the experiment, a large lexicon consisting of 70,568 types was pro- vided6.</S>
			<S sid ="153" ssid = "39">After adding the lexicon, there are 112 unknown tokens and 81 unknown types in the test corpus7.</S>
			<S sid ="154" ssid = "40">SVM tool again showed the best accuracy of 95.66%.</S>
			<S sid ="155" ssid = "41">Table 6 shows the accuracy of the taggers.</S>
			<S sid ="156" ssid = "42">The results of open class words significantly improve due to the smaller number of unknown words in the test corpus.</S>
			<S sid ="157" ssid = "43">The total accuracy of open class tags and their accuracy on unknown words are given in table 7 and table 8 respectively.</S>
			<S sid ="158" ssid = "44">“Table 8: Accuracies of open class tags on unknown words.</S>
			<S sid ="159" ssid = "45">The number of unknown words with tag VB and ADJ are less than 10 in this experiment.” The results of the taggers are analyzed by finding the most frequently confused pairs for all the taggers.</S>
			<S sid ="160" ssid = "46">It includes both the known and unknown words.</S>
			<S sid ="161" ssid = "47">Only those pairs are added in the table which have an occurrence of more than 10.</S>
			<S sid ="162" ssid = "48">Table 9 shows the results.</S>
			<S sid ="163" ssid = "49">Tn T tag ge r Tr ee Ta gg er RF ta gg er SV M to ol 94 .9 1 % 95 .1 7 % 95 .2 6 % 95 .6 6 % K n o w n 95 .4 2 % 95 .6 5 % 95 .6 6 % 96 .1 1 % U n k n o w n 56 .2 5 % 58 .0 4 % 64 .6 0 % 61 .6 1 % “Table 6: Accuracies of the taggers after adding the lexicon.</S>
			<S sid ="164" ssid = "50">SVM tool shows the best accuracy for known word disambiguation.</S>
			<S sid ="165" ssid = "51">RF tagger shows the best accuracy for unknown words.” Ta g Tn T ta gg er T r e e - T a g g e r RF ta gg er SV M to ol V B 95 .8 8 % 95 .8 8 % 96 .5 8 % 96 .8 0 % N N 94 .6 4 % 95 .8 5 % 94 .7 9 % 96 .6 4 % P N 86 .9 2 % 79 .7 3 % 84 .9 6 % 81 .7 0 % A D V 82 .2 8 % 79 .1 1 % 81 .6 4 % 81 .0 1 % A DJ 91 .5 9 % 89 .8 2 % 92 .3 7 % 88 .2 6 % “Table 7: Accuracies of open class tags after adding an external lexicon.”</S>
	</SECTION>
	<SECTION title="Additional lexicon is taken from CRULP,  Lahore, Paki-. " number = "6">
			<S sid ="166" ssid = "1">stan (www.crulp.org).vided by each tagger.</S>
			<S sid ="167" ssid = "2">No probability distribution informa tion was given with the lexicon.</S>
			<S sid ="168" ssid = "3">“Table 9: Most frequently confused tag pairs with total number of occurrences.” 5 Discussion The output of table 9 can be analyzed in many ways e.g. ambiguous tags, unknown words, open class tags, close class tags, etc. In the close class tags, the most frequent errors are between demonstrative and pronoun, and between KER tag and semantic marker (P).</S>
			<S sid ="169" ssid = "4">The difference between demonstrative and pronoun is at the phrase level.</S>
			<S sid ="170" ssid = "5">Demonstratives are followed by a noun which belongs to the same noun phrase whereas pronouns form a noun phrase by itself.</S>
			<S sid ="171" ssid = "6">Taggers analyze the language in a flat structure and are unable to handle the phrase level differences.</S>
			<S sid ="172" ssid = "7">It is interesting to see that the SVM tool shows a clear improvement in detecting the phrase level differences over the other taggers.</S>
			<S sid ="173" ssid = "8">It might be due to the SVM tool ability to look not only at the neighboring tags but at the neighboring words as well.</S>
			<S sid ="174" ssid = "9">“Table 11: (a) Verbal noun with semantic marker, (b) syntactic structure of KER tag.”8 c&apos;&quot;� - � (a) � ’ oJ All the taggers other than the SVM tool have difficulti es to disambigu ate between KER tags Gay gayain Gana log Voh TA VB NN NN PD Will sing Song people Those and semantic markers.</S>
			<S sid ="175" ssid = "10">(a) Those people will sing a song.</S>
			<S sid ="176" ssid = "11">(b) J }’ ’ u’�’ &quot;&quot;.:}JY do khoraak Ko log z a r o r a t m a n d c&apos;&quot;� - � � oJ VB N N P NN ADJ Gay Gayain gana Voh TA VB NN PP Will Sing Song those Those will sing a song.</S>
			<S sid ="177" ssid = "12">give food To people needy Give food to the needy people “Table 10: The word oJ /voh/ is occurring both as pronoun and demonstrative.</S>
			<S sid ="178" ssid = "13">In both of the cases, it is followed by a noun.</S>
			<S sid ="179" ssid = "14">But looking at the phrases, demonstrative oJ has the noun inside the noun phrase.” The second most frequent error among the closed class tags is the distinction between the KER tag c&apos;&quot; /kay/ and the semantic marker c&apos;&quot; /kay/.</S>
			<S sid ="180" ssid = "15">The KER tag always takes a verb before it and the semantic marker always takes a noun before it.</S>
			<S sid ="181" ssid = "16">The ambiguity arises when a verbal noun occurs.</S>
			<S sid ="182" ssid = "17">In the tagset, verbal nouns are handled as verb.</S>
			<S sid ="183" ssid = "18">Syntactically, verbal nouns occur at the place of a noun and can also take a semantic marker after them.</S>
			<S sid ="184" ssid = "19">This decreases the accuracy in two ways; the wrong disambiguation of KER tag and the wrong disambiguation of unknown verbal nouns.</S>
			<S sid ="185" ssid = "20">Due to the small amount of training data, unknown words are frequent in the test corpus.</S>
			<S sid ="186" ssid = "21">Whenever an unknown word occurs at the place of a noun, the most probable tag for that word will be noun which is wrong in our case.</S>
			<S sid ="187" ssid = "22">Table 11 shows an example of such a scenario.</S>
			<S sid ="188" ssid = "23">(a) o After doing work (b) c&apos;&quot; Y � kay ker kam KER VB NN -- Doing work (After) doing work Give food to the needy “Table 12: (a) Occurrence of adjective with noun, (b) dropping of main noun from the noun phrase.</S>
			<S sid ="189" ssid = "24">In that case, adjective becomes the noun.” Coming to open class tags, the most frequent errors are between noun and the other open class tags in the noun phrase like proper noun, adjective and adverb.</S>
			<S sid ="190" ssid = "25">In Urdu, there is no clear distinction between noun and proper noun.</S>
			<S sid ="191" ssid = "26">The phenomenon of dropping of words is also frequent in Urdu.</S>
			<S sid ="192" ssid = "27">If a noun in a noun phrase is dropped, the adjective becomes a noun in that phrase (see table 12).</S>
			<S sid ="193" ssid = "28">The ambiguity between noun and verb is due to verbal nouns as explained above (see table 11).</S>
			<S sid ="194" ssid = "29">6 Conclusion.</S>
			<S sid ="195" ssid = "30">In this paper, probabilistic part of speech tagging technologies are tested on the Urdu language.</S>
			<S sid ="196" ssid = "31">The main goal of this work is to investigate whether general disambiguation techniques and standard POS taggers can be used for the tagging of Urdu.</S>
			<S sid ="197" ssid = "32">The results of the taggers clearly answer this question positively.</S>
			<S sid ="198" ssid = "33">With the small training corpus, all the taggers showed accuracies around 95%.</S>
			<S sid ="199" ssid = "34">The SVM tool shows the best accuracy in 8 One possible solution to this problem could be to intro-.</S>
			<S sid ="200" ssid = "35">duce a separate tag for verbal nouns which will certainly remove the ambiguity between the KER tag and the semantic marker and reduce the ambiguity between verb and noun.</S>
			<S sid ="201" ssid = "36">disambiguating the known words and the RF tagger shows the best accuracy in detecting the tags of unknown words.</S>
			<S sid ="202" ssid = "37">Appendices Appendix A. Urdu part of speech tagset Following is the complete list of the tags of Urdu.</S>
			<S sid ="203" ssid = "38">There are some occurrences in which two Urdu words are mapped to the same translation of English.</S>
			<S sid ="204" ssid = "39">There are two reasons for that, either the Urdu words have different case or there is no significant meaning difference between the two words which can be described by different English translations.</S>
			<S sid ="205" ssid = "40">“Table 13: Tagset of Urdu” 9 Polite form of you which is used while talking with the elders and.</S>
			<S sid ="206" ssid = "41">with the strangers 10 They always occur with a verb and can not be translated.</S>
			<S sid ="207" ssid = "42">stand- alone.</S>
			<S sid ="208" ssid = "43">Appendix B. Examples of WALA, Noun with - c&apos;&quot;.: c&apos;&quot;,.</S>
			<S sid ="209" ssid = "44">�\ ’ u locative behavior, KAF pronoun and KAF demonstrative and multiplicative.</S>
			<S sid ="210" ssid = "45">WALA &apos;i J: Which one like mangoes?</S>
			<S sid ="211" ssid = "46">Adverbial KAF pronoun c&apos;&quot; -� YA oJ Where did he go?</S>
			<S sid ="212" ssid = "47">“Table 17: Examples of KAF pronoun and KAF demonstrative</S>
	</SECTION>
</PAPER>
