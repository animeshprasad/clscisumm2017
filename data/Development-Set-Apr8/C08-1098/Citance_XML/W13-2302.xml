<PAPER>
	<ABSTRACT></ABSTRACT>
	<SECTION title="Introduction1. " number = "1">
			<S sid ="1" ssid = "1">Part-of-speech (POS) tagging of modern language data is a well-explored field, commonly achieving accuracies around 97% (Brants, 2000; Schmid and Laws, 2008).</S>
			<S sid ="2" ssid = "2">For historical language varieties, the situation is worse, as specialized taggers are typically not available.</S>
			<S sid ="3" ssid = "3">As an example, a study by Scheible et al.</S>
			<S sid ="4" ssid = "4">(2011a) reports an average tagging accuracy of 69.6% for Early Modern German texts.</S>
			<S sid ="5" ssid = "5">However, with projects to create historical corpora being on the rise (Sa´nchezMarco et al., 2010; Scheible et al., 2011b, are recent examples), the need for more accurate tagging methods on these types of data increases.</S>
			<S sid ="6" ssid = "6">A common approach for historical texts is to use spelling normalization to map historical word- forms to modern ones (Baron and Rayson, 2008; Jurish, 2010).</S>
			<S sid ="7" ssid = "7">Manually normalized data was found to improve POS tagging accuracy for a variety of languages such as German, English, and Portuguese, with accuracies between 79% and 91% (Scheible et al., 2011a; Rayson et al., 2007; Hendrickx and Marquilhas, 2011).</S>
			<S sid ="8" ssid = "8">1 I would like to thank the anonymous reviewers for their helpful comments.</S>
			<S sid ="9" ssid = "9">The research reported here was supported by Deutsche Forschungsgemeinschaft (DFG), Grants DI 1558/41 and DI 1558/51.</S>
			<S sid ="10" ssid = "10">This paper presents results for POS tagging of historical German from 1400 to 1770, classified here as Early New High German (ENHG), using automatic spelling normalization to preprocess the data for a POS tagger trained on modern German corpora.</S>
			<S sid ="11" ssid = "11">To train the normalization tool, short fragments of a few hundred tokens are used for each text.</S>
			<S sid ="12" ssid = "12">This approach allows for a better adaptation to the individual spelling characteristics of each text while requiring only small amounts of training data.</S>
			<S sid ="13" ssid = "13">Additionally, different ways to deal with typical obstacles for processing historical texts (e.g., inconsistent use of punctuation) are compared.</S>
			<S sid ="14" ssid = "14">The structure of this paper is as follows.</S>
			<S sid ="15" ssid = "15">Sec.</S>
			<S sid ="16" ssid = "16">2 presents the historical texts used for the evaluation.</S>
			<S sid ="17" ssid = "17">Sec.</S>
			<S sid ="18" ssid = "18">3 describes the approach to normalization, while Sec.</S>
			<S sid ="19" ssid = "19">4 discusses problems and results of POS tagging on normalized data.</S>
			<S sid ="20" ssid = "20">Sec.</S>
			<S sid ="21" ssid = "21">5 presents related work, and Sec.</S>
			<S sid ="22" ssid = "22">6 concludes.</S>
	</SECTION>
	<SECTION title="Corpora. " number = "2">
			<S sid ="23" ssid = "1">This study considers texts from two corpora of historical German: the Anselm corpus (Dipper and SchultzBalluff, 2013) and the GerManC-GS corpus (Scheible et al., 2011b).</S>
			<S sid ="24" ssid = "2">The Anselm corpus consists of more than 50 different versions of a medieval religious treatise written up in various German dialects.</S>
			<S sid ="25" ssid = "3">As the creation of gold-standard annotations for the corpus is still in progress, only two texts are used here: a manuscript in an Eastern Upper German dialect kept in Melk, Austria; and an Eastern Central German manuscript kept in Berlin.</S>
			<S sid ="26" ssid = "4">Both manuscripts are dated to the 15th century.</S>
			<S sid ="27" ssid = "5">The GerManC-GS corpus aims to be a representative subcorpus of GerManC with additional gold-standard annotations.</S>
			<S sid ="28" ssid = "6">It contains texts from Early Modern German categorized by genre, region, and time period.</S>
			<S sid ="29" ssid = "7">For this study, the three texts of the genre “sermon” are used.</S>
			<S sid ="30" ssid = "8">They are 11 Proceedings of the 7th Linguistic Annotation Workshop &amp; Interoperability with Discourse, pages 11–18, Sofia, Bulgaria, August 89, 2013.</S>
			<S sid ="31" ssid = "9">Qc 2013 Association for Computational Linguistics Corpus Date Name Tokens Anselm 15c Berlin 5,399 15c Melk 4,783 3 Normalization Spelling normalization is performed using the Norma tool (Bollmann, 2012).</S>
			<S sid ="32" ssid = "10">It implements a chain of normalization methods—to the effect that GerManC- GS 1677 LeichS ermon 2,585 1730 JubelF este 2,523 1770 Gottes dienst 2,292 methods further down the chain are only called if previous ones failed to produce a result—in the following order: (1) wordlist mapping; (2) rule Table 1: Texts used for the evaluation dated from 1677 to 1770, which makes them considerably newer than the Anselm texts.</S>
			<S sid ="33" ssid = "11">Table 1 gives an overview of all texts used here.</S>
			<S sid ="34" ssid = "12">All texts are manually annotated with normalizations and POS tags.</S>
			<S sid ="35" ssid = "13">In the normalization layer, tokens are mapped to modern German equivalents.</S>
			<S sid ="36" ssid = "14">The normalization schemes are not identical, but roughly comparable for both GerManC-GS and Anselm (see Scheible et al.</S>
			<S sid ="37" ssid = "15">(2011b) and Bollmann et al.</S>
			<S sid ="38" ssid = "16">(2012) for details).</S>
			<S sid ="39" ssid = "17">In both corpora, POS tagging follows the STTS tagset (Schiller et al., 1999) without morphological information, though some additional tags were introduced in GerManC-GS.</S>
			<S sid ="40" ssid = "18">For our evaluation, they are mapped back to standard STTS tags; this mapping only affects 80 tokens from all three texts.</S>
			<S sid ="41" ssid = "19">Additionally, both corpora are annotated with modern punctuation and sentence boundaries; however, while modern punctuation is a separate annotation layer in Anselm, there is always a 1:1 correspondence between historical and modern (i.e., normalized) punctuation marks in GerManC-GS.</S>
			<S sid ="42" ssid = "20">Finally, both corpora preserve many spelling characteristics of the original manuscripts, e.g., superposition of characters such as u˚ , or abbreviation marks such as the nasal bar (as in v¯).</S>
			<S sid ="43" ssid = "21">Before any further processing, all wordforms are sim based normalization; and (3) weighted Levenshtein distance.</S>
			<S sid ="44" ssid = "22">Wordlist mapping considers simple 1:1 mappings of historical wordforms to modern ones (e.g., vnd → und “and”), while rule-based normalization applies context-sensitive character rewrite rules (e.g., transform v to u between a word boundary and n) to an input string from left to right.</S>
			<S sid ="45" ssid = "23">Weighted Levenshtein distance assigns individual weights to character replacements (e.g., v → u), and performs normalization by retrieving the wordform from a modern lexicon which can be derived from the historical input wordform with the lowest cost, i.e., using a sequence of edit operations with the lowest sum of weights.</S>
			<SUBSECTION>3.1 Normalization procedure.</SUBSECTION>
			<S sid ="46" ssid = "24">All normalization algorithms described above require some kind of parametrization to work (i.e., a wordlist; rewrite rules; Levenshtein weights).</S>
			<S sid ="47" ssid = "25">These parametrizations are neither hard-coded nor manually defined, but are derived automatically by the Norma tool from a set of manually normalized training data.</S>
			<S sid ="48" ssid = "26">For this purpose, short samples from the text to be normalized are used; i.e., training set and evaluation set are always disjoint parts of the same text.</S>
			<S sid ="49" ssid = "27">The reasons for choosing this approach lie in the individual spelling characteristics of the texts—the following examples show excerpts from Berlin, Melk, and LeichSermon, respectively, along with their (gold-standard) normalizations: plified to plain alphabetic characters; e.g., u˚ is mapped to uo.</S>
			<S sid ="50" ssid = "28">For some abbreviation marks in (1) dyn dein lybes liebes kynt kindthe Anselm corpus, there is no clear “best” sim plification: the nasal bar is a prime example here, “your dear child” which should be simplified most appropriately to e, (e)n, (e)m, or nothing, either before or after the (2) mein mein liebs liebes chind kind letter on which it is placed, depending on context.</S>
			<S sid ="51" ssid = "29">“my dear child” In these cases, manually defined heuristics were used to guess the most appropriate mapping.</S>
			<S sid ="52" ssid = "30">As capitalization is not used consistently in the texts, (3) eins eins ihrer ihrer andern anderen kinder kinder all letters were additionally lowercased.</S>
			<S sid ="53" ssid = "31">“one of their other children” Text Baseline Normalizations 1 0 0 2 5 0 5 0 0 1, 0 0 0 Be rli n 2 3.</S>
			<S sid ="54" ssid = "32">0 5 % 68.</S>
			<S sid ="55" ssid = "33">99 % 75 .0 2 % 79 .1 4 % 81 .8 3 % M el k 3 9.</S>
			<S sid ="56" ssid = "34">3 2 % 69.</S>
			<S sid ="57" ssid = "35">10 % 74 .3 9 % 75 .7 4 % 77 .9 8 % Le ic hS er m on 7 2.</S>
			<S sid ="58" ssid = "36">7 1 % 77.</S>
			<S sid ="59" ssid = "37">96 % 80 .5 1 % 82 .8 5 % 87 .2 3 % Ju be lF est e 7 9.</S>
			<S sid ="60" ssid = "38">4 7 % 88.</S>
			<S sid ="61" ssid = "39">50 % 89 .9 8 % 91 .8 7 % 93 .1 3 % G ott es di en st 8 3.</S>
			<S sid ="62" ssid = "40">4 1 % 93.</S>
			<S sid ="63" ssid = "41">77 % 95 .2 4 % 95 .2 7 % 95 .5 6 % Table 2: Normalization accuracy after training on n tokens and evaluating on 1,000 tokens (average of 10 random training and evaluation sets), compared to the “baseline” score of the full text without any normalization The first two examples, while both dated to the 15th century, show quite different spellings of the modern Kind “child”: Ex.</S>
			<S sid ="64" ssid = "42">(1) shows the frequent use of y for modern ei or i(e), while Ex.</S>
			<S sid ="65" ssid = "43">(2) demonstrates the frequent spelling ch for k. These differences are likely a cause of the different dialectal regions from which the manuscripts originate, but could also be attributed, at least in parts, to individual preferences by the manuscripts’ writers.</S>
			<S sid ="66" ssid = "44">The LeichSermon text from 1677 in Ex.</S>
			<S sid ="67" ssid = "45">(3), on the other hand, already has the modern German spelling Kind.</S>
			<S sid ="68" ssid = "46">Given this range of spelling variations, it seems implausible to achieve good normalization results using the same parametrization for each of the texts.</S>
			<S sid ="69" ssid = "47">Furthermore, for the older manuscripts showing more variation such as in Ex.</S>
			<S sid ="70" ssid = "48">(1), it is unclear what other training data could be used.</S>
			<S sid ="71" ssid = "49">The full GerManC-GS consists of texts from 1650 to 1800, while Jurish (2010) uses a corpus of German texts from 1780 to 1880; these texts are all considerably newer, consequently having less spelling variation than the Anselm texts.</S>
			<S sid ="72" ssid = "50">This lack of appropriate training data applies similarly to all kinds of less-resourced language varieties.</S>
			<S sid ="73" ssid = "51">Therefore, while this approach requires slightly more effort for manually normalizing parts of the texts beforehand, it does not depend on the availability of a large training corpus or a specialized tool for the language variety to be processed.</S>
			<SUBSECTION>3.2 Evaluation.</SUBSECTION>
			<S sid ="74" ssid = "52">Normalization is evaluated separately for each text, using a part of that text for training and evaluating on a different part of the same text.</S>
			<S sid ="75" ssid = "53">To address the question of how much training data is needed, evaluation is performed with different sizes of the training set in a range between 100 and 1,000 tokens.</S>
			<S sid ="76" ssid = "54">The evaluation set is kept at a fixed size of 1,000 tokens.</S>
			<S sid ="77" ssid = "55">Normalization accuracy is calculated by taking the average of 10 trials with randomly drawn training and evaluation sets.</S>
			<S sid ="78" ssid = "56">The results of this evaluation are shown in Table 2.</S>
			<S sid ="79" ssid = "57">The baseline score for a text is defined as the percentage of matching tokens between the unmodified, historical text and its gold-standard normalization.</S>
			<S sid ="80" ssid = "58">There is a clear difference between the Anselm texts, with scores of 23% and 39%, and the GerManC-GS texts, which range from 72% to 83%.</S>
			<S sid ="81" ssid = "59">This shows that spelling variation affects significantly more wordforms in the Anselm texts.</S>
			<S sid ="82" ssid = "60">The age of a text is likely to be the main factor for this, as even within the group of GerManC- GS texts, a clear tendency for newer texts to have higher baseline scores can be observed.</S>
			<S sid ="83" ssid = "61">Spelling normalization with the Norma tool shows rather positive results even for small training samples: with only 100 tokens used for training, it achieves a normalization accuracy of 69% for the Anselm texts, and raises the score for the GerManC-GS texts by 5–10 percentage points.</S>
			<S sid ="84" ssid = "62">Using 250 tokens results in another noticeable increase in accuracy, although the relative gain from increasing the training size even further attenuates after this point.</S>
			<S sid ="85" ssid = "63">4 Part-of-speech tagging.</S>
			<S sid ="86" ssid = "64">While spelling normalization can be useful in itself (e.g., for search queries in the corpus), our main focus is on its usefulness for further processing of the data such as part-of-speech tagging.</S>
			<S sid ="87" ssid = "65">The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) with an increased context size of 10, which we found to perform best on average on our data.</S>
			<S sid ="88" ssid = "66">Text OrigP ModP NoP Berlin 85.78% 87.29% 87.07% Melk 85.21% 87.76% 87.74% LeichSermon 81.22% 80.59% 81.04% JubelFeste 90.41% 90.41% 90.03% Gottesdienst 93.24% 93.24% 92.27% Table 3: Tagging accuracy on the gold-standard normalizations (OrigP = original punctuation, ModP = modern punctuation, NoP = no punctuation) 4.1 Impact of punctuation.</S>
			<S sid ="89" ssid = "67">Normalization tries to handle the problem of spelling inconsistencies found in historical language data.</S>
			<S sid ="90" ssid = "68">However, this is not the only challenge for processing the data with modern POS taggers.</S>
			<S sid ="91" ssid = "69">There is often no consistent capitalization, which can normally be used as a clue to detect nouns in modern German.</S>
			<S sid ="92" ssid = "70">This has already led to all word- forms being lowercased for the normalization process.</S>
			<S sid ="93" ssid = "71">Additionally, punctuation marks are also often used inconsistently or are missing completely: e.g., the Melk manuscript mostly uses virgules (visually resembling a modern slash ‘/’) where modern German would use a full stop, but this is far from a definite rule, and large parts of the Anselm texts feature no punctuation marks at all.</S>
			<S sid ="94" ssid = "72">This raises the question whether punctuation should be used for POS tagging at all for these texts.</S>
			<S sid ="95" ssid = "73">In order to test the impact of punctuation on tagging performance, three scenarios are considered: tagging with original, modern, and no punctuation marks.</S>
			<S sid ="96" ssid = "74">In order to provide a fairer comparison, instead of using the supplied parameter file for German, we retrain RFTagger on a prepared set of data.</S>
			<S sid ="97" ssid = "75">For this purpose, the TIGER corpus (Brants et al., 2002) and version 6 of Tu¨ baD/Z (Telljohann et al., 2004) are used.</S>
			<S sid ="98" ssid = "76">First, the two corpora are combined—with minor modifications to the POS tags to make them uniform—and lowercased.</S>
			<S sid ="99" ssid = "77">The combined corpus has a size of more than 1.6 million tokens.</S>
			<S sid ="100" ssid = "78">Additionally, for the evaluation without punctuation, a separate tagger model is trained on a version of the TIGER/Tu¨ ba corpus where all punctuation marks and sentence boundaries have been removed.Using these tagger models, tagging perfor Original 96.85% Lowercased 96.50% No punctuation and SB 96.22% Lowercased + no punctuation and SB 95.74% Table 4: Tagging accuracy on the combined TIGER/Tu¨ ba corpus, using 10-fold CV, evaluated with and without capitalization, punctuation, and sentence boundaries (SB) mance is evaluated on the gold-standard normalizations with different levels of punctuation.</S>
			<S sid ="101" ssid = "79">The results are shown in Table 3.</S>
			<S sid ="102" ssid = "80">For better comparability, accuracy was evaluated excluding punctuation marks in all scenarios.Tagging with modern punctuation or no punc tuation is shown to be best in all cases, with the difference between these two scenarios never being statistically significant (p &gt; 0.05).</S>
			<S sid ="103" ssid = "81">For the Anselm texts, using the original punctuation is worse than using none at all.</S>
			<S sid ="104" ssid = "82">This is not true for GerManC-GS, though the differences are minor; also, original and modern punctuation are identical for the JubelFeste and Gottesdienst texts, showing that they already follow modern German conventions in this regard.</S>
			<S sid ="105" ssid = "83">The results show that removing all punctuation marks does not lead to significant losses in POS tagging accuracy.</S>
			<S sid ="106" ssid = "84">Indeed, for texts with infrequent and/or inconsistent use of punctuation marks, discarding punctuation is shown to be preferable.</S>
			<S sid ="107" ssid = "85">For these reasons, the tagging approach without punctuation is used for all following experiments.</S>
			<S sid ="108" ssid = "86">4.2 Tagging “with handicaps”.</S>
			<S sid ="109" ssid = "87">So far, the preprocessing of the historical data includes removing all capitalization and punctuation.</S>
			<S sid ="110" ssid = "88">Consequently, information about sentence boundaries should also be removed, as it cannot easily be derived from texts without (consistent) punctuation.</S>
			<S sid ="111" ssid = "89">However, POS tagging with these “handicaps” potentially increases the difficulty of the task in general.</S>
			<S sid ="112" ssid = "90">To gauge the extent of this effect, an evaluation on modern data was performed using 10-fold cross-validation on the combined TIGER/Tu¨ ba corpus, both with and without these artificial modifications.</S>
			<S sid ="113" ssid = "91">Table 4 shows the results of Text Tokens Original Automatically normalized Gold 1 0 0 2 5 0 5 0 0 1, 0 0 0 Be rli n 4, 7 1 9 2 8.</S>
			<S sid ="114" ssid = "92">6 5 % 58.</S>
			<S sid ="115" ssid = "93">68 % 74 .8 9 % 75 .9 5 % 78 .0 3 % 87 .0 7 % M el k 4, 5 5 0 4 4.</S>
			<S sid ="116" ssid = "94">7 0 % 69.</S>
			<S sid ="117" ssid = "95">63 % 74 .0 2 % 76 .2 4 % 78 .6 6 % 87 .7 4 % Le ic hS er m on 2, 2 1 5 6 7.</S>
			<S sid ="118" ssid = "96">9 5 % 72.</S>
			<S sid ="119" ssid = "97">87 % 74 .6 3 % 75 .8 5 % 78 .0 1 % 81 .0 4 % Ju be lF est e 2, 1 3 7 8 2.</S>
			<S sid ="120" ssid = "98">2 6 % 82.</S>
			<S sid ="121" ssid = "99">64 % 83 .6 2 % 86 .5 2 % 87 .7 4 % 90 .0 3 % G ott es di en st 1, 9 5 3 8 8.</S>
			<S sid ="122" ssid = "100">0 7 % 88.</S>
			<S sid ="123" ssid = "101">84 % 90 .2 7 % 91 .3 0 % 91 .6 5 % 92 .2 7 % Table 5: POS tagging accuracy on texts without punctuation and capitalization, for tagging on the original data, the gold-standard normalization, and automatic normalizations using the first n tokens as training data this experiment; tagging accuracy drops from 96.85% to 95.74% when removing capitalization and punctuation.</S>
			<S sid ="124" ssid = "102">While this change is significant (p &lt; 0.01) considering the corpus size, with regard to the effort involved in manually annotating whole texts with modern capitalization and punctuation marks, it seems small enough to make tagging without this information a viable approach for historical data.</S>
			<S sid ="125" ssid = "103">4.3 Tagging historical data.</S>
			<S sid ="126" ssid = "104">POS tagging on the historical texts is evaluated in three different scenarios: first, tagging on the simplified, but otherwise unmodified, original texts; second, tagging on the gold-standard normalizations; and third, tagging on texts which have been normalized automatically as described in Sec.</S>
			<S sid ="127" ssid = "105">3.</S>
			<S sid ="128" ssid = "106">For automatic normalization, the first n tokens of a text were used for training the Norma tool, with different values for n (cf.</S>
			<S sid ="129" ssid = "107">Sec.</S>
			<S sid ="130" ssid = "108">3.2).</S>
			<S sid ="131" ssid = "109">Only the remainder of the text has then been automatically processed by Norma.</S>
			<S sid ="132" ssid = "110">This means that, e.g., for a text with 500 tokens used for training, POS tagging is performed on a version of the text consisting of 500 gold-standard normalizations plus automatically generated normalizations for the remainder of the text.</S>
			<S sid ="133" ssid = "111">This evaluation method models a typical application scenario, where a tradeoff is made between no manual effort (= tagging on the original) and full manual preprocessing (= tagging on the gold-standard).</S>
			<S sid ="134" ssid = "112">Full evaluation results are shown in Table 5.</S>
			<S sid ="135" ssid = "113">Tagging accuracy roughly correlates with normalization accuracy (cf.</S>
			<S sid ="136" ssid = "114">Table 2); it tends to be slightly above the normalization score for Anselm and a few points below that score for GerManC- GS.</S>
			<S sid ="137" ssid = "115">Tagging on the original, historical data is particularly inaccurate for the Anselm texts, with the Berlin text only achieving an accuracy of 28.7%.</S>
			<S sid ="138" ssid = "116">This again highlights the need for specialized tagging methods on such types of data.</S>
			<S sid ="139" ssid = "117">The GerManC-GS texts from the 18th century perform much better without normalization, with accuracies up to 88% for the Gottesdienst text.</S>
			<S sid ="140" ssid = "118">These results mainly confirm the observations that the Anselm texts show much more variety in spelling than the newer texts from GerManC-GS.</S>
			<S sid ="141" ssid = "119">Similar to the results for normalization, using only 100 tokens for training is enough to increase tagging accuracy for the Melk text from 45% to 70%.</S>
			<S sid ="142" ssid = "120">For Berlin, this method results in an even higher relative increase, more than doubling the number of correct POS tags.</S>
			<S sid ="143" ssid = "121">Results for these texts can be improved further to about 74% when using 250 tokens for training; after this figure, POS tagging seems to profit less from increasing the size of the training set, with accuracies around 78% for a training set of 1,000 tokens.</S>
			<S sid ="144" ssid = "122">The GerManC-GS texts, particularly JubelFeste and Gottesdienst, do not benefit as much from a small number of training tokens.</S>
			<S sid ="145" ssid = "123">With 100 tokens, POS tagging accuracy only increases by 0.38–0.77 percentage points.</S>
			<S sid ="146" ssid = "124">However, these texts already have a comparatively high baseline to start with (82–88%).</S>
			<S sid ="147" ssid = "125">As they are already much closer to modern German spelling, fewer wordforms have spelling variations at all; consequently, more training data is required to capture a similar amount of variant wordforms as in the Anselm texts.</S>
			<S sid ="148" ssid = "126">Indeed, when increasing the training portion to 1,000 tokens, the benefit of spelling normalization becomes more pronounced.</S>
			<S sid ="149" ssid = "127">Curiously, for the LeichSermon text, even the gold-standard normalization only achieves 81% accuracy, which is significantly lower than for any other text in the evaluation.</S>
			<S sid ="150" ssid = "128">This is un expected, considering that the text is much more recent than Berlin and Melk.</S>
			<S sid ="151" ssid = "129">The reason for this discrepancy is the frequent use of bible verse numbers in LeichSermon, which are written as numer which is mapped to the artificial lemma zehant, but would rather be expressed as sofort in modern German: als followed by a dot and annotated as CARD (cardinal number) in the gold-standard data.</S>
			<S sid ="152" ssid = "130">In the TIGER corpus and Tu¨ baD/Z, such numerals are (5) czuhant zehant ADV chust ku¨ sst VVFIN iudas judas NE mein mein PPOSAT chint kind NN treated as ordinal numbers and tagged as ADJA, leading to a high number of mismatching tags.</S>
			<S sid ="153" ssid = "131">4.4 Error analysis.</S>
			<S sid ="154" ssid = "132">POS tagging results for the historical texts are still considerably worse than those for modern data, even when tagging on gold-standard normalizations (81–92% vs. 95.74%).</S>
			<S sid ="155" ssid = "133">There are several factors responsible for this.</S>
			<S sid ="156" ssid = "134">It is important to observe that even perfectly normalized historical data has different characteristics than modern data, as normalization only affects the spelling of wordforms.</S>
			<S sid ="157" ssid = "135">One potential “Immediately, Judas kisses my child” Finally, a significant number of errors appears to result from limitations of the modern TIGER/Tu¨ ba corpus used to train the POS tag- ger.</S>
			<S sid ="158" ssid = "136">This corpus is created from newspaper texts, which are typically written in a rather formal style.</S>
			<S sid ="159" ssid = "137">The Anselm texts, on the other hand, consist of question/answer sets which contain a lot of direct speech.</S>
			<S sid ="160" ssid = "138">Similarly, the Gottesdienst text is a religious speech which addresses its audience right from the beginning.</S>
			<S sid ="161" ssid = "139">Ex.</S>
			<S sid ="162" ssid = "140">(6) shows a phrase that occurs frequently in the Berlin text: source of errors are semantic changes, as shown in Ex.</S>
			<S sid ="163" ssid = "141">(4) from the LeichSermon text: the word (6) sieh VVIMP anselm NE form so is an adverb in modern German, but is frequently used as a relative pronoun (PRELS2 ) in ENHG, which nevers occurs in the training data of the TIGER/Tu¨ ba corpus.</S>
			<S sid ="164" ssid = "142">“Look, Anselm” The imperative form sieh “look” is used 24 times in the Berlin text, but typically mistagged as a proper noun (NE) despite being correctly nor ART NN PRELS APPR NN not occur there at all; only the standard form siehe geschehen geschehen VVPP “the cases which occur out of weakness” Extinct wordforms are a major problem for the normalization approach.</S>
			<S sid ="165" ssid = "143">They cannot usually be normalized to a modern wordform by applying spelling changes, but would have to be mapped on a word-by-word basis.</S>
			<S sid ="166" ssid = "144">However, both GerManC- GS and the normalization layer of Anselm3 map extinct wordforms to artificial lemmas, which arestill useful to identify spelling variants, but im was learned.</S>
			<S sid ="167" ssid = "145">Imperative verb forms in general are very uncommon in TIGER/Tu¨ ba, only making up 397 tokens (0.02%).</S>
			<S sid ="168" ssid = "146">In comparison, the gold-standard POS annotation of Berlin already contains 43 imperative verb forms (0.91%).</S>
			<S sid ="169" ssid = "147">Similarly, the religious texts in Anselm and GerManC-GS often use vocabulary that is rarely used in newspaper text.</S>
			<S sid ="170" ssid = "148">Ex.</S>
			<S sid ="171" ssid = "149">(7) shows the finite verb form verschma¨ hten “despised/spurned”, which has only one occurrence in the TIGER/Tu¨ ba corpus where it was used as an adjective instead, inevitably leading to a tagging error.</S>
			<S sid ="172" ssid = "150">practical for this POS tagging approach.</S>
			<S sid ="173" ssid = "151">A common example in Melk is czuhant “immediately”, (7) vnd und KON vorsmeten verschma¨hten VVFIN yn ihn PPER 2 Actually, GerManC-GS annotates so in this example with the new tag PTKREL, which is mapped back to PRELS for reasons of compatibility.</S>
			<S sid ="174" ssid = "152">As PTKREL is not found in TIGER or Tu¨ baD/Z, keeping this tag would not solve the problem here, though.</S>
			<S sid ="175" ssid = "153">3 The Anselm corpus provides an additional “modernization” layer which maps extinct forms to actual modern words, but a first evaluation showed that using this layer has a negative impact on overall normalization accuracy.</S>
			<S sid ="176" ssid = "154">“and [they] despised him” These examples show that even if spelling normalization was done perfectly on historical texts, semantic/syntactic variation and domain adaptation of the POS tagger provide further obstacles for achieving higher tagging accuracies.</S>
			<S sid ="177" ssid = "155">5 Related work.</S>
			<S sid ="178" ssid = "156">For automatic spelling normalization, VARD 2 (Baron and Rayson, 2008) is another tool that has been developed for Early Modern English.</S>
			<S sid ="179" ssid = "157">It has been successfully adapted to other languages, e.g. Portuguese (Hendrickx and Marquilhas, 2011), though previous experiments found it to perform worse than Norma on the Anselm data (Bollmann, 2012).</S>
			<S sid ="180" ssid = "158">Jurish (2010) presents a normalization method that includes token context, which seems to be the logical next step to further improve normalization results.</S>
			<S sid ="181" ssid = "159">POS tagging on normalized data has been tried for the GerManC-GS corpus before with an average accuracy of 79.7% (Scheible et al., 2011a), however, only manual normalization was considered.</S>
			<S sid ="182" ssid = "160">For English, Rayson et al.</S>
			<S sid ="183" ssid = "161">(2007) report an accuracy of 89–91% on gold standard normalizations and 85–89% on automatically normalized texts.</S>
			<S sid ="184" ssid = "162">Hendrickx and Marquilhas (2011) perform a similar evaluation for Portuguese, achieving 86.6% and 83.4% on gold standard and automatic normalizations, respectively.</S>
			<S sid ="185" ssid = "163">There are some notable differences, however, between the aforementioned studies and the approach outlined here.</S>
			<S sid ="186" ssid = "164">Firstly, those studies using automatic normalization methods typically utilize either a much higher amount of training data or some kind of manually crafted resource.</S>
			<S sid ="187" ssid = "165">VARD, for instance, uses a manually compiled list of spelling variants totalling more than 45,000 entries (Rayson et al., 2005), while Hendrickx and Marquilhas (2011) use a training set of more than 37,000 tokens.</S>
			<S sid ="188" ssid = "166">While I certainly expect to improve the results in the future by using full texts from the Anselm and/or GerManC-GS corpora as basis for training, this approach might not always be feasible.</S>
			<S sid ="189" ssid = "167">The approach presented here, requiring only a few hundred tokens for training, seems especially suited for languages where projects to create historical corpora have only been started, and therefore do not have large amounts of previously annotated training material to fall back to.</S>
			<S sid ="190" ssid = "168">Secondly, the Anselm texts evaluated here show a much lower baseline than the texts evaluated in other studies.</S>
			<S sid ="191" ssid = "169">Without normalization, POS tagging accuracy is 82–88% in Rayson et al.</S>
			<S sid ="192" ssid = "170">(2007), 76.9% in Hendrickx and Marquilhas (2011), and 69.6% for the German data in Scheible et al.</S>
			<S sid ="193" ssid = "171">(2011a).</S>
			<S sid ="194" ssid = "172">The texts from Berlin and Melk, on the other hand, perform much worse without the nor malization step (28.7% and 44.7%, respectively).</S>
			<S sid ="195" ssid = "173">This suggests a higher amount of variance in the Anselm data compared to the types of text used in previous studies, making their automatic processing a potentially more challenging problem.</S>
			<S sid ="196" ssid = "174">Also, annotated data from these studies is less likely to be useful as training data for these texts.</S>
			<S sid ="197" ssid = "175">6 Conclusion.</S>
			<S sid ="198" ssid = "176">I presented an approach to part-of-speech tagging for historical texts that uses spelling normalization as a preprocessing step.</S>
			<S sid ="199" ssid = "177">Evaluation on texts from Early New High German showed that by manually normalizing 250 tokens of a text and using them as training data, automatic normalization of the remaining text performs well enough to result in a notable increase in POS tagging accuracy.</S>
			<S sid ="200" ssid = "178">Texts with more spelling variation were shown to benefit more from this approach than texts which are already closer to the modern target language.</S>
			<S sid ="201" ssid = "179">For one German manuscript from the 15th century, this method increased tagging accuracy from 28.65% to 74.89%.</S>
			<S sid ="202" ssid = "180">While this is still far from the accuracy scores reported for modern language data, and also quite a bit worse than tagging on the gold-standard normalization (87.07% for this text), it offers a way to facilitate the (semiautomatic) POS annotation of historical texts with relatively minor effort.</S>
			<S sid ="203" ssid = "181">Furthermore, as it does not require a sizeable amount of training data, this approach is potentially interesting for less-resourced language varieties in general, assuming some level of graphematic similarity to a well-resourced target language.</S>
			<S sid ="204" ssid = "182">Future work should likely consider inclusion of token context for the normalization as proposed by Jurish (2010).</S>
			<S sid ="205" ssid = "183">Analysis of the POS tagging errors also highlighted some of the problems that remain.</S>
			<S sid ="206" ssid = "184">Domain-specific differences can negatively impact tagging performance even on perfectly normalized data.</S>
			<S sid ="207" ssid = "185">Furthermore, spelling normalization cannot account for semantic and syntactic peculiarities of historical language.</S>
			<S sid ="208" ssid = "186">For a corpus of Old Spanish, this led Sa´nchezMarco et al.</S>
			<S sid ="209" ssid = "187">(2010) to abandon the normalization approach and use a customized POS tagger instead.</S>
			<S sid ="210" ssid = "188">On the other hand, a study by Dipper (2010) showed that normalization is still beneficial even when retraining a tagger on a corpus of historical data.</S>
			<S sid ="211" ssid = "189">Future research could try to combine a normalization step with a modified POS tagger to improve the results further.</S>
	</SECTION>
</PAPER>
