<PAPER>
	<ABSTRACT></ABSTRACT>
	<SECTION title="Predictions from Multilingual  Topics. " number = "1">
			<S sid ="1" ssid = "1">As its name suggests, MLSLDA is an extension of Latent Dirichlet allocation (LDA) (Blei et al., 2003), a modeling approach that takes a corpus of unannotated documents as input and produces two outputs, a set of “topics” and assignments of documents to topics.</S>
			<S sid ="2" ssid = "2">Both the topics and the assignments are probabilistic: a topic is represented as a probability distribution over words in the corpus, and each document is assigned a probability distribution over all the topics.</S>
			<S sid ="3" ssid = "3">Topic models built on the foundations of LDA are appealing for sentiment analysis because the learned topics can cluster together sentiment- bearing words, and because topic distributions are a parsimonious way to represent a document.1 LDA has been used to discover latent structure in text (e.g. for discourse segmentation (Purver et al., 2006) and authorship (RosenZvi et al., 2004)).</S>
			<S sid ="4" ssid = "4">MLSLDA extends the approach by ensuring that this latent structure — the underlying topics — is consistent across languages.</S>
			<S sid ="5" ssid = "5">We discuss multilingual topic modeling in Section 1.1, and in Section 1.2 we show how this enables supervised regression regardless of a document’s language.</S>
			<S sid ="6" ssid = "6">1.1 Capturing Semantic Correlations.</S>
			<S sid ="7" ssid = "7">Topic models posit a straightforward generative process that creates an observed corpus.</S>
			<S sid ="8" ssid = "8">For each document d, some distribution θd over unobserved topics is chosen.</S>
			<S sid ="9" ssid = "9">Then, for each word position in the document, a topic z is selected.</S>
			<S sid ="10" ssid = "10">Finally, the word for that position is generated by selecting from the topic indexed by z.</S>
			<S sid ="11" ssid = "11">(Recall that in LDA, a “topic” is a distribution over words).</S>
			<S sid ="12" ssid = "12">In monolingual topic models, the topic distribution is usually drawn from a Dirichlet distribution.</S>
			<S sid ="13" ssid = "13">Using Dirichlet distributions makes it easy to specify sparse priors, and it also simplifies posterior inference because Dirichlet distributions are conjugate to multinomial distributions.</S>
			<S sid ="14" ssid = "14">However, drawing topics from Dirichlet distributions will not suffice if our vocabulary includes multiple languages.</S>
			<S sid ="15" ssid = "15">If we are working with English, German, and Chinese at the same time, a Dirichlet prior has no way to fa vor distributions z such that p(good|z), p(gut|z), and 1 The latter property has also made LDA popular for information retrieval (Wei and Croft, 2006)).</S>
			<S sid ="16" ssid = "16">p(haˇo|z) all tend to be high at the same time, or low at the same time.</S>
			<S sid ="17" ssid = "17">More generally, the structure of our model must encourage topics to be consistent across languages, and Dirichlet distributions cannot encode correlations between elements.</S>
			<S sid ="18" ssid = "18">One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution.</S>
			<S sid ="19" ssid = "19">This has been done successfully in multilingual settings (Cohen and Smith, 2009).</S>
			<S sid ="20" ssid = "20">However, such models complicate inference by not being conjugate.</S>
			<S sid ="21" ssid = "21">Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (BoydGraber et al., 2007) and to encode clustering constraints (An- drzejewski et al., 2009).</S>
			<S sid ="22" ssid = "22">The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree.</S>
			<S sid ="23" ssid = "23">For con- creteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge, since it is well known, offers convenient and intuitive terminology, and demonstrates the full flexibility of our approach.</S>
			<S sid ="24" ssid = "24">However, the model we describe generalizes to any tree-structured representation of multilingual knowledge; we discuss some alternatives in Section 3.</S>
			<S sid ="25" ssid = "25">WordNet organizes a vocabulary into a rooted, directed acyclic graph of nodes called synsets, short for “synonym sets.” A synset is a child of another synset if it satisfies a hyponomy relationship; each child “is a” more specific instantiation of its parent concept (thus, hyponomy is often called an “isa” relationship).</S>
			<S sid ="26" ssid = "26">For example, a “dog” is a “canine” is an “animal” is a “living thing,” etc. As an approximation, it is not unreasonable to assume that WordNet’s structure of meaning is language independent, i.e. the concept encoded by a synset can be realized using terms in different languages that share the same meaning.</S>
			<S sid ="27" ssid = "27">In practice, this organization has been used to create many alignments of international WordNets to the original English WordNet (Ordan and Wintner, 2007; Sagot and Fisˇer, 2008; Isahara et al., 2008).</S>
			<S sid ="28" ssid = "28">Using the structure of WordNet, we can now describe a generative process that produces a distribution over a multilingual vocabulary, which encourages correlations between words with similar mean ings regardless of what language each word is in.</S>
			<S sid ="29" ssid = "29">For each synset h, we create a multilingual word distribution for that synset as follows: 1.</S>
			<S sid ="30" ssid = "30">Draw transition probabilities βh ∼ Dir (τh ).</S>
	</SECTION>
	<SECTION title="Draw stop probabilities ωh  ∼ Dir (κh ). " number = "2">
			<S sid ="31" ssid = "1">that synset φh,l ∼ Dir (πh,l ).</S>
			<S sid ="32" ssid = "2">For conciseness in the rest of the paper, we will refer to this generative process as multilingual Dirichlet hierarchy, or MULTDIRHIER(τ , κ, π).2 Each observed token can be viewed as the end result of a sequence of visited synsets λ.</S>
			<S sid ="33" ssid = "3">At each node in the tree, the path can end at node i with probability ωi,1, or it can continue to a child synset with probability ωi,0.</S>
			<S sid ="34" ssid = "4">If the path continues to another child synset, it visits child j with probability βi,j . If the path ends at a synset, it generates word k with probability φi,l,k .3 The probability of a word being emitted from a path with visited synsets r and final synset h in language l is therefore p(w, λ = r, h|l, β, ω, φ) =    n βi,j ωi,0 (1 − ωh,1)φh,l,w .</S>
			<S sid ="35" ssid = "5">(1) (i,j)∈r Note that the stop probability ωh is independent of language, but the emission φh,l is dependent on the language.</S>
			<S sid ="36" ssid = "6">This is done to prevent the following scenario: while synset A is highly probable in a topic and words in language 1 attached to that synset have high probability, words in language 2 have low probability.</S>
			<S sid ="37" ssid = "7">If this could happen for many synsets in a topic, an entire language would be effectively silenced, which would lead to inconsistent topics (e.g. 2 Variables τh , πh,l , and κh are hyperparameters.</S>
			<S sid ="38" ssid = "8">Their mean is fixed, but their magnitude is sampled during inference (i.e. τh,i Topic 1 is about baseball in English and about travel in German).</S>
			<S sid ="39" ssid = "9">Separating path from emission helps ensure that topics are consistent across languages.</S>
			<S sid ="40" ssid = "10">Having defined topic distributions in a way that can preserve cross-language correspondences, we now use this distribution within a larger model that can discover cross-language patterns of use that predict sentiment.</S>
			<S sid ="41" ssid = "11">1.2 The MLSLDA Model.</S>
			<S sid ="42" ssid = "12">We will view sentiment analysis as a regression problem: given an input document, we want to predict a real-valued observation y that represents the sentiment of a document.</S>
			<S sid ="43" ssid = "13">Specifically, we build on supervised latent Dirichlet allocation (SLDA, (Blei and McAuliffe, 2007)), which makes predictions based on the topics expressed in a document; this can be thought of projecting the words in a document to low dimensional space of dimension equal to the number of topics.</S>
			<S sid ="44" ssid = "14">Blei et al. showed that using this latent topic structure can offer improved predictions over regressions based on words alone, and the approach fits well with our current goals, since word-level cues are unlikely to be identical across languages.</S>
			<S sid ="45" ssid = "15">In addition to text, SLDA has been successfully applied to other domains such as social networks (Chang and Blei, 2009) and image classification (Wang et al., 2009).</S>
			<S sid ="46" ssid = "16">The key innovation in this paper is to extend SLDA by creating topics that are globally consistent across languages, using the bridging approach above.</S>
			<S sid ="47" ssid = "17">We express our model in the form of a probabilistic generative latent-variable model that generates documents in multiple languages and assigns a real- valued score to each document.</S>
			<S sid ="48" ssid = "18">The score comes from a normal distribution whose sum is the dot product between a regression parameter η that encodes the influence of each topic on the observation and 2 Pk τh,k is constant, but τh,i is not).</S>
			<S sid ="49" ssid = "19">For the bushier bridges, a variance σ . With this model in hand, we use sta-.</S>
			<S sid ="50" ssid = "20">(e.g. dictionary and flat), their mean is uniform.</S>
			<S sid ="51" ssid = "21">For GermaNet, we took frequencies from two balanced corpora of German and English: the British National Corpus (University of Oxford, 2006) and the Kern Corpus of the Digitales Wo¨ rterbuch der Deutschen Sprache des 20.</S>
			<S sid ="52" ssid = "22">Jahrhunderts project (Geyken, 2007).</S>
			<S sid ="53" ssid = "23">We took these frequencies and propagated them through the multilingual hierarchy, following LDAWN’s (BoydGraber et al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior.</S>
			<S sid ="54" ssid = "24">The variance of the priors was initialized to be 1.0, but could be sampled during inference.</S>
			<S sid ="55" ssid = "25">3 Note that the language and word are taken as given, but the path through the semantic hierarchy is a latent random variable.</S>
			<S sid ="56" ssid = "26">tistical inference to determine the distribution over latent variables that, given the model, best explains observed data.</S>
			<S sid ="57" ssid = "27">The generative model is as follows: 1.</S>
			<S sid ="58" ssid = "28">For each topic i = 1 . . .</S>
			<S sid ="59" ssid = "29">K , draw a topic distribution.</S>
			<S sid ="60" ssid = "30">{βi , ωi , φi } from MULTDIRHIER(τ , κ, π).</S>
			<S sid ="61" ssid = "31">2.</S>
			<S sid ="62" ssid = "32">For each document d = 1 . . .</S>
			<S sid ="63" ssid = "33">M with language ld :.</S>
			<S sid ="64" ssid = "34">(a) Choose a distribution over topics θd ∼ Dir (α).</S>
			<S sid ="65" ssid = "35">(b) For each word in the document n = 1 . . .</S>
			<S sid ="66" ssid = "36">Nd , choose a topic assignment zd,n ∼ Mult (θd ) and a path λd,n ending at word wd,n according to Equation 1 using {βzd,n , ωzd,n , φzd,n }.</S>
	</SECTION>
	<SECTION title="Choose	a	response	variable	from	y	∼. " number = "3">
			<S sid ="67" ssid = "1">α θd σ Norm η z¯, σ2 , where z¯d ≡ 1 N zd,n . τ β z y η N n=1 h i,h d,n d Crucially, note that the topics are not independent of the sentiment task; the regression encourages terms with similar effects on the observation y to be in the same topic.</S>
			<S sid ="68" ssid = "2">The consistency of topics described above allows the same regression to be done κh πh,l ωi,h φi,h,l L K H λd,n wd,n N M for the entire corpus regardless of the language of the underlying document.</S>
			<S sid ="69" ssid = "3">2 Inference.</S>
			<S sid ="70" ssid = "4">Finding the model parameters most likely to explain the data is a problem of statistical inference.</S>
			<S sid ="71" ssid = "5">We employ stochastic EM (Diebolt and Ip, 1996), using a Gibbs sampler for the E-step to assign words to paths and topics.</S>
			<S sid ="72" ssid = "6">After randomly initializing the topics, we alternate between sampling the topic and path Multilingual Topics Text Documents Sentiment Prediction Figure 1: Graphical model representing MLSLDA.</S>
			<S sid ="73" ssid = "7">Shaded nodes represent observations, plates denote replication, and lines show probabilistic dependencies.</S>
			<S sid ="74" ssid = "8">probability of taking a path r is then p(λn = r|zn = k, λ−n) =of a word (zd,n, λd,n) and finding the regression pa rameters η that maximize the likelihood.</S>
			<S sid ="75" ssid = "9">We jointly sample the topic and path conditioning on all of the n (i,j)∈r Bk,i,j + τi,j jj Bk,i,jj + τi,j Ok,i,1 + ωi \ s∈0,1 Ok,i,s + ωi,s other path and document assignments in the corpus, Tran s it ion _ selecting a path and topic with probability Ok,r end ,0 + ωr end Fk,r end ,wn + πrend ,l . s∈0,1 Ok,rend ,s + ωrend ,s wj Frend ,wj + πrend ,wj p(zn = k, λn = r|z−n, λ−n, wn, η, σ, Θ) = p(yd|z, η, σ)p(λn = r|zn = k, λ−n, wn, τ , κ, π) p(zn = k|z−n, α).</S>
			<S sid ="76" ssid = "10">(2) Each of these three terms reflects a different influence on the topics from the vocabulary structure, the document’s topics, and the response variable.</S>
			<S sid ="77" ssid = "11">In the next paragraphs, we will expand each of them to derive the full conditional topic distribution.</S>
			<S sid ="78" ssid = "12">As discussed in Section 1.1, the structure of the topic distribution encourages terms with the same meaning to be in the same topic, even across languages.</S>
			<S sid ="79" ssid = "13">During inference, we marginalize over possible multinomial distributions β, ω, and φ, using the observed transitions from i to j in topic k; Tk,i,j , stop counts in synset i in topic k, Ok,i,0; continue counts in synsets i in topic k, Ok,i,1; and emission counts in synset i in language l in topic k, Fk,i,l . The Em is sion _ (3) Equation 3 reflects the multilingual aspect of this model.</S>
			<S sid ="80" ssid = "14">The conditional topic distribution for SLDA (Blei and McAuliffe, 2007) replaces this term with the standard MultinomialDirichlet.</S>
			<S sid ="81" ssid = "15">However, we believe this is the first published SLDA-style model using MCMC inference, as prior work has used variational inference (Blei and McAuliffe, 2007; Chang and Blei, 2009; Wang et al., 2009).</S>
			<S sid ="82" ssid = "16">Because the observed response variable depends on the topic assignments of a document, the conditional topic distribution is shifted toward topics that explain the observed response.</S>
			<S sid ="83" ssid = "17">Topics that move the predicted response yˆd toward the true yd will be favored.</S>
			<S sid ="84" ssid = "18">We drop terms that are constant across all topics for the effect of the response variable, p(yd|z, η, σ) ∝ 3 Bridges Across Languages.</S>
			<S sid ="85" ssid = "19">In Section 1.1, we described connections across languages as offered by semantic networks in a general 1 exp yd − kj Nd,kj ηkj \ ηzk way, using WordNet as an example.</S>
			<S sid ="86" ssid = "20">In this section, σ2 kj Nd,kj Other words’ influence kj Nd,kj _ we provide more specifics, as well as alternative ways of building semantic connections across languages.</S>
			<S sid ="87" ssid = "21">r exp −ηzk 2 1 .</S>
			<S sid ="88" ssid = "22">(4) Flat First, we can consider a degenerate mapping 2σ2 kj Nd,kj This word’s influence _ The above equation represents the supervised aspect of the model, which is inherited from SLDA.</S>
			<S sid ="89" ssid = "23">Finally, there is the effect of the topics already assigned to a document; the conditional distribution favors topics already assigned in a document, Td,k + αkthat is nearly equivalent to running SLDA indepen dently across multiple languages, relating topics only based on the impact on the response variable.</S>
			<S sid ="90" ssid = "24">Consider a degenerate tree with only one node, with all words in all languages associated with that node.</S>
			<S sid ="91" ssid = "25">This is consistent with our model, but there is really no shared semantic space, as all emitted words must come from this degenerate “synset” and the model only represents the output distribution for this single node.</S>
			<S sid ="92" ssid = "26">p(zn = k|z−n, α) = kj Td,kj + αkj .</S>
			<S sid ="93" ssid = "27">(5) WordNet We took the alignment of GermaNet to WordNet 1.6 (Kunze and Lemnitzer, 2002) and re This term represents the document focus of this model; it is present in all Gibbs sampling inference schemes for LDA (Griffiths and Steyvers, 2004).</S>
			<S sid ="94" ssid = "28">Multiplying together Equations 3, 4, and 5 allows us to sample a topic using the conditional distribution from Equation 2, based on the topic and path of the other words in all languages.</S>
			<S sid ="95" ssid = "29">After sampling the path and topic for each word in a document, we then find new regression parameters η that maximize the likelihood conditioned on the current state of the sampler.</S>
			<S sid ="96" ssid = "30">This is simply a least squares regression using the topic assignments z¯d to predict yd.</S>
			<S sid ="97" ssid = "31">Prediction on documents for which we don’t have an observed yd is equivalent to marginalizing over yd and sampling topics for the document from Equations 3 and 5.</S>
			<S sid ="98" ssid = "32">The prediction for yd is then the dot product of η and the empirical topic distribution z¯d. We initially optimized all hyperparameters using slice sampling.</S>
			<S sid ="99" ssid = "33">However, we found that the regression variance σ2 was not stable.</S>
			<S sid ="100" ssid = "34">Optimizing σ2 seems to balance between modeling the language in the documents and the prediction, and thus is sensitive to documents’ length.</S>
			<S sid ="101" ssid = "35">Given this sensitivity, we did not optimize σ2 for our prediction experiments in Section 4, but instead kept it fixed at 0.25.</S>
			<S sid ="102" ssid = "36">We leave optimizing this variable, either through cross validation or adapting the model, to future work.</S>
			<S sid ="103" ssid = "37">moved all synsets that were had no mapped German words.</S>
			<S sid ="104" ssid = "38">Any German synsets that did not have English translations had their words mapped to the lowest extant English hypernym (e.g. “beinbruch,” a broken leg, was mapped to “fracture”).</S>
			<S sid ="105" ssid = "39">We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970).</S>
			<S sid ="106" ssid = "40">An example of the paths for the German word “wunsch” (wish, request) is shown in Figure 2(a).</S>
			<S sid ="107" ssid = "41">Dictionaries A dictionary can be viewed as a many to many mapping, where each entry ei maps one or more words in one language si to one or more words ti in another language.</S>
			<S sid ="108" ssid = "42">Entries were taken from an EnglishGerman dictionary (Richter, 2008) a ChineseEnglish dictionary (Denisowski, 1997), and a ChineseGerman dictionary (Hefti, 2005).</S>
			<S sid ="109" ssid = "43">As with WordNet, the words in entries for English and German were stemmed to improve coverage.</S>
			<S sid ="110" ssid = "44">An example for German is shown in Figure 2(b).</S>
			<S sid ="111" ssid = "45">Algorithmic Connections In addition to hand- curated connections across languages, one could also consider automatic means of mapping across languages, such as using edit distance or local context (Haghighi et al., 2008; Rapp, 1995) or using a lexical translation table obtained from parallel text (Melamed, 1998).</S>
			<S sid ="112" ssid = "46">While we experimented entity.n.01 abstraction.n.06 entiti objekt cognition.n.01 event.n.01 option.n.02 event ereignis vorgang act.n.02 root altern option choic option deed act handlung dict.1 dict.2 preference.n.03 speech_act.n.01 room gelass room raum platz wish.n.04 request.n.02 dict.3 wish wunsch ask request anfrag wunsch room zimm raum stub (a) GermaNet (b) Dictionary Figure 2: Two methods for constructing multilingual distributions over words.</S>
			<S sid ="113" ssid = "47">On the left, paths to the German word “wunsch” in GermaNet are shown.</S>
			<S sid ="114" ssid = "48">On the right, paths to the English word “room” are shown.</S>
			<S sid ="115" ssid = "49">Both English and German words are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines).</S>
			<S sid ="116" ssid = "50">Note that different senses are denoted by different internal paths, and that internal paths are distinct from the per-language expression.</S>
			<S sid ="117" ssid = "51">with these techniques, constructing appropriate hierarchies from these resources required many arbitrary decisions about cutoffs and which words to include.</S>
			<S sid ="118" ssid = "52">Thus, we do not consider them in this paper.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
			<S sid ="119" ssid = "1">We evaluate MLSLDA on three criteria: how well it can discover consistent topics across languages for matching parallel documents, how well it can discover sentiment-correlated word lists from nonaligned text, and how well it can predict sentiment.</S>
			<S sid ="120" ssid = "2">4.1 Matching on Multilingual Topics.</S>
			<S sid ="121" ssid = "3">We took the 1996 documents from the Europarl corpus (Koehn, 2005) using three bridges: GermaNet, dictionary, and the uninformative flat matching.4 The model is unaware that the translations of documents in one language are present in the other language.</S>
			<S sid ="122" ssid = "4">Note that this does not use the supervised framework 4 For English and German documents in all experiments,.</S>
			<S sid ="123" ssid = "5">(as there is no associated response variable for Europarl documents); this experiment is to demonstrate the effectiveness of the multilingual aspect of the model.</S>
			<S sid ="124" ssid = "6">To test whether the topics learned by the model are consistent across languages, we represent each document using the probability distribution θd over topic assignments.</S>
			<S sid ="125" ssid = "7">Each θd is a vector of length K and is a language-independent representation of the document.</S>
			<S sid ="126" ssid = "8">For each document in one language, we computed the Hellinger distance between it and all of the documents in the other language and sorted the documents by decreasing distance.</S>
			<S sid ="127" ssid = "9">The translation of the document is somewhere in that set; the higher the normalized rank (the percentage of documents with a rank lower than the translation of the document), the better the underlying topic model connects languages.</S>
			<S sid ="128" ssid = "10">We compare three bridges against what is to our knowledge the only other topic model for unaligned text, Multilingual Topics for Unaligned Text (BoydGraber and Blei, 2009).5 we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size).</S>
			<S sid ="129" ssid = "11">Documents shorter than fifty content words were excluded.</S>
	</SECTION>
	<SECTION title="The bipartite matching was initialized with the dictionary. " number = "5">
			<S sid ="130" ssid = "1">weights as specified by the Multilingual Topics for Unaligned Text algorithm.</S>
			<S sid ="131" ssid = "2">The matching size was limited to 250 and the bipartite matching was only updated on the initial iteration then held fixed.</S>
			<S sid ="132" ssid = "3">This yielded results comparable to when the matching Dictionary MuTo GermaNet Flat Dictionary MuTo GermaNet Flat Dictionary MuTo GermaNet Flat Dictionary MuTo GermaNet Flat 0.0 0.2 0.4 0.6 0.8 a WordNet-like resource is used as the bridge, the resulting topics are distributions over synsets, not just over words.</S>
			<S sid ="133" ssid = "4">As our demonstration corpus, we used the Amherst Sentiment Corpus (Constant et al., 2009), as it has documents in multiple languages (English, Chinese, and German) with numerical assessments of sentiment (number of stars assigned to the review).</S>
			<S sid ="134" ssid = "5">We segmented the Chinese text (Tseng et al., 2005) and used a classifier trained on character n grams to remove English-language documents that were mixed in among the Chinese and German language reviews.</S>
			<S sid ="135" ssid = "6">Figure 4 shows extracted topics from GermanEnglish and GermanChinese corpora.</S>
			<S sid ="136" ssid = "7">MLSLDA is able to distinguish sentiment-bearing topics from Average Parallel Document Rank Figure 3: Average rank of paired translation document recovered from the multilingual topic model.</S>
			<S sid ="137" ssid = "8">Random guessing would yield 0.5; MLSLDA with a dictionary based matching performed best.</S>
			<S sid ="138" ssid = "9">Figure 3 shows the results of this experiment.</S>
			<S sid ="139" ssid = "10">The dictionary-based bridge had the best performance on the task, ranking a large proportion of documents (0.95) below the translated document once enough topics were available.</S>
			<S sid ="140" ssid = "11">Although GermaNet is richer, its coverage is incomplete; the dictionary structure had a much larger vocabulary and could build a more complete multilingual topics.</S>
			<S sid ="141" ssid = "12">Using comparable input information, this more flexible model performed better on the matching task than the existing multilingual topic model available for unaligned text.</S>
			<S sid ="142" ssid = "13">The degenerate flat bridge did no better than the baseline of random guessing, as expected.</S>
			<S sid ="143" ssid = "14">4.2 Qualitative Sentiment-Correlated Topics.</S>
			<S sid ="144" ssid = "15">One of the key tasks in sentiment analysis has been the collection of lists of words that convey sentiment (Wilson, 2008; Riloff et al., 2003).</S>
			<S sid ="145" ssid = "16">These resources are often created using or in reference to resources like WordNet (Whitelaw et al., 2005; Baccianella and Sebastiani, 2010).</S>
			<S sid ="146" ssid = "17">MLSLDA provides a method for extracting topical and sentiment- correlated word lists from multilingual corpora.</S>
			<S sid ="147" ssid = "18">If was updated more frequently.content bearing topics.</S>
			<S sid ="148" ssid = "19">For example; in the German English corpus, “food” and “children” topics are not associated with a consistent sentiment signal, while “religion” is associated with a more negative sentiment.</S>
			<S sid ="149" ssid = "20">In contrast, in the GermanChinese corpus, the “religion/society” topic is more neutral, and the gender-oriented topic is viewed more negatively.</S>
			<S sid ="150" ssid = "21">Negative sentiment-bearing topics have reasonable words such as “pages,” “koˇ ng pa`” (Chinese for “I’m afraid that . . .</S>
			<S sid ="151" ssid = "22">”) and “tuo” (Chienese for “discard”), and positive sentiment-bearing topics have reasonable words such as “great,” “good,” and “juwel” (German for “jewel”).</S>
			<S sid ="152" ssid = "23">The qualitative topics also betray some of the weaknesses of the model.</S>
			<S sid ="153" ssid = "24">For example, in one of the negative sentiment topics, the German word “gut” (good) is present.</S>
			<S sid ="154" ssid = "25">Because topics are distributions over words, they can encode the presence of negations like “kein” (no) and “nicht” (not), but not collocations like “nicht gut.” More elaborate topic models that can model local syntax and collocations (Johnson, 2010) provide options for addressing such problems.</S>
			<S sid ="155" ssid = "26">We do not report the results for sentiment prediction for this corpus because the baseline of predicting a positive review is so strong; most algorithms do extremely well by always predicting a positive review, ours included.</S>
			<S sid ="156" ssid = "27">4.3 Sentiment Prediction.</S>
			<S sid ="157" ssid = "28">We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over god himmel us gedanken religion glaube church unsere human kirche wahrheit diet essen food diät eat verlieren weight befinden eating körper healthy rezepte fat book buch books immer one leben life art person lesen people thema autor (harry) harry OEf (harry) (volume) band &amp; (belt) (sky) himmel ::K (sky) (universe) all � (both) (vampire) vampir ff (section) (last) letzt .&amp;ltr.m (vampire) (part) teil $.7.</S>
			<S sid ="158" ssid = "29">(strong) �J§ (last) ( g o d ) g o t t _ _ * ( g o d ) ( l o r d ) h e r r ) t � ( l o r d ) (religion) religi on � (both ) (univ erse) all *)( (relig ion) (worl d) welt H* (scie nce) (science) wissenschaft ±� (community) (medicine) medizin (society) gesellschaft 0 . 81.20.80.4 0.0 0.4 0.8 1.2 &quot; ( b o o k ) (book) buch �8 ([I&apos;m afraid that...]) (itself) sich #$ (myself) � (quick) � (a little) (good) gut (sentence) satz (two) zwei T (good) � (set) book buch movie film film great good gesellschaft (that) dass � (both) (much) viel .:kff:5 (mostly) (woman) frau (point) punkt tl:A.</S>
			<S sid ="159" ssid = "30">(woman) 51A.</S>
			<S sid ="160" ssid = "31">(man ) (story) story (treasure) schatz :i: (treasure) !</S>
			<S sid ="161" ssid = "32">(handsome) novel roman children kind episode filme business genau (no) kein &quot;% (book) (man) mann tl: (female) (attractiv e) attraktiv � (both) reader leser baby kinder movies episode all überzeugt (equal) gleich (good) gut #1 ([really isn&apos;t]) 51 (male) (elegant) elegant .9� (story) pages seiten child eltern scenes star one ergebnis (when) wenn 5(, (discard) (fast) schnell (gem) juwel J (small) books geschichte parents baby separate story companies mittel (female) weiblich tl:1 (female) tale handlung sleep nacht gives gibt right verlangen (soon) bald � (both) (a) German / English (b) German / Chinese Figure 4: Topics, along with associated regression coefficient η from a learned 25-topic model on GermanEnglish (left) and GermanChinese (right) documents.</S>
			<S sid ="162" ssid = "33">Notice that theme-related topics have regression parameter near zero, topics discussing the number of pages have negative regression parameters, topics with “good,” “great,” “haˇo” (good) and “u¨ berzeugt” (convinced) have positive regression parameters.</S>
			<S sid ="163" ssid = "34">For the GermanChinese corpus, note the presence of “gut” (good) in one of the negative sentiment topics, showing the difficulty of learning collocations.</S>
			<S sid ="164" ssid = "35">prevent over-fitting.</S>
			<S sid ="165" ssid = "36">Table 1: Mean squared error on a film review corpus.</S>
			<S sid ="166" ssid = "37">All results are on the same German test data, varying the training data.</S>
			<S sid ="167" ssid = "38">Over-fitting prevents the model learning on the German data alone; adding English data to the mix allows the model to make better predictions.</S>
			<S sid ="168" ssid = "39">5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus.6 The results for predicting sentiment in German documents with 25 topics are presented in Table 1.</S>
			<S sid ="169" ssid = "40">On a small monolingual corpus, prediction is very poor.</S>
			<S sid ="170" ssid = "41">The model over-fits, especially when it has the entire vocabulary to select from.</S>
			<S sid ="171" ssid = "42">The slightly better performance using GermaNet and a dictionary as topic priors can be viewed as basic feature selection, removing proper names from the vocabulary to 6 We followed Pang and Lee’s method for creating a numerical score between 0 and 1 from a star rating.</S>
			<S sid ="172" ssid = "43">We then converted that to an integer by multiplying by 100; this was done because initial data preprocessing assumed integer values (although downstream processing did not assume integer values).</S>
			<S sid ="173" ssid = "44">The German movie review corpus is available at http://www.umiacs.umd.edu/˜jbg/ static/downloads_and_media.html One would expect that prediction improves with a larger training set.</S>
			<S sid ="174" ssid = "45">For this model, such an improvement is seen even when the training set includes no documents in the target language.</S>
			<S sid ="175" ssid = "46">Note that even the degenerate flat bridge across languages provides useful information.</S>
			<S sid ="176" ssid = "47">After introducing English data, the model learns to prefer smaller regression parameters (this can be seen as a form of regularization).</S>
			<S sid ="177" ssid = "48">Performance is best when a reasonably large corpus is available including some data in the target language.</S>
			<S sid ="178" ssid = "49">For each bridge, performance improves dramatically, showing that MLSLDA is successfully able to incorporate information learned from both languages to build a single, coherent picture of how sentiment is expressed in both languages.</S>
			<S sid ="179" ssid = "50">With the GermaNet bridge, performance is better than both the degenerate and dictionary based bridges, showing that the model is sharing information both through the multilingual topics and the regression parameters.</S>
			<S sid ="180" ssid = "51">Performance on English prediction is comparable to previously published results on this dataset (Blei and McAuliffe, 2007); with enough data, a monolingual model is no longer helped by adding additional multilingual data.</S>
			<S sid ="181" ssid = "52">5 Relationship to Previous Research.</S>
			<S sid ="182" ssid = "53">The advantages of MLSLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector.</S>
			<S sid ="183" ssid = "54">By not assuming parallel text, this approach can be applied to a broad class of corpora.</S>
			<S sid ="184" ssid = "55">Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006).</S>
			<S sid ="185" ssid = "56">Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009).</S>
			<S sid ="186" ssid = "57">In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system.</S>
			<S sid ="187" ssid = "58">Rather than viewing one language through the lens of another language, MLSLDA views all languages through the lens of the topics present in a document.</S>
			<S sid ="188" ssid = "59">This is a modeling decision with pros and cons.</S>
			<S sid ="189" ssid = "60">It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways.</S>
			<S sid ="190" ssid = "61">First, it throws away information important to sentiment analysis like syntactic constructions (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating.</S>
			<S sid ="191" ssid = "62">Second, a single real number is not always sufficient to capture the nuances of sentiment.</S>
			<S sid ="192" ssid = "63">Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews.</S>
			<S sid ="193" ssid = "64">We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predictions.</S>
			<S sid ="194" ssid = "65">Other probabilistic models for sentiment classification view sentiment as a word level feature.</S>
			<S sid ="195" ssid = "66">Some models use sentiment word lists, either given or learned from a corpus, as a prior to seed topics so that they attract other sentiment bearing words (Mei et al., 2007; Lin and He, 2009).</S>
			<S sid ="196" ssid = "67">Other approaches view sentiment or perspective as a perturbation of a log-linear topic model (Lin et al., 2008).</S>
			<S sid ="197" ssid = "68">Such techniques could be combined with the multilingual approach presented here by using distributions over words that not only bridge different languages but also encode additional information.</S>
			<S sid ="198" ssid = "69">For example, the vocabulary hierarchies could be structured to encourage topics that encourage correlation among similar sentiment-bearing words (e.g. clustering words associated with price, size, etc.).</S>
			<S sid ="199" ssid = "70">Future work could also more rigorously validate that the multilingual topics discovered by MLSLDA are sentiment-bearing via human judgments.</S>
			<S sid ="200" ssid = "71">In contrast, MLSLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008).</S>
			<S sid ="201" ssid = "72">Extending these models to multilingual data would be more straightforward.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "6">
			<S sid ="202" ssid = "1">MLSLDA is a “holistic” statistical model for multilingual corpora that does not require parallel text or expensive multilingual resources.</S>
			<S sid ="203" ssid = "2">It discovers connections across languages that can recover latent structure in parallel corpora, discover sentiment- correlated word lists in multiple languages, and make accurate predictions across languages that improve with more multilingual data, as demonstrated in the context of sentiment analysis.</S>
			<S sid ="204" ssid = "3">More generally, MLSLDA provides a formalism that can be used to incorporate the many insights of topic modeling-driven sentiment analysis to multilingual corpora by tying together word distributions across languages.</S>
			<S sid ="205" ssid = "4">MLSLDA can also contribute to the development of word list-based sentiment systems: the topics discovered by MLSLDA can serve as a first-pass means of sentiment-based word lists for languages that might lack annotated resources.</S>
			<S sid ="206" ssid = "5">MLSLDA also can be viewed as a sentiment- informed multilingual word sense disambiguation (WSD) algorithm.</S>
			<S sid ="207" ssid = "6">When the multilingual bridge is an explicit representation of sense such as WordNet, part of the generative process is an explicit assignment of every word to sense (the path latent variable λ); this is discovered during inference.</S>
			<S sid ="208" ssid = "7">The dictionary- based technique may be viewed as a disambiguation via a transfer dictionary.</S>
			<S sid ="209" ssid = "8">How sentiment prediction impacts the implicit WSD is left to future work.</S>
			<S sid ="210" ssid = "9">Better capturing local syntax and meaningful collocations would also improve the model’s ability to predict sentiment and model multilingual topics, as would providing a better mechanism for representing words not included in our bridges.</S>
			<S sid ="211" ssid = "10">We intend to develop such models as future work.</S>
	</SECTION>
	<SECTION title="Acknowledgments. " number = "7">
			<S sid ="212" ssid = "1">This research was funded in part by the Army Research Laboratory through ARL Cooperative Agreement W911NF09-20072 and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the Army Research Laboratory.</S>
			<S sid ="213" ssid = "2">All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of ARL, IARPA, the ODNI, or the U.S. Government.</S>
			<S sid ="214" ssid = "3">The authors thank the anonymous reviewers, Jonathan Chang, Christiane Fellbaum, and Lawrence Watts for helpful comments.</S>
			<S sid ="215" ssid = "4">The authors especially thank Chris Potts for providing help in obtaining and processing reviews.</S>
	</SECTION>
</PAPER>
