NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 100? 104, Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics Hierarchical clustering of word class distributions Grzegorz Chrupa?a gchrupala@lsv.uni-saarland.de Spoken Language Systems Saarland University Abstract We propose an unsupervised approach to POS tagging where first we associate each word type with a probability distribution over word classes using Latent Dirichlet Alloca.tion. Then we create a hierarchical cluster.ing of the word types: we use an agglomer.ative clustering algorithm where the distance between clusters is defined as the Jensen-Shannon divergence between the probability distributions over classes associated with each word-type. When assigning POS tags, we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag. This simple labeler outper.forms a baseline based on Brown clusters on 9 out of 10 datasets. 1 Introduction Unsupervised induction of word categories has been approached from three broad perspectives. First, it is of interest to cognitive scientists who model syntac.tic category acquisition by children (Redington et al 1998, Mintz 2003, Parisien et al 2008, Chrupa ? a and Alishahi 2010), where the primary concern is match .ing human performance patterns and satisfying cog.nitively motivated constraints such as incremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al (2010), Lamar et al (2010), Christodoulopoulos et al (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been re.searched from the point of view of feature learning, where the induced categories provide an interme.diate level of representation, abstracting away and generalizing over word form features in an NLP ap.plication (Brown et al 1992, Miller et al 2004, Lin and Wu 2009, Turian et al 2010, Chrupala 2011, Ta? ckstro ? m et al 2012). The main difference from the part-of-speech setting is that the focus is on eval.uating the performance of the learned categories in real tasks rather than on measuring how closely they match gold part-of-speech tags. Some researchers have used both approaches to evaluation. This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations. For part-of-speech tagging what is needed is a mapping from word to.kens to a small set of discrete, atomic labels. For feature learning, there are is no such limitation, and other types of representations have been used, such as low-dimensional continuous vectors learned by neural network language models as in Bengio et al (2006), Mnih and Hinton (2009), or distributions over word classes learned using Latent Dirichlet Al.location as in Chrupala (2011). In this paper we propose a simple method of map.ping distributions over word classes to a set of dis.crete labels by hierarchically clustering word class distributions using Jensen-Shannon divergence as a distance metric. This allows us to effectively use the algorithm of Chrupala (2011) and similar ones in settings where using distributions directly is not pos.sible or desirable. Equivalently, our approach can be seen as a generic method to convert a soft clus.tering to hard clustering while conserving much of the information encoded in the original soft cluster assignments. We evaluate this method on the unsu.pervised part-of-speech tagging task on ten datasets 100 in nine languages as part of the shared task at the NAACL-HLT 2012 Workshop on Inducing Linguis.tic Structure. 2 Architecture Our system consists of the following components (i) a soft word-class induction model (ii) a hierarchi.cal clustering algorithm which builds a tree of word class distributions (iii) a labeler which for each word type finds the leaf in the tree with the most similar word-class distribution and outputs a prefix of the path leading to that leaf. 2.1 Soft word-class model We use the probabilistic soft word-class model pro.posed by Chrupala (2011), which is based on Latent Dirichlet Allocation (LDA). LDA was introduced by Blei et al (2003) and applied to modeling the topic structure in document collections. It is a generative, probabilistic hierarchical Bayesian model which in.duces a set of latent variables, which correspond to the topics. The topics themselves are multinomial distributions over words. The generative structure of the LDA model is the following: ?k ? Dirichlet(? ), k ? [1,K] ?d ? Dirichlet(? ), d ? [1, D] znd ? Categorical(?d), nd ? [1, Nd] wnd ? Categorical(?znd ), nd ? [1, Nd] (1) Chrupala (2011) interprets the LDA model in terms of word classes as follows: K is the number of classes, D is the number of unique word types, Nd is the number of context features (such as right or left neighbor) associated with word type d, znd is the class of word type d in the nthd context, and wnd is the n th d context feature of word type d. Hy.perparameters ? and ? control the sparseness of the vectors ? d and ? k. Inference in LDA in general can be performed us.ing either variational EM or Gibbs sampling. Here we use a collapsed Gibbs sampler to estimate two sets of parameters: the ? d parameters correspond to word class probability distributions given a word type while the ?k correspond to feature distributions given a word class. In the current paper we focus on ?d which we use to represent a word type d as a distribution over word classes. Soft word classes are more expressive than hard categories. They make it easy and efficient to ex .press shared ambiguities: Chrupala (2011) gives an example of words used as either first names or sur.names, where this shared ambiguity is reflected in the similarity of their word class distributions. Another important property of soft word classes is that they make it easy to express graded similar.ity between words types. With hard classes, a pair of words either belong to the same class or to differ.ent classes, i.e. similarity is a binary indicator. With soft word classes, we can use standard measures of similarity between probability distributions to judge how similar words are to each other. We take advan.tage of this feature to build a hierarchical clustering of word types. 2.2 Hierarchical clustering of word types In some settings, e.g. in the unsupervised part-of.speech tagging scenario, words should be labeled with a small set of discrete labels. The question then arises how to map a probability distribution over word classes corresponding to each word type in the soft word class setting to a discrete label. The most obvious method would be to simply output the high.est scoring word class, but this has the disadvantage of discarding much of the information present in the soft labeling. What we do instead is to create a hierarchical clustering of word types using the Jensen-Shannon (JS) divergence between the word-class distribu.tions as a distance function. JS divergence is an information-theoretic measure of dissimilarity be .tween two probability distributions (Lin 1991). It is defined as follows: JS (P,Q) = 
1 
2 
(DKL (P,M) +DKL (Q,M)) (2) 
where M is the mean distribution P+Q2 and DKL is 
the Kullback-Leibler (KL) divergence: 
DKL(P,Q) = 
? 
i 
P (i) log2 
P (i) 
Q(i) 
(3) Unlike KL divergence, JS divergence is symmetric and is defined for any pair of discrete probability dis.tributions over the same domain. 101 We use a simple agglomerative clustering algo.rithm to build a tree hierarchy over the word class distributions corresponding to word types (see Al.gorithm 1). We start with a set of leaf nodes, one for each of D word types, containing the unnormalized word-class probabilities for the corresponding word type: i.e. the co-occurrence counts of word-type and word-class, n(z, d), output by the Gibbs sampler. We then merge that pair of nodes (P,Q) whose JS divergence is the smallest, remove these two nodes from the set, and add the new merged node with two branches. We proceed in this fashion until we obtain a single root node. When merging two nodes we sum their co .occurrence count tables: thus the nodes always con .tain unnormalized probabilities which are normal.ized only when computing JS scores. Algorithm 1 Bottom -up clustering of word types S = {n( ? , d) | d ? [1, D]} while |S| > 1 do (P,Q) = argmin(P,Q)? S?S JS (P,Q) S ? S \ {P,Q} ? {merge(P,Q)} The algorithm is simple but not very efficient: if implemented carefully it can be at best quadratic in the number of word types. However, in practice it is unnecessary to run it on more than a few hun.dred word types which can be done very quickly. In the experiments reported on below we build the tree based only on the 1000 most frequent words. Figure 1 shows two small fragments of a hierar.chy built from 200 most frequent words of the En.glish CHILDES dataset using 10 LDA word classes. 2.3 Tree paths as labels Once the tree is built, it can be used to assign a label to any word which has an associated word class dis.tribution. In principle, it could be used to perform either type-level or token-level tagging: token-level distributions could be composed from the distribu.tions associated with current word type (?) and the distributions associated with the current context fea.tures (? ). Since preliminary experiments with token.level tagging were not successful, here we focus ex.clusively on type-level tagging. Given the tree and a word-type paired with a class distribution, we generate a path to a leaf in the tree DaddyMommy Paul Fraser itthat thesethose ?ll goinggoin(g) couldcan Figure 1: Two fragments of a hierarchy over word class distributions as follows. If the word is one of the ones used to construct the tree, we simply record the path from the root to the leaf containing this word. If the word is not at any of the leaves (i.e. it is not one of the 1000 most frequent words), we traverse the tree, at each node comparing the JS divergence between the word and the left and right branches, and then de .scend along the branch for which JS is smaller. We record the path until we reach a leaf node. We can control the granularity of the labeling by varying the length of the prefix of the path from the root to the leaf. 3 Experiments We evaluate our method on the unsupervised part.of-speech tagging task on ten dataset in nine lan.guages as part of the shared task. For each dataset we run LDA word class induc.tion1 on the union of the unlabeled sentences in the train, development and test sets, setting the num.ber of classes K ? {10, 20, 40, 80}, and build a hierarchy on top of the learned word-class proba.bility distributions as explained above. We then la.bel the development set using path prefixes of length L ? {8, 9, . . . , 20} for each of the trees, and record 1We ran 200 Gibbs sampling passes, and set the LDA hyper.parameters to ? = 10K and ? = 0.1. 102 Dataset K L Brown HCD Arabic 40 13 39.6 51.4 Basque 40 16 39.5 48.3 Czech 80 8 42.1 42.4 Danish 40 19 50.2 56.8 Dutch 40 10 43.3 54.8 English CH 10 12 64.1 67.8 English PTB 40 8 61.6 60.2 Portuguese 80 10 51.7 52.4 Slovene 80 19 44.5 46.6 Swedish 20 17 51.8 56.1 Table 1: Evaluation of coarse-grained POS tagging on test data Dataset K L Brown HCD Arabic 40 13 42.2 52.9 Basque 40 16 38.5 54.4 Czech 40 19 45.3 46.8 Danish 40 20 49.2 63.6 Dutch 20 12 49.4 53.4 English CH 10 12 66.0 78.2 English PTB 80 14 62.0 61.3 Portuguese 80 11 52.9 54.7 Slovene 80 20 45.8 51.9 Swedish 20 17 51.8 56.1 Table 2: Evaluation of coarse-grained POS tagging on test data the V-measure (Rosenberg and Hirschberg 2007) against gold part-of-speech tags. We choose the best-performing pair ofK and L and use this setting to label the test set. We tune separately for coarse.grained and fine-grained POS tags. Other than using the development set labels to tune these two param.eters our system is unsupervised and uses no data other than the sentences in the provided data files. Table 1 and Table 2 show the best settings for the coarse- and fine-grained POS tagging for all the datasets, and the V-measure scores on the test set achieved by our labeler (HCD for Hierarchy over Class Distributions). Also included are the scores of the official baseline, i.e. labeling with Brown clus.ters (Brown et al 1992), with the number of clusters set to match the number of POS tags in each dataset. The best K stays the same when increasing the granularity in the majority of cases (7 out of 10). On the CHILDES dataset of child-directed speech, l l l l l l l l l l 0 20 40 60 80 100 120 0.00 0.05 0.10 0.15 0.20 Vocabulary size in thousands Erro r Re duc tion ar eu cz da nl en?ch en?ptb ptsl sv Figure 2: Error reduction as a function of vocabulary size which has the smallest vocabulary of all, the optimal number of LDA classes is also the smallest (10). As expected, the best path prefix length L is typically larger for the fine-grained labeling. Our labels outperform the baseline on 9 out of 10 datasets, for both levels of granularity. The only ex .ception is the English Penn Treebank dataset, where the HCD V-measure scores are slightly lower than Brown cluster scores. This may be taken as an il.lustration of the danger arising if NLP systems are exclusively evaluated on a single dataset: such a dataset may well prove to not be very representative. Part of the story seems to be that our method tends to outperform the baseline by larger margins on datasets with smaller vocabularies2. The scatter.plot in Figure 2 illustrates this tendency for coarse.grained POS tagging: Pearson? s correlation is ? 0.6. 4 Conclusion We have proposed a simple method of convert.ing a set of soft class assignments to a set of dis.crete labels by building a hierarchical clustering over word-class distributions associated with word types. This allows to use the efficient and effective LDA.based word-class induction method in cases where a hard clustering is required. We have evaluated this 2We suspect performance on datasets with large vocabular .ies could be improved by increasing the number of frequent words used to build the word-type hierarchy; due to time con .straints we had to postpone verifying it. 103 method on the POS tagging task on which our ap .proach outperforms a baseline based on Brown clus.ters in 9 out of 10 cases, often by a substantial mar.gin. In future it would be interesting to investigate whether the hierarchy over word-class distributions would also be useful as a source of features in a semi-supervised learning scenario, instead, or in ad .dition to using word-class probabilities as features directly. We would also like to revisit and further in.vestigate the challenging problem of token-level la.beling. References Bengio, Y., Schwenk, H., Sene ? cal, J., Morin, F., and Gauvain, J. (2006). Neural Probabilistic Lan.guage Models. Innovations in Machine Learning, pages 137? 186. Blei, D., Ng, A., and Jordan, M. (2003). La.tent dirichlet alocation. The Journal of Machine Learning Research, 3:993? 1022. Brown, P. F., Mercer, R. L., Della Pietra, V. J., and Lai, J. C. (1992). Class-based n-gram models of natural language. Computational Linguistics, 18(4):467?479. Christodoulopoulos, C., Goldwater, S., and Steed.man, M. (2011). A bayesian mixture model for part-of-speech induction using multiple features. In EMNLP. Chrupala, G. (2011). Efficient induction of proba.bilistic word classes with LDA. In IJCNLP. Chrupa?a, G. and Alishahi, A. (2010). Online Entropy-based Model of Lexical Category Acqui .sition. In CoNLL. Lamar, M., Maron, Y., Johnson, M., and Bienen.stock, E. (2010). Svd and clustering for unsuper.vised pos tagging. In ACL. Lee, Y., Haghighi, A., and Barzilay, R. (2010). Simple type-level unsupervised pos tagging. In EMNLP. Lin, D. and Wu, X. (2009). Phrase clustering for discriminative learning. In ACL/IJCNLP. Lin, J. (1991). Divergence measures based on the shannon entropy. Information Theory, IEEE Transactions on, 37(1):145 ? 151. Miller, S., Guinness, J., and Zamanian, A. (2004). Name tagging with word clusters and discrimina.tive training. In HLT/NAACL. Mintz, T. (2003). Frequent frames as a cue for gram.matical categories in child directed speech. Cog.nition, 90(1):91 ? 117. Mnih, A. and Hinton, G. (2009). A scalable hierar.chical distributed language model. In NIPS. Parisien, C., Fazly, A., and Stevenson, S. (2008). An incremental bayesian model for learning syntactic categories. In CoNLL. Ravi, S. and Knight, K. (2009). Minimized mod.els for unsupervised part-of-speech tagging. In ACL/IJCNLP. Redington, M., Crater, N., and Finch, S. (1998). Dis.tributional information: A powerful cue for ac.quiring syntactic categories. Cognitive Science: A Multidisciplinary Journal, 22(4):425?469. Rosenberg, A. and Hirschberg, J. (2007). V.measure: A conditional entropy-based external cluster evaluation measure. In EMNLP/CoNLL. Turian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: A simple and general method for semi-supervised learning. In ACL. Ta? ckstro ? m, O., McDonald, R., and Uszkoreit, J. (2012). Cross-lingual word clusters for direct transfer of linguistic structure. In NAACL. 104 