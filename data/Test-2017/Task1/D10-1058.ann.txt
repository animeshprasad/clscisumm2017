Citance Number: 1 | Reference Article: D10-1058 | Citing Article: C16-1060 | Citation Marker Offset: '48' | Citation Marker: 2010 | Citation Offset: '48' | Citation Text: <S sid ="48" ssid = "27">Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.</S> | Reference Offset: [168] |  Reference Text: <S sid ="168" ssid = "14">We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.</S> |  Discourse Facet:  
Citance Number: 2 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '24' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '24' | Citation Text: <S sid ="24" ssid = "24">Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.</S> | Reference Offset: [168] |  Reference Text: <S sid ="168" ssid = "14">We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.</S> |  Discourse Facet:  
Citance Number: 3 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '33' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '33' | Citation Text: <S sid ="33" ssid = "4">, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).</S> | Reference Offset: [156] |  Reference Text: <S sid ="156" ssid = "2">We use the alignment error rate (AER) as the word alignment evaluation criterion.</S> |  Discourse Facet:  
Citance Number: 4 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '43' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '43' | Citation Text: <S sid ="43" ssid = "14">Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.</S> | Reference Offset: [164] |  Reference Text: <S sid ="164" ssid = "10">We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.</S> |  Discourse Facet:  
Citance Number: 5 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '82' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '82' | Citation Text: <S sid ="82" ssid = "3">Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).</S> | Reference Offset: [87] |  Reference Text: <S sid ="87" ssid = "2">IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn.</S> |  Discourse Facet:  
Citance Number: 6 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: '99' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '99' | Citation Text: <S sid ="99" ssid = "7">The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).</S> | Reference Offset: [52] |  Reference Text: <S sid ="52" ssid = "4">, eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions.</S> |  Discourse Facet:  
Citance Number: 7 | Reference Article: D10-1058 | Citing Article: P59105ca | Citation Marker Offset: '45' | Citation Marker: 15 | Citation Offset: '45' | Citation Text: <S sid ="45" ssid = "45">Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.</S> | Reference Offset: [177] |  Reference Text: <S sid ="177" ssid = "23">Initially, the fertility IBM Model 1 and fertility HMM did not perform well.</S> |  Discourse Facet:  
Citance Number: 8 | Reference Article: D10-1058 | Citing Article: P87-94 | Citation Marker Offset: '79' | Citation Marker: Zhao | Citation Offset: '79' | Citation Text: <S sid ="79" ssid = "2">Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.</S> | Reference Offset: [26] |  Reference Text: <S sid ="26" ssid = "26">Our model is a coherent generative model that combines the HMM and IBM Model 4.</S> |  Discourse Facet:  
Citance Number: 9 | Reference Article: D10-1058 | Citing Article: Pbulletin | Citation Marker Offset: '104' | Citation Marker: 2010 | Citation Offset: 103','104' | Citation Text: <S sid ="103" ssid = "24">For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).</S><S sid ="104" ssid = "25">134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).</S> | Reference Offset: [167] |  Reference Text: <S sid ="167" ssid = "13">AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.</S> |  Discourse Facet:  
Citance Number: 10 | Reference Article: D10-1058 | Citing Article: Pbulletin | Citation Marker Offset: '131' | Citation Marker: 2010 | Citation Offset: '131' | Citation Text: <S sid ="131" ssid = "13">Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.</S> | Reference Offset: [150] |  Reference Text: <S sid ="150" ssid = "44">IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.</S> |  Discourse Facet:  
Citance Number: 11 | Reference Article: D10-1058 | Citing Article: Pcoling_D10 | Citation Marker Offset: '232' | Citation Marker: 2010 | Citation Offset: '232' | Citation Text: <S sid ="232" ssid = "9">Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.</S> | Reference Offset: [168] |  Reference Text: <S sid ="168" ssid = "14">We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.</S> |  Discourse Facet:  
Citance Number: 12 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '24' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '24' | Citation Text: <S sid ="24" ssid = "24">Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.</S> | Reference Offset: [87] |  Reference Text: <S sid ="87" ssid = "2">IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn.</S> |  Discourse Facet:  
Citance Number: 13 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '38' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '38' | Citation Text: <S sid ="38" ssid = "9">Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.</S> | Reference Offset: [168] |  Reference Text: <S sid ="168" ssid = "14">We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.</S> |  Discourse Facet:  
Citance Number: 14 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '39' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '39' | Citation Text: <S sid ="39" ssid = "10">I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).</S> | Reference Offset: [199] |  Reference Text: <S sid ="199" ssid = "4">The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.</S> |  Discourse Facet:  
Citance Number: 15 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '86' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '86' | Citation Text: <S sid ="86" ssid = "3">Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).</S> | Reference Offset: [168] |  Reference Text: <S sid ="168" ssid = "14">We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.</S> |  Discourse Facet:  
Citance Number: 16 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: '104' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '104' | Citation Text: <S sid ="104" ssid = "7">The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).</S> | Reference Offset: [167] |  Reference Text: <S sid ="167" ssid = "13">AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.</S> |  Discourse Facet:  
Citance Number: 17 | Reference Article: D10-1058 | Citing Article: Q13-1024 | Citation Marker Offset: '60' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '60' | Citation Text: <S sid ="60" ssid = "3">The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).</S> | Reference Offset: [14] |  Reference Text: <S sid ="14" ssid = "14">Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.</S> |  Discourse Facet:  
Citance Number: 18 | Reference Article: D10-1058 | Citing Article: Q13-1024 | Citation Marker Offset: '130' | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: '130' | Citation Text: <S sid ="130" ssid = "22">Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.</S> | Reference Offset: [133] |  Reference Text: <S sid ="133" ssid = "27">Gibbs sampling for the fertility IBM Model 1 is similar but simpler.</S> |  Discourse Facet:  
